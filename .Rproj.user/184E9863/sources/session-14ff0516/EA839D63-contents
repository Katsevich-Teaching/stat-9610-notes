# Linear models: Inference

We now understand the least squares estimator $\boldsymbol{\widehat{\beta}}$ from geometric and algebraic points of view. In Chapter 2, we will switch to a probabilistic perspective to derive inferential statements for linear models, in the form of hypothesis tests and confidence intervals. In order to facilitate this, we will assume that the error terms are normally distributed:

$$
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \text{where} \ \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_n).
$$

## Building blocks for linear model inference

*See also Agresti 3.1.1, 3.1.2, 3.1.4*

First we put in place some building blocks: The multivariate normal distribution (Section [-@sec-mvrnorm]), the distributions of linear regression estimates and residuals (Section [-@sec-lin-reg-dist]), and estimation of the noise variance $\sigma^2$ (Section [-@sec-noise-estimation]).

### The multivariate normal distribution {#sec-mvrnorm}

Recall that a random vector $\boldsymbol{w} \in \mathbb{R}^d$ has a multivariate normal distribution with mean $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$ if it has probability density

$$
p(\boldsymbol{w}) = \frac{1}{\sqrt{(2\pi)^{d}\text{det}(\boldsymbol{\Sigma})}}\exp\left(-\frac{1}{2}(\boldsymbol{w} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{w} - \boldsymbol{\mu})\right).
$$

These random vectors have lots of special properties, including:

- **Linear transformation**: If $\boldsymbol{w} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, then $\boldsymbol{A} \boldsymbol{w} + \boldsymbol{b} \sim N(\boldsymbol{A} \boldsymbol{\mu} + \boldsymbol{b}, \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^\top)$.
- **Independence**: If 
$$
\begin{pmatrix}\boldsymbol{w}_1 \\ \boldsymbol{w}_2 \end{pmatrix} \sim N\left(\begin{pmatrix}\boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix} , \begin{pmatrix}\boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{12}^\top & \boldsymbol{\Sigma}_{22}\end{pmatrix}\right),
$$ 
then $\boldsymbol{w}_1 \perp\!\!\!\perp \boldsymbol{w}_2$ if and only if $\boldsymbol{\Sigma}_{12} = \boldsymbol{0}$.

An important distribution related to the multivariate normal is the $\chi^2_d$ (chi-squared with $d$ degrees of freedom) distribution, defined as

$$
\chi^2_d \equiv \sum_{j = 1}^d w_j^2 \quad \text{for} \quad w_1, \dots, w_d \overset{\text{i.i.d.}}{\sim} N(0, 1).
$$

### The distributions of linear regression estimates and residuals {#sec-lin-reg-dist}

*See also Dunn and Smyth 2.8.2*

The most important distributional result in linear regression is that

$$
\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}).
$$ {#eq-beta-hat-dist}

Indeed, by the linear transformation property of the multivariate normal distribution,

$$
\begin{split}
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}_n) &\Longrightarrow \boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{y} \sim N((\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{\beta}, (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \sigma^2 \boldsymbol{I}_n \boldsymbol{X}(\boldsymbol{X}^\top \boldsymbol{X})^{-1}) \\
&= N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}).
\end{split}
$$

Next, let's consider the joint distribution of $\boldsymbol{\widehat{\mu}} = \boldsymbol{X} \boldsymbol{\widehat{\beta}}$ and $\boldsymbol{\widehat{\epsilon}} = \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}$. We have

$$
\begin{split}
\begin{pmatrix} \boldsymbol{\widehat{\mu}} \\ \boldsymbol{\widehat{\epsilon}} \end{pmatrix} = \begin{pmatrix} \boldsymbol{H} \boldsymbol{y} \\ (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y} \end{pmatrix} = \begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\boldsymbol{y} &\sim N\left(\begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\boldsymbol{X} \boldsymbol{\beta}, \begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\cdot \sigma^2 \boldsymbol{I} \begin{pmatrix} \boldsymbol{H} & \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\right) \\
&= N\left(\begin{pmatrix} \boldsymbol{X} \boldsymbol{\beta} \\ \boldsymbol{0} \end{pmatrix}, \begin{pmatrix} \sigma^2 \boldsymbol{H} & \boldsymbol{0} \\ \boldsymbol{0} & \sigma^2(\boldsymbol{I} - \boldsymbol{H}) \end{pmatrix} \right).
\end{split}
$$

In other words,

$$
\boldsymbol{\widehat{\mu}} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{H}) \quad \text{and} \quad \boldsymbol{\widehat{\epsilon}} \sim N(\boldsymbol{0}, \sigma^2(\boldsymbol{I} - \boldsymbol{H})), \quad \text{with} \quad \boldsymbol{\widehat{\mu}} \perp\!\!\!\perp \boldsymbol{\widehat{\epsilon}}.
$$ {#eq-fit-and-error-dist}

The statistical independence between $\boldsymbol{\widehat{\mu}}$ and $\boldsymbol{\widehat{\epsilon}}$ is a result of the fact that these two quantities are projections of $\boldsymbol{y}$ onto two orthogonal subspaces: $C(\boldsymbol{X})$ and $C(\boldsymbol{X})^\perp$ (Figure [-@fig-orthogonality-fit-residuals]).

![The fitted vector $\boldsymbol{\widehat{\mu}}$ and the residual vector $\boldsymbol{\widehat{\epsilon}}$ are projections of $\boldsymbol{y}$ onto orthogonal subspaces.](figures/orthogonality-fit-residuals.jpg){#fig-orthogonality-fit-residuals width=50%}

Since $\boldsymbol{\widehat{\beta}}$ is a deterministic function of $\boldsymbol{\widehat{\mu}}$ (in particular, $\boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{\widehat{\mu}}$), it also follows that

$$
\boldsymbol{\widehat{\beta}} \perp\!\!\!\perp \boldsymbol{\widehat{\epsilon}}.
$$ {#eq-beta-ind-eps}

### Estimation of the noise variance $\sigma^2$ {#sec-noise-estimation}

*See also Dunn and Smyth 2.4.2, 2.5.3*

We can't quite do inference for $\boldsymbol{\beta}$ based on the distributional result [-@eq-beta-hat-dist] because the noise variance $\sigma^2$ is unknown to us. Intuitively, since $\sigma^2 = \mathbb{E}[\epsilon_i^2]$, we can get an estimate of $\sigma^2$ by looking at the quantity $\|\boldsymbol{\widehat{\epsilon}}\|^2$. To get the distribution of this quantity, we need the following lemma:

::: {#lem-normal-projection}
Let $\boldsymbol{w} \sim N(\boldsymbol{0}, \boldsymbol{P})$ for some projection matrix $\boldsymbol{P}$. Then, $\|\boldsymbol{w}\|^2 \sim \chi^2_d$, where $d = \text{trace}(\boldsymbol{P})$ is the dimension of the subspace onto which $\boldsymbol{P}$ projects.
:::

::: {.proof}
Let $\boldsymbol{P} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{U}^\top$ be an eigenvalue decomposition of $\boldsymbol{P}$, where $\boldsymbol{U}$ is orthogonal and $\boldsymbol{D}$ is a diagonal matrix with $D_{ii} \in \{0,1\}$. We have $\boldsymbol{w} \overset{d}{=} \boldsymbol{U} \boldsymbol{D} \boldsymbol{z}$ for $\boldsymbol{z} \sim N(0, \boldsymbol{I}_n)$. Therefore,

$$
\|\boldsymbol{w}\|^2 = \|\boldsymbol{D} \boldsymbol{z}\|^2 = \sum_{i: D_{ii} = 1} z_i^2 \sim \chi^2_d, \quad \text{where } d = |\{i: D_{ii} = 1\}| = \text{trace}(D) = \text{trace}(\boldsymbol{P}).
$$
:::

Recall that $\boldsymbol{I} - \boldsymbol{H}$ is a projection onto the $(n-p)$-dimensional space $C(\boldsymbol{X})^\perp$, so by Lemma [-@lem-normal-projection] and equation [-@eq-fit-and-error-dist] we have

$$
\|\boldsymbol{\widehat{\epsilon}}\|^2 \sim \sigma^2 \chi^2_{n-p}.
$$ {#eq-eps-norm-dist}

From this result, it follows that $\mathbb{E}[\|\boldsymbol{\widehat{\epsilon}}\|^2] = n-p$, so

$$
\widehat{\sigma}^2 \equiv \frac{1}{n-p}\|\boldsymbol{\widehat{\epsilon}}\|^2
$$ {#eq-unbiased-noise-estimate}

is an unbiased estimate for $\sigma^2$. Why does the denominator need to be $n-p$ rather than $n$ for the estimator above to be unbiased? The reason for this is that the residuals $\boldsymbol{\widehat{\epsilon}}$ are the projection of the true noise vector $\boldsymbol{\epsilon} \in \mathbb{R}^n$ onto the $(n-p)$-dimensional subspace $C(\boldsymbol{X})^\perp$ (Figure [-@fig-residuals-as-noise-projection]). To see this, note that

$$
\boldsymbol{\widehat{\epsilon}} = (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y} = (\boldsymbol{I} - \boldsymbol{H})(\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) = (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{\epsilon}.
$$

Therefore, the norm of the residual vector will be smaller than that of the noise vector, especially to the extent that $p$ is close to $n$.

![The residual vector $\boldsymbol{\widehat{\epsilon}}$ is the projection of the noise vector $\boldsymbol{\epsilon}$ onto $C(\boldsymbol{X})^\perp$.](figures/residuals-as-noise-projection.jpg){#fig-residuals-as-noise-projection width=50%}

## Hypothesis Testing

*See also Agresti 3.2.1, 3.2.2, 3.2.4, 3.2.8*

Typically, two types of null hypotheses are tested in a regression setting: those involving one-dimensional parameters and those involving multi-dimensional parameters. For example, consider the null hypotheses $H_0: \beta_j = 0$ and $H_0: \boldsymbol{\beta}_S = \boldsymbol{0}$ for $S \subseteq \{0, 1, \dots, p-1\}$, respectively. We discuss tests of these two kinds of hypotheses in Sections [-@sec-one-dim-testing] and [-@sec-multi-dim-testing], and then discuss the power of these tests in Section [-@sec-power].

### Testing a One-Dimensional Parameter {#sec-one-dim-testing}

*See also Dunn and Smyth 2.8.3*

#### $t$-test for a Single Coefficient

The most common question to ask in a linear regression context is: Is the $j$th predictor associated with the response when controlling for the other predictors? In the language of hypothesis testing, this corresponds to the null hypothesis:

$$
H_0: \beta_j = 0
$$ {#eq-one-dim-null}

According to [-@eq-beta-hat-dist], we have $\widehat{\beta}_j \sim N(0, \sigma^2/s_j^2)$, where, as we learned in Chapter 1:

$$
s_j^{2} \equiv [(\boldsymbol{X}^T \boldsymbol{X})^{-1}_{jj}]^{-1} = \|\boldsymbol{x}_{*j}^\perp\|^2.
$$

Therefore,

$$
\frac{\widehat{\beta}_j}{\sigma/s_j} \sim N(0,1),
$$ {#eq-oracle-z-stat}

and we are tempted to define a level $\alpha$ test of the null hypothesis [-@eq-one-dim-null] based on this normal distribution. While this is infeasible since we don't know $\sigma^2$, we can substitute in the unbiased estimate [-@eq-unbiased-noise-estimate] derived in Section [-@sec-noise-estimation]. Then,

$$
\text{SE}(\widehat{\beta}_j) \equiv \frac{\widehat{\sigma}}{s_j}
$$

is the standard error of $\widehat{\beta}_j$, which is an approximation to the standard deviation of $\widehat{\beta}_j$. Dividing $\widehat{\beta}_j$ by its standard error gives us the $t$-statistic:

$$
t_j \equiv \frac{\widehat{\beta}_j}{\text{SE}(\widehat{\beta}_j)} = \frac{\widehat{\beta}_j}{\sqrt{\frac{1}{n-p}\|\boldsymbol{\widehat{\epsilon}}\|^2}/s_j}.
$$

This statistic is *pivotal*, in the sense that it has the same distribution for any $\boldsymbol{\beta}$ such that $\beta_j = 0$. Indeed, we can rewrite it as:

$$
t_j = \frac{\frac{\widehat{\beta}}{\sigma/s_j}}{\sqrt{\frac{\sigma^{-2}\|\boldsymbol{\widehat{\epsilon}}\|^2}{n-p}}}.
$$

Recalling the independence of $\boldsymbol{\widehat{\beta}}$ and $\boldsymbol{\widehat{\epsilon}}$ [-@eq-beta-ind-eps], the scaled chi-square distribution of $\|\boldsymbol{\widehat{\epsilon}}\|^2$ [-@eq-eps-norm-dist], and the standard normal distribution of $\frac{\widehat{\beta}}{\sigma/s_j}$ [-@eq-oracle-z-stat], we find that:

$$
\text{Under } H_0:\beta_j = 0, \quad t_j \sim \frac{N(0,1)}{\sqrt{\frac{1}{n-p}\chi^2_{n-p}}}, \quad \text{with numerator and denominator independent.}
$$

The latter distribution is called the *$t$ distribution with $n-p$ degrees of freedom* and is denoted $t_{n-p}$. This paves the way for the two-sided $t$-test:

$$
\phi_t(\boldsymbol{X}, \boldsymbol{y}) = \mathbbm{1}(|t_j| > t_{n-p}(1-\alpha/2)),
$$

where $t_{n-p}(1-\alpha/2)$ denotes the $1-\alpha/2$ quantile of $t_{n-p}$. Note that, by the law of large numbers,

$$
\frac{1}{n-p}\chi^2_{n-p} \overset{P}{\rightarrow} 1 \quad \text{as} \quad n - p \rightarrow \infty,
$$

so for large $n-p$ we have $t_{j} \sim t_{n-p} \approx N(0,1)$. Hence, the $t$-test is approximately equal to the following $z$-test:

$$
\phi_t(\boldsymbol{X}, \boldsymbol{y}) \approx \phi_z(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}(|t_j| > z(1-\alpha/2)),
$$

where $z(1-\alpha/2)$ is the $1-\alpha/2$ quantile of $N(0,1)$. The $t$-test can also be defined in a one-sided fashion if power against one-sided alternatives is desired.

#### Example: One-Sample Model

Consider the intercept-only linear regression model $y = \beta_0 + \epsilon$, and let's apply the $t$-test derived above to test the null hypothesis $H_0: \beta_0 = 0$. We have $\widehat{\beta}_0 = \bar{y}$. Furthermore, we have

$$
\text{SE}^2(\widehat{\beta}_0) = \frac{\widehat{\sigma}^2}{n}, \quad \text{where} \quad \widehat{\sigma}^2 = \frac{1}{n-1}\|\boldsymbol{y} - \bar{y} \boldsymbol{1}_n\|^2.
$$

Hence, we obtain the $t$ statistic:

$$
t = \frac{\widehat{\beta}_0}{\text{SE}(\widehat{\beta}_0)} = \frac{\sqrt{n} \bar{y}}{\sqrt{\frac{1}{n-1}\|\boldsymbol{y} - \bar{y} \boldsymbol{1}_n\|^2}}.
$$

According to the theory above, this test statistic has a null distribution of $t_{n-1}$.

#### Example: Two-Sample Model

Suppose we have $x_1 \in \{0,1\}$, in which case the linear regression $y = \beta_0 + \beta_1 x_1 + \epsilon$ becomes a two-sample model. We can rewrite this model as:

$$
y_i \sim \begin{cases}
N(\beta_0, \sigma^2) \quad &\text{for } x_i = 0; \\
N(\beta_0 + \beta_1, \sigma^2) \quad &\text{for } x_i = 1.
\end{cases}
$$

It is often of interest to test the null hypothesis $H_0: \beta_1 = 0$, i.e., that the two groups have equal means. Let's define:

$$
\bar{y}_0 \equiv \frac{1}{n_0}\sum_{i: x_i = 0} y_i, \quad \bar{y}_1 \equiv \frac{1}{n_1}\sum_{i: x_i = 1} y_i, \quad \text{where} \quad n_0 = |\{i: x_i = 0\}| \text{ and } n_1 = |\{i: x_i = 1\}|.
$$

Then, we have seen before that $\widehat{\beta}_0 = \bar{y}_0$ and $\widehat{\beta}_1 = \bar{y}_1 - \bar{y}_0$. We can compute that:

$$
s_1^2 \equiv \|\boldsymbol{x}_{*1}^{\perp}\|^2 = \|\boldsymbol{x}_{*1} - \frac{n_1}{n}\boldsymbol{1}\|^2 = n_1\frac{n_0^2}{n^2} + n_0\frac{n_1^2}{n^2} = \frac{n_0 n_1}{n} = \frac{1}{\frac{1}{n_0} + \frac{1}{n_1}}
$$

and

$$
\widehat{\sigma}^2 = \frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar{y}_0)^2 + \sum_{i: x_i = 1}(y_i - \bar{y}_1)^2\right).
$$

Therefore, we arrive at a $t$-statistic of:

$$
t = \frac{\sqrt{\frac{1}{\frac{1}{n_0} + \frac{1}{n_1}}}(\bar{y}_1 - \bar{y}_0)}{\sqrt{\frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar{y}_0)^2 + \sum_{i: x_i = 1}(y_i - \bar{y}_1)^2\right)}}.
$$

Under the null hypothesis, this statistic has a distribution of $t_{n-2}$.

#### $t$-test for a Contrast Among Coefficients

Given a vector $\boldsymbol{c} \in \mathbb{R}^p$, the quantity $\boldsymbol{c}^T \boldsymbol{\beta}$ is sometimes called a *contrast*. For example, suppose $\boldsymbol{c} = (1,-1, 0, \dots, 0)$. Then, $\boldsymbol{c}^T \boldsymbol{\beta} = \beta_1 - \beta_2$ is the difference in effects of the first and second predictors. We are sometimes interested in testing whether such a contrast is equal to zero, i.e., $H_0: \boldsymbol{c}^T \boldsymbol{\beta} = 0$. While this hypothesis can involve two or more of the predictors, the parameter $\boldsymbol{c}^T \boldsymbol{\beta}$ is still one-dimensional, and therefore we can still apply a $t$-test. Going back to the distribution $\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2(\boldsymbol{X}^T \boldsymbol{X})^{-1})$, we find that:

$$
\boldsymbol{c}^T\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{c}^T\boldsymbol{\beta}, \sigma^2\boldsymbol{c}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{c}).
$$

Therefore, under the null hypothesis that $\boldsymbol{c}^T \boldsymbol{\beta} = 0$, we can derive that:

$$
\frac{\boldsymbol{c}^T \boldsymbol{\widehat{\beta}}}{\widehat{\sigma} \sqrt{\boldsymbol{c}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{c}}} \sim t_{n-p},
$$ {#eq-contrasts-t-dist}

giving us another $t$-test. Note that the $t$-tests described above can be recovered from this more general formulation by setting $\boldsymbol{c} = \boldsymbol{e}_j$, the indicator vector with the $j$th coordinate equal to 1 and all others equal to zero.

### Testing a Multi-Dimensional Parameter {#sec-multi-dim-testing}

*See also Dunn and Smyth 2.10.1*

#### $F$-Test for a Group of Coefficients

Now we move on to the case of testing a multi-dimensional parameter: $H_0: \boldsymbol{\beta}_S = \boldsymbol{0}$ for some $S \subseteq \{0, 1, \dots, p-1\}$. In other words, we would like to test

$$
H_0: \boldsymbol{y} = \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} + \boldsymbol{\epsilon} \quad \text{versus} \quad H_1: \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}.
$$

To test this hypothesis, let us fit least squares coefficients $\boldsymbol{\widehat{\beta}}_{-S}$ and $\boldsymbol{\widehat{\beta}}$ for the partial model as well as the full model. If the partial model fits well, then the residuals $\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}$ from this model will not be much larger than the residuals $\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}$ from the full model. To quantify this intuition, let us recall our analysis of variance decomposition from Chapter 1:

$$
\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 = \|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 + \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2.
$$

Let's consider the ratio

$$
\frac{\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2} = \frac{\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2},
$$

which is the relative increase in the residual sum of squares when going from the full model to the partial model. Let us rewrite this ratio in terms of projection matrices. Let $\boldsymbol{H}$ be the projection matrix for the full model, and let $\boldsymbol{H}_{\text{-}S}$ be the projection matrix for the partial model. Note that $\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}$ is the projection matrix onto the $|S|$-dimensional space $C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{\text{-}S})^\perp$ (@fig-f-test-geometry).

![Geometry of the $F$-test. Orthogonality relationships stem from $C(\boldsymbol{X}_{*,\text{-}S}) \perp C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^\perp \perp C(\boldsymbol{X})^\perp$.](figures/F-test-geometry.png){#fig-f-test-geometry width=75%}

We have

$$
\frac{\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2} = \frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}\|^2},
$$

so the numerator and denominator are the squared norms of the projections of $\boldsymbol{y}$ onto $C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^\perp$ and $C(\boldsymbol{X})^\perp$, respectively (@fig-f-test-geometry). Under the null hypothesis, we have $\boldsymbol{y} = \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} + \boldsymbol{\epsilon}$, and

$$
(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{X}_{*,\text{-}S} \boldsymbol{\beta}_{-S} = (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{X}_{*,\text{-}S} \boldsymbol{\beta}_{-S} = 0
$$

because $\boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} \in C(\boldsymbol{X}_{*, \text{-}S}) \perp C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^T \perp C(\boldsymbol{X})^\perp$. It follows that

$$
\frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}\|^2} = \frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\|^2}.
$$

Since the projection matrices in the numerator and denominator project onto orthogonal subspaces, we have $(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon} \perp\!\!\!\perp (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}$, with $\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon}\|^2 \sim \sigma^2 \chi^2_{|S|}$ and $\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\|^2 \sim \sigma^2 \chi^2_{n-p}$. Renormalizing numerator and denominator to have expectation 1 under the null, we arrive at the $F$-statistic

$$
F \equiv \frac{(\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2)/|S|}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2/(n-p)}.
$$

We have derived that under the null hypothesis,

$$
F \sim \frac{\chi^2_{|S|}/|S|}{\chi^2_{n-p}/(n-p)}, \quad \text{with numerator and denominator independent.}
$$

This distribution is called the $F$-distribution with $|S|$ and $n-p$ degrees of freedom, and is denoted $F_{|S|, n-p}$. Denoting by $F_{|S|, n-p}(1-\alpha)$ the $1-\alpha$ quantile of this distribution, we arrive at the $F$-test

$$
\phi_F(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}(F > F_{|S|, n-p}(1-\alpha)).
$$

Note that the $F$-test searches for deviations of $\boldsymbol{\beta}_{S}$ in all directions, and does not have one-sided variants like the $t$-test.

#### Example: Testing for Any Significant Coefficients Except the Intercept

Suppose $\boldsymbol{x}_{*,0} = \boldsymbol{1}_n$ is an intercept term. Then, consider the null hypothesis $H_0: \beta_1 = \cdots = \beta_{p-1} = 0$. In other words, the null hypothesis is the intercept-only model, and the alternative hypothesis is the regression model with an intercept and $p-1$ additional predictors. In this case, $S = \{1, \dots, p-1\}$ and $-S = \{0\}$. The corresponding $F$ statistic is

$$
F \equiv \frac{(\|\boldsymbol{y} - \bar{y} \boldsymbol{1}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2)/(p-1)}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2/(n-p)},
$$

with null distribution $F_{p-1, n-p}$.

#### Example: Testing for Equality of Group Means in $C$-Groups Model

As a further special case, consider the $C$-groups model from Chapter 1. Recall the ANOVA decomposition

$$
\sum_{i = 1}^n (y_i - \bar{y})^2 = \sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2 + \sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2 = \text{SSB} + \text{SSW}.
$$

The $F$-statistic in this case becomes

$$
F = \frac{\sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2/(C-1)}{\sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2/(n-C)} = \frac{\text{SSB}/(C-1)}{\text{SSW}/(n-C)},
$$

with null distribution $F_{C-1, n-C}$.

## Power {#sec-power}

*See also Agresti 3.2.5*

So far we've been focused on finding the null distributions of various test statistics in order to construct tests with Type-I error control. Now let's shift our attention to examining the power of these tests.

### The power of a $t$-test

Consider the $t$-test of the null hypothesis $H_0: \beta_j = 0$. Suppose that, in reality, $\beta_j \neq 0$. What is the probability the $t$-test will reject the null hypothesis? To answer this question, recall that $\widehat \beta_j \sim N(\beta_j, \sigma^2/s_j^2)$. Therefore,

$$
t = \frac{\widehat \beta_j}{\text{SE}(\widehat \beta_j)} = \frac{\beta_j}{\text{SE}(\widehat \beta_j)} + \frac{\widehat \beta_j - \beta_j}{\text{SE}(\widehat \beta_j)} \overset{\cdot}{\sim} N\left(\frac{\beta_j s_j}{\sigma}, 1\right)
$$ {#eq-t-alt-dist-1}

Here we have made the approximation $\text{SE}(\widehat \beta_j) \approx \frac{\sigma}{s_j}$, which is pretty good when $n-p$ is large. Therefore, the power of the two-sided $t$-test is

$$
\mathbb{E}[\phi_t] = \mathbb{P}[\phi_t = 1] \approx \mathbb{P}[|t| > z_{1-\alpha/2}] \approx \mathbb{P}\left[\left|N\left(\frac{\beta_j s_j}{\sigma}, 1\right)\right| > z_{1-\alpha/2}\right]
$$

Therefore, the quantity $\frac{\beta_j s_j}{\sigma}$ determines the power of the $t$-test. To understand $s_j$ a little better, let's assume that the rows $\boldsymbol{x}_{i*}$ of the model matrix are drawn i.i.d. from some distribution $(x_0, \dots, x_{p-1})$. Then we have roughly

$$
\boldsymbol{x}_{*j}^\perp \approx \boldsymbol{x}_{*j} - \mathbb{E}[\boldsymbol{x}_{*j}|\boldsymbol{X}_{*, \text{-}j}],
$$

so $x_{ij}^\perp \approx x_{ij} - \mathbb{E}[x_{ij}|\boldsymbol{x}_{i,\text{-}j}]$. Hence,

$$
s_j^2 \equiv \|\boldsymbol{x}_{*j}^\perp\|^2 \approx n\mathbb{E}[(x_j-\mathbb{E}[x_j|\boldsymbol{x}_{\text{-}j}])^2] = n\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]].
$$

Hence, we can rewrite the alternative distribution ([-@eq-t-alt-dist-1]) as

$$
t \overset{\cdot}{\sim} N\left(\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]}}{\sigma}, 1\right)
$$ {#eq-t-alt-dist-2}

We can see clearly now how the power of the $t$-test varies with the effect size $\beta_j$, the sample size $n$, the degree of collinearity $\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]$, and the noise standard deviation $\sigma$.

### The power of an $F$-test

Now let's turn our attention to computing the power of the $F$-test. We have

$$
F = \frac{\|\boldsymbol{X}\boldsymbol{\widehat \beta} - \boldsymbol{X}_{*, \text{-}S}\boldsymbol{\widehat \beta}_{-S}\|^2/|S|}{\|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat \beta}\|^2/|n-p|} = \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\|(\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y}\|^2/|n-p|} \approx \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\sigma^2}.
$$

To calculate the distribution of the numerator, we need to introduce the notion of a non-central chi-squared random variable.

::: {#def-noncentral-chi-square}
For some vector $\boldsymbol{\mu} \in \mathbb{R}^d$, suppose $\boldsymbol{z} \sim N(\boldsymbol{\mu}, \boldsymbol{I}_d)$. Then, we define the distribution of $\|\boldsymbol{z}\|^2$ as the noncentral chi-square random variable with $d$ degrees of freedom and noncentrality parameter $\|\boldsymbol{\mu}\|^2$ and denote this distribution by $\chi^2_d(\|\boldsymbol{\mu}\|^2)$.
:::

The following proposition states two useful facts about noncentral chi-square distributions.

::: {#prp-noncentral-chi-square}
The following two relations hold:
1. The mean of a $\chi^2_d(\|\boldsymbol{\mu}\|^2)$ random variable is $d + \|\boldsymbol{\mu}\|^2$.
2. If $\boldsymbol{P}$ is a projection matrix and $\boldsymbol{y} = \boldsymbol{\mu} + \boldsymbol{\epsilon}$, then $\frac{1}{\sigma^2}\|\boldsymbol{P} \boldsymbol{y}\|^2 \sim \chi^2_{\text{tr}(\boldsymbol{P})}\left(\frac{1}{\sigma^2}\|\boldsymbol{P} \boldsymbol{\mu}\|^2\right).
:::

It therefore follows that

$$
F \approx \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\sigma^2} \sim \frac{1}{|S|}\chi^2_{|S|}\left(\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S})\boldsymbol{X} \boldsymbol{\beta}\|^2\right) = \frac{1}{|S|}\chi^2_{|S|}\left(\frac{1}{\sigma^2}\|\boldsymbol{X}^\perp_{*, S}\boldsymbol{\beta}_S\|^2\right).
$$

Assuming as before that the rows of $\boldsymbol{X}$ are samples from a joint distribution, we can write

$$
\|\boldsymbol{X}^\perp_{*, S}\boldsymbol{\beta}_S\|^2 \approx n\boldsymbol{\beta}_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S.
$$

Therefore,

$$
F \overset{\cdot}{\sim} \frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{\sigma^2}\right)
$$

which is similar in spirit to equation ([-@eq-t-alt-dist-2]). To get a better sense of what this relationship implies for the power of the $F$-test, we find from the first part of @prp-noncentral-chi-square that, under the alternative,

$$
\mathbb{E}[F] \approx \mathbb{E}\left[\frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{\sigma^2}\right)\right] = 1 + \frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{|S| \cdot \sigma^2}.
$$

By contrast, under the null, the mean of the $F$-statistic is 1. The $|S|$ term in the denominator above suggests that testing larger sets of variables explaining the same amount of variation in $\boldsymbol{y}$ will hurt power. The test must accommodate for the fact that larger sets of variables will explain more of the variability in $y$ even under the null hypothesis.

### Power of the $t$-test when predictors are added to the model

As we know, the outcome of a regression is a function of the predictors that are used. What happens to the $t$-test $p$-value for $H_0: \beta_j = 0$ when a predictor is added to the model? To keep things simple, let's consider the

$$
\text{true underlying model:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
$$

Let's consider the power of testing $H_0: \beta_0 = 0$ in the regression models

$$
\text{model 0:}\ y = \beta_0 x_0 + \epsilon \quad \text{versus} \quad \text{model 1:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
$$

There are four cases based on $\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}]$ and the value of $\beta_1$ in the true model:

1. $\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] \neq 0$ and $\beta_1 \neq 0$. In this case, in model 0 we have omitted an important variable that is correlated with $\boldsymbol{x}_{*0}$. Therefore, the meaning of $\beta_0$ differs between model 0 and model 1, so it may not be meaningful to compare the $p$-values arising from these two models.
2. $\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] \neq 0$ and $\beta_1 = 0$. In this case, we are adding a null predictor that is correlated with $x_{*0}$. Recall that the power of the $t$-test hinges on the quantity $\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]}}{\sigma}$. Adding the predictor $x_1$ has the effect of reducing the conditional predictor variance $\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]$, therefore reducing the power. This is a case of *predictor competition*.
3. $\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0$ and $\beta_1 \neq 0$. In this case, we are adding a non-null predictor that is orthogonal to $\boldsymbol{x}_{*0}$. While the conditional predictor variance $\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]$ remains the same due to orthogonality, the residual variance $\sigma^2$ is reduced when going from model 0 to model 1.\footnote{If $\beta_1$ is small enough, then the unbiased estimate of the residual variance may actually increase due to a reduction in the residual degrees of freedom in the denominator.} Therefore, in this case adding $x_1$ to the model increases the power for testing $H_0: \beta_0 = 0$. This is a case of *predictor collaboration*.
4. $\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0$ and $\beta_1 = 0$. In this case, we are adding an orthogonal null variable, which does not change the conditional predictor variance or the residual variance, and therefore keeps the power of the test the same.

In conclusion, adding a predictor can either increase or decrease the power of a $t$-test. Similar reasoning can be applied to the $F$-test.

**Remark: Adjusting for covariates in randomized experiments.** Case 3 above, i.e., $\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0$ and $\beta_1 \neq 0$, arises in the context of randomized experiments in causal inference. In this case, $y$ represents the outcome, $x_0$ represents the treatment, and $x_1$ represents a covariate. Because the treatment is randomized, there is no correlation between $x_0$ and $x_1$. Therefore, it is not necessary to adjust for $x_1$ in order to get an unbiased estimate of the average treatment effect. However, it is known that adjusting for covariates can lead to more *precise* estimates of the treatment effect due to the phenomenon discussed in case 3 above. This point is also related to the discussion in Chapter 1 about the fact that if $x_0$ and $x_1$ are orthogonal, then the least squares coefficient $\widehat \beta_0$ is the same regardless of whether $x_1$ is included in the model. As we see here, either including $x_1$ in the model or adjusting $y$ for $x_1$ is necessary to get better power.

## Confidence and prediction intervals

*See also Agresti 3.3, Dunn and Smyth 2.8.4-2.8.5*

In addition to hypothesis testing, we often want to construct confidence intervals for the coefficients.

### Confidence interval for a coefficient

Under $H_0: \beta_j = 0$, we showed that $\frac{\widehat{\beta_j}}{\widehat{\sigma}/s_j} \sim t_{n-p}$. The same argument shows that for arbitrary $\beta_j$, we have

$$
\frac{\widehat{\beta_j} - \beta_j}{\widehat{\sigma}/s_j} \sim t_{n-p}.
$$

We can use this relationship to construct a confidence interval for $\beta_j$ as follows:

$$
\begin{split}
1-\alpha = \mathbb{P}[|t_{n-p}| \leq t_{n-p}(1-\alpha/2)] &= \mathbb{P}\left[\left|\frac{\widehat{\beta_j} - \beta_j}{\widehat{\sigma}/s_j}\right| \leq t_{n-p}(1-\alpha/2) \right] \\
&= \mathbb{P}\left[\beta_j \in \left[\widehat{\beta_j} - \frac{\widehat{\sigma}}{s_j}t_{n-p}(1-\alpha/2), \widehat{\beta_j} + \frac{\widehat{\sigma}}{s_j}t_{n-p}(1-\alpha/2) \right]\right] \\
&\equiv \mathbb{P}\left[\beta_j \in \left[\widehat{\beta_j} - \text{SE}(\widehat{\beta_j})t_{n-p}(1-\alpha/2), \widehat{\beta_j} + \text{SE}(\widehat{\beta_j})t_{n-p}(1-\alpha/2) \right]\right] \\
&\equiv \mathbb{P}[\beta_j \in \text{CI}(\beta_j)].
\end{split}
$$ {#eq-pointwise-interval-beta}

The confidence interval $\text{CI}(\beta_j)$ defined above therefore has $1-\alpha$ coverage. Because of the duality between confidence intervals and hypothesis tests, the factors contributing to powerful tests ([-@sec-power]) also lead to shorter confidence intervals.

### Confidence interval for $\mathbb{E}[y|\boldsymbol{x_0}]$

Suppose now that we have a new predictor vector $\boldsymbol{x_0} \in \mathbb{R}^p$. The mean of the response for this predictor vector is $\mathbb{E}[y|\boldsymbol{x_0}] = \boldsymbol{x_0}^T \boldsymbol{\beta}$. Plugging in $\boldsymbol{x_0}$ for $\boldsymbol{c}$ in the relation, we obtain

$$
\frac{\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} - \boldsymbol{x_0}^T \boldsymbol{\beta}}{\widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}}} \sim t_{n-p}.
$$

From this, we can derive that

$$
\text{CI}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}) \cdot t_{n-p}(1-\alpha/2) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}} \cdot t_{n-p}(1-\alpha/2)
$$

is a $1-\alpha$ confidence interval for $\boldsymbol{x_0}^T \boldsymbol{\beta}$. We see that the width of this confidence interval depends on $\boldsymbol{x_0}$ through the quantity $\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}$. Let's give this quantity a closer look, in the case when the regression contains an intercept, i.e., $\boldsymbol{x_{*,0}} = \boldsymbol{1}$. Then, we have $\boldsymbol{x_0} = (1, \boldsymbol{x^T_{0,\text{-}0}})$. Then, defining $\bar{x} \in \mathbb{R}^{p-1}$ as the vector of column-wise means of $\boldsymbol{X_{*,\text{-}0}}$, we can rewrite the regression as

$$
y = \beta_0 + \boldsymbol{x_{\text{-}0}}^T \boldsymbol{\beta_{\text{-}0}} + \epsilon \equiv \beta'_0 + (\boldsymbol{x_{\text{-}0}}-\bar{x})^T  \boldsymbol{\beta_{\text{-}0}} + \epsilon.
$$

Therefore, we seek a prediction interval for $\boldsymbol{x_{0}}^T \boldsymbol{\beta} = \beta'_0 + (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T \boldsymbol{\beta_{\text{-}0}}$. With this reformulation, we can compute

$$
\begin{split}
\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0} &= (1 \ (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T)\begin{pmatrix}\boldsymbol{1}^T \boldsymbol{1} & 0 \\ 0 &\boldsymbol{X_{*,\text{-}0}}^T \boldsymbol{X_{*,\text{-}0}} \end{pmatrix}^{-1}{1 \choose \boldsymbol{x_{0, \text{-}0}}-\bar{x}} \\
&= \frac{1}{n} + (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T (\boldsymbol{X_{*,\text{-}0}}^T \boldsymbol{X_{*,\text{-}0}})^{-1}(\boldsymbol{x_{0, \text{-}0}}-\bar{x}).
\end{split}
$$

Hence, we see that this quantity grows larger as $\boldsymbol{x_{0, \text{-}0}}-\bar{x}$ grows larger, and achieves its minimum when $\boldsymbol{x_{0, \text{-}0}}=\bar{x}$. Let's look at the special case when $p = 2$, so there is just one predictor except the intercept. Then, we have $\boldsymbol{X_{*,\text{-}0}} = \boldsymbol{x_{*,1}}-\bar{x_1}$, so

$$
\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0} = \frac{1}{n} + \frac{(x_{01}-\bar{x_1})^2}{\|\boldsymbol{x_{*,1}}-\bar{x_1}\|^2}.
$$

### Prediction interval for $y|\boldsymbol{x_0}$

Instead of creating a confidence interval for a point on the regression line, we may want to create a confidence interval for a new draw $y_0$ of $y$ for $\boldsymbol{x} = \boldsymbol{x_0}$, i.e., a *prediction interval*. Note that

$$
y_0 - \boldsymbol{x_0}^T \widehat{\beta} = \boldsymbol{x_0}^T \beta + \epsilon_0 - \boldsymbol{x_0}^T \widehat{\beta} = \epsilon_0 + \boldsymbol{x_0}^T (\beta-\widehat{\beta}) \sim N(0, \sigma^2 + \sigma^2 \boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}).
$$

Therefore, we have

$$
\frac{y_0 - \boldsymbol{x_0}^T \widehat{\beta}}{\widehat{\sigma}\sqrt{1 + \boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}}} \sim t_{n-p},
$$

which leads to the $1-\alpha$ prediction interval

$$
\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{1+\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}} \cdot t_{n-p}(1-\alpha/2) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}) \cdot t_{n-p}(1-\alpha/2).
$$ {#eq-pointwise-contrast-interval}

**Remark: Prediction with confidence in machine learning.**

The entire field of supervised machine learning is focused on accurately predicting $y_0$ from $\boldsymbol{x_0}$, usually using nonlinear functions $\widehat{f}(\boldsymbol{x_0})$. In addition to providing a guess $\widehat{y_0}$ for $y_0$, it is often useful to quantify the uncertainty in this guess. In other words, it is useful to come up with a prediction interval (or prediction region) $\text{PI}(y_0)$ such that

$$
\mathbb{P}[y_0 \in \text{PI}(y_0) \mid \boldsymbol{x_0}] \geq 1-\alpha.
$$ {#eq-conditional-prediction-interval}

For example, in safety-critical applications of machine learning like self-driving cars, it is essential to have confidence in predictions. Unfortunately, beyond the realm of linear regression, it is hard to come up with intervals satisfying ([-@eq-conditional-prediction-interval]) for each point $\boldsymbol{x_0}$. However, the emerging field of *conformal inference* provides guarantees on average over possible values of $\boldsymbol{x}$:

$$
\mathbb{P}[y \in \text{PI}(y)] = \mathbb{E}[\mathbb{P}[y \in \text{PI}(y) \mid \boldsymbol{x}]] \geq 1-\alpha.
$$ {#eq-unconditional-prediction-interval}

Remarkably, these guarantees place no assumption on the machine learning method used and require only that the data points on which $\widehat{f}$ is trained are exchangeable (an even weaker condition than i.i.d.). While the unconditional guarantee ([-@eq-unconditional-prediction-interval]) is weaker than the conditional one ([-@eq-conditional-prediction-interval]), it can be obtained for modern machine learning and deep learning models.

### Simultaneous intervals

Note that the intervals in the preceding sections have *pointwise coverage*. For example, we have

$$
\mathbb{P}[\beta_j \in \text{CI}(\beta_j)] \geq 1-\alpha \quad \text{for each } j.
$$

or

$$
\mathbb{P}[\boldsymbol{x_0}^T \boldsymbol{\beta} \in \text{CI}(\boldsymbol{x_0}^T \boldsymbol{\beta})] \geq 1-\alpha \quad \text{for each } \boldsymbol{x_0}.
$$

Sometimes a stronger *simultaneous coverage* guarantee is desired, e.g.,

$$
\mathbb{P}[\beta_j \in \text{CI}^{\text{sim}}(\beta_j) \ \text{for each } j] \geq 1-\alpha
$$ {#eq-simultaneous-coordinatewise}

or

$$
\mathbb{P}[\boldsymbol{x_0}^T \boldsymbol{\beta} \in \text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \ \text{for each } \boldsymbol{x_0}] \geq 1-\alpha.
$$ {#eq-simultaneous-contrasts}

Simultaneous confidence intervals are possible to construct as well. As a starting point, note that

$$
\frac{\frac{1}{p}\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X} \boldsymbol{\beta}\|^2}{\widehat{\sigma}^2} \sim F_{p, n-p}.
$$

Hence, we have

$$
\mathbb{P}[\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X} \boldsymbol{\beta}\|^2 \leq p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha)] \geq 1-\alpha.
$$

Hence, the region

$$
\text{CR}(\boldsymbol{\beta}) \equiv \{\boldsymbol{\beta}: (\boldsymbol{\widehat{\beta}} - \boldsymbol{\beta})^T \boldsymbol{X}^T \boldsymbol{X} (\boldsymbol{\widehat{\beta}} - \boldsymbol{\beta})  \leq p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha)\} \subseteq \mathbb{R}^p
$$

is a $1-\alpha$ confidence region for the vector $\boldsymbol{\beta}$:

$$
\mathbb{P}[\boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})] \geq 1-\alpha.
$$

It's easy to see that $\text{CR}(\boldsymbol{\beta})$ is an ellipse centered at $\boldsymbol{\widehat{\beta}}$.

![Confidence region and simultaneous and pointwise confidence intervals.](figures/confidence-regions.jpg){#fig-confidence-region width=40%}

Since the confidence region is for the entire vector $\boldsymbol{\beta}$, we can define simultaneous confidence intervals for each coordinate as follows:

$$
\text{CI}^{\text{sim}}(\beta_j) \equiv \{\beta_j: \boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\}.
$$

Then, these confidence intervals will satisfy the simultaneous coverage property ([-@eq-simultaneous-coordinatewise]). We will obtain a more explicit expression for $\text{CI}^{\text{sim}}(\beta_j)$ shortly.

Similarly, we may define the simultaneous confidence regions

$$
\text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \equiv \{\boldsymbol{x_0}^T \boldsymbol{\beta}: \boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\}.
$$

Let us find a more explicit expression for the latter interval. For notational ease, let us define $\boldsymbol{\Sigma} \equiv \boldsymbol{X}^T \boldsymbol{X}$. Then, note that if $\boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})$, then by the Cauchy-Schwarz inequality we have

$$
\begin{split}
(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}-\boldsymbol{x_0}^T \boldsymbol{\beta})^2 = \|\boldsymbol{x_0}^T (\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 &= \|(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{x_0})^T \boldsymbol{\Sigma}^{1/2}(\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 \\
&\leq \|(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{x_0})\|^2\|\boldsymbol{\Sigma}^{1/2}(\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 \leq \boldsymbol{x_0}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{x_0} p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha),
\end{split}
$$

i.e.,

$$
\boldsymbol{x_0}^T \boldsymbol{\beta} \in \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{x_0}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}})\cdot\sqrt{pF_{p, n-p}(1-\alpha)}.
$$ {#eq-simultaneous-fit-se}

Defining the above interval as $\text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta})$ gives us the simultaneous coverage property ([-@eq-simultaneous-contrasts]). Comparing to equation ([-@eq-pointwise-contrast-interval]), we see that the simultaneous interval is the pointwise interval expanded by a factor of $\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)$. Specializing to the case $\boldsymbol{x_0} \equiv \boldsymbol{e_j}$, we get an expression for the simultaneous intervals for each coordinate:

$$
\text{CI}^{\text{sim}}(\beta_j) \equiv \widehat{\beta_j} \pm \widehat{\sigma} \sqrt{(\boldsymbol{X}^T \boldsymbol{X})^{-1}_{jj}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \text{SE}(\widehat{\beta_j})\sqrt{pF_{p, n-p}(1-\alpha)},
$$ {#eq-simultaneous-coordinatewise-se}

which again is the pointwise interval ([-@eq-pointwise-interval-beta]) expanded by a factor of $\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)$. These simultaneous intervals are called *Working-Hotelling intervals*.

## Practical considerations

### Practical versus statistical significance

You can have a statistically significant effect that is not practically significant. The hypothesis testing framework is most useful in the case when the signal-to-noise ratio is relatively small. Otherwise, constructing a confidence interval for the effect size is a more meaningful approach.

### Correlation versus causation, and Simpson's paradox

Causation can be elusive for several reasons. One is reverse causation, where it is not clear whether $X$ causes $Y$ or $Y$ causes $X$. Another is confounding, where there is a third variable $Z$ that causes both $X$ and $Y$. For the latter reason, linear regression coefficients can be sensitive to the choice of other predictors to include and can be misleading if you omit important variables from the regression. A special and sometimes overlooked case of this is *Simpson's paradox*, where an important discrete variable is omitted. Consider the example in Figure [-@fig-simpson-paradox]. Sometimes this discrete variable may seem benign, such as the year in which the data was collected. Such variables might or might not be measured.

![An example of Simpson's paradox (source: Wikipedia).](figures/kidney-stones.png){#fig-simpson-paradox width=100%}

### Dealing with correlated predictors

It depends on the goal. If we're trying to tease apart effects of correlated predictors, then we have no choice but to proceed as usual despite lower power. Otherwise, we can test predictors in groups via the $F$-test to get higher power at the cost of lower "resolution." Sometimes, it is recommended to simply remove predictors that are correlated with other predictors. This practice, however, is somewhat arbitrary and not recommended.

### Model selection

We need to ask ourselves: Why do we want to do model selection? It can either be for prediction purposes or for inferential purposes. If it is for prediction purposes, then we can apply cross-validation to select a model and we don't need to think very hard about statistical significance. If it is for inference, then we need to be more careful. There are various classical model selection criteria (e.g., AIC, BIC), but it is not entirely clear what statistical guarantee we are getting for the resulting models. A simpler approach is to apply a $t$-test for each variable in the model, apply a multiple testing correction to the resulting $p$-values, and report the set of significant variables and the associated guarantee. Re-fitting the linear regression after model selection leads us into some dicey inferential territory due to selection bias. This is the subject of ongoing research, and the jury is still out on the best way of doing this.

## R demo

*See also Agresti 3.4.1, 3.4.3, Dunn and Smyth 2.6, 2.14*

Let's put into practice what we've learned in this chapter by analyzing data about house prices.

```{r, message = FALSE}
library(tidyverse)
library(GGally)

houses_data <- read_tsv("data/Houses.dat")
houses_data
```

### Exploration

Let's first do a bit of exploration:

```{r, fig.width = 4, fig.height = 3}
# visualize distribution of housing prices, superimposing the mean
houses_data |>
  ggplot(aes(x = price)) +
  geom_histogram(color = "black", bins = 30) +
  geom_vline(aes(xintercept = mean(price)),
    colour = "red",
    linetype = "dashed"
  )
```

```{r}
# compare median and mean price
houses_data |>
  summarise(
    mean_price = mean(price),
    median_price = median(price)
  )
```

```{r, fig.width = 4.5, fig.height = 4}
# create a pairs plot of continuous variables
houses_data |>
  select(price, size, taxes) |>
  ggpairs()
```

```{r, fig.width = 3, fig.height = 3}
# see how price relates to beds
houses_data |>
  ggplot(aes(x = factor(beds), y = price)) +
  geom_boxplot(fill = "dodgerblue")
```

```{r, fig.width = 3, fig.height = 3}
# see how price relates to baths
houses_data |>
  ggplot(aes(x = factor(baths), y = price)) +
  geom_boxplot(fill = "dodgerblue")
```

```{r, fig.width = 2, fig.height = 3}
# see how price relates to new
houses_data |>
  ggplot(aes(x = factor(new), y = price)) +
  geom_boxplot(fill = "dodgerblue")
```

### Hypothesis testing

Let's run a linear regression and interpret the summary. But first, we must decide whether to model beds/baths as categorical or continuous? We should probably model these as categorical, given the potentially nonlinear trend observed in the box plots.

```{r}
lm_fit <- lm(price ~ factor(beds) + factor(baths) + new + size,
  data = houses_data
)
summary(lm_fit)
```

We can read off the test statistics and $p$-values for each variable from the regression summary, as well as for the $F$-test against the constant model from the bottom of the summary.

Let's use an $F$-test to assess whether the categorical `baths` variable is important.

```{r}
lm_fit_partial <- lm(price ~ factor(beds) + new + size,
  data = houses_data
)
anova(lm_fit_partial, lm_fit)
```

What if we had not coded `baths` as a factor?

```{r}
lm_fit_not_factor <- lm(price ~ factor(beds) + baths + new + size,
  data = houses_data
)
anova(lm_fit_partial, lm_fit_not_factor)
```

If we want to test for the equality of means across groups of a categorical predictor, without adjusting for other variables, we can use the ANOVA $F$-test. There are several equivalent ways of doing so:

```{r}
# just use the summary function
lm_fit_baths <- lm(price ~ factor(baths), data = houses_data)
summary(lm_fit_baths)

# use the anova function as before
lm_fit_const <- lm(price ~ 1, data = houses_data)
anova(lm_fit_const, lm_fit_baths)

# use the aov function
aov_fit <- aov(price ~ factor(baths), data = houses_data)
summary(aov_fit)
```

We can also use an $F$-test to test for the presence of an interaction with a multi-class categorical predictor.

```{r}
lm_fit_interaction <- lm(price ~ size * factor(beds), data = houses_data)
summary(lm_fit_interaction)

lm_fit_size <- lm(price ~ size + factor(beds), data = houses_data)
anova(lm_fit_size, lm_fit_interaction)
```

Contrasts of regression coefficients can be tested using the `glht()` function from the `multcomp` package.

### Confidence intervals

We can construct pointwise confidence intervals for each coefficient using `confint()`:

```{r}
confint(lm_fit)
```

To create simultaneous confidence intervals, we need a somewhat more manual approach. We start with the coefficients and standard errors:

```{r}
coef(summary(lm_fit))
```

Then we add lower and upper confidence interval endpoints based on the formula ([-@eq-simultaneous-coordinatewise-se]):

```{r}
alpha <- 0.05
n <- nrow(houses_data)
p <- length(coef(lm_fit))
f_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)
coef(summary(lm_fit)) |>
  as.data.frame() |>
  rownames_to_column(var = "Variable") |>
  select(Variable, Estimate, `Std. Error`) |>
  mutate(
    CI_lower = Estimate - `Std. Error` * sqrt(p * f_quantile),
    CI_upper = Estimate + `Std. Error` * sqrt(p * f_quantile)
  )
```

Note that the simultaneous intervals are substantially larger.

To construct pointwise confidence intervals for the fit, we can use the `predict()` function:

```{r}
predict(lm_fit, newdata = houses_data, interval = "confidence") |> head()
```

To get pointwise prediction intervals, we switch `"confidence"` to `"prediction"`:

```{r}
predict(lm_fit, newdata = houses_data, interval = "prediction") |> head()
```

To construct simultaneous confidence intervals for the fit or predictions, we again need a slightly more manual approach. We call `predict()` again, but this time asking it for the standard errors rather than the confidence intervals:

```{r}
predictions <- predict(lm_fit, newdata = houses_data, se.fit = TRUE)
head(predictions$fit)
head(predictions$se.fit)
```

Now we can construct the simultaneous confidence intervals via the formula ([-@eq-simultaneous-fit-se]):

```{r}
f_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)
tibble(
  lower = predictions$fit - predictions$se.fit * sqrt(p * f_quantile),
  upper = predictions$fit + predictions$se.fit * sqrt(p * f_quantile)
)
```

In the case of simple linear regression, we can plot these pointwise and simultaneous confidence intervals as bands:

```{r, fig.width = 3, fig.height = 3}
# to produce confidence intervals for fits in general, use the predict() function
n <- nrow(houses_data)
p <- 2
alpha <- 0.05
lm_fit <- lm(price ~ size, data = houses_data)
predictions <- predict(lm_fit, se.fit = TRUE)
t_quantile <- qt(1 - alpha / 2, df = n - p)
f_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)
houses_data |>
  mutate(
    fit = predictions$fit,
    se = predictions$se.fit,
    ptwise_width = t_quantile * se,
    simultaneous_width = sqrt(p * f_quantile) * se
  ) |>
  ggplot(aes(x = size)) +
  geom_point(aes(y = price)) +
  geom_line(aes(y = fit), color = "blue") +
  geom_line(aes(y = fit + ptwise_width, color = "Pointwise")) +
  geom_line(aes(y = fit - ptwise_width, color = "Pointwise")) +
  geom_line(aes(y = fit + simultaneous_width, color = "Simultaneous")) +
  geom_line(aes(y = fit - simultaneous_width, color = "Simultaneous")) +
  theme(legend.title = element_blank(), legend.position = "bottom")
```

### Predictor competition and collaboration

Let's look at the power of detecting the association between `price` and `beds`. We can imagine that `beds` and `baths` are correlated:

```{r, fig.width = 3.5, fig.height = 3}
houses_data |>
  ggplot(aes(x = beds, y = baths)) +
  geom_count()
```

So let's see how significant `beds` is, with and without `baths` in the model:

```{r}
lm_fit_only_beds <- lm(price ~ factor(beds), data = houses_data)
summary(lm_fit_only_beds)
```

```{r}
lm_fit_only_baths <- lm(price ~ factor(baths), data = houses_data)
lm_fit_beds_baths <- lm(price ~ factor(beds) + factor(baths), data = houses_data)
anova(lm_fit_only_baths, lm_fit_beds_baths)
```

We see that the significance of `beds` dropped by two orders of magnitude. This is an example of predictor competition.

On the other hand, note that the variable `new` is not very correlated with `beds`:

```{r}
lm_fit <- lm(new ~ beds, data = houses_data)
summary(lm_fit)
```

but we know it has a substantial impact on `price`. Let's look at the significance of the test that `beds` is not important when we add `new` to the model.

```{r}
lm_fit_only_new <- lm(price ~ new, data = houses_data)
lm_fit_beds_new <- lm(price ~ new + factor(beds), data = houses_data)
anova(lm_fit_only_new, lm_fit_beds_new)
```

Adding `new` to the model made the $p$-value more significant by a factor of 10. This is an example of predictor collaboration.

