% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{book}
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{lemma}[proposition]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{1}

\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\title{STAT 9610 Lecture Notes}
\author{Eugene Katsevich}
\date{Fall 2023}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if (is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse = ", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)
@

\begin{document}

\frontmatter

\maketitle

\chapter*{Preface}

This is a set of lecture notes developed for the PhD statistics course ``STAT 9610: Statistical Methodology'' at the University of Pennsylvania. Much of the content is adapted from Alan Agresti's book \textit{Foundations of Linear and Generalized Linear Models} (2015). These notes may contain typos and errors, and will be updated in subsequent iterations of STAT 9610.

\tableofcontents

\mainmatter

<<echo=FALSE, message=FALSE>>=
conflicted::conflicts_prefer(dplyr::filter)
conflicted::conflicts_prefer(dplyr::select)
@


\chapter{Linear models: Estimation}

\section{Introduction}

\textit{See also Agresti 1.1, Dunn and Smyth 1.1-1.2, 1.5-1.6, 1.8-1.12} \\

The overarching statistical goal addressed in this class is to learn about relationships between a response $y$ and predictors $x_0, x_1, \dots, x_{p-1}$. This abstract formulation encompasses an extremely wide variety of applications. The most widely used set of statistical models to address such problems are \textit{generalized linear models}, which are the focus of this class.

Let's start by recalling the \textit{linear model}, the most fundamental of the generalized linear models. In this case, the response is continuous ($y \in \mathbb R$) and modeled as
\begin{equation}
y = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} + \epsilon,
\label{eq:lm1}
\end{equation}
where
\begin{equation}
\epsilon \sim (0, \sigma^2), \quad \text{i.e.}\ \mathbb E[\epsilon] = 0 \ \text{and} \ \text{Var}[\epsilon] = \sigma^2.
\label{eq:lm2}
\end{equation}
We view the predictors $x_0, \dots, x_{p-1}$ as fixed, so the only source of randomness in $y$ is $\epsilon$. Another way of writing the linear model is
\begin{equation*}
\mu \equiv \mathbb E[y] = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} \equiv \eta.
\end{equation*}

Not all responses are continuous, however. In some cases, we have binary responses ($y \in \{0,1\}$) or count responses ($y \in \mathbb Z$). In these cases, there is a mismatch between the
\begin{equation*}
\textit{linear predictor } \eta \equiv \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1}
\end{equation*}
and the
\begin{equation*}
\textit{mean response } \mu \equiv \mathbb E[y].
\end{equation*}
The linear predictor can take arbitrary real values $(\eta \in \mathbb R)$, but the mean response can lie in a restricted range, depending on the response type. For example, $\mu \in [0,1]$ for binary $y$ and $\mu \in [0, \infty)$ for count $y$.

For these kinds of responses, it makes sense to model a \textit{transformation} of the mean as linear, rather than the mean itself:
\begin{equation}
g(\mu) = g(\mathbb E[y]) = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} = \eta.
\label{eq:glm}
\end{equation}
This transformation $g$ is called the link function. For binary $y$, a common choice of link function is the \textit{logit link}, which transforms a probability into a log-odds:
\begin{equation*}
\text{logit}(\pi) \equiv \log \frac{\pi}{1-\pi}.
\end{equation*}
So the predictors contribute linearly on the log-odds scale rather than on the probability scale. For count $y$, a common choice of link function is the \textit{log link}.

Models of the form~\eqref{eq:glm} are called \textit{generalized linear models} (GLMs). They specialize to linear models for identity link function, i.e. $g(\mu) = \mu$. The focus of this course are methodologies to learn about the coefficients $\bm \beta \equiv (\beta_0, \dots, \beta_{p-1})^T$ of a GLM based on a sample $(\bm X, \bm y) \equiv \{(x_{i,0}, \dots, x_{i,p-1}, y_i)\}_{i = 1}^n$ drawn from this distribution. Learning about the coefficient vector helps us learn about the relationship between the response and the predictors. This course is broken up into five units.

\begin{itemize}
\item \textbf{Chapter 1. Linear model: Estimation.} The \textit{least squares} point estimate $\bm{\widehat \beta}$ of $\bm \beta$ based on a dataset $(\bm X, \bm y)$ under the linear model assumptions~\eqref{eq:lm1} and~\eqref{eq:lm2}.
\item \textbf{Chapter 2. Linear model: Inference.} Under the additional assumption that $\epsilon \sim N(0,\sigma^2)$, how to carry out statistical inference (hypothesis testing and confidence intervals) for the coefficients.
\item \textbf{Chapter 3. Linear model: Misspecification.} What to do when the linear model assumptions are not correct: What issues can arise, how to diagnose them, and how to fix them.
\item \textbf{Chapter 4. GLMs: General theory.} Estimation and inference for GLMs (generalizing Chapters 1 and 2). GLMs fit neatly into a unified theory based on \textit{exponential families}.
\item \textbf{Chapter 5. GLMs: Special cases.} Looking more closely at the most important special cases of GLMs, including logistic regression and Poisson regression.
\end{itemize}

If time permits, we will cover further topics, including multiple testing (how to correct for multiplicity when testing many hypotheses---in GLMs or otherwise) and high-dimensional inference (how to carry out inference in situations where there are more predictors than samples).

We will use the following notations in this course. Vector and matrix quantities will be bolded, whereas scalar quantities will not be. Capital letters will be used for matrices, and lowercase for vectors and scalars. No notational distinction will be made between random quantities and their realizations. The letters $i = 1, \dots, n$ and $j = 0, \dots, p-1$ will index samples and predictors, respectively. The predictors $\{x_{ij}\}_{i,j}$ will be gathered into an $n \times p$ matrix $\bm X$. The rows of $\bm X$ correspond to samples, with the $i$th row denoted $\bm x_{i*}$. The columns of $\bm X$ correspond to predictors, with the $j$th column denoted $\bm x_{*j}$. The responses $\{y_i\}_i$ will be gathered into an $n \times 1$ response vector $\bm y$. The notation $\equiv$ will be used for definitions.

\section{Types of predictors; interpreting linear model coefficients}

\textit{See also Agresti 1.2, Dunn and Smyth 1.4, 1.7, 2.7} \\

The types of predictors $x_j$ (e.g. binary or continuous) has less of an effect on the regression than the type of response, but it is still important to pay attention to the former.

\paragraph{Intercepts.} It is common to include an \textit{intercept} in a linear regression model, a predictor $x_0$ such that $x_{i0} = 1$ for all $i$. When an intercept is present, we index it as the 0th predictor. The simplest kind of linear model is the \textit{intercept-only model} or the \textit{one-sample model}:
\begin{equation}
y = \beta_0 + \epsilon.
\label{eq:one-sample-model}
\end{equation}
The parameter $\beta_0$ is the mean of the response.

\paragraph{Binary predictors.} In addition to an intercept, suppose we have a binary predictor $x_1 \in \{0,1\}$ (e.g. $x_1 = 1$ for patients who took blood pressure medication and $x_1 = 0$ for those who didn't). This leads to the following linear model:
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \epsilon.
\label{eq:two-sample-model}
\end{equation}
Here, $\beta_0$ is the mean response (say blood pressure) for observations with $x_1 = 0$ and $\beta_0 + \beta_1$ is the mean response for observations with $x_1 = 1$. Therefore, the parameter $\beta_1$ is the difference in mean response between observations with $x_1 = 1$ and $x_1 = 0$. This parameter is sometimes called the \textit{effect} or \textit{effect size} of $x_1$, though a causal relationship might or might not be present. The model~\eqref{eq:two-sample-model} is sometimes called the \textit{two-sample model}, because the response data can be split into two ``samples'': those corresponding to $x_1 = 0$ and those corresponding to $x_1 = 1$.

\paragraph{Categorical predictors.} A binary predictor is a special case of a categorical predictor: A predictor taking two or more discrete values. Suppose we have a predictor $w \in \{w_0, w_1, \dots, w_{C-1}\}$, where $C \geq 2$ is the number of categories and $w_0, \dots, w_{C-1}$ are the \textit{levels} of $w$. E.g. suppose $\{w_0, \dots, w_{C-1}\}$ is the collection of U.S. states, so that $C = 50$. If we want to regress a response on the categorical predictor $w$, we cannot simply set $x_1 = w$ in the context of the linear regression~\eqref{eq:two-sample-model}. Indeed, $w$ does not necessarily take numerical values. Instead, we need to add a predictor $x_j$ for each of the levels of $w$. In particular, define $x_j \equiv \mathbbm 1(w = w_j)$ for $j = 1, \dots, C-1$ and consider the regression
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_{C-1}x_{C-1} + \epsilon.
\label{eq:C-sample-model}
\end{equation}
Here, category 0 is the \textit{base category}, and $\beta_0$ represents the mean response in the base category. The coefficient $\beta_j$ represents the difference in mean response between the $j$th category and the base category.

\paragraph{Quantitative predictors.} A quantitative predictor is one that can take on any real value. For example, suppose that $x_1 \in \mathbb R$, and consider the linear model
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \epsilon.
\label{eq:simple-linear-regression}
\end{equation}
Now, the interpretation of $\beta_1$ is that an increase in $x_1$ by 1 is associated with an increase in $y$ by $\beta_1$. We must be careful to avoid saying ``an increase in $x_1$ by 1 \textit{causes} $y$ to increase by $\beta_1$'' unless we make additional causal assumptions. Note that the units of $x_1$ matter. If $x_1$ is the height of a person, then the value and the interpretation of $\beta_1$ changes depending on whether that height is measured in feet or in meters.

\paragraph{Ordinal predictors.} There is an awkward category of predictor in between categorical and continuous called \textit{ordinal}. An ordinal predictor is one that takes a discrete number of values, but these values have an intrinsic ordering, e.g. $x_1 \in \{\texttt{small}, \texttt{medium}, \texttt{large}\}$. It can be treated as categorical at the cost of losing the ordering information, or as continuous if one is willing to assign quantatitive values to each category.

\paragraph{Multiple predictors.} A linear regression need not contain just one predictor (aside from an intercept). For example, let's say $x_1$ and $x_2$ are two predictors. Then, a linear model with both predictors is
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon.
\label{eq:multiple-regression}
\end{equation}
When there are multiple predictors, the interpretation of coefficients must be revised somewhat. For example, $\beta_1$ in the above regression is the effect of an increase in $x_1$ by 1 \textit{while holding $x_2$ constant} or \textit{while adjusting for $x_2$} or \textit{while controlling for $x_2$}. If $y$ is blood pressure, $x_1$ is a binary predictor indicating blood pressure medication taken and $x_2$ is sex, then $\beta_1$ is the effect of the medication on blood pressure while controlling for sex. In general, the coefficient of a predictor depends on what other predictors are in the model. As an extreme case, suppose the medication has no actual effect, but that men generally have higher blood pressure and higher rates of taking the medication. Then, the coefficient $\beta_1$ in the single regression model~\eqref{eq:two-sample-model} would be nonzero but the coefficient in the multiple regression model~\eqref{eq:multiple-regression} would be equal to zero. In this case, sex acts as a \textit{confounder}.


\paragraph{Interactions.} Note that the multiple regression model~\eqref{eq:multiple-regression} has the built-in assumption that the effect of $x_1$ on $y$ is the same for any fixed value of $x_2$ (and vice versa). In some cases, the effect of one variable on the response may depend on the value of another variable. In this case, it's appropriate to add another predictor called an \textit{interaction}. Suppose $x_2$ is quantitative (e.g. years of job experience) and $x_2$ is binary (e.g. sex, with $x_2 = 1$ meaning male). Then, we can define a third predictor $x_3$ as the product of the first two, i.e. $x_3 = x_1x_2$. This gives the regression model
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon.
\label{eq:interaction}
\end{equation}
Now, the effect of adding another year of job experience is $\beta_1$ for females and $\beta_1 + \beta_3$ for males. The coefficient $\beta_3$ is the difference in the effect of job experience between males and females.

\section{Model matrices, model vector spaces, and identifiability}

\textit{See also Agresti 1.3-1.4, Dunn and Smyth 2.1, 2.2, 2.5.1} \\

The matrix $\bm X$ is called the \textit{model matrix} or the \textit{design matrix}. Concatenating the linear model equations~\eqref{eq:lm1} and~\eqref{eq:lm2} across observations give us an equivalent formulation:
\begin{equation*}
\bm y = \bm X \bm \beta + \bm \epsilon; \quad \mathbb E[\bm \epsilon] = \bm 0,\ \text{Var}[\bm \epsilon] = \sigma^2 \bm I_n
\end{equation*}
or
\begin{equation*}
\mathbb E[\bm y] = \bm X \bm \beta = \bm \mu.
\end{equation*}
As $\bm \beta$ varies in $\mathbb R^p$, the set of possible vectors $\bm \mu \in \mathbb R^n$ is defined
\begin{equation*}
C(\bm X) \equiv \{\bm \mu = \bm X \bm \beta: \bm \beta \in \mathbb R^p\}.
\end{equation*}
$C(\bm X)$, called the \textit{model vector space}, is a subspace of $\mathbb R^n$: $C(\bm X) \subseteq \mathbb R^n$. Since
\begin{equation*}
\bm X \bm \beta = \beta_0 \bm x_{*0} + \cdots + \beta_{p-1} \bm x_{*p-1},
\end{equation*}
the model vector space is the column space of the matrix $\bm X$ (Figure~\ref{fig:model-vector-space}).

\begin{figure}[h!]
\centering
\includegraphics[width = 0.5\textwidth]{figures/model-vector-space.png}
\caption{The model vector space.}
\label{fig:model-vector-space}
\end{figure}

The \textit{dimension} of $C(\bm X)$ is the rank of $\bm X$, i.e. the number of linearly independent columns of $\bm X$. If $\text{rank}(\bm X) < p$, this means that there are two different vectors $\bm \beta$ and $\bm \beta'$ such that $\bm X \bm \beta = \bm X \bm \beta'$. Therefore, we have two values of the parameter vector that give the same model for $\bm y$. This makes $\bm \beta$ \textit{not identifiable}, and makes it impossible to reliably determine $\bm \beta$ based on the data. For this reason, we will generally assume that $\bm \beta$ is \textit{identifiable}, i.e. $\bm X \bm \beta \neq \bm X \bm \beta'$ if $\bm \beta \neq \bm \beta'$. This is equivalent to the assumption that $\text{rank}(\bm X) = p$. Note that this cannot hold when $p > n$, so for the majority of the course we will assume that $p \leq n$. In this case, $\text{rank}(\bm X) = p$ if and only if $\bm X$ has \textit{full-rank}.

As an example when $p \leq n$ but when $\bm \beta$ is still not identifiable, consider the case of a categorical predictor. Suppose the categories of $w$ were $\{w_1, \dots, w_{C-1}\}$, i.e. the baseline category $w_0$ did not exist. In this case, the model~\eqref{eq:C-sample-model} would not be identifiable because $x_0 = 1 = x_1 + \cdots + x_{C-1}$ and thus $x_{*0} = 1 = x_{*1} + \cdots + x_{*,C-1}$. Indeed, this means that one of the predictors can be expressed as a linear combination of the others, so $\bm X$ cannot have full rank. A simpler way of phrasing the problem is that we are describing $C-1$ intrinsic parameters (the means in each of the $C-1$ groups) with $C$ model parameters. There must therefore be some redundancy. For this reason, if we include an intercept term in the model then we must designate one of our categories as the baseline and exclude its indicator from the model.

\section{Least squares estimation}

\subsection{Algebraic perspective}

\textit{See also Agresti 2.1.1, Dunn and Smyth 2.4.1, 2.5.2}

Now, suppose that we are given a dataset $(\bm X, \bm y)$. How do we go about estimating $\bm \beta$ based on this data? The canonical approach is the \textit{method of least squares}:
\begin{equation}
\bm {\widehat \beta} \equiv \underset{\bm \beta}{\arg \min}\ \|\bm y - \bm X \bm \beta\|^2.
\end{equation}
The quantity
\begin{equation}
\|\bm y - \bm X \bm{\widehat \beta}\|^2 = \|\bm y - \bm{\widehat \mu}\|^2 = \sum_{i = 1}^n (y_i - \widehat \mu_i)^2
\end{equation}
is called the \textit{residual sum of squares (RSS)}, and it measures the lack of fit of the linear regression model. We therefore want to choose $\bm{\widehat \beta}$ to minimize this lack of fit. Letting $L(\bm{\beta}) = \frac12\|\bm y - \bm X \bm \beta\|^2$, we can do some calculus to derive that
\begin{equation}
\frac{\partial}{\partial \bm \beta}L(\bm \beta) = -\bm X^T(\bm y - \bm X \bm \beta).
\end{equation}
Setting this vector of partial derivatives equal to zero, we arrive at the \textit{normal equations}:
\begin{equation}
-\bm X^T(\bm y - \bm X \bm{\widehat \beta}) = 0 \quad \Longleftrightarrow \quad \bm X^T \bm X \bm {\widehat \beta} = \bm X^T \bm y.
\label{eq:normal-equations}
\end{equation}
If $\bm X$ is full rank, the matrix $\bm X^T \bm X$ is invertible and we can therefore conclude that
\begin{equation}
\bm {\widehat \beta} = (\bm X^T \bm X)^{-1}\bm X^T \bm y.
\label{eq:beta-hat}
\end{equation}

\subsection{Probabilistic perspective}

\textit{See also Agresti 2.7.1}

\paragraph{Least squares as maximum likelihood estimation.}

Note that if $\bm \epsilon$ is assumed to be $N(0,\sigma^2 \bm I_n)$, then the least squares solution would also be the maximum likelihood solution. Indeed, for $y_i \sim N(\mu_i, \sigma^2)$, the log-likelihood is
\begin{equation*}
\log \left[\prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)\right] = \text{constant} - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \mu_i)^2.
\end{equation*}

\paragraph{Gauss-Markov theorem.}

Now that we have derived the least squares estimator, we can compute its bias and variance. To obtain the bias, we first calculate that
\begin{equation*}
\mathbb E[\widehat{\bm\beta}] = \mathbb E[(\bm X^T \bm X)^{-1}\bm X^T \bm y] = (\bm X^T \bm X)^{-1}\bm X^T \mathbb E[\bm y] = (\bm X^T \bm X)^{-1}\bm X^T \bm X \bm \beta = \bm \beta.
\end{equation*}
Therefore, the least squares estimator is unbiased. To obtain the variance, we compute
\begin{equation}
\begin{split}
\text{Var}[\bm{\widehat\beta}] &= \text{Var}[(\bm X^T \bm X)^{-1}\bm X^T \bm y] \\
&= (\bm X^T \bm X)^{-1}\bm X^T\text{Var}[\bm y]\bm X (\bm X^T \bm X)^{-1} \\
&= (\bm X^T \bm X)^{-1}\bm X^T(\sigma^2 \bm I_n)\bm X (\bm X^T \bm X)^{-1} \\
&= \sigma^2 (\bm X^T \bm X)^{-1}.
\label{eq:var-of-beta-hat}
\end{split}
\end{equation}

\begin{theorem}[Gauss-Markov theorem]
For homoskedastic linear models (eqs. \ref{eq:lm1} and \ref{eq:lm2}), the least squares coefficient estimates have the smallest covariance matrix (in the sense of positive semidefinite matrices) among all linear unbiased estimates of $\bm \beta$.
\end{theorem}

\subsection{Geometric perspective}

\textit{See also Agresti 2.2.1-2.2.3} \\

\noindent The following is the key geometric property of least squares (Figure~\ref{fig:least-squares-as-projection}).

\begin{proposition}
The mapping $\bm y \mapsto \bm{\widehat \mu} = \bm X\bm{\widehat \beta} \in C(\bm X)$ is an \textit{orthogonal projection} onto $C(\bm X)$, with projection matrix
\begin{equation}
\bm H \equiv  \bm X(\bm X^T \bm X)^{-1}\bm X^T \quad (\textit{the hat matrix}).
\label{eq:hat-matrix}
\end{equation}
\end{proposition}
Geometrically it makes sense, since we define $\bm{\widehat \beta}$ so that $\bm{\widehat \mu} \in C(\bm X)$ is as close to $\bm y$ as possible. The shortest path between a point and a plane is the perpendicular. A simple example of $\bm H$ can be obtained by considering the intercept-only regression.

\begin{proof}
To prove that $\bm y \mapsto \bm{\widehat \mu}$ is an orthogonal projection onto $C(\bm X)$, it suffices to show that
\begin{equation}
\bm v^T (\bm y - \bm X \bm{\widehat \beta}) = 0 \text{ for each } \bm v \in C(\bm X).
\end{equation}
Since the columns $\{\bm x_{*0}, \dots, \bm x_{*p-1}\}$ of $\bm X$ form a basis for $C(\bm X)$, it suffices to show that $\bm x_{*j}^T (\bm y - \bm X \bm{\widehat \beta}) = 0$ for each $j = 0, \dots, p-1$. This is a consequence of the normal equations $\bm X^T(\bm y - \bm X\bm{\widehat \beta}) = 0$ derived in~\eqref{eq:normal-equations}.

To show that the projection matrix is $\bm H$ \eqref{eq:hat-matrix}, it suffices to check that
\begin{equation}
\bm{\widehat \mu} = \bm X\bm{\widehat \beta} = \bm X(\bm X^T \bm X)^{-1}\bm X^T \bm y \equiv \bm H \bm y.
\end{equation}
\end{proof}

\begin{figure}[h!]
\centering
\includegraphics[width = 0.5\textwidth]{figures/least-squares-as-projection.png}
\caption{Least squares as orthogonal projection.}
\label{fig:least-squares-as-projection}
\end{figure}

The following proposition summarizes a few important facts about projection matrices.

\begin{proposition}
If $\bm P$ is an orthogonal projection onto a subspace $\bm W$, then
\begin{enumerate}
\item $\bm P$ is idempotent, i.e. $\bm P^2 = \bm P$.
\item For all $\bm v \in \bm W$, we have $\bm P\bm v = \bm v$ and for all $\bm v \in \bm W^{\perp}$, we have $\bm P \bm v = 0$.
\item $\textnormal{trace}(\bm P) = \textnormal{dim}(\bm W)$.
\end{enumerate}
\end{proposition}

One consequence of the geometric interpretation of least squares is that the fitted values $\bm{\widehat \mu}$ depend on $\bm X$ only through $C(\bm X)$. As we will see in Homework 1, there are many different model matrices $\bm X$ leading to the same model space. Essentially, this reflects the fact that there are many different bases for the same vector space. Consider for example changing the units on the columns of $\bm X$. It can be verified that not just the fitted values $\bm{\widehat \mu}$ but also the predictions on a new set of features remain invariant to reparametrization (this follows from parts (a) and (b) of Homework 1 Problem 1). Therefore, while reparametrization can have a huge impact on the fitted coefficients, it has no impact on the predictions of linear regression.

\section{Analysis of variance and $R^2$}

\textit{See also Agresti 2.4.2, 2.4.3, 2.4.6, Dunn and Smyth 2.9}

\paragraph{Analysis of variance.}

The orthogonality property of least squares, together with the Pythagorean theorem, leads to a fundamental relationship called \textit{the analysis of variance}.

Let's say that $S \subset \{0, 1, \dots, p-1\}$ is a subset of the predictors we wish to exclude from the model. First regress $\bm y$ on $\bm X$ to get $\bm{\widehat \beta}$ as usual. Then, we consider the \textit{partial model matrix} $\bm X_{*,\text{-}S}$ obtained by selecting all predictors except those in $S$. Regressing $\bm y$ on $\bm X_{*, \text{-}S}$ results in $\bm{\widehat \beta}_{\text{-}S}$ (note: $\bm{\widehat \beta}_{\text{-}S}$ is not necessarily obtained from $\bm{\widehat\beta}$ by extracting the coefficients corresponding to $\text{-}S$).
\begin{theorem}[Analysis of variance]
\begin{equation}
\|\bm{y} -  \bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S}\|^2 = \|\bm{X}\bm{\widehat \beta}- \bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S}\|^2 + \|\bm{y} - \bm{X}\bm{\widehat \beta}\|^2.
\label{eq:pythagorean-theorem}
\end{equation}
\end{theorem}
\begin{proof}
Consider the three points $\bm y, \bm X\bm{\widehat \beta}, \bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S} \in \mathbb R^n$. Since $\bm X\bm{\widehat \beta}$ and $\bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S}$ are both in $C(\bm X)$, it follows by the orthogonal projection property that $\bm y - \bm X\bm{\widehat \beta}$ is orthogonal to $\bm X\bm{\widehat \beta}- \bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S}$. In other words, these three points form a right triangle (Figure~\ref{fig:sum-of-squares}). The relationship~\eqref{eq:pythagorean-theorem} is then a consequence of the Pythagorean theorem.
\end{proof}

\begin{figure}[h!]
\centering
\includegraphics[width = 0.5\textwidth]{figures/sum-of-squares.jpeg}
\caption{Pythagorean theorem for regression on a subset of predictors.}
\label{fig:sum-of-squares}
\end{figure}

We will rely on this fundamental relationship throughout this course. One important special case is when $S = \{1, \dots, p-1\}$, i.e. the model without $S$ is the intercept-only model. In this case, $\bm X_{*, \text{-}S} = \bm 1_n$ and $\bm{\widehat \beta}_{\text{-}S} = \bar y$. Therefore, equation~\eqref{eq:pythagorean-theorem} implies the following.

\begin{proposition}
We have
\begin{equation}
\|\bm y -  \bar y \bm 1_n\|^2 = \|\bm X\bm{\widehat \beta}- \bar y \bm 1_n\|^2 + \|\bm y - \bm X\bm{\widehat \beta}\|^2.
\end{equation}
Equivalently, we can rewrite this equation as follows:
\begin{equation}
\textnormal{SST} \equiv \sum_{i = 1}^n (y_i - \bar y)^2 = \sum_{i = 1}^n (\widehat \mu_i - \bar y)^2 + \sum_{i = 1}^n (y_i - \widehat \mu_i)^2 \equiv \textnormal{SSR} + \textnormal{SSE}.
\label{eq:anova}
\end{equation}
\end{proposition}

\paragraph{$R^2$ and multiple correlation.} The ANOVA decomposition~\eqref{eq:anova} of the variation in $\bm y$ into that explained by the linear regression model (SSR) and that left over (SSE) leads naturally to the definition of $R^2$ as the fraction of variation in $\bm y$ explained by the linear regression model:
\begin{equation}
R^2 \equiv \frac{\text{SSR}}{\text{SST}} = \frac{\sum_{i = 1}^n (\widehat \mu_i - \bar y)^2}{\sum_{i = 1}^n (y_i - \bar y)^2} = \frac{\|\bm X\bm{\widehat \beta}- \bar y \bm 1_n\|^2}{\|\bm y -  \bar y \bm 1_n\|^2}.
\end{equation}
By the decomposition~\eqref{eq:anova}, we have $R^2 \in [0,1]$. The closer $R^2$ is to 1, the closely the data follow the fitted linear regression model. This intuition is formalized in the following result.
\begin{proposition} \label{prop:multiple-correlation}
$R^2$ is the squared sample correlation between $\bm X \bm{\widehat \beta}$ and $\bm y$.
\end{proposition}
\noindent For this reason, the positive square root of $R^2$ is called the \textit{multiple correlation coefficient}.
\begin{proof}
The first step is to observe that the mean of $\bm X \bm{\widehat \beta}$ is $\bar y$ (this follows from the normal equations). Therefore, the sample correlation between $\bm X \bm{\widehat \beta}$ and $\bm y$ is the inner product of the unit-normalized vectors $\bm X \bm{\widehat \beta} - \bar y \bm 1$ and $\bm y - \bar y \bm 1$, which is the cosine of the angle between them. From the geometry of Figure~\ref{fig:sum-of-squares}, we find that the cosine of the angle between $\bm X \bm{\widehat \beta} - \bar y \bm 1$ and $\bm y - \bar y \bm 1$ is $\|\bm X \bm{\widehat \beta} - \bar y \bm 1\|/\|\bm y - \bar y \bm 1\|$. Squaring this relation gives the desired conclusion.
\end{proof}

\paragraph{$R^2$ increases as predictors are added.}

The $R^2$ is an \textit{in-sample} measure, i.e. it uses the same data to fit the model and to assess the quality of the fit. Therefore, it is generally an optimistic measure of the (out-of-sample) prediction error. One manifestation of this is that the $R^2$ increases if any predictors are added to the model (even if these predictors are ``junk''). To see this, it suffices to show that SSE decreases as we add predictors. Without loss of generality, suppose that we start with a model all predictors but those in $S \subset \{0, 1, \dots, p-1\}$ and compare it to the model including all the predictors $\{0,1,\dots,p-1\}$. We can read off from the Pythagorean theorem~\eqref{eq:pythagorean-theorem} that
\begin{equation*}
\text{SSE}(\bm X_{*, \text{-}S}, \bm y) \equiv \|\bm y -  \bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S}\|^2 \geq  \|\bm y -  \bm X\bm{\widehat \beta}\|^2 \equiv \text{SSE}(\bm X, \bm y).
\end{equation*}
Adding many junk predictors will have the effect of degrading predictive performance but will nevertheless increase $R^2$.

\section{Special cases}

\subsection{The $C$-groups model}

\textit{See also Agresti 2.3.2-2.3.3}

\paragraph{ANOVA decomposition for $C$ groups model.} Let's consider the special case of the ANOVA decomposition~\eqref{eq:anova} when the model matrix $\bm X$ represents a single categorical predictor $w$. In this case, each observation $i$ is associated to one of the $C$ classes of $w$, which we denote $c(i) \in \{1, \dots, C\}$. Let's consider the $C$ groups of observations $\{i: c(i) = c\}$ for $c \in \{1, \dots, C\}$. For example, $w$ may be the type of a car (compact, midsize, minivan, etc.) and $y$ might be its fuel efficiency in miles per gallon.
<<out.width = "75%", fig.width = 5, fig.height = 4, fig.align='center', echo = FALSE, message = FALSE>>=
library(tidyverse)
mpg |>
  ggplot() +
  geom_boxplot(aes(x = fct_reorder(class, hwy), y = hwy)) +
  labs(x = "Car class", y = "Gas mileage (mpg)")
@

\noindent It is easy to check that the least squares fitted values $\widehat \mu_i$ are simply the means of the corresponding groups:
\begin{equation}
\widehat \mu_i = \bar y_{c(i)}, \quad \text{where}\ \bar y_{c(i)} \equiv \frac{\sum_{i: c(i) = c} y_i}{|\{i: c(i) = c\}|}.
\end{equation}
Therefore, we have
\begin{equation}
\text{SSR} = \sum_{i = 1}^n (\widehat \mu_i - \bar y)^2 = \sum_{i = 1}^n (\bar y_{c(i)} - \bar y)^2 \equiv \text{between-groups sum of squares (SSB)}
\end{equation}
and
\begin{equation}
\text{SSE} = \sum_{i = 1}^n (y_i - \widehat \mu_i)^2 = \sum_{i = 1}^n (y_i - \bar y_{c(i)})^2 \equiv \text{within-groups sum of squares (SSW)}.
\end{equation}
We therefore obtain the following corollary of the ANOVA decomposition~\eqref{eq:anova}:
\begin{equation}
\text{SST} = \text{SSB} + \text{SSW}.
\label{eq:anova-C-groups}
\end{equation}

\subsection{Simple linear regression}

\textit{See also Agresti 2.1.3} \\

Consider a linear regression model with an intercept and one quantitative predictor, $x$:
\begin{equation}
y = \beta_0 + \beta_1 x + \epsilon.
\label{eq:simple-regression}
\end{equation}
This is the simple linear regression model.

\paragraph{ANOVA decomposition for simple linear regression.}

Figure~\ref{fig:anova-simple-linear-regression} gives an interpretation of the ANOVA decomposition~\eqref{eq:anova} in the case of the simple linear regression model~\eqref{eq:simple-linear-regression}.
\begin{figure}[ht!]
\centering
\includegraphics[width = 0.4\textwidth]{figures/anova-simple-linear-regression.png}
\caption{ANOVA decomposition for simple linear regression.}
\label{fig:anova-simple-linear-regression}
\end{figure}

\paragraph{Connection between $R^2$ and correlation.}

There is a connection between $R^2$ and correlation in simple linear regression.

\begin{proposition}
Let $\rho_{xy}$ denote the sample correlation between $x$ and $y$, and let $R^2_{xy}$ be the $R^2$ from the simple linear regression~\eqref{eq:simple-regression}. Then, we have
\begin{equation}
R^2 = \rho_{xy}^2.
\end{equation}
\end{proposition}
\begin{proof}
This fact is a consequence of Proposition~\ref{prop:multiple-correlation}.
\end{proof}

\paragraph{Regression to the mean.}

Simple linear regression can be used to study the relationship between the same quantity across time (or generations). For example, let $x$ and $y$ be the height of a parent and child. This example motivated Sir Francis Galton to study linear regression in the first place. Alternatively, $x$ and $y$ can be a student's score on a standardized test in two consecutive years, or the number of games won by a given sports team in two consecutive seasons. In this situation, it is reasonable to assume that the sample standard deviations of $x$ and $y$ are the same (or to normalize these variables to achieve this). In this case, one can show that
\begin{equation}
\widehat \beta_0 = \bar y - \rho_{xy} \bar x \quad \text{and} \quad \widehat \beta_1 = \rho_{xy}.
\label{eq:coefficient-as-correlation}
\end{equation}
It follows that
\begin{equation*}
|\widehat \mu_i - \bar y| = |\widehat \beta_0 + \widehat \beta_1 x_i - \bar y| = |\rho_{xy}(x_i - \bar x)| = |\rho_{xy}| \cdot |x_i - \bar x|.
\end{equation*}
Since $|\rho_{xy}| < 1$ unless $\bm x$ and $\bm y$ are perfectly correlated (by the Cauchy-Schwarz inequality), this means that
\begin{equation}
|\widehat \mu_i - \bar y| < |x_i - \bar x| \quad \text{for each } i.
\label{eq:regression-to-the-mean}
\end{equation}
Therefore, we expect $y_i$ to be closer to its mean than $x_i$ is to its mean. This phenomenon is called \textit{regression to the mean} (and is in fact the origin of the term ``regression''). Many mistakenly attribute a causal mechanism to this phenomenon, when in reality it is simply a statistical artifact. For example, suppose $x_i$ is the number of games a sports team won last season and $y_i$ is the number of games it won this season. It is widely observed that teams with exceptional performance in a given season suffer a ``winner's curse'', performing worse in the next season. The reason for the winner's curse is simple: teams perform exceptionally well due to a combination of skill and luck. While skill stays roughly constant from year to year, the team which performed exceptionally well in a given season is unlikely to get as lucky as it did next season.

\section{Collinearity, adjustment, and partial correlation}

\textit{See also Agresti 2.2.4, 2.5.6, 2.5.7, 4.6.5} \\

An important part of linear regression analysis is the dependence of the least squares coefficient for a predictor on what other predictors are in the model. This relationship is dictated by the extent to which the given predictor is correlated with the other predictors. In this section, we'll use some additional notation. Let $S \subset \{0, \dots, p-1\}$ be a group of predictors (we can assume without loss of generality that $S = \{0, \dots, s-1\}$ for some $1 \leq s < p$). Then, denote $\text{-}S \equiv \{0, \dots, p-1\} \setminus S$. Let $\bm{\widehat \beta}_S$ denote the least squares coefficients when regressing $\bm y$ on $\bm X_{*S}$ and let $\bm{\widehat \beta}_{S|\text{-}S}$ denote the least squares coefficients corresponding to $S$ when regressing $\bm y$ on $\bm X = (\bm X_{*S}, \bm X_{*,\text{-}S})$.

\paragraph{Least squares estimates in the orthogonal case.}

The simplest case to analyze is when a groups of predictors $\bm X_{*S}$ is orthogonal to the rest of the predictors $\bm X_{*,\text{-}S}$ in the sense that
\begin{equation}
\bm X_{*S}^T \bm X_{*,\text{-}S} = \bm 0.
\end{equation}
In this case, we can derive the least squares coefficient vector $\bm{\widehat \beta} = (\bm{\widehat \beta}_{S|\text{-}S}, \bm{\widehat \beta}_{\text{-}S|S})$ from the normal equations:
\begin{equation}
\begin{split}
{\bm{\widehat \beta}_{S|\text{-}S} \choose \bm{\widehat \beta}_{\text{-}S|S}} &= (\bm X^T \bm X)^{-1}\bm X^T \bm y \\
&=
\begin{pmatrix}
\bm X_S^T \bm X_S & 0 \\
0 & \bm X_{\text{-}S}^T \bm X_{\text{-}S}
\end{pmatrix}^{-1}{\bm X_S^T \choose \bm X_{\text{-}S}^T}\bm y \\
&= {(\bm X_S^T \bm X_S)^{-1}\bm X_S^T\bm y \choose (\bm X_{\text{-}S}^T \bm X_{\text{-}S})^{-1}\bm X_{\text{-}S}^T\bm y} \\
&= {\bm{\widehat \beta}_{S} \choose \bm{\widehat \beta}_{\text{-}S}}.
\label{eq:orthogonality}
\end{split}
\end{equation}
Therefore, the least squares coefficients when regressing $\bm y$ on $(\bm X_S, \bm X_{\text{-}S})$ are the same as those obtained from regressing $\bm y$ separately on $\bm X_S$ and $\bm X_{\text{-}S}$, i.e.
\begin{equation}
\bm{\widehat \beta}_{S|\text{-}S} = \bm{\widehat \beta}_{S}.
\label{eq:orthogonality-consequence}
\end{equation}

\paragraph{Least squares estimates via orthogonalization.}

Let's now focus our attention on a single predictor $x_j$. If this predictor is orthogonal to the remaining predictors, then the result~\eqref{eq:orthogonality-consequence} states that $\widehat \beta_{j|\text{-}j}$ can be obtained from simply regressing $y$ on $x_j$. However, this is usually not the case. Usually, $\bm x_{*j}$ has a nonzero projection $\bm X_{*,\text{-}j}\bm{\widehat \gamma}$ onto $C(\bm X_{*,\text{-}j})$:
\begin{equation}
\bm x_{*j} = \bm X_{*,\text{-}j}\bm{\widehat \gamma} + \bm x^{\perp}_{*j},
\end{equation}
where $\bm x^{\perp}_{*j}$ is the residual from regressing $\bm x_{*j}$ onto $\bm X_{*,\text{-}j}$ and is therefore orthogonal to $C(\bm X_{*,\text{-}j})$. In other words, $\bm x^{\perp}_{*j}$ is the projection of $\bm x_{*j}$ onto the orthogonal complement of $C(\bm X_{*,\text{-}j})$.

With this decomposition, let us change basis from $(\bm x_{*j}, \bm X_{*,\text{-}j})$ to $(\bm x^{\perp}_{*j}, \bm X_{*,\text{-}j})$ by the process explored in Homework 1 Question 1. Let us write
\begin{equation*}
\begin{split}
\bm y = \bm x_{*j} \beta_{j|\text{-}j} + \bm X_{*,\text{-}j}\bm \beta_{\text{-}j|j} + \bm \epsilon \ &\Longleftrightarrow \ \bm y = (\bm X_{*,\text{-}j}\bm{\widehat \gamma} + \bm x^{\perp}_{*j})\beta_{j|\text{-}j} + \bm X_{*,\text{-}j}\bm \beta_{\text{-}j|j} + \bm \epsilon\\
\ &\Longleftrightarrow \ \bm y = \bm x^{\perp}_{*j}\beta_{j|\text{-}j} + \bm X_{*,\text{-}j}\bm \beta'_{\text{-}j|j} + \bm \epsilon.
\end{split}
\end{equation*}
What this means is that $\widehat \beta_{j|\text{-}j}$, the least squares coefficient of $\bm x_{*j}$ in the regression of $\bm y$ on $(\bm x_{*j}, \bm X_{*,\text{-}j})$ is also the least squares coefficient of $\bm x^{\perp}_{*j}$ in the regression of $\bm y$ on $(\bm x^{\perp}_{*j}, \bm X_{*,\text{-}j})$. However, since $\bm x^{\perp}_{*j}$ is orthogonal to $\bm X_{*,\text{-}j}$ by construction, we can use the result~\eqref{eq:orthogonality} to conclude that
\begin{equation*}
\widehat \beta_{j|\text{-}j} \text{ is the least squares coefficient of } \bm x^{\perp}_{*j} \text{ in the \textit{univariate} regression of } \bm y \text{ on } \bm x^{\perp}_{*j} \text{ (without intercept).}
\end{equation*}
We can solve this univariate regression explicitly to obtain
\begin{equation}
\widehat \beta_{j|\text{-}j} = \frac{(\bm x^{\perp}_{*j})^T \bm y}{\|\bm x^{\perp}_{*j}\|^2}.
\label{eq:orthogonal-univariate}
\end{equation}

\paragraph{Adjustment and partial correlation.}

Equivalently, letting $\bm{\widehat \beta}_{\text{-}j}$ be the least squares estimate in the regression of $\bm y$ on $\bm X_{*,\text{-}j}$ (note that this is \textit{not} the same as $\bm{\widehat \beta}_{\text{-}j|j}$), we can write
\begin{equation}
\widehat \beta_{j|\text{-}j} = \frac{(\bm x^{\perp}_{*j})^T(\bm y - \bm X_{*,\text{-}j}\bm{\widehat \beta_{\text{-}j}})}{\|\bm x^{\perp}_{*j}\|^2} = \frac{(\bm x_{*j} - \bm X_{*,\text{-}j}\bm{\widehat \gamma})^T(\bm y - \bm X_{*,\text{-}j}\bm{\widehat \beta_{\text{-}j}})}{\|\bm x_{*j} -\bm X_{*,\text{-}j}\bm{\widehat \gamma}\|^2}.
\end{equation}
We can interpret this result as follows: 
\begin{theorem}[Frisch, Waugh, Lovell]
The linear regression coefficient $\widehat \beta_{j|\text{-}j}$ results from first adjusting $\bm y$ and $\bm x_{*j}$ for the effects of all other variables, and then regressing the residuals from $\bm y$ onto the residuals from $\bm x_{*j}$. 
\end{theorem}
In this sense, \textit{the least squares coefficient for a predictor in a multiple linear regression reflects the effect of the predictor on the response after controlling for the effects of all other predictors.} A related quantity is the \textit{partial correlation} between $\bm x_{*j}$ and $\bm y$ after controlling for $\bm X_{*,\text{-}j}$, defined as the correlation between $\bm x_{*j} - \bm X_{*,\text{-}j}\bm{\widehat \gamma}$ and $\bm y - \bm X_{*,\text{-}j}\bm{\widehat \beta_{\text{-}j}}$. We can then connect the least squares coefficient $\widehat \beta_j$ to this partial correlation in a similar spirit to equation~\eqref{eq:coefficient-as-correlation}.

\paragraph{Effects of collinearity.}

Collinearity between a predictor $x_j$ and the other predictors tends to make the estimate $\widehat \beta_{j|\text{-}j}$ unstable. Intuitively, this makes sense because it becomes harder to distinguish between the effects of predictor $x_j$ and those of the other predictors on the response. To find the variance of $\widehat \beta_{j|\text{-}j}$ for a model matrix $\bm X$, we could in principle use the formula~\eqref{eq:var-of-beta-hat}. However, this formula involves the inverse of the matrix $\bm X^T \bm X$, which is hard to reason about. Instead, we can employ the formula~\eqref{eq:orthogonal-univariate} to calculate directly that
\begin{equation}
\text{Var}[\widehat \beta_{j|\text{-}j}] = \frac{\sigma^2}{\|\bm x_{*j}^\perp\|^2}.
\label{eq:conditional-variance}
\end{equation}
We see that the variance of $\widehat \beta_{j|\text{-}j}$ is inversely proportional to $\|\bm x_{*j}^\perp\|^2$. This means that the greater the collinearity, the less of $\bm x_{*j}$ is left over after adjusting for $\bm X_{*,\text{-}j}$, and the greater the variance of $\widehat \beta_{j|\text{-}j}$. To quantify the effect of this adjustment, suppose there were no other predictors other than the intercept term. Then, we would have
\begin{equation}
\text{Var}[\widehat \beta_j] = \frac{\sigma^2}{\|\bm x_{*j}-\bar x_j \bm 1_n\|^2}.
\end{equation}
Therefore, we can rewrite the variance~\eqref{eq:conditional-variance} as
\begin{equation}
\text{Var}[\widehat \beta_{j|\text{-}j}] = \frac{\|\bm x_{*j}-\bar x_j \bm 1_n\|^2}{\|\bm x_{*j}-\bm X_{*,\text{-}j}\bm{\widehat \gamma}\|^2} \cdot \text{Var}[\widehat \beta_j] = \frac{1}{1-R_j^2} \cdot \text{Var}[\widehat \beta_j] \equiv \text{VIF}_j \cdot \text{Var}[\widehat \beta_j],
\label{eq:vif}
\end{equation}
where $R_j^2$ is the $R^2$ value when regressing $\bm x_{*j}$ on $\bm X_{*,\text{-}j}$ and VIF stands for \textit{variance inflation factor}. The higher $R_j^2$, the more of the variance in $\bm x_{*j}$ is explained by other predictors, the higher the variance in $\widehat \beta_{j|\text{-}j}$.

\paragraph{Remark: Average treatment effect estimation in causal inference.}

Suppose we'd like to study the effect of an exposure or treatment (e.g. taking a blood pressure medication) on a response $y$ (e.g. blood pressure). In the Neyman-Rubin causal model, for a given individual $i$ we denote by $y_i(1)$ and $y_i(0)$ the outcomes that would have occurred had the individual received the treatment and the control, respectively. These are called \textit{potential outcomes}. Let $t_i \in \{0,1\}$ indicate whether the $i$th individual actually received treatment or control. Therefore, the observed outcome is $y_i^{\text{obs}} = y_i(t_i)$.\footnote{This requires the \textit{stable unit treatment value assumption} (SUTVA), which prevents issues like \textit{interference}, where the treatment of individual $i$ can be affected by the assigned treatments of other individuals.} Based on the data $\{(t_i, y_i)\}_{i = 1, \dots, n}$, the most basic goal is to estimate the
\begin{equation*}
\textit{average treatment effect} \  \tau \equiv \mathbb E[y(1) - y(0)],
\end{equation*}
where averaging is done over the population of individuals (often called \textit{units} in causal inference). Of course, we do not observe both $y(1)$ and $y(0)$ for any unit. Additionally, usually in observational studies we have \textit{confounding variables} $z_2, \dots, z_{p-1}$: variables that influence both the treatment assignment and the response (e.g. degree of health-seeking activity). It is important to control for these confounders in order to get an unbiased estimate of the treatment effect. Suppose the following linear model holds:
\begin{equation}
y(t) = \beta_0 + \beta_1 t + \beta_2 z_2 + \cdots + \beta_{p-1} z_{p-1} + \epsilon \quad \text{for } t \in \{0, 1\}, \quad \text{where} \ \epsilon \perp\!\!\!\!\perp t.
\end{equation}
This assumption implies that the treatment effect is constant, and the response is a linear function of the treatment and observed confounders, and there is no unmeasured confounding. Note that
\begin{equation}
\tau \equiv \mathbb E[y(1) - y(0)] = \beta_1.
\end{equation}
Furthermore,
\begin{equation}
y^{\text{obs}} = \beta_0 + \beta_1 t + \beta_2 z_2 + \cdots + \beta_{p-1} z_{p-1} + \epsilon \quad \text{for } t \in \{0, 1\}.
\end{equation}
In this case, the average treatment effect $\tau$ is \textit{identified} as the coefficient $\beta_1$ in the above regression, i.e. $\tau = \beta$. Therefore, the least squares estimate $\widehat \beta_1$ is an unbiased estimate of the average treatment effect. (Causal inference is beyond the scope of STAT 9610; see STAT 9210 instead.)

\section{R demo}

\textit{See also Agresti 2.6, Dunn and Smyth 2.6} \\

The R demo will be based on the \texttt{ScotsRaces} data from the textbook. Data description (quoted from the textbook):
\begin{quote}
``Each year the Scottish Hill Runners Association publishes a list of hill races in Scotland for the year. The table below shows data on the record time for some of the races (in minutes). Explanatory variables listed are the distance of the race (in miles) and the cumulative climb (in thousands of feet).''
\end{quote}

We will also familiarize ourselves with several important functions from the \verb|tidyverse| packages, including the \verb|ggplot2| package for data visualization and \verb|dplyr| package for data manipulation.
<<cache = FALSE, message = FALSE>>=
library(tidyverse) # for data import, manipulation, and plotting
library(GGally)    # for ggpairs() function
library(ggrepel)   # for geom_text_repel() function
library(car)       # for vif() function
library(conflicted)
conflicts_prefer(dplyr::filter)
@

<<message = FALSE>>=
# read the data into R
scots_races <- read_tsv("data/ScotsRaces.dat") # read_tsv from readr for data import
scots_races
@

\paragraph{Exploration.}

Before modeling our data, let's first explore it.

<<fig.width = 5, fig.height = 4, fig.align='center'>>=
# pairs plot

# Q: What are the typical ranges of the variables?
# Q: What are the relationships among the variables?

scots_races |>
  select(-race) |> # select() from dplyr for selecting columns
  ggpairs() # ggpairs() from GGally to create pairs plot

# mile time versus distance

# Q: How does mile time vary with distance?
# Q: What races deviate from this trend?
# Q: How does climb play into it?

# add mile time variable to scots_races
scots_races <- scots_races |>
  mutate(mile_time = time / distance) # mutate() from dplyr to add column
@

<<fig.width = 4, fig.height = 4, fig.align='center'>>=
# plot mile time versus distance
scots_races |>
  ggplot(aes(x = distance, y = mile_time)) +
  geom_point()
@

<<fig.width = 4.5, fig.height = 4, fig.align='center'>>=
# add climb information as point color
scots_races |>
  ggplot(aes(x = distance, y = mile_time, colour = climb)) +
  geom_point()
@

<<fig.width = 4.5, fig.height = 4, fig.align='center'>>=
# highlight extreme points
scots_races_extreme <- scots_races |>
  filter(distance > 15 | mile_time > 9) # filter() from dplyr to subset rows

# plot mile time versus distance
scots_races |>
  ggplot(aes(x = distance, y = mile_time, label = race, colour = climb)) +
  geom_point() +
  geom_text_repel(aes(label = race), data = scots_races_extreme)
@

<<fig.width = 5.5, fig.height = 4, fig.align='center'>>=
# clean up plot
scots_races |>
  ggplot(aes(x = distance, y = mile_time, label = race, color = climb)) +
  geom_point() +
  geom_text_repel(aes(label = race), data = scots_races_extreme) +
  labs(
    x = "Distance (miles)",
    y = "Mile Time (minutes per mile)",
    color = "Climb\n(thousands of ft)"
  )
@

\paragraph{Linear model coefficient interpretation.}

Let's fit some linear models and interpret the coefficients.

<<>>=
# Q: What is the effect of an extra mile of distance on time?

lm_fit <- lm(time ~ distance + climb, data = scots_races)
coef(lm_fit)
@

<<>>=
# Linear model with interaction

# Q: What is the effect of an extra mile of distance on time
#  for a run with low climb?

# Q: What is the effect of an extra mile of distance on time
#  for a run with high climb?

lm_fit_int <- lm(time ~ distance * climb, data = scots_races)
coef(lm_fit_int)

scots_races |>
  summarise(min_climb = min(climb), max_climb = max(climb))
@

Let's take a look at the regression summary for \verb|lm_fit|:
<<>>=
lm_fit <- lm(time ~ distance + climb, data = scots_races)
summary(lm_fit)
@

\noindent We get a coefficient of 6.35 with standard error 0.36 for \verb|distance|, where the standard error is an estimate of the quantity~\eqref{eq:conditional-variance}.

\paragraph{$R^2$ and sum-of-squared decompositions.}
\noindent We can extract the $R^2$ from this fit by reading it off from the bottom of the summary, or by typing
<<>>=
summary(lm_fit)$r.squared
@

\noindent We can construct sum-of-squares decompositions~\eqref{eq:pythagorean-theorem} using the \verb|anova| function. This function takes as arguments the partial model and the full model. For example, consider the partial model \verb|time ~ distance|.
<<>>=
lm_fit_partial <- lm(time ~ distance, data = scots_races)
anova(lm_fit_partial, lm_fit)
@
\noindent We find that adding the predictor \texttt{climb} reduces the RSS by 7106, from 9547 to 2441. As another example, we can compute the $R^2$ by comparing the full model with the null model:
<<>>=
lm_fit_null <- lm(time ~ 1, data = scots_races)
anova(lm_fit_null, lm_fit)
@
\noindent Therefore, the $R^2$ is 83899/86340 = 0.972, consistent with the above regression summary.

\paragraph{Adjustment and collinearity.} We can also test the adjustment formula~\eqref{eq:orthogonal-univariate} numerically. Let's consider the coefficient of \verb|distance| in the regression \verb|time ~ distance + climb|. We can obtain this coefficient by first regressing \verb|climb| out of \verb|distance| and \verb|time|:
<<>>=
lm_dist_on_climb <- lm(distance ~ climb, data = scots_races)
lm_time_on_climb <- lm(time ~ climb, data = scots_races)

scots_races_resid <- tibble(
  dist_residuals = residuals(lm_dist_on_climb),
  time_residuals = residuals(lm_time_on_climb)
)

lm_adjusted <- lm(time_residuals ~ dist_residuals - 1,
  data = scots_races_resid
)
summary(lm_adjusted)
@
\noindent We find a coefficient of 6.35 with standard error 0.35, which matches that obtained in the original regression.

\noindent We can get the partial correlation between \verb|distance| and \verb|time| by taking the empirical correlation between the residuals. We can compare this quantity to the usual correlation.
<<>>=
scots_races_resid |>
  summarise(cor(dist_residuals, time_residuals)) |>
  pull()

scots_races |>
  summarise(cor(distance, time)) |>
  pull()
@
\noindent In this case, the two correlation quantities are similar.

To obtain the variance inflation factors defined in equation~\eqref{eq:vif}, we can use the \verb|vif| function from the \verb|car| package:
<<>>=
vif(lm_fit)
@
\noindent Why are these two VIF values the same?

\chapter{Linear models: Inference}

We now understand the least squares estimator $\bm{\widehat \beta}$ from geometric and algebraic points of view. In Chapter 2, we will switch to a probabilistic perspective to derive inferential statements for linear models, in the form of hypothesis tests and confidence intervals. In order to facilitate this, we will assume that the error terms are normally distributed:
\begin{equation}
\bm y = \bm X \bm \beta + \bm \epsilon, \quad \text{where} \ \bm \epsilon \sim N(\bm 0, \sigma^2 \bm I_n).
\end{equation}

\section{Building blocks for linear model inference}

\textit{See also Agresti 3.1.1, 3.1.2, 3.1.4} \\

First we put in place some building blocks: The multivariate normal distribution (Section~\ref{sec:mvrnorm}), the distributions of linear regression estimates and residuals (Section~\ref{sec:lin-reg-dist}), and estimation of the noise variance $\sigma^2$ (Section~\ref{sec:noise-estimation}).

\subsection{The multivariate normal distribution} \label{sec:mvrnorm}

Recall that a random vector $\bm w \in \mathbb R^d$ has a multivariate normal distribution with mean $\bm \mu$ and covariate matrix $\bm \Sigma$ if it has probability density
\begin{equation*}
p(\bm w) = \frac{1}{\sqrt{(2\pi)^{d}\text{det}(\bm \Sigma)}}\exp\left(-\frac{1}{2}(\bm w - \bm \mu)^T\Sigma^{-1}(\bm w - \bm \mu)\right).
\end{equation*}
These random vectors have lots of special properties, including:
\begin{itemize}
\item (Linear transformation) If $\bm w \sim N(\bm \mu, \bm \Sigma)$, then $\bm A \bm w + \bm b \sim N(\bm A \bm \mu + \bm b, \bm A \bm \Sigma \bm A^T)$.
% \item If $\bm w_1 \sim N(\bm \mu_1, \bm \Sigma_1)$ and $\bm w_2 \sim N(\bm \mu_2, \bm \Sigma_2)$ are independent random vectors of the same dimension, then $\bm w_1 + \bm w_2 \sim N(\bm \mu_1 + \bm \mu_2, \bm \Sigma_1 + \bm \Sigma_2)$.
\item (Independence) If $\begin{pmatrix}\bm w_1 \\ \bm w_2 \end{pmatrix} \sim N\left(\begin{pmatrix}\bm \mu_1 \\ \bm \mu_2 \end{pmatrix} , \begin{pmatrix}\bm \Sigma_{11} & \bm \Sigma_{12} \\ \bm \Sigma_{12}^T & \Sigma_{22}\end{pmatrix}\right)$, then $\bm w_1 \perp\!\!\!\perp \bm w_2$ if and only if $\bm \Sigma_{12} = \bm 0$.
\end{itemize}

\noindent An important distribution related to the multivariate normal is the $\chi^2_d$ (chi-squared with $d$ degrees of freedom) distribution, defined as
\begin{equation*}
\chi^2_d \equiv \sum_{j = 1}^d w_j^2 \quad \text{ for } \quad w_1, \dots, w_d \overset{\text{i.i.d.}}\sim N(0, 1).
\end{equation*}

\subsection{The distributions of linear regression estimates and residuals} \label{sec:lin-reg-dist}

\textit{See also Dunn and Smyth 2.8.2} \\

The most important distributional result in linear regression is that
\begin{equation}
\bm{\widehat\beta} \sim N(\bm \beta, \sigma^2 (\bm X^T \bm X)^{-1}).
\label{eq:beta-hat-dist}
\end{equation}
Indeed, by the linear transformation property of the multivariate normal distribution,
\begin{equation*}
\begin{split}
\bm y \sim N(\bm X \bm \beta, \sigma^2 \bm I_n) \Longrightarrow \bm{\widehat \beta} = (\bm X^T \bm X)^{-1}\bm X^T \bm y &\sim N((\bm X^T \bm X)^{-1}\bm X^T \bm X \bm \beta, (\bm X^T \bm X)^{-1}\bm X^T \sigma^2 \bm I_n \bm X(\bm X^T \bm X)^{-1}) \\
&= N(\bm \beta, \sigma^2 (\bm X^T \bm X)^{-1}).
\end{split}
\end{equation*}
Next, let's consider the joint distribution of $\bm{\widehat \mu} = \bm X \bm{\widehat \beta}$ and $\bm{\widehat \epsilon} = \bm y - \bm X \bm{\widehat \beta}$. We have
\begin{equation}
\begin{split}
\begin{pmatrix} \bm{\widehat \mu} \\ \bm{\widehat \epsilon} \end{pmatrix} = \begin{pmatrix} \bm H \bm y \\ (\bm I - \bm H) \bm y \end{pmatrix} = \begin{pmatrix} \bm H \\ \bm I - \bm H \end{pmatrix}\bm y \sim N\left(\begin{pmatrix} \bm H \\ \bm I - \bm H \end{pmatrix}\bm X \bm \beta, \begin{pmatrix} \bm H \\ \bm I - \bm H \end{pmatrix}\cdot \sigma^2 \bm I \begin{pmatrix} \bm H & \bm I - \bm H \end{pmatrix}\right) \\
= N\left(\begin{pmatrix} \bm X \bm \beta \\ \bm 0 \end{pmatrix}, \begin{pmatrix} \sigma^2 \bm H & \bm 0 \\ \bm 0 & \sigma^2(\bm I - \bm H) \end{pmatrix} \right).
\end{split}
\end{equation}
In other words,
\begin{equation}
\bm{\widehat \mu} \sim N(\bm X \bm \beta, \sigma^2 \bm H) \quad \text{and} \quad \bm{\widehat \epsilon} \sim N(\bm 0, \sigma^2(\bm I - \bm H)), \quad \text{with} \quad \bm{\widehat \mu} \perp\!\!\!\perp \bm{\widehat \epsilon}.
\label{eq:fit-and-error-dist}
\end{equation}

The statistical independence between $\bm{\widehat \mu}$ and $\bm{\widehat \epsilon}$ is a result of the fact that these two quantities are projections of $\bm y$ onto two orthogonal subspaces: $C(\bm X)$ and $C(\bm X)^\perp$ (Figure~\ref{fig:orthogonality-fit-residuals})

\begin{figure}[ht!]
\centering
\includegraphics[width = 0.5\textwidth]{figures/orthogonality-fit-residuals.jpg}
\caption{The fitted vector $\bm{\widehat \mu}$ and the residual vector $\bm{\widehat \epsilon}$ are projections of $\bm y$ onto orthogonal subspaces.}
\label{fig:orthogonality-fit-residuals}
\end{figure}


Since $\bm{\widehat \beta}$ is a deterministic function of $\bm{\widehat \mu}$ (in particular, $\bm{\widehat \beta} = (\bm X^T \bm X)^{-1}\bm X^T \bm{\widehat \mu}$), it also follows that
\begin{equation}
\bm{\widehat \beta} \perp\!\!\!\perp \bm{\widehat \epsilon}.
\label{eq:beta-ind-eps}
\end{equation}

\subsection{Estimation of the noise variance $\sigma^2$} \label{sec:noise-estimation}

\textit{See also Dunn and Smyth 2.4.2, 2.5.3} \\

We can't quite do inference for $\bm \beta$ based on the distributional result~\eqref{eq:beta-hat-dist} because the noise variance $\sigma^2$ is unknown to us. Intuitively, since $\sigma^2 = \mathbb E[\epsilon_i^2]$, we can get an estimate of $\sigma^2$ by looking at the quantity $\|\bm{\widehat \epsilon}\|^2$. To get the distribution of this quantity, we need the following lemma:
\begin{lemma} \label{lem:normal-projection}
Let $\bm w \sim N(\bm 0, \bm P)$ for some projection matrix $\bm P$. Then, $\|\bm w\|^2 \sim \chi^2_d$, where $d = \textnormal{trace}(\bm P)$ is the dimension of the subspace onto which $\bm P$ projects.
\end{lemma}
\begin{proof}
Let $\bm P = \bm U \bm D \bm U^T$ be an eigenvalue decomposition of $\bm P$, where $\bm U$ is orthogonal and $\bm D$ is a diagonal matrix with $D_{ii} \in \{0,1\}$. We have $\bm w \overset d = \bm U \bm D \bm z$ for $\bm z \sim N(0, \bm I_n)$. Therefore,
\begin{equation*}
\|\bm w\|^2 = \|\bm D \bm z\|^2 = \sum_{i: D_{ii} = 1} z_i^2 \sim \chi^2_d, \quad \text{where } d = |\{i: D_{ii} = 1\}| = \text{trace}(D) = \text{trace}(\bm P).
\end{equation*}
\end{proof}
Recall that $\bm I - \bm H$ is a projection onto the $(n-p)$-dimensional space $C(\bm X)^\perp$, so by Lemma~\ref{lem:normal-projection} and equation~\eqref{eq:fit-and-error-dist} we have
\begin{equation}
\|\bm{\widehat \epsilon}\|^2 \sim \sigma^2 \chi^2_{n-p}.
\label{eq:eps-norm-dist}
\end{equation}
From this result, it follows that $\mathbb E[\|\bm{\widehat \epsilon}\|^2] = n-p$, so
\begin{equation}
\widehat \sigma^2 \equiv \frac{1}{n-p}\|\bm{\widehat \epsilon}\|^2
\label{eq:unbiased-noise-estimate}
\end{equation}
is an unbiased estimate for $\sigma^2$. Why does the denominator need to be $n-p$ rather than $n$ for the estimator above to be unbiased? The reason for this is that the residuals $\bm{\widehat \epsilon}$ are the projection of the true noise vector $\bm \epsilon \in \mathbb R^n$ onto the $(n-p)$-dimensional subspace $C(\bm X)^\perp$ (Figure~\ref{fig:residuals-as-noise-projection}). To see this, note that
\begin{equation}
\bm{\widehat \epsilon} = (\bm I - \bm H)\bm y = (\bm I - \bm H)(\bm X \bm \beta + \bm \epsilon) = (\bm I - \bm H)\bm \epsilon.
\end{equation}
Therefore, the norm of the residual vector will be smaller than that of the noise vector, especially to the extent that $p$ is close to $n$.
\begin{figure}[ht!]
\centering
\includegraphics[width = 0.5\textwidth]{figures/residuals-as-noise-projection.jpg}
\caption{The residual vector $\bm{\widehat \epsilon}$ is the projection of the noise vector $\bm \epsilon$ onto $C(\bm X)^\perp$.}
\label{fig:residuals-as-noise-projection}
\end{figure}


\section{Hypothesis testing}

\textit{See also Agresti 3.2.1, 3.2.2, 3.2.4, 3.2.8} \\

Typically two types of null hypotheses are tested in a regression setting: Those involving one-dimensional parameters and those involving multi-dimensional parameters. For example, consider the null hypotheses $H_0: \beta_j = 0$ and $H_0: \bm \beta_S = \bm 0$ for $S \subseteq \{0, 1, \dots, p-1\}$, respectively. We discuss tests of these two kinds of hypothesis in Sections~\ref{sec:one-dim-testing} and~\ref{sec:multi-dim-testing}, and then discuss the power of these tests in Section~\ref{sec:power}.

\subsection{Testing a one-dimensional parameter} \label{sec:one-dim-testing}

\textit{See also Dunn and Smyth 2.8.3} \\

\paragraph{$t$-test for a single coefficient.} The most common question to ask in a linear regression context is: Is the $j$th predictor associated with the response, when controlling for the other predictors? In the language of hypothesis testing, this corresponds to the null hypothesis
\begin{equation}
H_0: \beta_j = 0.
\label{eq:one-dim-null}
\end{equation}
According to~\eqref{eq:beta-hat-dist}, we have $\widehat \beta_j \sim N(0, \sigma^2/s_j^2)$, where, as we learned in Chapter 1,
\begin{equation}
s_j^{2} \equiv [(\bm X^T \bm X)^{-1}_{jj}]^{-1} = \|\bm x_{*j}^\perp\|^2  .
\end{equation}
Therefore,
\begin{equation}
\frac{\widehat \beta_j}{\sigma/s_j} \sim N(0,1),
\label{eq:oracle-z-stat}
\end{equation}
and we are tempted to define a level $\alpha$ test of the null hypothesis~\eqref{eq:one-dim-null} based on this normal distribution. While this is infeasible since we don't know $\sigma^2$, we can substitute in the unbiased estimate~\eqref{eq:unbiased-noise-estimate} derived in Section~\ref{sec:noise-estimation}. Then,
\begin{equation}
\text{SE}(\widehat \beta_j) \equiv \frac{\widehat \sigma}{s_j} \quad \text{is the standard error of } \widehat \beta_j,
\end{equation}
which is an approximation to the standard deviation of $\widehat \beta_j$. Dividing $\widehat \beta_j$ by its standard error gives us the $t$-statistic
\begin{equation}
t_j \equiv \frac{\widehat \beta_j}{\text{SE}(\widehat \beta_j)} = \frac{\widehat \beta_j}{\sqrt{\frac{1}{n-p}\|\bm{\widehat \epsilon}\|^2}/s_j}.
\end{equation}
This statistic is \textit{pivotal}, in the sense that it has the same distribution for any $\bm \beta$ such that $\beta_j = 0$. Indeed, we can rewrite it as
\begin{equation}
t_j = \frac{\frac{\widehat \beta}{\sigma/s_j}}{\sqrt{\frac{\sigma^{-2}\|\bm{\widehat \epsilon}\|^2}{n-p}}}.
\end{equation}
Recalling the independence of $\bm{\widehat \beta}$ and $\bm{\widehat \epsilon}$~\eqref{eq:beta-ind-eps}, the scaled chi square distribution of $\|\bm{\widehat \epsilon}\|^2$~\eqref{eq:eps-norm-dist}, the standard normal distribution of $\frac{\widehat \beta}{\sigma/s_j}$~\eqref{eq:oracle-z-stat}, we find that
\begin{equation}
\text{under } H_0:\beta_j = 0, \quad t_j \sim \frac{N(0,1)}{\sqrt{\frac{1}{n-p}\chi^2_{n-p}}}, \quad \text{with numerator and denominator independent.}
\end{equation}
The latter distribution is called the \textit{$t$ distribution with $n-p$ degrees of freedom} and denoted $t_{n-p}$. This paves the way for the two-sided $t$-test:
\begin{equation}
\phi_t(\bm X, \bm y) = \mathbbm 1(|t_j| >t_{n-p}(1-\alpha/2)),
\end{equation}
where $t_{n-p}(1-\alpha/2)$ denotes the $1-\alpha/2$ quantile of $t_{n-p}$. Note that, by the law of large numbers,
\begin{equation}
\frac{1}{n-p}\chi^2_{n-p} \overset{P}\rightarrow 1 \quad \text{as} \quad n - p \rightarrow \infty,
\end{equation}
so for large $n-p$ we have $t_{j} \sim t_{n-p} \approx N(0,1)$. Hence, the $t$-test is approximately equal to the following $z$-test:
\begin{equation}
\phi_t(\bm X, \bm y) \approx \phi_z(\bm X, \bm y) \equiv  \mathbbm 1(|t_j| >z(1-\alpha/2)),
\end{equation}
where $z(1-\alpha/2)$ is the $1-\alpha/2$ quantile of $N(0,1)$. The $t$-test can also be defined in a one-sided fashion, if power against one-sided alternatives is desired.

\paragraph{Example: One-sample model.}

Consider the intercept-only linear regression model $y = \beta_0 + \epsilon$, and let's apply the $t$-test derived above to test the null hypothesis $H_0: \beta_0 = 0$. We have $\widehat \beta_0 = \bar y$. Furthermore, we have
\begin{equation}
\text{SE}^2(\widehat \beta_0) = \frac{\widehat \sigma^2}{n}, \quad \text{where} \quad \widehat \sigma^2 = \frac{1}{n-1}\|\bm y - \bar y \bm 1_n\|^2.
\end{equation}
Hence, we obtain the $t$ statistic
\begin{equation}
t = \frac{\widehat \beta_0}{\text{SE}(\widehat \beta_0)} = \frac{\sqrt n \bar y }{\sqrt{\frac{1}{n-1}\|\bm y - \bar y \bm 1_n\|^2}}.
\end{equation}
According to the theory above, this test statistic has a null distribution of $t_{n-1}$.

\paragraph{Example: Two-sample model.}

Suppose we have $x_1 \in \{0,1\}$, in which case the linear regression $y = \beta_0 + \beta_1 x_1 + \epsilon$ becomes a two-sample model. We can rewrite this model as
\begin{equation}
y_i \sim \begin{cases}N(\beta_0, \sigma^2) \quad &\text{for } x_i = 0; \\ N(\beta_0 + \beta_1, \sigma^2) \quad &\text{for } x_i = 1.\end{cases}
\end{equation}
It is often of interest to test the null hypothesis $H_0: \beta_1 = 0$, i.e. that the two groups have equal means. Let's define
\begin{equation}
\bar y_0 \equiv \frac{1}{n_0}\sum_{i: x_i = 0} y_i, \quad \bar y_1 \equiv \frac{1}{n_1}\sum_{i: x_i = 1} y_i, \quad \text{where} \quad n_0 = |\{i: x_i = 0\}| \text{ and } n_1 = |\{i: x_i = 1\}|.
\end{equation}
Then, we have seen before that $\widehat \beta_0 = \bar y_0$ and $\widehat \beta_1 = \bar y_1 - \bar y_0$. We can compute that
\begin{equation}
s_1^2 \equiv \|\bm x_{*1}^{\perp}\|^2 = \|\bm x_{*1} - \frac{n_1}{n}\bm 1\|^2 = n_1\frac{n^2_0}{n^2} + n_0\frac{n_1^2}{n^2} = \frac{n_0 n_1}{n} = \frac{1}{\frac1{n_0} + \frac1{n_1}}
\end{equation}
and
\begin{equation}
\widehat \sigma^2 = \frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar y_0)^2 + \sum_{i: x_i = 1}(y_i - \bar y_1)^2\right).
\end{equation}
Therefore, we arrive at a $t$-statistic of
\begin{equation}
t = \frac{\sqrt{\frac{1}{\frac1{n_0} + \frac1{n_1}}}(\bar y_1 - \bar y_0)}{\sqrt{\frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar y_0)^2 + \sum_{i: x_i = 1}(y_i - \bar y_1)^2\right)}}.
\end{equation}
Under the null hypothesis, this statistic has a distribution of $t_{n-2}$.

\paragraph{$t$-test for a contrast among coefficients.}

Given a vector $\bm c \in \mathbb R^{p}$, the quantity $\bm c^T \bm \beta$ is sometimes called a \textit{contrast}. For example, suppose $\bm c = (1,-1, 0, \dots, 0)$. Then, $\bm c^T \bm \beta = \beta_1 - \beta_2$ is the difference in effects of the first and second predictors. We are sometimes interested in testing whether such a contrast is equal to zero, i.e. $H_0: \bm c^T \bm \beta = 0$. While this hypothesis can involve two or more of the predictors, the parameter $\bm c^T \bm \beta$ is still one-dimensional and therefore we can still apply a $t$-test. Going back to the distribution $\bm{\widehat \beta} \sim N(\bm \beta, \sigma^2(\bm X^T \bm X)^{-1})$, we find that
\begin{equation*}
\bm c^T\bm{\widehat \beta} \sim N(\bm c^T\bm \beta, \sigma^2\bm c^T (\bm X^T \bm X)^{-1} \bm c).
\end{equation*}
Therefore, under the null hypothesis that $\bm c^T \bm \beta = 0$, we can derive that
\begin{equation}
\frac{\bm c^T \bm{\widehat \beta}}{\widehat \sigma \sqrt{\bm c^T (\bm X^T \bm X)^{-1} \bm c}} \sim t_{n-p},
\label{eq:contrasts-t-dist}
\end{equation}
giving us another $t$-test. Note that the $t$-tests described above can be recovered from this more general formulation by setting $\bm c = \bm e_j$, the indicator vector with $j$th coordinate equal to 1 and all others equal to zero.

\subsection{Testing a multi-dimensional parameter} \label{sec:multi-dim-testing}

\textit{See also Dunn and Smyth 2.10.1} \\

\paragraph{$F$-test for a group of coefficients.} Now we move on to the case of testing a multi-dimensional parameter: $H_0: \bm \beta_S = \bm 0$ for some $S \subseteq \{0, 1, \dots, p-1\}$. In other words, we would like to test
\begin{equation}
H_0: \bm y = \bm X_{*, \text{-}S}\bm \beta_{-S} + \bm \epsilon \quad \text{versus} \quad H_1: \bm X \bm \beta + \bm \epsilon.
\end{equation}
To test this hypothesis, let us fit least squares coefficients $\bm{\widehat \beta}_{-S}$ and $\bm{\widehat \beta}$ for the partial model as well as the full model. If the partial model fits well, then the residuals $\bm y - \bm X_{*, \text{-}S}\bm{\widehat \beta_{-S}}$ from this model will not be much larger than the residuals $\bm y - \bm X\bm{\widehat \beta}$ from the full model. To quantify this intuition, let us recall our analysis of variance decomposition from Chapter 1:
\begin{equation}
\|\bm y - \bm X_{*, \text{-}S}\bm{\widehat \beta_{-S}}\|^2 = \|\bm X\bm{\widehat \beta} - \bm X_{*, \text{-}S}\bm{\widehat \beta_{-S}}\|^2 + \|\bm y - \bm X\bm{\widehat \beta}\|^2.
\end{equation}
Let's consider the ratio
\begin{equation}
\frac{\|\bm y - \bm X_{*, \text{-}S}\bm{\widehat \beta_{-S}}\|^2 - \|\bm y - \bm X\bm{\widehat \beta}\|^2}{\|\bm y - \bm X\bm{\widehat \beta}\|^2} = \frac{\|\bm X\bm{\widehat \beta} - \bm X_{*, \text{-}S}\bm{\widehat \beta}_{-S}\|^2}{\|\bm y - \bm X\bm{\widehat \beta}\|^2},
\end{equation}
which is the relative increase in the residual sum of squares when going from the full model to the partial model. Let us rewrite this ratio in terms of projection matrices. Let $\bm H$ be the projection matrix for the full model, and let $\bm H_{\text{-}S}$ be the projection matrix for the partial model. Note that $\bm H - \bm H_{\text{-}S}$ is the projection matrix onto the $|S|$-dimensional space $C(\bm X) \cap C(\bm X_{\text{-}S})^\perp$ (Figure~\ref{fig:f-test-geometry}).
\begin{figure}[ht!]
\centering
\includegraphics[width = 0.75\textwidth]{figures/F-test-geometry.png}
\caption{Geometry of the $F$-test. Orthogonality relationships stem from $C(\bm X_{*,\text{-}S}) \perp C(\bm X) \cap C(\bm X_{*, \text{-}S})^\perp \perp C(\bm X)^\perp$.}
\label{fig:f-test-geometry}
\end{figure}

We have
\begin{equation}
\frac{\|\bm X\bm{\widehat \beta} - \bm X_{*, \text{-}S}\bm{\widehat \beta}_{\text{-}S}\|^2}{\|\bm y - \bm X\bm{\widehat \beta}\|^2} = \frac{\|(\bm H - \bm H_{\text{-}S})\bm y\|^2}{\|(\bm I - \bm H)\bm y\|^2},
\end{equation}
so the numerator and denominator are the squared norms of the projections of $y$ onto $C(\bm X) \cap C(\bm X_{*, \text{-}S})^\perp$ and $C(\bm X)^\perp$, respectively (Figure~\ref{fig:f-test-geometry}). Under the null hypothesis, we have $\bm y = \bm X_{*, \text{-}S}\bm \beta_{\text{-S}} + \bm \epsilon$, and
\begin{equation}
(\bm H - \bm H_{\text{-}S}) \bm X_{*,\text{-}S}\bm \beta_{\text{-S}} = (\bm I - \bm H)\bm X_{*,\text{-}S}\bm \beta_{\text{-S}} = 0
\end{equation}
because $\bm X_{*, \text{-}S}\bm \beta_{\text{-S}} \in C(\bm X_{*, \text{-}S}) \perp C(\bm X) \cap C(\bm X_{*, \text{-}S})^T \perp C(\bm X)^\perp$. It follows that
\begin{equation}
\frac{\|(\bm H - \bm H_{\text{-}S})\bm y\|^2}{\|(\bm I - \bm H)\bm y\|^2} = \frac{\|(\bm H - \bm H_{\text{-}S})\bm \epsilon\|^2}{\|(\bm I - \bm H)\bm \epsilon\|^2}.
\end{equation}
Since the projection matrices in the numerator and denominator project onto orthogonal subspaces, we have $(\bm H - \bm H_{\text{-}S})\bm \epsilon \perp\!\!\!\perp (\bm I - \bm H)\bm \epsilon$, with $\|(\bm H - \bm H_{\text{-}S})\bm \epsilon\|^2 \sim \sigma^2 \chi^2_{|S|}$ and $\|(\bm I - \bm H)\bm \epsilon\|^2 \sim \sigma^2\chi^2_{n-p}$. Renormalizing numerator and denominator to have expectation 1 under the null, we arrive at the $F$-statistic
\begin{equation}
F \equiv \frac{(\|\bm y - \bm X_{*, \text{-}S}\bm{\widehat \beta_{-S}}\|^2 - \|\bm y - \bm X\bm{\widehat \beta}\|^2)/|S|}{\|\bm y - \bm X\bm{\widehat \beta}\|^2/(n-p)}.
\end{equation}
We have derived that under the null hypothesis,
\begin{equation}
F \sim \frac{\chi^2_{|S|}/|S|}{\chi^2_{n-p}/(n-p)}, \quad \text{with numerator and denominator independent.}
\end{equation}
This distribution is called the $F$-distribution with $|S|$ and $n-p$ degrees of freedom, and denoted $F_{|S|, n-p}$. Denoting by $F_{|S|, n-p}(1-\alpha)$ the $1-\alpha$ quantile of this distribution, we arrive at the $F$-test
\begin{equation}
\phi_F(\bm X, \bm y) \equiv \mathbbm 1(F > F_{|S|, n-p}(1-\alpha)).
\end{equation}
Note that the $F$-test searches for deviations of $\bm \beta_{S}$ in all directions, and does not have one-sided variants like the $t$-test.
\paragraph{Example: Testing for any significant coefficients except the intercept.}

Suppose $\bm x_{*,0} = \bm 1_n$ is an intercept term. Then, consider the null hypothesis $H_0: \beta_1 = \cdots = \beta_{p-1} = 0$. In other words, the null hypothesis is the intercept-only model and the alternative hypothesis is the regression model with an intercept and $p-1$ additional predictors. In this case, $S = \{1, \dots, p-1\}$ and -$S = \{0\}$. The corresponding $F$ statistic is
\begin{equation}
F \equiv \frac{(\|\bm y - \bar y \bm 1\|^2 - \|\bm y - \bm X\bm{\widehat \beta}\|^2)/(p-1)}{\|\bm y - \bm X\bm{\widehat \beta}\|^2/(n-p)},
\end{equation}
with null distribution $F_{p-1, n-p}$.

\paragraph{Example: Testing for equality of group means in $C$-groups model.}

As a further special case, consider the $C$-groups model from Chapter 1. Recall the ANOVA decomposition
\begin{equation}
\sum_{i = 1}^n (y_i - \bar y)^2 = \sum_{i = 1}^n (\bar y_{c(i)} - \bar y)^2 + \sum_{i = 1}^n (y_i - \bar y_{c(i)})^2 = \text{SSB} + \text{SSW}.
\end{equation}
The $F$-statistic in this case becomes
\begin{equation}
F = \frac{\sum_{i = 1}^n (\bar y_{c(i)} - \bar y)^2/(C-1)}{\sum_{i = 1}^n (y_i - \bar y_{c(i)})^2/(n-C)} = \frac{\text{SSB}/(C-1)}{\text{SSW}/(n-C)},
\end{equation}
with null distribution $F_{C-1,n-C}$.

\section{Power} \label{sec:power}

\textit{See also Agresti 3.2.5} \\

So far we've been focused on finding the null distributions of various test statistics in order to construct tests with Type-I error control. Now let's shift our attention to examining the power of these tests.

\paragraph{The power of a $t$-test.}

Consider the $t$-test of the null hypothesis $H_0: \beta_j = 0$. Suppose that, in reality, $\beta_j \neq 0$. What is the probability the $t$-test will reject the null hypothesis? To answer this question, recall that $\widehat \beta_j \sim N(\beta_j, \sigma^2/s_j^2)$. Therefore,
\begin{equation}
t = \frac{\widehat \beta_j}{\text{SE}(\widehat \beta_j)} = \frac{\beta_j}{\text{SE}(\widehat \beta_j)} + \frac{\widehat \beta_j - \beta_j}{\text{SE}(\widehat \beta_j)} \overset \cdot \sim N\left(\frac{\beta_j s_j}{\sigma}, 1\right).
\label{eq:t-alt-dist-1}
\end{equation}
Here we have made the approximation $\text{SE}(\widehat \beta_j) \approx \frac{\sigma}{s_j}$, which is pretty good when $n-p$ is large. Therefore, the power of the two-sided $t$-test is
\begin{equation}
\mathbb E[\phi_t] = \mathbb P[\phi_t = 1] \approx \mathbb P[|t| > z_{1-\alpha/2}] \approx \mathbb P\left[\left|N\left(\frac{\beta_j s_j}{\sigma}, 1\right)\right| > z_{1-\alpha/2}\right].
\end{equation}
Therefore, the quantity $\frac{\beta_j s_j}{\sigma}$ determines the power of the $t$-test. To understand $s_j$ a little better, let's assume that the rows $\bm x_{i*}$ of the model matrix are drawn i.i.d. from some distribution $(x_0, \dots, x_{p-1})$. Then we have roughly
\begin{equation}
\bm x_{*j}^\perp \approx \bm x_{*j} - \mathbb E[\bm x_{*j}|\bm X_{*, \text{-}j}],
\end{equation}
so $x_{ij}^\perp \approx x_{ij} - \mathbb E[x_{ij}|\bm x_{i,\text{-}j}]$. Hence,
\begin{equation}
s_j^2 \equiv \|\bm x_{*j}^\perp\|^2 \approx n\mathbb E[(x_j-\mathbb E[x_j|\bm x_{\text{-}j}])^2] = n\mathbb E[\text{Var}[x_j|\bm x_{\text{-}j}]].
\end{equation}
Hence, we can rewrite the alternative distribution~\eqref{eq:t-alt-dist-1} as
\begin{equation}
t \overset \cdot \sim N\left(\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb E[\text{Var}[x_j|\bm x_{\text{-}j}]]}}{\sigma}, 1\right).
\label{eq:t-alt-dist-2}
\end{equation}
We can see clearly now how the power of the $t$-test varies with the effect size $\beta_j$, the sample size $n$, the degree of collinearity $\mathbb E[\text{Var}[x_j|\bm x_{\text{-}j}]]$, and the noise standard deviation $\sigma$.

\paragraph{The power of an $F$-test.}

Now let's turn our attention to computing the power of the $F$-test. We have
\begin{equation}
F = \frac{\|\bm X\bm{\widehat \beta} - \bm X_{*, \text{-}S}\bm{\widehat \beta}_{-S}\|^2/|S|}{\|\bm y - \bm X\bm{\widehat \beta}\|^2/|n-p|} = \frac{\|(\bm H-\bm H_{\text{-}S}) \bm y\|^2/|S|}{\|(\bm I - \bm H)\bm y\|^2/|n-p|} \approx \frac{\|(\bm H-\bm H_{\text{-}S}) \bm y\|^2/|S|}{\sigma^2}.
\end{equation}
To calculate the distribution of the numerator, we need to introduce the notion of a non-central chi-squared random variable.
\begin{definition}
For some vector $\bm \mu \in \mathbb R^d$, suppose $\bm z \sim N(\bm \mu, \bm I_d)$. Then, we define the distribution of $\|\bm z\|^2$ as the noncentral chi-square random variable with $d$ degrees of freedom and noncentrality parameter $\|\bm \mu\|^2$ and denote this distribution by $\chi^2_d(\|\bm \mu\|^2)$.
\end{definition}

\noindent The following proposition states two useful facts about noncentral chi-square distributions.
\begin{proposition} \label{prop:noncentral-chi-square}
The following two relations hold:
\begin{enumerate}
\item The mean of a $\chi^2_d(\|\bm \mu\|^2)$ random variable is $d + \|\bm \mu\|^2$.
\item If $\bm P$ is a projection matrix and $\bm y = \bm \mu + \bm \epsilon$, then $\frac{1}{\sigma^2}\|\bm P \bm y\|^2 \sim \chi^2_{\text{tr}(\bm P)}(\frac{1}{\sigma^2}\|\bm P \bm \mu\|^2)$.
\end{enumerate}
\end{proposition}
\noindent It therefore follows that
\begin{equation}
F \approx \frac{\|(\bm H-\bm H_{\text{-}S}) \bm y\|^2/|S|}{\sigma^2} \sim \frac{1}{|S|}\chi^2_{|S|}(\|(\bm H-\bm H_{\text{-}S})\bm X \bm \beta\|^2) = \frac{1}{|S|}\chi^2_{|S|}\left(\frac{1}{\sigma^2}\|\bm X^\perp_{*, S}\bm \beta_S\|^2\right).
\end{equation}
Assuming as before that the rows of $\bm X$ are samples from a joint distribution, we can write
\begin{equation}
\|\bm X^\perp_{*, S}\bm \beta_S\|^2 \approx n\bm \beta_S^T \mathbb E[\text{Var}[\bm x_{S}|\bm x_{\text{-}S}]] \bm \beta_S.
\end{equation}
Therefore,
\begin{equation}
F \overset \cdot \sim \frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb E[\text{Var}[\bm x_{S}|\bm x_{\text{-}S}]] \bm \beta_S}{\sigma^2}\right),
\end{equation}
which is similar in spirit to equation~\eqref{eq:t-alt-dist-2}. To get a better sense for what this relationship implies for the power of the $F$-test, we find from the first part of Proposition~\ref{prop:noncentral-chi-square} that, under the alternative,
\begin{equation*}
\mathbb E[F] \approx \mathbb E\left[\frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb E[\text{Var}[\bm x_{S}|\bm x_{\text{-}S}]] \bm \beta_S}{\sigma^2}\right)\right] = 1 + \frac{n\beta_S^T \mathbb E[\text{Var}[\bm x_{S}|\bm x_{\text{-}S}]] \bm \beta_S}{|S| \cdot \sigma^2}.
\end{equation*}
By contrast, under the null, the mean of the $F$-statistic is 1. The $|S|$ term in the denominator above suggests that testing larger sets of variables explaining the same amount of variation in $\bm y$ will hurt power. The test must accommodate for the fact that larger sets of variables will explain more of the variability in $y$ even under the null hypothesis.

\paragraph{Power of the $t$-test when predictors are added to the model.}

As we know, the outcome of a regression is a function of the predictors that are used. What happens to the $t$-test $p$-value for $H_0: \beta_j = 0$ when a predictor is added to the model? To keep things simple, let's consider the
\begin{equation}
\text{true underlying model:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
\end{equation}
Let's consider the power of testing $H_0: \beta_0 = 0$ in the regression models
\begin{equation}
\text{model 0:}\ y = \beta_0 x_0 + \epsilon \quad \text{versus} \quad \text{model 1:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
\end{equation}

\noindent There are four cases based on $\text{cor}[\bm x_{*0}, \bm x_{*1}]$ and the value of $\beta_1$ in the true model:
\begin{enumerate}
\item $\text{cor}[\bm x_{*0}, \bm x_{*1}] \neq 0$ and $\beta_1 \neq 0$. In this case, in model 0 we have omitted an important variable that is correlated with $\bm x_{*0}$. Therefore, the meaning of $\beta_0$ differs between model 0 and model 1, so it may not be meaningful to compare the $p$-values arising from these two models.
\item $\text{cor}[\bm x_{*0}, \bm x_{*1}] \neq 0$ and $\beta_1 = 0$. In this case, we are adding a null predictor that is correlated with $x_{*0}$. Recall that the power of the $t$-test hinges on the quantity $\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb E[\text{Var}[x_j|\bm x_{\text{-}j}]]}}{\sigma}$. Adding the predictor $x_1$ has the effect of reducing the conditional predictor variance $\mathbb E[\text{Var}[x_j|\bm x_{\text{-}j}]]$, therefore reducing the power. This is a case of \textit{predictor competition}.
\item $\text{cor}[\bm x_{*0}, \bm x_{*1}] = 0$ and $\beta_1 \neq 0$. In this case, we are adding a non-null predictor that is orthogonal to $\bm x_{*0}$. While the conditional predictor variance $\mathbb E[\text{Var}[x_j|\bm x_{\text{-}j}]]$ remains the same due to orthogonality, the residual variance $\sigma^2$ is reduced when going from model 0 to model 1.\footnote{If $\beta_1$ is small enough, then the unbiased estimate of the residual variance may actually increase due to a reduction in the residual degrees of freedom in the denominator.} Therefore, in this case adding $x_1$ to the model increases the power for testing $H_0: \beta_0 = 0$. This is a case of \textit{predictor collaboration}.
\item $\text{cor}[\bm x_{*0}, \bm x_{*1}] = 0$ and $\beta_1 = 0$. In this case, we are adding an orthogonal null variable, which does not change the conditional predictor variance or the residual variance, and therefore keeps the power of the test the same.
\end{enumerate}
In conclusion, adding a predictor can either increase or decrease the power of a $t$-test. Similar reasoning can be applied to the $F$-test.

\paragraph{Remark: Adjusting for covariates in randomized experiments.} Case 3 above, i.e. $\text{cor}[\bm x_{*0}, \bm x_{*1}] = 0$ and $\beta_1 \neq 0$, arises in the context of randomized experiments in causal inference. In this case, $y$ represents the outcome, $x_0$ represents the treatment, and $x_1$ represents a covariate. Because the treatment is randomized, there is no correlation between $x_0$ and $x_1$. Therefore, it is not necessary to adjust for $x_1$ in order to get an unbiased estimate of the average treatment effect. However, it is known that adjusting for covariates can lead to more \text{precise} estimates of the treatment effect due to the phenomenon discussed in case 3 above. This point is also related to the discussion in Chapter 1 about the fact that if $x_0$ and $x_1$ are orthogonal, then the least squares coefficient $\widehat \beta_0$ is the same regardless of whether $x_1$ is included in the model. As we see here, either including $x_1$ in the model or adjusting $y$ for $x_1$ is necessary to get better power. 

\section{Confidence and prediction intervals}

\textit{See also Agresti 3.3, Dunn and Smyth 2.8.4-2.8.5} \\

In addition to hypothesis testing, we often want to construct confidence intervals for the coefficients.

\paragraph{Confidence interval for a coefficient.}

Under $H_0: \beta_j = 0$, we showed that $\frac{\widehat \beta_j}{\widehat \sigma/s_j} \sim t_{n-p}$. The same argument shows that for arbitrary $\beta_j$, we have
\begin{equation}
\frac{\widehat \beta_j - \beta_j}{\widehat \sigma/s_j} \sim t_{n-p}.
\end{equation}
We can use this relationship to construct a confidence interval for $\beta_j$ as follows:
\begin{equation}
\begin{split}
1-\alpha = \mathbb P[|t_{n-p}| \leq t_{n-p}(1-\alpha/2)] &= \mathbb P\left[\left|\frac{\widehat \beta_j - \beta_j}{\widehat \sigma/s_j}\right| \leq t_{n-p}(1-\alpha/2) \right] \\
&= \mathbb P\left[\beta_j \in \left[\widehat \beta_j - \frac{\widehat \sigma}{s_j}t_{n-p}(1-\alpha/2), \widehat \beta_j + \frac{\widehat \sigma}{s_j}t_{n-p}(1-\alpha/2) \right]\right] \\
&\equiv \mathbb P\left[\beta_j \in \left[\widehat \beta_j - \text{SE}(\widehat \beta_j)t_{n-p}(1-\alpha/2), \widehat \beta_j + \text{SE}(\widehat \beta_j)t_{n-p}(1-\alpha/2) \right]\right] \\
&\equiv \mathbb P[\beta_j \in \text{CI}(\beta_j)].
\label{eq:pointwise-interval-beta}
\end{split}
\end{equation}
The confidence interval $\text{CI}(\beta_j)$ defined above therefore has $1-\alpha$ coverage. Because of the duality between confidence intervals and hypothesis tests, the factors contributing to powerful tests (Section~\ref{sec:power}) also lead to shorter confidence intervals. 

\paragraph{Confidence interval for $\mathbb E[y|\bm x_0]$.}

Suppose now that we have a new predictor vector $\bm x_0 \in \mathbb R^p$. The mean of the response for this predictor vector is $\mathbb E[y|\bm x_0] = \bm x_0^T \bm \beta$. Plugging in $\bm x_0$ for $\bm c$ in the relation~\eqref{eq:contrasts-t-dist}, we obtain
\begin{equation*}
\frac{\bm x_0^T \bm{\widehat \beta} - \bm x_0^T \bm \beta}{\widehat \sigma \sqrt{\bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0}} \sim t_{n-p}.
\end{equation*}
From this we can derive that
\begin{equation}
\text{CI}(\bm x_0^T \bm{\beta}) \equiv \bm x_0^T \bm{\widehat \beta} \pm \text{SE}(\bm x_0^T \bm{\widehat \beta}) \cdot t_{n-p}(1-\alpha/2) \equiv \bm x_0^T \bm{\widehat \beta} \pm \widehat \sigma \sqrt{\bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0} \cdot t_{n-p}(1-\alpha/2)
\end{equation}
is a $1-\alpha$ confidence interval for $\bm x_0^T \bm \beta$. We see that the width of this confidence interval depends on $\bm x_0$ through the quantity $\bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0$. Let's give this quantity a closer look, in the case when the regression contains an intercept, i.e. $\bm x_{*,0} = \bm 1$. Then, we have $\bm x_0 = (1, \bm x^T_{0,\text{-}0})$. Then, defining $\bar x \in \mathbb R^{p-1}$ as the vector of column-wise means of $\bm X_{*,\text{-}0}$, we can rewrite the regression as
\begin{equation}
y = \beta_0 + \bm x_{\text{-}0}^T \bm \beta_{\text{-}0} + \epsilon \equiv \beta'_0 + (\bm x_{\text{-}0}-\bar x)^T  \bm \beta_{\text{-}0} + \epsilon.
\end{equation}
Therefore, we seek a prediction interval for $\bm x_{0}^T \bm \beta = \beta'_0 + (\bm x_{0, \text{-}0}-\bar x)^T \bm \beta_{\text{-}0}$. With this reformulation, we can compute
\begin{equation}
\begin{split}
\bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0 &= (1 \ (\bm x_{0, \text{-}0}-\bar x)^T)\begin{pmatrix}\bm 1^T \bm 1 & 0 \\ 0 &\bm X_{*,\text{-}0}^T \bm X_{*,\text{-}0} \end{pmatrix}^{-1}{1 \choose \bm x_{0, \text{-}0}-\bar x} \\
&= \frac{1}{n} + (\bm x_{0, \text{-}0}-\bar x)^T (\bm X_{*,\text{-}0}^T \bm X_{*,\text{-}0})^{-1}(\bm x_{0, \text{-}0}-\bar x).
\end{split}
\end{equation}
Hence, we see that this quantity grows larger as $\bm x_{0, \text{-}0}-\bar x$ grows larger, and achieves its minimum when $\bm x_{0, \text{-}0}=\bar x$. Let's look at the special case when $p = 2$, so there is just one predictor except the intercept. Then, we have $\bm X_{*,\text{-}0} = \bm x_{*,1}-\bar x_1$, so
\begin{equation}
\bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0 = \frac{1}{n} + \frac{(x_{01}-\bar x_1)^2}{\|\bm x_{*,1}-\bar x_1\|^2}.
\end{equation}

\paragraph{Prediction interval for $y|\bm x_0$.}

Instead of creating a confidence interval for a point on the regression line, we may want to create a confidence interval for a new draw $y_0$ of $y$ for $\bm x = \bm x_0$, i.e. a \textit{prediction interval}. Note that
\begin{equation}
y_0 - \bm x_0^T \widehat \beta = \bm x_0^T \beta + \epsilon_0 - \bm x_0^T \widehat \beta = \epsilon_0 + \bm x_0^T (\beta-\widehat \beta) \sim N(0, \sigma^2 + \sigma^2 \bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0).
\end{equation}
Therefore, we have
\begin{equation}
\frac{y_0 - \bm x_0^T \widehat \beta}{\widehat \sigma\sqrt{1 + \bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0}} \sim t_{n-p},
\end{equation}
which leads to the $1-\alpha$ prediction interval
\begin{equation}
\bm x_0^T \bm{\widehat \beta} \pm \widehat \sigma \sqrt{1+\bm x_0^T (\bm X^T \bm X)^{-1} \bm x_0} \cdot t_{n-p}(1-\alpha/2) \equiv \bm x_0^T \bm{\widehat \beta} \pm \text{SE}(\bm x_0^T \bm{\widehat \beta}) \cdot t_{n-p}(1-\alpha/2).
\label{eq:pointwise-contrast-interval}
\end{equation}

\paragraph{Remark: Prediction with confidence in machine learning.}

The entire field of supervised machine learning is focused on accurately predicting $y_0$ from $\bm x_0$, usually using nonlinear functions $\widehat f(\bm x_0)$. In addition to providing a guess $\widehat y_0$ for $y_0$, it is often useful to quantify the uncertainty in this guess. In other words, it is useful to come up with an prediction interval (or prediction region) $\text{PI}(y_0)$ such that 
\begin{equation}
\mathbb P[y_0 \in \text{PI}(y_0) \mid \bm x_0] \geq 1-\alpha. 
\label{eq:conditional-prediction-interval}
\end{equation}
For example, in safety-critical applications of machine learning like self-driving cars, it is essential to have confidence in predictions. Unfortunately, beyond the realm of linear regression, it is hard to come up with intervals satisfying~\eqref{eq:conditional-prediction-interval} for each point $\bm x_0$. However, the emerging field of \textit{conformal inference} provides guarantees on average over possible values of $\bm x$:
\begin{equation}
\mathbb P[y \in \text{PI}(y)] = \mathbb E[\mathbb P[y \in \text{PI}(y) \mid \bm x]] \geq 1-\alpha. 
\label{eq:unconditional-prediction-interval}
\end{equation}
Remarkably, these guarantees place no assumption on the machine learning method used and require only that the data points on which $\widehat f$ is trained are exchangeable (an even weaker condition than i.i.d.). While the unconditional guarantee~\eqref{eq:unconditional-prediction-interval} is weaker than the conditional one~\eqref{eq:conditional-prediction-interval}, it can be obtained for modern machine learning and deep learning models.

\paragraph{Simultaneous intervals.}

Note that the intervals in the preceding sections have \textit{pointwise coverage}. For example, we have
\begin{equation}
\mathbb P[\beta_j \in \text{CI}(\beta_j)] \geq 1-\alpha \quad \text{for each } j.
\end{equation}
or
\begin{equation}
\mathbb P[\bm x_0^T \bm \beta \in \text{CI}(\bm x_0^T \bm \beta)] \geq 1-\alpha \quad \text{for each } \bm x_0.
\end{equation}
Sometimes a stronger \textit{simultaneous coverage} guarantee is desired, e.g.
\begin{equation}
\mathbb P[\beta_j \in \text{CI}^{\text{sim}}(\beta_j) \ \text{for each } j] \geq 1-\alpha
\label{eq:simultaneous-coordinatewise}
\end{equation}
or
\begin{equation}
\mathbb P[\bm x_0^T \bm \beta \in \text{CI}^{\text{sim}}(\bm x_0^T \bm \beta) \ \text{for each } \bm x_0] \geq 1-\alpha.
\label{eq:simultaneous-contrasts}
\end{equation}
Simultaneous confidence intervals are possible to construct as well. As a starting point, note that
\begin{equation}
\frac{\frac{1}{p}\|\bm X \bm{\widehat \beta} - \bm X \bm{\beta}\|^2}{\widehat \sigma^2} \sim F_{p, n-p}.
\end{equation}
Hence, we have
\begin{equation}
\mathbb P[\|\bm X \bm{\widehat \beta} - \bm X \bm{\beta}\|^2 \leq p \widehat \sigma^2 F_{p, n-p}(1-\alpha)] \geq 1-\alpha.
\end{equation}
Hence, the region
\begin{equation}
\text{CR}(\bm \beta) \equiv \{\bm \beta: (\bm{\widehat \beta} - \bm \beta)^T \bm X^T \bm X (\bm{\widehat \beta} - \bm \beta)  \leq p \widehat \sigma^2 F_{p, n-p}(1-\alpha)\} \subseteq \mathbb R^p
\end{equation}
is a $1-\alpha$ confidence region for the vector $\bm \beta$:
\begin{equation}
\mathbb P[\bm \beta \in \text{CR}(\bm \beta)] \geq 1-\alpha.
\end{equation}
It's easy to see that $\text{CR}(\bm \beta)$ is an ellipse centered at $\bm{\widehat \beta}$ (Figure~\ref{fig:confidence-region}). 
\begin{figure}[ht!]
\centering
\includegraphics[width = 0.4\textwidth]{figures/confidence-regions.jpg}
\caption{Confidence region and simultaneous and pointwise confidence intervals.}
\label{fig:confidence-region}
\end{figure}
Since the confidence region is for the entire vector $\bm \beta$, we can define simultaneous confidence intervals for each coordinate as follows:
\begin{equation}
\text{CI}^{\text{sim}}(\beta_j) \equiv \{\beta_j: \bm \beta \in \text{CR}(\bm \beta)\}.
\end{equation}
Then, these confidence intervals will satisfy the simultaneous coverage property~\eqref{eq:simultaneous-coordinatewise}. We will obtain a more explicit expression for $\text{CI}^{\text{sim}}(\beta_j)$ shortly.

Similarly, we may define the simultaneous confidence regions
\begin{equation}
\text{CI}^{\text{sim}}(\bm x_0^T \bm \beta) \equiv \{\bm x_0^T \bm \beta: \bm \beta \in \text{CR}(\bm \beta)\}.
\end{equation}
Let us find a more explicit expression for the latter interval. For notational ease, let us define $\bm \Sigma \equiv \bm X^T \bm X$. Then, note that if $\bm \beta \in \text{CR}(\bm \beta)$, then by the Cauchy-Schwarz inequality we have
\begin{equation}
\begin{split}
(\bm x_0^T \bm{\widehat \beta}-\bm x_0^T \bm\beta)^2 = \|\bm x_0^T (\bm{\widehat \beta}-\bm\beta)\|^2 &= \|(\bm \Sigma^{-1/2}\bm x_0)^T \bm \Sigma^{1/2}(\bm{\widehat \beta}-\bm\beta)\|^2 \\
&\leq \|(\bm \Sigma^{-1/2}\bm x_0)\|^2\|\bm \Sigma^{1/2}(\bm{\widehat \beta}-\bm\beta)\|^2 \leq \bm x_0^T \bm \Sigma^{-1}\bm x_0 p \widehat \sigma^2 F_{p, n-p}(1-\alpha),
\end{split}
\end{equation}
i.e.
\begin{equation}
\bm x_0^T \bm\beta \in \bm x_0^T \bm{\widehat \beta} \pm \widehat \sigma \sqrt{\bm x_0^T (\bm X^T \bm X)^{-1}\bm x_0} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \bm x_0^T \bm{\widehat \beta} \pm \text{SE}(\bm x_0^T \bm{\widehat \beta})\cdot\sqrt{pF_{p, n-p}(1-\alpha)}.
\label{eq:simultaneous-fit-se}
\end{equation}
Defining the above interval as $\text{CI}^{\text{sim}}(\bm x_0^T \bm\beta)$ gives us the simultaneous coverage property~\eqref{eq:simultaneous-contrasts}. Comparing to equation~\eqref{eq:pointwise-contrast-interval}, we see that the simultaneous interval is the pointwise interval expanded by a factor of $\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)$. Specializing to the case $\bm x_0 \equiv \bm e_j$, we get an expression for the simultaneous intervals for each coordinate:
\begin{equation}
\text{CI}^{\text{sim}}(\beta_j) \equiv \widehat \beta_j \pm \widehat \sigma \sqrt{(\bm X^T \bm X)^{-1}_{jj}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \text{SE}(\widehat \beta_j)\sqrt{pF_{p, n-p}(1-\alpha)},
\label{eq:simultaneous-coordinatewise-se}
\end{equation}
which again is the pointwise interval~\eqref{eq:pointwise-interval-beta} expanded by a factor of $\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)$. These simultaneous intervals are called \textit{Working-Hotelling intervals}.

\section{Practical considerations}

\paragraph{Practical versus statistical significance.}

You can have a statistically significant effect that is not practically significant. The hypothesis testing framework is most useful in the case when the signal to noise ratio is relatively small. Otherwise, constructing a confidence interval for the effect size is a more meaningful approach.

\paragraph{Correlation versus causation, and Simpson's paradox.}

Causation can be elusive for several reasons. One is reverse causation, where it is not clear whether $X$ causes $Y$ or $Y$ causes $X$. Another is confounding, where there is a third variable $Z$ that causes both $X$ and $Y$. For the latter reason, linear regression coefficients can be sensitive to the choice of other predictors to include, and can be misleading if you omit important variables from the regression. A special case and sometimes overlooked case of this is \textit{Simpson's paradox}, where an important discrete variable is omitted. Consider the example in Figure~\ref{fig:simpson-paradox}. Sometimes this discrete variable may seem benign, such as the year in which the data was collected. Such variables might or might not be measured. 
\begin{figure}[ht!]
\includegraphics[width = \textwidth]{figures/kidney-stones.png}
\caption{An example of Simpson's paradox (source: Wikipedia).}
\label{fig:simpson-paradox}
\end{figure}

\paragraph{Dealing with correlated predictors.}

It depends on the goal. If we're trying to tease apart effects of correlated predictors, then we have no choice but to proceed as usual despite lower power. Otherwise, we can test predictors in groups via the $F$-test to get higher power at the cost of lower ``resolution.'' Sometimes, it is recommended to simply remove predictors that are correlated with other predictors. This practice, however, is somewhat arbitrary and not recommended.

\paragraph{Model selection.}

We need to ask ourselves: Why do we want to do model selection? It can either be for prediction purposes or for inferential purposes. If it is for prediction purposes, then we can apply cross-validation to select a model and we don't need to think very hard about statistical significance. If it is for inference, then we need to be more careful. There are various classical model selection criteria (e.g. AIC, BIC), but it is not entirely clear what statistical guarantee we are getting for the resulting models. A simpler approach is to apply a $t$-test for each variable in the model, apply a multiple testing correction to the resulting $p$-values, and report the set of significant variables and the associated guarantee. Re-fitting the linear regression after model selection leads us into some dicey inferential territory due to selection bias. This is the subject of ongoing research and the jury is still out on the best way of doing this.

\section{R demo}

\textit{See also Agresti 3.4.1, 3.4.3, Dunn and Smyth 2.6, 2.14} \\

Let's put into practice what we've learned in this chapter by analyzing data about house prices.
<<message = FALSE>>=
library(tidyverse)
library(GGally)

houses_data <- read_tsv("data/Houses.dat")
houses_data
@

\paragraph{Exploration.}

Let's first do a bit of exploration:
<<fig.width = 4, fig.height = 3>>=
# visualize distribution of housing prices, superimposing the mean
houses_data |>
  ggplot(aes(x = price)) +
  geom_histogram(color = "black", bins = 30) +
  geom_vline(aes(xintercept = mean(price)),
    colour = "red",
    linetype = "dashed"
  )
@

<<>>=
# compare median and mean price
houses_data |>
  summarise(
    mean_price = mean(price),
    median_price = median(price)
  )
@

<<fig.width = 4.5, fig.height = 4>>=
# create a pairs plot of continuous variables
houses_data |>
  select(price, size, taxes) |>
  ggpairs()
@

<<fig.width = 3, fig.height = 3>>=
# see how price relates to beds
houses_data |>
  ggplot(aes(x = factor(beds), y = price)) +
  geom_boxplot(fill = "dodgerblue")
@

<<fig.width = 3, fig.height = 3>>=
# see how price relates to baths
houses_data |>
  ggplot(aes(x = factor(baths), y = price)) +
  geom_boxplot(fill = "dodgerblue")
@

<<fig.width = 2, fig.height = 3>>=
# see how price relates to new
houses_data |>
  ggplot(aes(x = factor(new), y = price)) +
  geom_boxplot(fill = "dodgerblue")
@


\paragraph{Hypothesis testing.} Let's run a linear regression and interpret the summary. But first, we must decide whether to model beds/baths as categorical or continuous? We should probably model these as categorical, given the potentially nonlinear trend observed in the box plots.
<<>>=
lm_fit <- lm(price ~ factor(beds) + factor(baths) + new + size,
  data = houses_data
)
summary(lm_fit)
@

\noindent We can read off the test statistics and $p$-values for each variable from the regression summary, as well as for the $F$-test against the constant model from the bottom of the summary.

Let's use an $F$-test to assess whether the categorical \verb|baths| variable is important.
<<>>=
lm_fit_partial <- lm(price ~ factor(beds) + new + size,
  data = houses_data
)
anova(lm_fit_partial, lm_fit)
@

\noindent What if we had not coded \verb|baths| as a factor?
<<>>=
lm_fit_not_factor <- lm(price ~ factor(beds) + baths + new + size,
  data = houses_data
)
anova(lm_fit_partial, lm_fit_not_factor)
@

If we want to test for the equality of means across groups of a categorical predictor, without adjust for other variables, we can use the ANOVA $F$-test. There are several equivalent ways of doing so:
<<>>=
# just use the summary function
lm_fit_baths <- lm(price ~ factor(baths), data = houses_data)
summary(lm_fit_baths)

# use the anova function as before
lm_fit_const <- lm(price ~ 1, data = houses_data)
anova(lm_fit_const, lm_fit_baths)

# use the aov function
aov_fit <- aov(price ~ factor(baths), data = houses_data)
summary(aov_fit)
@

We can also use an $F$-test to test for the presence of an interaction with a multi-class categorical predictor.
<<>>=
lm_fit_interaction <- lm(price ~ size * factor(beds), data = houses_data)
summary(lm_fit_interaction)

lm_fit_size <- lm(price ~ size + factor(beds), data = houses_data)
anova(lm_fit_size, lm_fit_interaction)
@

Contrasts of regression coefficients can be tested using the \verb|glht()| function from the \verb|multcomp| package.

\paragraph{Confidence intervals.}

We can construct pointwise confidence intervals for each coefficient using \verb|confint()|:
<<>>=
confint(lm_fit)
@
To create simultaneous confidence intervals, we need a somewhat more manual approach. We start with the coefficients and standard errors
<<>>=
coef(summary(lm_fit))
@
Then we add lower and upper confidence interval endpoints based on the formula~\eqref{eq:simultaneous-coordinatewise-se}:
<<>>=
alpha <- 0.05
n <- nrow(houses_data)
p <- length(coef(lm_fit))
f_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)
coef(summary(lm_fit)) |>
  as.data.frame() |>
  rownames_to_column(var = "Variable") |>
  select(Variable, Estimate, `Std. Error`) |>
  mutate(
    CI_lower = Estimate - `Std. Error` * sqrt(p * f_quantile),
    CI_upper = Estimate + `Std. Error` * sqrt(p * f_quantile)
  )
@
Note that the simultaneous intervals are substantially larger.

To construct pointwise confidence intervals for the fit, we can use the \verb|predict()| function:
<<>>=
predict(lm_fit, newdata = houses_data, interval = "confidence") |> head()
@
To get pointwise prediction intervals, we switch \verb|"confidence"| to \verb|"prediction"|:
<<>>=
predict(lm_fit, newdata = houses_data, interval = "prediction") |> head()
@
To construct simultaneous confidence intervals for the fit or predictions, we again need a slightly more manual approach. We call \verb|predict()| again, but this time asking it for the standard errors rather than the confidence intervals
<<>>=
predictions <- predict(lm_fit, newdata = houses_data, se.fit = TRUE)
head(predictions$fit)
head(predictions$se.fit)
@
Now we can construct the simultaneous confidence intervals via the formula~\eqref{eq:simultaneous-fit-se}:
<<>>=
f_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)
tibble(
  lower = predictions$fit - predictions$se.fit * sqrt(p * f_quantile),
  upper = predictions$fit + predictions$se.fit * sqrt(p * f_quantile)
)
@

In the case of simple linear regression, we can plot these pointwise and simultaneous confidence intervals as bands:
<<fig.width = 3, fig.height = 3>>=
# to produce confidence intervals for fits in general, use the predict() function
n <- nrow(houses_data)
p <- 2
alpha <- 0.05
lm_fit <- lm(price ~ size, data = houses_data)
predictions <- predict(lm_fit, se.fit = TRUE)
t_quantile <- qt(1 - alpha / 2, df = n - p)
f_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)
houses_data |>
  mutate(
    fit = predictions$fit,
    se = predictions$se.fit,
    ptwise_width = t_quantile * se,
    simultaneous_width = sqrt(p * f_quantile) * se
  ) |>
  ggplot(aes(x = size)) +
  geom_point(aes(y = price)) +
  geom_line(aes(y = fit), color = "blue") +
  geom_line(aes(y = fit + ptwise_width, color = "Pointwise")) +
  geom_line(aes(y = fit - ptwise_width, color = "Pointwise")) +
  geom_line(aes(y = fit + simultaneous_width, color = "Simultaneous")) +
  geom_line(aes(y = fit - simultaneous_width, color = "Simultaneous")) +
  theme(legend.title = element_blank(), legend.position = "bottom")
@

\paragraph{Predictor competition and collaboration.}

Let's look at the power of detecting the association between \verb|price| and \verb|beds|. We can imagine that \verb|beds| and \verb|baths| are correlated:
<<fig.width = 3.5, fig.height = 3>>=
houses_data |>
  ggplot(aes(x = beds, y = baths)) +
  geom_count()
@

So let's see how significant \verb|beds| is, with and without \verb|baths| in the model:
<<>>=
lm_fit_only_beds <- lm(price ~ factor(beds), data = houses_data)
summary(lm_fit_only_beds)
@

<<>>=
lm_fit_only_baths <- lm(price ~ factor(baths), data = houses_data)
lm_fit_beds_baths <- lm(price ~ factor(beds) + factor(baths), data = houses_data)
anova(lm_fit_only_baths, lm_fit_beds_baths)
@
We see that the significance of \verb|beds| dropped by two orders of magnitude. This is an example of predictor competition.

On the other hand, note that the variable \verb|new| is not very correlated with \verb|beds|:
<<>>=
lm_fit <- lm(new ~ beds, data = houses_data)
summary(lm_fit)
@
\noindent but we know has a substantial impact on \verb|price|. Let's look at the significance of the test that \verb|beds| is not important when we add \verb|new| to the model.

<<>>=
lm_fit_only_new <- lm(price ~ new, data = houses_data)
lm_fit_beds_new <- lm(price ~ new + factor(beds), data = houses_data)
anova(lm_fit_only_new, lm_fit_beds_new)
@
Adding \verb|new| to the model made the $p$-value more significant by a factor of 10. This is an example of predictor collaboration.

\chapter{Linear models: Misspecification}

In our discussion of linear model inference in Chapter 2, we assumed the normal linear model throughout:
\begin{equation}
\bm y = \bm X \bm \beta + \bm \epsilon, \quad \text{where} \ \bm \epsilon \sim N(\bm 0, \sigma^2 \bm I_n).
\end{equation}
In this unit, we will discuss what happens when this model is misspecified:
\begin{itemize}
\item Non-normality (Section~\ref{sec:non-normality}): $\bm \epsilon \sim (0, \sigma^2 \bm I_n)$ but not $N(0, \sigma^2 \bm I_n)$.
\item Heteroskedastic and/or correlated errors (Section~\ref{sec:heteroskedasticity}): $\bm \epsilon \sim (0, \bm \Sigma)$, where $\bm \Sigma \neq \sigma^2 \bm I$. This includes the case of heteroskedastic errors ($\bm \Sigma$ is diagonal but not a constant multiple of the identity) and correlated errors ($\bm \Sigma$ is not diagonal).
\item Model bias (Section~\ref{sec:model-bias}): It is not the case that $\mathbb E[\bm y] = \bm X \bm \beta$ for some $\bm \beta \in \mathbb R^p$.
\item Outliers (Section~\ref{sec:outliers}): For one or more $i$, it is not the case that $y_i \sim N(\bm x_{i*}^T \bm \beta, \sigma^2)$.
\end{itemize}
For each type of misspecification, we will discuss its origins, consequences, detection, and fixes (Sections~\ref{sec:non-normality}-\ref{sec:outliers}). We conclude with an R demo (Section~\ref{sec:R-demo-misspecification}).

\section{Origins, consequences, diagnostics, and overview of fixes}

\subsection{Non-normality} \label{sec:non-normality}

\paragraph{Origin.}

Non-normality occurs when the distribution of $y|\bm x$ is either skewed or has heavier tails than the normal distribution. This may happen, for example, if there is some discreteness in $y$.

\paragraph{Consequences.}

Non-normality is the most benign of linear model misspecifications. While we derived linear model inferences under the normality assumption, all the corresponding statements hold asymptotically without this assumption. Recall Homework 2 Question 1, or take for example the simpler problem of estimating the mean $\mu$ of a distribution based on $n$ samples from it: We can test $H_0: \mu = 0$ and build a confidence interval for $\mu$ even if the underlying distribution is not normal. So if $n$ is relatively large and $p$ is relatively small, you need not worry too much. If $n$ is small and the errors are highly skewed or heavy-tailed, we may have issues with incorrect standard errors.

\paragraph{Detection.}

Non-normality is a property of the error-terms $\epsilon_i$. We do not observe these directly, but we can approximate these using the residuals
\begin{equation}
\widehat \epsilon_i = y_i - \bm x_{i*}^T \bm{\widehat \beta}.
\end{equation}
Recall from Chapter 2 that $\text{Var}[\bm{\widehat \epsilon}] = \sigma^2(\bm I - \bm H)$. Letting $h_i$ be the $i$th diagonal entry of $\bm H$, it follows that $\widehat \epsilon_i \sim (0, \sigma^2(1-h_i))$. The \textit{standardized residuals} are defined as
\begin{equation}
r_i = \frac{\widehat \epsilon_i}{\widehat \sigma \sqrt{1-h_i}}.
\label{eq:standardized-residuals}
\end{equation}
Under normality, we would expect $r_i \overset \cdot \sim N(0,1)$. We can therefore assess normality by producing a histogram or normal QQ-plot of these residuals (see Figure~\ref{fig:qqplot}).

\begin{figure}[h!]
\centering
\includegraphics[width = 0.9\textwidth]{figures/qqplot.png}
\caption{Histogram and normal QQ plot of standardized residuals.}
\label{fig:qqplot}
\end{figure}

\paragraph{Fixes.}

As mentioned above, non-normality is not necessarily a problem that needs to be fixed, except in small samples. In small samples (but not too small!), we can apply the residual bootstrap for robust standard error computation and/or robust hypothesis testing.

\subsection{Heteroskedastic and correlated errors} \label{sec:heteroskedasticity}

\paragraph{Origin.}

\textbf{Heteroskedasticity} can arise as follows. Suppose each observation $y_i$ is actually the average of $n_i$ underlying observations, each with variance $\sigma^2$. Then, the variance of $y_i$ is $\sigma^2/n_i$, which will differ across $i$ if $n_i$ differ. It is also common to see the variance of a distribution increase as the mean increases (as in Figure~\ref{fig:heteroskedasticity}), whereas for a linear model the variance of $y$ stays constant as the mean of $y$ varies.

\textbf{Correlated errors} can arise when observations have group, spatial, or temporal structure. Below are examples:
\begin{itemize}
\item Group/clustered structure: We have 10 samples $(\bm x_{i*}, y_i)$ each from 100 schools.
\item Spatial structure: We have 100 soil samples from a 10$\times$10 grid on a 1km$\times$1km field.
\item Temporal structure: We have 366 COVID positivity rate measurements, one from each day of the year 2020.
\end{itemize}
The issue arises because there are common sources of variation among sample that are in the same group or spatially/temporally close to one another.

\paragraph{Consequences.}

All normal linear model inference from Chapter 2 hinges on the assumption that $\bm \epsilon \sim N(\bm 0, \sigma^2 \bm I)$. If instead of $\sigma^2 I$ we have $\text{Var}[\bm \epsilon] = \bm \Sigma$ for some matrix $\bm \Sigma$, then we may suffer two consequences: wrong inference (in terms of confidence interval coverage and hypothesis test levels) and inefficient inference (in terms of confidence interval width and hypothesis test power). One way of seeing the consequence of heteroskedasticity for confidence interval coverage is the width of prediction intervals; see Figure~\ref{fig:heteroskedasticity} for intuition.

\begin{figure}[h!]
\centering
\includegraphics[width = 0.6\textwidth]{figures/heteroskedasticity.png}
\caption{Heteroskedasticity in a simple bivariate linear model (\href{http://www3.wabash.edu/econometrics/EconometricsBook/chap19.htm}{image source}).}
\label{fig:heteroskedasticity}
\end{figure}


Like with heteroskedastic errors, correlated errors can cause invalid standard errors. In particular, positively correlated errors typically cause standard errors to be smaller than they should be, leading to inflated Type-I error rates. For intuition, consider estimating the mean of a distribution based on $n$ samples. Consider the cases when these samples are independent, compared to when they are perfectly correlated. The effective sample size in the former case is $n$ and in the latter case is 1.

\paragraph{Detection.}

Heteroskedasticity is usually assessed via the \textit{residual plot} (Figure~\ref{fig:residual-plots}). In this plot, the standardized residuals $r_i$~\eqref{eq:standardized-residuals} are plotted against the fitted values $\widehat \mu_i$. In the absence of heteroskedasticity, the spread of the points around the origin should be roughly constant as a function of $\widehat \mu$ (Figure~\ref{fig:residual-plots}(a)). A common sign of heteroskedasticity is the fan shape where variance increases as a function of $\widehat \mu$ (Figure~\ref{fig:residual-plots}(c)).

\begin{figure}[h!]
\centering
\includegraphics[width = \textwidth]{figures/residual-plots.png}
\caption{Residuals plotted against linear-model fitted values that reflect (a) model ade- quacy, (b) quadratic rather than linear relationship, and (c) nonconstant variance(image source: Agresti Figure 2.8).}
\label{fig:residual-plots}
\end{figure}

Residual plots once again come in handy to detect correlated errors. Instead of plotting the standardized residuals against the fitted values, we should plot the residuals against whatever variables we think might explain variation in the response that the regression does not account for. In the presence of group structures, we can plot residuals versus group (via a boxplot); in the presence of spatial or temporal structure, we can plot residuals as a function of space or time. If the residuals show a dependency on these variables, this suggests they are correlated. This dependency can be checked via formal means as well, e.g. via an ANOVA test in the case of groups or by estimating the autocorrelation function in the case of temporal structure.


\subsection{Model bias} \label{sec:model-bias}

\paragraph{Origin.}

Model bias arises when predictors are left out of the regression model:
\begin{equation}
\text{assumed model: } \bm y = \bm X \bm \beta + \bm \epsilon; \quad \text{actual model: } \bm y = \bm X \bm \beta + \bm Z \bm \gamma + \bm \epsilon.
\label{eq:confounding}
\end{equation}
We may not always know about or measure all the variables that impact a response $\bm y$.

Model bias can also arise when the predictors do not impact the response on the linear scale. For example:
\begin{equation}
\text{assumed model: } \mathbb E[\bm y] = \bm X \bm \beta; \quad \text{actual model: } g(\mathbb E[\bm y]) = \bm X \bm \beta.
\label{eq:wrong-scale}
\end{equation}

\paragraph{Consequences.}

In cases of model bias, the parameters $\bm \beta$ in the assumed linear model lose their meanings. The least squares estimate $\bm{\widehat \beta}$ will be a biased estimate for the parameter we probably actually want to estimate. In the case~\eqref{eq:confounding} when predictors are left out of the regression model, these additional predictors $\bm Z$ will act as confounders and create bias in $\bm{\widehat \beta}$ as an estimate of the $\bm \beta$ parameters in the true model, unless $\bm X^T \bm Z = 0$. As discussed in Chapter 2, this can lead to misleading conclusions.

\paragraph{Detection.}

Similarly to the detection of correlated errors, we can try to identify model bias by plotting the standardized residuals against predictors that may have been left out of the model. A good place to start is to plot standardized residuals against the predictors $\bm X$ (one at a time) that are in the model, since nonlinear transformations of these might have been left out. In this case, you would see something like Figure~\ref{fig:residual-plots}(b).

It is possible to formally test for model bias in cases when we have repeated observations of the response for each value of the predictor vector. In particular, suppose that $\bm x_{i*} = \bm x_c$ for $c = c(i)$ and predictor vectors $\bm x_1, \dots, \bm x_C \in \mathbb R^p$. Then, consider testing the following hypothesis:
\begin{equation}
H_0: y_i = \bm x_{i*}^T \bm \beta + \epsilon_i \quad \text{versus} \quad H_1: y_i = \beta_{c(i)} + \epsilon_i.
\end{equation}
The model under $H_0$ (the linear model) is nested in the model for $H_1$ (the saturated model), and we can test this hypothesis using an $F$-test called the \textit{lack of fit $F$-test}.

\paragraph{Overview of fixes.}

To fix model bias in the case~\eqref{eq:confounding}, ideally we would identify the missing predictors $\bm Z$ and add them to the regression model. This may not always be feasible or possible. To fix model bias in the case~\eqref{eq:wrong-scale}, it is sometimes advocated to find a transformation $g$ (e.g. a square root or a logarithm) of $\bm y$ such that $\mathbb E[g(\bm y)] = \bm X \bm{\beta}$. However, a better solution is to use a \textit{generalized linear model}, which we will discuss starting in Chapter 4.

\subsection{Outliers} \label{sec:outliers}

\paragraph{Origin.}

Outliers often arise due to measurement or data entry errors. An observation can be an outlier in $\bm x$, in $y$, or both.

\paragraph{Consequences.}

An outlier can have the effect of biasing the estimate $\bm{\widehat \beta}$. This occurs when an observation has outlying $\bm x$ as well as outlying $y$.

\paragraph{Detection.}

There are a few measures associated to an observation that can be used to detect outliers, though none are perfect. The first quantity is called the \textit{leverage}, defined as
\begin{equation}
\text{leverage of observation } i \equiv \text{corr}^2(y_i, \widehat \mu_i)^2.
\end{equation}
This quantity measures the extent to which the fitted value $\widehat \mu_i$ is sensitive to the (noise in the) observation $y_i$. It can be derived that
\begin{equation}
\text{leverage of observation } i = h_{i},
\end{equation}
which is the $i$th diagonal element of the hat matrix $\bm H$. This is related to the fact that $\text{Var}[\widehat \epsilon_i] = \sigma^2(1-h_{i})$. The larger the leverage, the smaller the variance of the residual, so the closer the line passes to the $i$th observation. The leverage of an observation is larger to the extent that $\bm x_{i*}$ is far from $\bm{\bar x}$. For example, in the bivariate linear model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$,
\begin{equation*}
h_{i} = \frac{1}{n} + \frac{(x_i - \bar x)^2}{\sum_{i' = 1}^n (x_{i'} - \bar x)^2}.
\end{equation*}
Note that the average of the leverages is 
\begin{equation}
\frac{1}{n}\sum_{i = 1}^n h_{i} = \frac{1}{n}\text{trace}(\bm H) = \frac{p}{n}.
\end{equation}
An observation's leverage is considered large if it is significantly larger than this after, e.g. three times larger.

Note that the leverage is not a function of $y_i$, so a high-leverage point might or might not be an outlier in $y_i$ and therefore might or might not have a strong impact on the regression. To assess more directly whether an observation is \textit{influential}, we can compare the least squares fits with and without that observation. To this end, we define the \textit{Cook's distance}
\begin{equation}
D_i = \frac{\sum_{i' = 1}^n (\widehat \mu_{i'} - \widehat \mu^{\text{-}i}_{i'})^2}{p\widehat \sigma^2},
\end{equation}
where $\widehat \mu^{\text{-}i}_{i'} = \bm x_{i*}^T \bm{\widehat{\beta}}^{\text{-}i}$ and $\bm{\widehat{\beta}}^{\text{-}i}$ is the least squares estimate based on $(\bm X_{\text{-}i,*}, \bm y_{\text{-}i})$. An observation is considered influential if it has Cooks distance greater than one.

There is a connection between Cook's distance and leverage:
\begin{equation}
D_i = \left(\frac{y_i - \widehat \mu_i}{\widehat \sigma \sqrt{1-h_{ii}}}\right)^2 \cdot \frac{h_{ii}}{p(1-h_{ii})}.
\end{equation}
We recognize the first term as the standardized residual; therefore a point is influential if its residual and leverage are large.

Note that Cook's distance may not successfully identify outliers. For example, if there are groups of outliers, then they will \textit{mask} each other in the calculation of Cook's distance.

\paragraph{Overview of fixes.}

If outliers can be detected, then the fix is to remove them from the regression. But, we need to be careful. Definitively determining whether observations are outliers can be tricky. Outlier detection can even be used as a way to commit fraud with data, as now-defunct blood testing start-up \href{https://arstechnica.com/tech-policy/2021/09/cherry-picking-data-was-routine-practice-at-theranos-former-lab-worker-says/}{Theranos is alleged to have done}. As an alternative to removing outliers, we can fit estimators $\bm{\widehat \beta}$ that are less sensitive to outliers; see Section~\ref{sec:robust-estimation}.

\section{Asymptotic methods for heteroskedastic and correlated errors}

Broadly speaking, approaches to fixing heteroskedastic or correlated errors can be divided into (1) those based on estimating $\bm \Sigma$ and (2) those based on resampling. Methods based on estimating $\bm \Sigma$ can use this estimate to either (i) build a better estimate $\bm {\widehat \beta}$ or (ii) build better standard errors for the least squares estimate. Resampling methods include the bootstrap (for estimation) and the permutation test (for testing).

\subsection{Methods that build a better estimate of $\bm{\widehat \beta}$}
Suppose $\bm y \sim N(\bm X \bm \beta, \Sigma)$. This is a \textit{generalized least squares} problem for which inference can be carried out. The generalized least squares estimate is $\bm{\widehat \beta} = (\bm X^T \bm \Sigma^{-1}\bm X)^{-1}\bm X^T \bm \Sigma^{-1}\bm y$, which is distributed as $\bm{\widehat \beta} \sim N(\bm \beta, (\bm X^T \bm \Sigma^{-1}\bm X)^{-1})$. This is the best linear unbiased estimate of $\bm \beta$, recovering efficiency. We can carry out inference based on the latter distributional result analogously to how we did so in Chapter 2. The issue, of course, is that we usually do not know $\bm \Sigma$. Therefore, we can consider the following approach: (1) estimate $\bm{\widehat \beta}$ using OLS, (2) use this estimate to get an estimate $\bm{\widehat \Sigma}$ of $\bm \Sigma$, (3) use $\bm{\widehat \Sigma}$ to get a (hopefully) more efficient estimator
\begin{equation}
\bm{\widehat \beta}^{\text{FGLS}} \equiv (\bm X^T \bm{\widehat \Sigma}^{-1}\bm X)^{-1}\bm X^T \bm{\widehat\Sigma}^{-1}\bm y.
\end{equation}
This is called the \textit{feasible generalized least squares estimate} (FGLS), to contrast it with the infeasible estimate that assumes $\bm \Sigma$ is known exactly. The procedure above can be iterated until convergence. To estimate $\bm{\widehat \Sigma}$, we usually need to make some parametric assumptions. For example, in the case of grouped structure, we might assume a \textit{random effects model}. In the case of a temporal structure, we might assume an \textit{AR(1) model}.

\subsection{Methods that build better standard errors for OLS estimate}

Sometimes we don't feel comfortable enough with our estimate of $\bm \Sigma$ to actually modify the least squares estimator. So we want to keep using our least squares estimator, but still get standard errors robust to heteroskedastic or correlated errors. There are several strategies to computing valid standard errors in such situations.

\subsubsection*{Sandwich standard errors}
Let's say that $\bm y = \bm X \bm \beta + \bm \epsilon$, where $\bm \epsilon \sim N(\bm 0, \bm \Sigma)$. Then, we can compute that the covariance matrix of the least squares estimate $\bm{\widehat \beta}$ is
\begin{equation}
\text{Var}[\bm{\widehat \beta}] = (\bm X^T \bm X)^{-1}(\bm X^T \bm \Sigma \bm X)(\bm X^T \bm X)^{-1}.
\label{eq:sandwich}
\end{equation}
Note that this expression reduces to the usual $\sigma^2(\bm X^T \bm X)^{-1}$ when $\bm \Sigma = \sigma^2 \bm I$. It is called the sandwich variance between we have the $(\bm X^T \bm \Sigma \bm X)$ term sandwiched between two $(\bm X^T \bm X)^{-1}$ terms. If we have some estimate $\bm{\widehat \Sigma}$ of the covariance matrix, we can construct
\begin{equation}
\widehat{\text{Var}}[\bm{\widehat \beta}] \equiv (\bm X^T \bm X)^{-1}(\bm X^T \bm{\widehat\Sigma} \bm X)(\bm X^T \bm X)^{-1}.
\end{equation}
Different estimates $\bm{\widehat \Sigma}$ are appropriate in different situations. Below we consider three of the most common choices: one for heteroskedasticity (due to Huber-White), one for group-correlated errors (due to Liang-Zeger), and one for temporally-correlated errors (due to Newey-West).

\subsubsection*{Specific instances of sandwich standard errors}

\paragraph{Huber-White standard errors.}

Suppose $\bm \Sigma = \text{diag}(\sigma_1^2, \dots, \sigma_n^2)$ for some variances $\sigma_1^2, \dots, \sigma_n^2 > 0$. The Huber-White sandwich estimator is defined by~\eqref{eq:sandwich}, with
\begin{equation}
\bm{\widehat\Sigma} \equiv \text{diag}(\widehat \sigma_1^2, \dots, \widehat \sigma_n^2), \quad \text{where} \quad \widehat \sigma_i^2 = (y_i - \bm x_{i*}^T \bm{\widehat \beta})^2.
\end{equation}
While each estimator $\widehat \sigma_i^2$ is very poor, Huber and White's insight was that the resulting estimate of the (averaged) quantity $\bm X^T \bm{\widehat \Sigma}\bm X$ is not bad. To see why, assume that $(\bm x_{i*}, y_i) \overset{\text{i.i.d.}} \sim F$ for some joint distribution $F$. Then, we have that
\begin{equation}
\begin{split}
\frac{1}{n}(\bm X^T \widehat{\bm \Sigma} \bm X - \bm X^T \bm \Sigma \bm X) &= \frac{1}{n} \sum_{i=1}^n (\widehat \sigma_i^2 - \sigma_i^2) \bm x_{i*} \bm x_{i*}^T \\
&= \frac{1}{n} \sum_{i=1}^n ((\epsilon_i + \bm x_{i*}^T(\widehat{\bm \beta} - \bm \beta))^2 - \sigma_i^2) \bm x_{i*} \bm x_{i*}^T \\
&= \frac{1}{n} \sum_{i=1}^n \epsilon_i^2 \bm x_{i*} \bm x_{i*}^T + o_p(1) \\
&\to_p 0.
\end{split}
\end{equation}
The last step holds by the large of large numbers, since $\mathbb E[\epsilon_i^2 \bm x_{i*} \bm x_{i*}^T] = 0$ for each $i$.

\paragraph{Liang-Zeger standard errors.}

Next, let's consider the case of group-correlated errors. Suppose that the observations are \textit{clustered}, with correlated errors among clusters but not between clusters. Suppose there are $C$ clusters of observations, with the $i$th observation belonging to cluster $c(i) \in \{1, \dots, C\}$. Suppose for the sake of simplicity that the observations are ordered so that clusters are contiguous. Let $\bm{\widehat \epsilon}_c$ be the vector of residuals in cluster $c$, so that $\bm{\widehat \epsilon} = (\bm{\widehat \epsilon}_1, \dots, \bm{\widehat \epsilon}_C)$. Then, the true covariance matrix is $\bm \Sigma = \text{block-diag}(\bm \Sigma_1, \dots, \bm \Sigma_C)$ for some positive definite $\bm \Sigma_1, \dots, \bm \Sigma_C$. The Liang-Zeger estimator is then defined by~\eqref{eq:sandwich}, with
\begin{equation}
\bm{\widehat\Sigma} \equiv \text{block-diag}(\bm{\widehat\Sigma_1}, \dots, \bm{\widehat \Sigma_C}), \quad \text{where} \quad  \bm{\widehat\Sigma_c} \equiv \bm{\widehat \epsilon}_c \bm{\widehat \epsilon}_c^T.
\end{equation}
Note that the Liang-Zeger estimator is a generalization of the Huber-White estimator. Its justification is similar as well: while each $\bm{\widehat\Sigma_c}$ is a poor estimator, the resulting estimate of the (averaged) quantity $\bm X^T \bm{\widehat \Sigma}\bm X$ is not bad as long as the number of clusters is large. Liang-Zeger standard errors are referred to as ``clustered standard errors'' in the econometrics community.

\paragraph{Newey-West standard errors.}

Finally, consider the case when our observations $i$ have a temporal structure, and we believe there to be nontrivial correlations between $\epsilon_{i1}$ and $\epsilon_{i2}$ for $|i1 - i2| \leq L$. Then, a natural extension of the Huber-White estimate of $\bm \Sigma$ is $\bm{\widehat \Sigma}_{i1,i2} = \widehat \epsilon_{i1}\widehat \epsilon_{i2}$ for each pair $(i1, i2)$ such that $|i1 - i2| \leq L$. Unfortunately, this is not guaranteed to give a positive semidefinite matrix $\bm{\widehat \Sigma}$. Therefore, Newey and West proposed a slightly modified estimator:
\begin{equation*}
\bm{\widehat \Sigma}_{i1,i2} = \max\left(0, 1-\frac{|i1-i2|}{L}\right)\widehat \epsilon_{i1}\widehat \epsilon_{i2}.
\end{equation*}
This estimator shrinks the off-diagonal estimates $\widehat \epsilon_{i1}\widehat \epsilon_{i2}$ based on their distance to the diagonal. It can be shown that this modification restores positive semidefiniteness of $\bm{\widehat \Sigma}$.

\subsubsection*{Inference based on sandwich standard errors}

We now have a matrix $\widehat{\bm \Omega}$ such that 
\begin{equation}
\bm{\widehat \beta} \overset \cdot \sim N(\bm \beta, \widehat{\bm \Omega}).
\end{equation}
This allows us to construct confidence intervals and hypothesis tests each $\beta_j$, by simply replacing $\text{SE}(\beta_j)$ with $\sqrt{\widehat \Omega_{jj}}$. For contrasts and prediction intervals, we can use the fact that $\bm c^T \bm \beta \overset \cdot \sim N(\bm c^T \bm \beta, \bm c^T \widehat{\bm \Omega} \bm c)$, so that $\text{CE}(\bm c^T \bm \beta) = \sqrt{\bm c^T \widehat{\bm \Omega} \bm c}$. It is less obvious how to use the matrix $\widehat{\bm \Omega}$ to test the hypothesis $H_0: \bm \beta_S = \bm 0$. To this end, we can use a Wald test (we will discuss Wald tests in more detail in Chapter~\ref{ch:glm-theory}). The Wald test statistic is
\begin{equation}
W = \bm{\widehat \beta}_S^T (\widehat{\bm \Omega}_{S, S})^{-1} \bm{\widehat \beta}_S,
\end{equation}
which is asymptotically distributed as $\chi^2_{|S|}$ under the null hypothesis. It turns out that the usual regression $F$-test is asymptotically equivalent to this Wald test.

\section{The bootstrap}

\subsection{The residual bootstrap}

\paragraph{Standard errors via the residual bootstrap.}

The \textit{bootstrap} is one way of carrying out robust inference. The core idea of the bootstrap is to use the data to construct an approximation to the data-generating distribution, and then to approximate the sampling distribution of any test statistic by simulating from this approximate data-generating distribution. This approach, pioneered by Brad Efron in 1979, replaces mathematical derivations with computation. The bootstrap is extremely flexible, and can be adapted to apply in a variety of settings.

Suppose that $y_i = \bm x_{i*}^T \bm \beta + \epsilon_i$, where $\epsilon_i \overset{\text{i.i.d.}}\sim F$ for some distribution $F$. Then, the data-generating distribution is specified by $(\bm \beta, F)$, which we approximate by substituting $\bm{\widehat \beta}$ for $\bm \beta$ and the empirical distribution of the residuals $\widehat \epsilon_i$ (call it $\widehat F$) for $F$. We can then sample new response vectors based on this approximate data-generating distribution:
\begin{equation}
y_i^b = \bm x_{i*}^T \bm{\widehat \beta} + \epsilon_i^b, \quad \epsilon_i^b \overset{\text{i.i.d.}}\sim \widehat F \quad \text{for } b = 1, \dots, B.
\label{eq:residual_bootstrap}
\end{equation}
Note that i.i.d. sampling $\epsilon_i^b$ from $\widehat F$ amounts to sampling $(\epsilon_1^b, \dots, \epsilon_n^b)$ with replacement from $(\widehat \epsilon_1, \dots, \widehat \epsilon_n)$. Then, as with the parametric bootstrap, we fit a least squares coefficient vector $\bm{\widehat \beta}^b$ to $(\bm X, \bm y^b)$ for each $b$ and obtain standard errors by treating $\{\bm{\widehat \beta}^b\}_{b = 1}^B$ as though it were the sampling distribution of $\bm{\widehat \beta}$.

\paragraph{Hypothesis testing via the residual bootstrap.}

While the bootstrap is commonly associated with the construction of standard errors, it can also be used directly for hypothesis testing. Suppose we wish to test the linear regression null hypothesis $H_0: \bm \beta_S = \bm 0$ for some $S \subseteq \{1, \dots, p-1\}$ (which recall we cannot do using a permutation test). We compute some test statistic $T(\bm X, \bm y)$ measuring the significance of $\bm \beta_S$ (e.g. an $F$-statistic but it could be anything else). Then, we can use a variant of the residual bootstrap. We fit the least squares estimate $\bm{\widehat \beta}$ as usual and extract the residuals $\widehat \epsilon_i \equiv y_i - \bm x_{i*}^T \bm{\widehat \beta}$ and their empirical distribution $\widehat F$. Then, placing ourselves under the null hypothesis, we generate new samples $\bm y^b$ from the null distribution analogously to the usual residual bootstrap~\eqref{eq:residual_bootstrap}:
\begin{equation}
y_i^b = \bm x_{i,\text{-}S}^T \bm{\widehat \beta}_{\text{-}S} + \epsilon_i^b, \quad \epsilon_i^b \overset{\text{i.i.d.}}\sim \widehat F \quad \text{for } b = 1, \dots, B.
\end{equation}
We can then build a null distribution by recomputing $T(\bm X, \bm y^b)$ for each $b$ and then define the bootstrap-based $p$-value
\begin{equation}
p^{\text{boot}} \equiv \frac{1}{B+1}\left(1+\sum_{b = 1}^B \mathbbm 1(T(\bm X, \bm y^b) \geq T(\bm X, \bm y))\right).
\end{equation}

\subsection{Pairs bootstrap}

The residual bootstrap corrects for non-normality, but not heteroskedasticity or correlated errors, since it assumes that the noise terms are i.i.d. from some distribution.

Weakening the assumptions further, let's assume only that $(\bm x_{i*}, y_i) \overset{\text{i.i.d.}} \sim F$ for some joint distribution $F$. We then resample our observations by sampling with replacement from the original observations.

Note that, unlike the parametric or residual bootstrap, the pairs bootstrap treats the predictors $\bm X$ as random rather than fixed. The benefit of the pairs bootstrap is that it does not assume homoskedasticity, since the error variance is allowed to depend on $\bm x_{i*}$. Therefore, the pairs bootstrap addresses both non-normality and heteroskedasticity, though it does not address correlated errors (though variants of the pairs bootstrap do; see below). Note that the pairs bootstrap does not even assume that $\mathbb E[y_i] = \bm x_{i*}^T \bm \beta$ for some $\bm \beta$. However, in the presence of model bias, it is unclear for what parameters we are even doing inference. While the pairs bootstrap assumes less than the residual bootstrap, it may be somewhat less efficient in the case when the assumptions of the latter are met.

The pairs bootstrap has several variants that help it overcome correlated errors, in addition to heteroskedasticity. The \textit{cluster bootstrap} is applicable in the case when errors have a clustered/grouped structure. In this case, we sample entire clusters of observations, with replacement, from the original set of clusters. The \textit{moving blocks bootstrap} is applicable in the case of spatially or temporally structured errors. In this variant of the pairs bootstrap, we resample spatially or temporally adjacent blocks of observations together to preserve their joint correlation structure.

\section{The permutation test}

Unlike the residual bootstrap, the pairs bootstrap cannot accommodate hypothesis testing. If we would like resampling-based hypothesis tests in the presence of heteroskedasticity, we can consider permutation tests instead. Permutation tests are an easy way of testing the null hypothesis of independence between two random variables (or vectors). For our purposes, suppose that $(\bm x_{i*}, y_i)$ are drawn i.i.d. from some joint distribution $F$ (as opposed to the usual assumption that $\bm X$ is fixed). Then, consider the null hypothesis
\begin{equation}
H_0: \bm x \perp\!\!\!\perp y.
\label{eq:independence-1}
\end{equation}
This null hypothesis is related to the null hypothesis $H_0: \bm \beta_{\text{-}0} = 0$ in a linear regression, as formalized by the following lemma.
\begin{lemma}
Suppose $\bm x \in \mathbb R^{p-1}$ has a nondegenerate distribution $F_{\bm x}$ in the sense that there does not exist a vector $c \in \mathbb R^{p-1}$ such that $\bm c^T \bm x$ is deterministic. Suppose also that $F_{y|\bm x}$ is a distribution such that $\mathbb E[y|\bm x] = \beta_0 + \bm x^T\bm \beta_{\text{-}0}$ and that the distribution $F_{y|\bm x}$ is specified by its mean. Then,
\begin{equation}
\bm x \perp\!\!\!\perp y \quad \Longleftrightarrow \quad \bm \beta_{\text{-}0} = \bm 0.
\end{equation}
\end{lemma}
\begin{proof}
If $\bm \beta_{\text{-}0} = \bm 0$, then $\mathbb E[y|\bm x] = \beta_0$. Therefore, the mean of $y$ does not depend on $\bm x$. By the assumption on $F_{y|\bm x}$, it follows that the entire distribution $F_{y|\bm x}$ does not depend on $\bm x$, i.e. $y \perp\!\!\!\perp \bm x$. If $\bm \beta_{\text{-}0} \neq \bm 0$, then $\mathbb E[y|\bm x] = \beta_0 + \bm x^T\bm \beta_{\text{-}0}$, which by assumption is non-constant. Since $\mathbb E[y|\bm x]$ depends on $\bm x$, it follows that $y$ is not independent of $\bm x$.
\end{proof}

Therefore, any valid independence test automatically gives a non-normality-robust and heteroskedasticity-robust test of $H_0: \bm \beta_{\text{-}0} = \bm 0$ in a linear regression.

Now, suppose we have $n$ i.i.d. samples $(\bm x_{i*}, y_i)$ from $F$. Under the independence null hypothesis~\eqref{eq:independence-1}, the distribution of the data is unchanged if we permute the response variables $y_i$. Formally, let $\bm y_{()}$ be the order statistics of the response variable, let $S_n$ be the permutation group on $\{1, \dots, n\}$, and let $\bm y_\tau$ denote the permutation of $\bm y$ by $\tau \in S_n$. Then,
\begin{equation}
\bm y | \bm X, \bm y_{()} \sim \frac{1}{n!}\sum_{\tau \in S_n} \delta(\bm y_{\tau}).
\end{equation}
Now, let $T(\bm X, \bm y)$ be any test statistic measuring the association between $\bm y$ and $\bm X$, e.g. a linear regression $F$-statistic. Then, the above distributional result implies that
\begin{equation}
T(\bm X, \bm y) | \bm X, \bm y_{()} \sim \frac{1}{n!}\sum_{\tau \in S_n} \delta(T(\bm X, \bm y_{\tau})).
\end{equation}
Hence, we can compute the null distribution of $T$ by repeatedly permuting the response $\bm y$ and recomputing $T(\bm X, \bm y_{\tau})$. This gives rise to the permutation $p$-value
\begin{equation}
p^{\text{perm}} \equiv \frac{1}{n!}\sum_{\tau \in S_n} \mathbbm 1(T(\bm X, \bm y_{\tau}) \geq T(\bm X, \bm y)).
\end{equation}
The uniform distribution of $T(\bm X, \bm y) | \bm X, \bm y_{()}$ implies that
\begin{equation}
\mathbb P[p^{\text{perm}} \leq t | \bm X, \bm y_{()}] \leq t \quad \Longrightarrow \quad \mathbb P[p^{\text{perm}} \leq t] = \mathbb E[\mathbb P[p^{\text{perm}} \leq t | \bm X, \bm y_{()}]] \leq t \quad \text{for all } t \in [0,1]..
\end{equation}
In practice, $p^{\text{perm}}$ is approximated by independently sampling $B$ permutations $\tau_1, \dots, \tau_B$ from the uniform distribution over $S_n$. Letting $\tau_0$ be the identity permutation, it follows that
\begin{equation}
\bm y | \bm X, \bm y \in \{\bm y_{\tau_0}, \dots, \bm y_{\tau_B}\} \sim \frac{1}{B+1}\sum_{b = 0}^B \delta(\bm y_{\tau_b}).
\end{equation}
Similar logic as above leads to the approximate permutation $p$-value
\begin{equation}
\widehat p^{\text{perm}} \equiv \frac{1}{B+1}\sum_{b = 0}^B \mathbbm 1(T(\bm X, \bm y_{\tau_b}) \geq T(\bm X, \bm y)) = \frac{1}{B+1}\left(1 + \sum_{b = 1}^B \mathbbm 1(T(\bm X, \bm y_{\tau_b}) \geq T(\bm X, \bm y))\right).
\label{eq:p-perm-hat}
\end{equation}
Although $\widehat p^{\text{perm}}$ can be viewed as an approximation to $\bm p^{\text{perm}}$, it is also stochastically larger than the uniform distribution in finite samples:
\begin{equation}
\mathbb P[\widehat p^{\text{perm}} \leq t] \leq t \quad \text{for all } t \in [0,1].
\label{eq:permutation-pvalue-validity}
\end{equation}
Warning: A common mistake is to omit the ``1+'' in the numerator and denominator of the definition~\eqref{eq:p-perm-hat}. The resulting $p$-value is \textit{not valid} in the sense of equation~\eqref{eq:permutation-pvalue-validity}.

\paragraph{Example.}

A common application of the permutation test is testing for equality of distributions in the two-sample problem, where the permutation test amounts to generating a null distribution for any test statistic (e.g. a difference in means) by pooling together the two samples and randomly reassigning the classes of the samples.

\paragraph{Strengths and weaknesses.}

The strength of the permutation test is that it is valid under almost no assumptions on the data-generating process. Its main weakness is that it is not applicable to the hypothesis $H_0: \beta_S = 0$ for any group of predictors $S \neq \{1, \dots, p-1\}$. Intuitively, this would require a fancy kind of permutation that breaks the association between $\bm y$ and $\bm X_{*, S}$ while preserving the association between $\bm X_{*, S}$ and $\bm X_{*, \text{-}S}$. This amounts to a test of \textit{conditional} independence, which requires more assumptions on the joint distribution $F_{\bm x, y}$ than an independence test. Another weakness of a permutation test is that it is computationally expensive, although in the 21st century this is not a huge issue.

\section{Robust estimation} \label{sec:robust-estimation}

The squared error loss $\sum_{i = 1}^n (y_i - \bm x_{i*}^T \bm \beta)^2$ is sensitive to outliers in the sense that a large value of $y_i - \bm x_{i*}^T \bm \beta$ can have a significant impact on the loss function. The least squares estimate, as the minimizer of this loss function, is therefore sensitive to outliers. One way of addressing this challenge is to replace the squared error loss by a different loss that does not grow so quickly in $y_i - \bm x_{i*}^T \bm \beta$. A popular choice for such a loss function is the Huber loss:
\begin{equation}
L_\delta(y_i - \bm x_{i*}^T \bm \beta) =
\begin{cases}
\frac{1}{2}(y_i - \bm x_{i*}^T \bm \beta)^2, \quad &\text{if } |y_i - \bm x_{i*}^T \bm \beta| \leq \delta; \\
\delta(|y_i - \bm x_{i*}^T \bm \beta|-\delta), \quad &\text{if } |y_i - \bm x_{i*}^T \bm \beta| > \delta.
\end{cases}
\end{equation}
This function is differentiable, like the squared error loss, but grows linearly as opposed to quadratically. We can then define
\begin{equation*}
\bm{\widehat{\beta}}^{\text{Huber}} \equiv \underset{\bm \beta}{\arg \min}\ \sum_{i = 1}^n L_\delta(y_i - \bm x_{i*}^T \bm \beta).
\end{equation*}
This is an \textit{M-estimator}; it is consistent and has an asymptotic normal distribution that can be used for inference.

% \paragraph{Parametric bootstrap.}
%
% The parametric bootstrap proceeds by fitting a parametric model, and then by resampling from this model. In the linear regression case, we use the original data to fit $(\bm{\widehat \beta}, \widehat \sigma^2)$. Then, we sample new response vectors
% \begin{equation}
% y^b_i = \bm x_{i*}^T\bm{\widehat \beta} + \epsilon_i^b, \quad \epsilon_i^b \overset{\text{i.i.d.}}\sim N(0, \widehat \sigma^2) \quad \text{for } b = 1, \dots, B.
% \end{equation}
% We then fit a least squares coefficient vector $\bm{\widehat \beta}^b$ to $(\bm X, \bm y^b)$ for each $b$, and then get variance estimates by treating $\{\bm{\widehat \beta}^b\}_{b = 1}^B$ as though it were the sampling distribution of $\bm{\widehat \beta}$. For example, we could use the sample standard deviation of $\widehat \beta_j^b$ as the standard error for $\beta_j$.
%
% This the most model-based of the bootstrap variants. It assumes a completely well-specified model, and gives equivalent results to traditional parametric inference. It is typically not applied in regression settings, and presented here mainly for pedagogical purposes.



% \subsubsection{Bootstrap-based tests} \label{sec:bootstrap-tests}
%
%
%
% This bootstrap-based hypothesis test is not as robust as a permutation test or a heteroskedasticity-robust standard error, since the residual bootstrap implicitly assumes homoskedasticity. However, compared to parametric linear model inference, this bootstrap test affords the additional flexibility of using \textit{any} test statistic $T$ (including one based on, say, machine learning). Note that, while the pairs bootstrap is more robust than the residual bootstrap, the pairs bootstrap does not allow one to create samples under the null distribution and therefore cannot be used for hypothesis testing.

\section{R demo} \label{sec:R-demo-misspecification}

We illustrate how to deal with heteroskedasticity, group-correlated errors, autocorrelated errors, and outliers in the following sections.

\subsection{Heteroskedasticity}

Next let's look at another dataset, from the Current Population Survey (CPS).
<<message = FALSE>>=
library(tidyverse)
cps_data <- read_tsv("data/cps2.tsv")
cps_data
@

Suppose we want to regress \verb|wage| on \verb|educ|, \verb|exper|, and \verb|metro|.
<<>>=
lm_fit <- lm(wage ~ educ + exper + metro, data = cps_data)
@

\subsubsection{Diagnostics}

Let's take a look at the standard linear model diagnostic plots built into R.
<<fig.width = 4, fig.height = 4, fig.align='center'>>=
# residuals versus fitted
plot(lm_fit, which = 1)

# residual QQ plot
plot(lm_fit, which = 2)

# residuals versus leverage (with Cook's distance)
plot(lm_fit, which = 5)
@

The residuals versus fitted plot suggests significant heteroskedasticity, with variance growing as a function of the fitted value.

\subsubsection{Sandwich standard errors}

To get standard errors robust to this heteroskedasticity, we can use one of the robust estimators discussed in Section~\ref{sec:heteroskedasticity}. Most of the robust standard error constructions discussed in that section are implemented in the R package \verb|sandwich|.
<<>>=
library(sandwich)
@
For example, Huber-White's heteroskedasticity-consistent estimate $\widehat{\text{Var}}[\bm{\widehat \beta}]$ can be obtain via \verb|vcovHC|:
<<>>=
HW_cov <- vcovHC(lm_fit)
HW_cov
@
\noindent Compare this to the traditional estimate:
<<>>=
usual_cov <- vcovHC(lm_fit, type = "const")
usual_cov

# extract the variance estimates from the diagonal
tibble(
  variable = rownames(usual_cov),
  usual_variance = sqrt(diag(usual_cov)),
  HW_variance = sqrt(diag(HW_cov))
)
@

\noindent Bootstrap standard errors are also implemented in \verb|sandwich|:
<<>>=
# pairs bootstrap
bootstrap_cov <- vcovBS(lm_fit, type = "xy")
tibble(
  variable = rownames(usual_cov),
  usual_variance = diag(usual_cov),
  HW_variance = diag(HW_cov),
  boostrap_variance = diag(bootstrap_cov)
)
@
\noindent Note that the bootstrap standard errors are closer to the HW ones than the standard ones.

The covariance estimate produced by \verb|sandwich| can be easily integrated into linear model inference using the package \verb|lmtest|.
<<message = FALSE>>=
library(lmtest)

# fit linear model as usual
lm_fit <- lm(wage ~ educ + exper + metro, data = cps_data)

# robust t-tests for coefficients
coeftest(lm_fit, vcov. = vcovHC)

# robust confidence intervals for coefficients
coefci(lm_fit, vcov. = vcovHC)

# robust F-test
lm_fit_partial <- lm(wage ~ educ, data = cps_data) # a partial model
waldtest(lm_fit_partial, lm_fit, vcov = vcovHC)
@


\subsubsection{Bootstrap confidence intervals}

One R package for performing bootstrap inference is \verb|simpleboot|. Let's see how to get pairs bootstrap distributions for the coefficient estimates.
<<message = FALSE>>=
library(simpleboot)
boot_out <- lm.boot(
  lm.object = lm_fit, # input the fit object from lm()
  R = 1000
) # R is the number of bootstrap replicates
perc(boot_out) # get the percentile 95% confidence intervals

# Note: lm.boot implements the residual bootstrap as well. For this option, set rows = FALSE.
@

\noindent We can extract the resampling distributions for the coefficient estimates using the \verb|samples| function:
<<>>=
samples(boot_out, name = "coef")[, 1:5]
@
\noindent We can plot these as follows:
<<fig.width = 6, fig.height = 2.25,fig.align='center'>>=
boot_pctiles <- boot_out |>
  perc() |>
  t() |>
  as.data.frame() |>
  rownames_to_column(var = "var") |>
  filter(var != "(Intercept)")

samples(boot_out, name = "coef") |>
  as.data.frame() |>
  rownames_to_column(var = "var") |>
  filter(var != "(Intercept)") |>
  pivot_longer(-var, names_to = "resample", values_to = "coefficient") |>
  group_by(var) |>
  ggplot(aes(x = coefficient)) +
  geom_histogram(bins = 30, colour = "black") +
  geom_vline(aes(xintercept = `2.5%`), data = boot_pctiles, linetype = "dashed") +
  geom_vline(aes(xintercept = `97.5%`), data = boot_pctiles, linetype = "dashed") +
  facet_wrap(~var, scales = "free")
@

In this case, the bootstrap sampling distributions look roughly normal.

\subsection{Group-correlated errors}

Credit for this data example: \url{https://www.r-bloggers.com/2021/05/clustered-standard-errors-with-r/}.

Let's consider the \verb|nslwork| data from the \verb|webuse| package:
<<>>=
library(webuse)
nlswork_orig <- webuse("nlswork")
nlswork <- nlswork_orig |>
  filter(idcode <= 100) |>
  select(idcode, year, ln_wage, age, tenure, union) |>
  na.omit() |>
  mutate(
    union = as.integer(union),
    idcode = as.factor(idcode)
  )
nlswork
@

The data comes from the US National Longitudinal Survey (NLS) and contains information about more than 4,000 young working women. We’re interested in the relationship between wage (here as log-scaled GNP-adjusted wage) \verb|ln_wage| and survey participant’s current age, job tenure in years and union membership as independent variables. It’s a longitudinal survey, so subjects were asked repeatedly between 1968 and 1988 and each subject is identified by an unique idcode \verb|idcode|. Here we restrict attention to the first 100 subjects, and remove any rows with missing data.

Let's start by fitting a linear regression of the log wage on \verb|age|, \verb|tenure|, \verb|union|, and the interaction between \verb|tenure| and \verb|union|:
<<>>=
lm_fit <- lm(ln_wage ~ age + tenure + union + tenure:union,
  data = nlswork
)
summary(lm_fit)
@
Let's plot the residuals against the individuals:
<<fig.width = 4, fig.height = 3,fig.align='center'>>=
nlswork |>
  mutate(resid = lm_fit$residuals) |>
  ggplot(aes(x = idcode, y = resid)) +
  geom_boxplot() +
  labs(
    x = "Subject",
    y = "Residual"
  ) +
  theme(axis.text.x = element_blank())
@

Clearly, there is dependency among the residuals within subjects. Therefore, we have either model bias, or correlated errors, or both. To help assess whether we have model bias or not, we must check whether the variables of interest are correlated with the grouping variable \verb|idcode|. We can check this with a plot, e.g. for the \verb|tenure| variable:
<<fig.width = 4, fig.height = 3,fig.align='center'>>=
nlswork |>
  ggplot(aes(x = idcode, y = tenure)) +
  geom_boxplot() +
  labs(
    x = "Subject",
    y = "Tenure"
  ) +
  theme(axis.text.x = element_blank())
@

Again, there seems to be nontrivial association between \verb|tenure| and \verb|idcode|. We can check this more formally with an ANOVA test:
<<>>=
summary(aov(tenure ~ idcode, data = nlswork))
@
So in this case we do have model bias on our hands. We can address this using fixed effects for each subject.
<<>>=
lm_fit_FE <- lm(ln_wage ~ age + tenure + union + tenure:union + idcode,
  data = nlswork
)
lm_fit_FE |>
  summary() |>
  coef() |>
  as.data.frame() |>
  rownames_to_column(var = "var") |>
  filter(!grepl("idcode", var)) |> # remove coefficients for fixed effects
  column_to_rownames(var = "var")
@
Note the changes in the standard errors and p-values. Sometimes, we may have remaining correlation among residuals even after adding cluster fixed effects. Therefore, it is common practice to compute clustered (i.e. Liang-Zeger) standard errors in conjunction with cluster fixed effects. We can get clustered standard errors via the \verb|vcovCL| function from \verb|sandwich|:
<<>>=
LZ_cov <- vcovCL(lm_fit_FE, cluster = nlswork$idcode)
coeftest(lm_fit_FE, vcov. = LZ_cov)[, ] |>
  as.data.frame() |>
  rownames_to_column(var = "var") |>
  filter(!grepl("idcode", var)) |> # remove coefficients for fixed effects
  column_to_rownames(var = "var")
@
Again, note the changes in the standard errors and p-values.

\subsection{Autocorrelated errors}

Let's take a look at the \verb|EuStockMarkets| data built into R, containing the daily closing prices of major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. Let's regress \verb|DAX| on \verb|FTSE| and take a look at the residuals:
<<>>=
lm_fit <- lm(DAX ~ FTSE, data = EuStockMarkets)
summary(lm_fit)
@
We find an extremely significant association between the two stock indices. But let's examine the residuals for autocorrelation:
<<fig.width = 4, fig.height = 3,fig.align='center'>>=
EuStockMarkets |>
  as.data.frame() |>
  mutate(
    date = row_number(),
    resid = lm_fit$residuals
  ) |>
  ggplot(aes(x = date, y = resid)) +
  geom_line() +
  labs(
    x = "Day",
    y = "Residual"
  )
@

There is clearly some autocorrelation in the residuals. Let's quantify it using the autocorrelation function (\verb|acf()| in R):
<<fig.width = 4, fig.height = 4,fig.align='center'>>=
acf(lm_fit$residuals, lag.max = 1000)
@
We see that the autocorrelation gets into a reasonably low range around lag 200. We can then construct Newey-West standard errors based on this lag:
<<>>=
NW_cov <- NeweyWest(lm_fit)
coeftest(lm_fit, vcov. = NW_cov)
@
We see that the p-value for the association goes from \verb|2e-16| to \verb|0.46|, after accounting for autocorrelation.

\subsection{Outliers}

Let's take a look at the crime data from HW2:
<<message=FALSE>>=
# read crime data
crime_data <- read_tsv("data/Statewide_crime.dat")

# read and transform population data
population_data <- read_csv("data/state-populations.csv")
population_data <- population_data |>
  filter(State != "Puerto Rico") |>
  select(State, Pop) |>
  rename(state_name = State, state_pop = Pop)

# collate state abbreviations
state_abbreviations <- tibble(
  state_name = state.name,
  state_abbrev = state.abb
) |>
  add_row(state_name = "District of Columbia", state_abbrev = "DC")

# add CrimeRate to crime_data
crime_data <- crime_data |>
  mutate(STATE = ifelse(STATE == "IO", "IA", STATE)) |>
  rename(state_abbrev = STATE) |>
  left_join(state_abbreviations, by = "state_abbrev") |>
  left_join(population_data, by = "state_name") |>
  mutate(CrimeRate = Violent / state_pop) |>
  select(state_abbrev, CrimeRate, Metro, HighSchool, Poverty)

crime_data
@

Let's fit the linear regression:
<<>>=
# note: we make the state abbreviations row names for better diagnostic plots
lm_fit <- lm(CrimeRate ~ Metro + HighSchool + Poverty,
  data = crime_data |> column_to_rownames(var = "state_abbrev")
)
@

We can get the standard linear regression diagnostic plots as follows:
<<fig.width = 4, fig.height = 4, fig.align='center'>>=
# residuals versus fitted
plot(lm_fit, which = 1)

# residual QQ plotxw
plot(lm_fit, which = 2)

# residuals versus leverage (with Cook's distance)
plot(lm_fit, which = 5)
@

The information underlying these diagnostic plots can be extracted as follows:
<<>>=
tibble(
  state = crime_data$state_abbrev,
  std_residual = rstandard(lm_fit),
  fitted_value = fitted.values(lm_fit),
  leverage = hatvalues(lm_fit),
  cooks_dist = cooks.distance(lm_fit)
)
@

Clearly DC is an outlier. We can either run a robust estimation procedure or we can redo the analysis without DC. Let's try both. First, we try robust regression using \verb|rlm()| from the \verb|MASS| package:
<<>>=
rlm_fit <- MASS::rlm(CrimeRate ~ Metro + HighSchool + Poverty, data = crime_data)
summary(rlm_fit)
@
For some reason, the p-values are not computed automatically. We can compute them ourselves instead:
<<>>=
summary(rlm_fit)$coef |>
  as.data.frame() |>
  rename(Estimate = Value) |>
  mutate(`p value` = 2 * dnorm(-abs(`t value`)))
@
To see the robust estimation action visually, let's consider a univariate example:
<<>>=
lm_fit <- lm(CrimeRate ~ Metro, data = crime_data)
rlm_fit <- MASS::rlm(CrimeRate ~ Metro, data = crime_data)

# collate the fits into a tibble
line_fits <- tibble(
  method = c("Usual", "Robust"),
  intercept = c(
    coef(lm_fit)["(Intercept)"],
    coef(rlm_fit)["(Intercept)"]
  ),
  slope = c(
    coef(lm_fit)["Metro"],
    coef(rlm_fit)["Metro"]
  )
)
@

<<fig.width = 4, fig.height = 3, fig.align='center'>>=
# usual and robust univariate fits
# plot the fits
crime_data |>
  ggplot() +
  geom_point(aes(x = Metro, y = CrimeRate)) +
  geom_abline(aes(intercept = intercept, slope = slope, colour = method),
    data = line_fits
  )
@

Next, let's try removing DC and running a usual linear regression.
<<fig.width = 4, fig.height = 4, fig.align='center'>>=
lm_fit_no_dc <- lm(CrimeRate ~ Metro + HighSchool + Poverty,
  data = crime_data |>
    filter(state_abbrev != "DC") |>
    column_to_rownames(var = "state_abbrev")
)

# residuals versus fitted
plot(lm_fit_no_dc, which = 1)

# residual QQ plot
plot(lm_fit_no_dc, which = 2)

# residuals versus leverage (with Cook's distance)
plot(lm_fit_no_dc, which = 5)
@

\chapter{Generalized linear models: General theory} \label{ch:glm-theory}

Chapters 1-3 focused on the most common class of models used in applications: linear models. Despite their versatility, linear models do not apply in all situations. In particular, they are not designed to deal with binary or count responses. In Chapter 4, we introduce \textit{generalized linear models} (GLMs), a generalization of linear models that encompasses a wide variety of incredibly useful models including logistic regression and Poisson regression.

We'll start Chapter 4 by introducing exponential dispersion models (Section~\ref{sec:edm}), a generalization of the Gaussian distribution that serves as the backbone of GLMs. Then we formally define a GLM, demonstrating logistic regression and Poisson regression as special cases (Section~\ref{sec:glm-def}). Next we discuss maximum likelihood inference in GLMs (Section~\ref{sec:glm-max-lik}). Finally, we discuss how to carry out statistical inference in GLMs (Section~\ref{sec:glm-inf}).

\section{Exponential dispersion model (EDM) distributions} \label{sec:edm}

\subsection{Definition}

Let's start with the Gaussian distribution. If $y \sim N(\mu, \sigma^2)$, then it has the following density with respect to the Lebesgue measure $\nu$ on $\mathbb R$:
\begin{equation}
f_{\mu, \sigma^2}(y) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y-\mu)^2\right) = \exp\left(\frac{\mu y - \frac{1}{2}\mu^2}{\sigma^2}\right) \cdot \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac1{2\sigma^2} y^2\right).
\end{equation}
We can consider a more general class of densities with respect to any measure $\nu$:
\begin{equation}
f_{\theta, \phi}(y) \equiv \exp\left(\frac{\theta y - \psi(\theta)}{\phi}\right)h(y, \phi), \quad \theta \in \Theta \subseteq \mathbb R, \ \phi > 0.
\label{eq:edm-def}
\end{equation}
Here $\theta$ is called the \textit{natural parameter}, $\psi$ is called the \textit{log-partition function}, $\Theta \equiv \{\theta: \psi(\theta) < \infty\}$ is called the natural parameter space, $\phi > 0$ is called the \textit{dispersion parameter}, and $h$ is called the \textit{base density}. The distribution with density $f_{\theta, \phi}$ with respect to a measure $\nu$ on $\mathbb R$ is called an \textit{exponential dispersion model} (EDM). Sometimes, we parameterize this distribution using its mean and dispersion, writing
\begin{equation}
y \sim \text{EDM}(\mu, \phi).
\end{equation}
When $\phi = 1$, the distribution becomes a \textit{one-parameter natural exponential family}. The support of an EDM distribution remains fixed as $(\theta, \phi)$ vary.

\subsection{Examples}

\paragraph{Normal distribution.}

As derived above, $y \sim N(\mu, \sigma^2)$ is an EDM with
\begin{equation}
\theta = \mu, \quad \psi(\theta) = -\frac 12 \theta^2, \quad \phi = \sigma^2, \quad h(y, \phi) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac1{2\sigma^2} y^2\right).
\end{equation}

\paragraph{Bernoulli distribution.}

Suppose $y \sim \text{Ber}(\mu)$. Then, we have
\begin{equation}
f(y) = \mu^{y}(1-\mu)^{1-y} = \exp\left(y \log \frac{\mu}{1-\mu} + \log(1-\mu) \right).
\end{equation}
Therefore, we have $\theta = \log \frac{\mu}{1-\mu}$, so that $\log(1-\mu) = -\log(1+e^\theta)$. It follows that
\begin{equation}
\theta = \log \frac{\mu}{1-\mu}, \quad \psi(\theta) = \log(1+e^\theta), \quad \phi = 1, \quad h(y) = 1.
\end{equation}
Hence, the Bernoulli distribution is an EDM, as well as a one-parameter exponential family. Note that $\text{Ber}(0)$ and $\text{Ber}(1)$ are not included in this class of EDMs, because there is no $\theta \in \Theta = \mathbb R$ that gives rise to $\mu = 0$ or $\mu = 1$. Hence, $\mu \in (0,1)$, and the support of any Bernoulli EDM is $\{0,1\}$.

\paragraph{Binomial distribution.}

Consider the binomial proportion $y$: $my \sim \text{Bin}(m, \mu)$. We have
\begin{equation}
f(y) = {m \choose my}\mu^{my}(1-\mu)^{m(1-y)} = \exp\left(m\left(y \log \frac{\mu}{1-\mu} + \log(1-\mu)\right)\right){m \choose my},
\end{equation}
so
\begin{equation}
\theta = \log\frac{\mu}{1-\mu}, \quad \psi(\theta) = \frac{e^{\theta}}{1 + e^{\theta}}, \quad \phi = 1/m, \quad h(y, \phi) = {m \choose my}.
\end{equation}
Note that $\text{Bin}(m, 0)$ and $\text{Bin}(m, 1)$ are not included in this class of EDMs, for the same reason as above. Hence, $\mu \in (0,1)$, and the support of any binomial EDM is $\{0,\frac{1}{m}, \frac{2}{m}, \dots, 1\}$.

\paragraph{Poisson distribution.}

Suppose $y \sim \text{Poi}(\mu)$. We have
\begin{equation}
f(y) = e^{-\mu}\frac{\mu^y}{y!} = \exp(y \log \mu - \mu)\frac{1}{y!}.
\end{equation}
Therefore, we have $\theta = \log \mu$, so that $\mu = e^\theta$. It follows that
\begin{equation}
\theta = \log \mu, \quad \psi(\theta) = e^\theta,\quad \phi = 1, \quad h(y) = \frac{1}{y!}.
\end{equation}
Hence, the Poisson distribution is an EDM, as well as a one-parameter exponential family. Note that $\text{Poi}(0)$is not included in this class of EDMs, because there is no $\theta \in \Theta = \mathbb R$ that gives rise to $\mu = 0$. Hence, $\mu \in (0,\infty)$, and the support of any Poisson EDM is $\mathbb N$.

\paragraph{}

Many other examples fall into this class, including the negative binomial, gamma, and inverse-Gaussian distributions. We will see at least some of these in the next chapter.

\subsection{Moments of exponential dispersion model distributions.}

It turns out that the derivatives of the log-partition function $\psi$ give the moments of $y$. Indeed, let's start with the relationship
\begin{equation}
\int f_{\theta, \phi}(y)d\nu(y) = \int \exp\left(\frac{\theta y - \psi(\theta)}{\phi}\right)h(y, \phi) d\nu(y) = 1.
\end{equation}
Differentiating in $\theta$ and interchanging the derivative and the integral, we obtain
\begin{equation}
0 = \frac{d}{d\theta} \int f_{\theta, \phi}(y)dy = \int \frac{y - \dot \psi(\theta)}{\phi}f_{\theta, \phi}(y) dy,
\end{equation}
from which it follows that
\begin{equation}
\dot \psi(\theta) = \int \dot \psi(\theta)f_{\theta, \phi}(y)dy = \int y f_{\theta, \phi}(y)dy = \mathbb E[y] \equiv \mu.
\label{eq:psi-dot}
\end{equation}
Thus, the first derivative of the log partition function is the mean of $y$. Differentiating again, we get
\begin{equation}
\phi \cdot \ddot \psi(\theta) = \phi \int \ddot \psi(\theta) f_{\theta, \phi}(y) d\nu(y) = \int (y - \dot \psi(\theta))^2 f_{\theta, \phi}(y) dy = \int (y - \mu)^2f_{\theta, \phi}(y) d\nu(y) = \text{Var}[y].
\end{equation}
Thus, the second derivative of the log-partition function multiplied by the dispersion parameter is the variance of $y$.

\subsection{Relationships among the mean, variance, and natural parameter}

\paragraph{Relationship between the mean and the natural parameter.}

The log-partition function $\psi$ induces a connection~\eqref{eq:psi-dot} between the natural parameter $\theta$ and the mean $\mu$. Because
\begin{equation}
\frac{d\mu}{d\theta} = \frac{d}{d\theta}\dot \psi(\theta) = \ddot \psi(\theta) = \frac{1}{\phi}\text{Var}[y] > 0,
\label{eq:dmu-dtheta}
\end{equation}
it follows that $\mu$ is a strictly increasing function of $\theta$, so in particular the mapping between $\mu$ and $\theta$ is bijective. Therefore, we can think of equivalently parameterizing the distribution via $\mu$ or $\theta$. % In the context of GLMs (see Section~\ref{sec:glm-def}), the mean-variance relationship is quantified in terms of the \textit{canonical link function} $g$, which maps the mean to the natural parameter:
% \begin{equation}
% \theta = \dot \psi^{-1}(\mu) \equiv g(\mu).
% \end{equation}

\paragraph{Relationship between the mean and variance.}

Note that the mean of an EDM, together with the dispersion parameter, determines its variance (since it determines the natural parameter $\theta$). Define
\begin{equation}
V(\mu) \equiv \frac{d\mu}{d\theta},
\end{equation}
so that $\text{Var}[y] = \phi V(\mu)$. For example, a Poisson random variable with mean $\mu$ has variance $\mu$ and a Bernoulli random variable with mean $\mu$ has $V(\mu) = \mu(1-\mu)$. The mean-variance relationship turns out to characterize the EDM, i.e. an EDM with mean equal to its variance is the Poisson distribution. For all EDMs except the normal distribution, the variance depends nontrivially on the mean. Therefore, heteroskedasticity is a natural feature of EDMs (rather than a pathology that needs to be corrected for).

\subsection{The unit deviance}

It's possible to rewrite the EDM distribution in terms of $\mu$ rather than in terms of $\theta$. Take the quantity in the numerator of the exponential in the EDM density~\eqref{eq:edm-def} and call it $t(y, \mu)$:
\begin{equation}
t(y, \mu) \equiv \theta y - \psi(\theta).
\end{equation}
Let's consider the shape of this function by taking the first two derivatives with respect to $\theta$:
\begin{equation}
\frac{\partial}{\partial \theta}t(y, \mu) = y - \mu
\end{equation}
and
\begin{equation}
  \frac{\partial^2}{\partial \theta^2}t(y, \mu) = -V(\mu) < 0.
\end{equation}
Hence, $t(y, \mu)$ has a unique global maximum at $\mu = y$. We can then define the \textit{unit deviance} $d(y, \mu)$ as twice the distance between the value $t(y, \mu)$ and the optimal value $t(y,y)$:
\begin{equation}
d(y, \mu) \equiv 2(t(y, y) - t(y, \mu)).
\end{equation}
The unit deviance is nonnegative, and minimized by $\mu = y$. For the normal distribution, the unit deviance is $d(y, \mu) = (y - \mu)^2$. The unit deviance can therefore be viewed as a  ``distance'' between the mean $\mu$ and the observation $y$.

\paragraph{Example.}

For the Poisson distribution, we have $t(y, \mu) = y\log \mu - \mu$, so
\begin{equation}
d(y, \mu) \equiv 2(t(y, y) - t(y, \mu)) = 2(y\log y - y - y \log \mu + \mu) = 2\left(y\log \frac{y}{\mu} - (y - \mu)\right).
\end{equation}
See Figure~\ref{fig:poisson-unit-deviance} for an example of the shape of this function.
\begin{figure}[h!]
\centering
\includegraphics{figures/poisson-unit-deviance.pdf}
\caption{The Poisson unit deviance for $y = 4$.}
\label{fig:poisson-unit-deviance}
\end{figure}
Note that the Poisson deviance is asymmetric about $\mu = y$. This is a consequence of the nontrivial mean-variance relationship for the Poisson distribution. In particular, the Poisson distribution's variance grows with its mean. Therefore, an observation of $y = 4$ is less likely to have come from a Poisson distribution with mean $\mu = 2$ than from a Poisson distribution with mean $\mu = 6$. 

\subsection{Small-dispersion approximations to an EDM}

If the dispersion $\phi$ is small, then that means that $y$ is a fairly precise estimate of $\mu$, similar to an average of multiple independent samples from a mean-$\mu$ distribution. Consider, for example, that $\frac{1}{m}\text{Bin}(m, \mu)$ is the mean of $m$ i.i.d. draws from $\text{Ber}(\mu)$. In this case, we can use either the normal approximation or the saddlepoint approximation to approximate the EDM density. For the sake of this section, we will abuse notation by denoting by $f_{\mu, \phi}$ the EDM with mean $\mu$ and dispersion $\phi$.

\subsubsection{The normal approximation}

\paragraph{The approximation.}

For small values of $\phi$, we can hope to approximate $f_{\mu, \phi}$ with a normal distribution. Recall that the mean and variance of this distribution are $\mu$ and $\phi \cdot V(\mu)$, respectively. The central limit theorem gives
\begin{equation}
\frac{y - \mu}{\sqrt{\phi \cdot V(\mu)}} \rightarrow_d N(0,1) \quad \text{as} \quad \phi \rightarrow 0,
\end{equation}
so
\begin{equation}
y \overset \cdot \sim N(\mu, \phi \cdot V(\mu)) \equiv \tilde f^{\text{normal}}_{\mu, \phi}.
\end{equation}
For example, we have
\begin{equation}
\text{Poi}(\mu) \approx N(\mu, \mu).
\end{equation}
For the normal EDM, note that the normal approximation is exact. One consequence of the normal approximation is 
\begin{equation}
\frac{(y - \mu)^2}{\phi \cdot V(\mu)} \overset \cdot \sim \chi^2_1.
\end{equation}
This fact will be useful to us as we carry out inference for GLMs.


\paragraph{Approximation accuracy.}

We have
\begin{equation}
\tilde f^{\text{normal}}_{\mu, \phi}(y) = f_{\mu, \phi}(y) + O(\sqrt{\phi}).
\end{equation}
In practice, the rule of thumb for the applicability of this approximation is that
\begin{equation}
\tau \equiv \frac{\phi \cdot V(\mu)}{(\mu - \text{boundary})^2} \leq \frac{1}{5}.
\end{equation}
Here, ``boundary'' represents the nearest boundary of the parameter space to $\mu$. For example, if $y \sim \frac{1}{m}\text{Bin}(m, \mu)$, then we have
\begin{equation}
\tau = \frac{\frac{1}{m} \cdot \mu \cdot (1-\mu)}{\min(\mu, 1-\mu)^2} = \frac{1}{m} \cdot \max\left(\frac{\mu}{1-\mu}, \frac{1-\mu}{\mu}\right) \approx \frac{1}{m} \cdot \max\left(\frac 1 \mu, \frac 1 {1-\mu}\right),
\end{equation}
so $\tau \leq 1/5$ roughly if $m \mu \leq 5$ and $m (1-\mu) \leq 5$. For Poisson distributions, we always have $\tau = 1$, but for some reason small-dispersion asymptotics still applies as $\mu \rightarrow \infty$ as opposed to $\tau \rightarrow 0$. The criterion $\tau \leq 1/5$ is satisfied when $\mu \leq 5$. 

\subsubsection{The saddlepoint approximation}


\paragraph{The approximation.}

Another approximation to the EDM density is the saddlepoint approximation, which tends to be more accurate than the normal approximation. The reason the normal approximation may be inaccurate is that the quality of the central limit approximation degrades as one enters the tails of the distribution. In particular, the normal approximation to $f_{\mu, \phi}(y)$ may be poor if $\mu$ is far from $y$. The saddlepoint approximation is build on the observation that the EDM density for $f_{\mu, \phi}(y)$ can be written in terms of the density $f_{y, \phi}(y)$; the latter density is by definition evaluated at its mean. Indeed,
\begin{equation}
\begin{split}
f_{\mu, \phi}(y) &\equiv \exp\left(\frac{\theta y - \psi(\theta)}{\phi}\right)h(y, \phi) \\
&= \exp\left(\frac{t(y, \mu)}{\phi}\right)h(y, \phi) \\
&= \exp\left(\frac{-2(t(y, y) - t(y, \mu)) + 2t(y,y)}{2\phi}\right)h(y, \phi) \\
&= \exp\left(-\frac{d(y, \mu)}{2\phi}\right)\exp\left(\frac{t(y,y)}{\phi}\right) h(y, \phi) \\
&= \exp\left(-\frac{d(y, \mu)}{2\phi}\right)f_{y, \phi}(y).
\label{eq:deviance-form}
\end{split}
\end{equation}
Now, we apply the central limit theorem to approximate $f_{y, \phi}(y)$:
\begin{equation}
f_{y, \phi}(y) \approx \frac{1}{\sqrt{2\pi \phi V(y)}}.
\end{equation}
Substituting this approximation into~\eqref{eq:deviance-form}, we obtain the \textit{saddlepoint approximation}:
\begin{equation}
f_{\mu, \phi}(y) \approx \frac{1}{\sqrt{2\pi \phi V(y)}}\exp\left(-\frac{d(y, \mu)}{2\phi}\right) \equiv \widetilde f^{\text{saddle}}_{\mu, \phi}(y).
\end{equation}
For the normal EDM, note that the normal approximation is exact. For the Poisson distribution, we get
\begin{equation}
\widetilde f^{\text{saddle}}_{\mu, \phi}(y) = \frac{1}{\sqrt{2\pi y}}\exp\left(y \log \frac y \mu - (y - \mu)\right).
\end{equation}

The approximation can be shown to lead to the following consequence:
\begin{equation}
\frac{d(y, \mu)}{\phi} \overset \cdot \sim \chi^2_1.
\end{equation}
Here, we are using the unit deviance rather than the squared distance to measure the deviation of $\mu$ from $y$. This fact will be useful to us as we carry out inference for GLMs.

\paragraph{Approximation accuracy.}

We have still used a normal approximation, but this time we have used it to approximate $f_{y, \phi}(y)$ instead of $f_{\mu, \phi}(y)$. Since the normal approximation is applied to a distribution ($f_{y, \phi}$) at its mean, we expect it to be more accurate than a normal approximation applied to a distribution ($f_{\mu, \phi}$) at a point potentially far from its mean. The saddlepoint approximation yields an approximation to the density that is \textit{multiplicative} rather than \textit{additive}, and of order $O(\phi)$ rather than $O(\sqrt{\phi})$:
\begin{equation}
\tilde f^{\text{saddle}}_{\mu, \phi}(y) = f_{\mu, \phi}(y) \cdot (1 + O(\phi)).
\end{equation}
In practice, the rule of thumb for the applicability of this approximation is that $\tau \leq 1/3$; the looser requirement on $\tau$ reflects the greater accuracy of the saddlepoint approximation. This translates to $m\mu \geq 3$ and $m(1-\mu) \geq 3$ for the binomial and $\mu \geq 3$ for the Poisson.

\subsubsection{Comparing the two approximations}

The saddlepoint approximation is more accurate than the normal approximation, as discussed above. However, the accuracy of the saddlepoint approximation relies on the assumption that the entire parametric form of the EDM is correctly specified. On the other hand, the accuracy of the normal distribution requires only that the first two moments of the EDM are correctly specified.

\section{Generalized linear models and examples} \label{sec:glm-def}

In this class, the focus is on building models that tie a vector of predictors $(\bm x_{i*})$ to a response $y_i$. For linear regression, the mean of $y$ was modeled as a linear combination of the predictors $\bm x_{i*}^T \bm \beta$: $\mu_i = \bm x_{i*}^T \bm \beta$. More generally, we might want to model \textit{a function} of the mean $\eta_i = g(\mu_i)$ as a linear combination of the predictors; $g$ is called the \textit{link function} and $\eta_i$ the \textit{linear predictor}. Pairing a link function with an EDM gives us a \textit{generalized linear model} (GLM):

\paragraph{Definition.}

We define $\{(y_i, \bm x_{i*})\}_{i = 1}^n$ as following a generalized linear model based on the exponential dispersion model $f_{\theta, \phi}$, monotonic and differentiable link function $g$, and observation weights $w_i$ if
\begin{equation}
y_i \overset{\text{ind}} \sim \text{EDM}(\mu_i, \phi_0/w_i), \quad \eta_i \equiv g(\mu_i) = o_i + \bm x^T_{i*}\bm \beta.
\label{eq:glm-def}
\end{equation}
The offset terms $o_i$ and observation weights $w_i$ are both known in advance. The free parameters in a GLM are the coefficients $\bm \beta$ and, possibly, the parameter $\phi_0$ controlling the dispersion. We will see examples where $\phi_0$ is known (e.g. Poisson regression) and those where $\phi_0$ is unknown (e.g. linear regression).

The ``default'' choice for the link function $g$ is the \textit{canonical link function}
\begin{equation}
  g(\mu) = \dot \psi^{-1}(\mu),
\end{equation}
which, given the relationship~\eqref{eq:psi-dot}, gives $\eta = \dot \psi^{-1}(\mu) = \theta$, i.e. the linear predictor coincides with the natural parameter. As discussed in the context of equation~\eqref{eq:dmu-dtheta}, $\dot \psi^{-1}$ is a valid link function because it is monotonic and differentiable. Canonical link functions are very commonly used with EDMs because they lead to various nice properties that general EDMs do not enjoy (e.g. concave log-likelihood).

\paragraph{Example: Linear regression model.}

The linear regression models is a special case of a GLM, with $\phi_0 = \sigma^2$ (unknown), $w_i = 1$, $o_i = 0$, and identity (canonical) link function:
\begin{equation}
  y_i \overset{\text{ind}}\sim N(\mu_i, \sigma^2); \quad \eta_i = \mu_i = \bm x_{i*}^T \bm \beta.
\end{equation}

\paragraph{Example: Weighted linear regression model.}

If each observation $y_i$ is the mean of $m_i$ independent repeated observations, then we get a weighted linear regression model, with $\phi_0 = \sigma^2$ (unknown), $w_i = m_i$, $o_i = 0$, and identity (canonical) link function:
\begin{equation}
  y_i \overset{\text{ind}}\sim N(\mu_i, {\textstyle \frac{\sigma^2}{m_i}}); \quad \eta_i = \mu_i = \bm x_{i*}^T \bm \beta.
\end{equation}

\paragraph{Example: Ungrouped logistic regression model.} The \textit{(ungrouped) logistic regression model} is the GLM based on the Bernoulli EDM with $\phi_0 = 1$ (known), $w_i = 1$, $o_i = 0$, and the canonical link function:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Ber}(\mu_i); \quad \eta_i = \theta_i = \log\frac{\mu_i}{1-\mu_i} = \bm x_{i*}^T \bm \beta.
\end{equation}
Thus the canonical link function for logistic regression is the \textit{logistic link function} $g(\mu) = \log \frac{\mu}{1-\mu}$.

\paragraph{Example: Grouped logistic regression model.}

Suppose $y_i$ is a binomial proportion based on $m_i$ trials. The \textit{(grouped) logistic regression model} is the GLM based on the binomial EDM with $\phi_0 = 1$ (known), $w_i = 1/m_i$, $o_i = 0$, and the canonical link function:
\begin{equation}
  m_i y_i \sim \text{Bin}(m_i, \mu_i); \quad \eta_i = \log \frac{\mu_i}{1-\mu_i} = o_i + \bm x^T_{i*}\bm \beta.
\end{equation}
Note that a binomial proportion $y_i$ based on on $m_i$ trials and a success probability of $\mu_i$ can be equivalently represented as $m_i$ independent Bernoulli draws with the same success probability $\mu_i$. Therefore, any grouped logistic regression model can be equivalently represented as an ungrouped logistic regression model with $\sum_{i = 1}^n m_i$ observations. We will see that, despite this equivalence, grouped logistic regression models have some useful properties that ungrouped logistic regression models do not.

\paragraph{Example: Poisson regression model.}

\textit{Poisson regression} is the Poisson EDM with $\phi_0 = 1$ (known), $w_i = 1$, $o_i = 0$, and the canonical link function:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \eta_i = \theta_i = \log \mu_i = \bm x_{i*}^T \bm \beta.
\end{equation}
Thus the canonical link function for Poisson regression is the \textit{log link function} $g(\mu) = \log \mu$.

\section{Parameter estimation in GLMs} \label{sec:glm-max-lik}

\subsection{The GLM likelihood, score, and Fisher information}

The log-likelihood of a GLM is
\begin{equation}
\log \mathcal L(\bm \beta) = \sum_{i = 1}^n \frac{\theta_i y_i - \psi(\theta_i)}{\phi_0/w_i} + \sum_{i = 1}^n \log h(y_i, \phi_0/w_i).
\label{eq:glm-log-likelihood}
\end{equation}
Let's differentiate this with respect to $\bm \beta$, using the chain rule:
\begin{equation}
\begin{split}
  \frac{\partial \log \mathcal L(\bm \beta)}{\partial \bm \beta} &= \frac{\partial \log \mathcal L(\bm \beta)}{\partial \bm \theta}\frac{\partial \bm \theta}{\partial \bm \mu} \frac{\partial \bm \mu}{\partial \bm \eta}\frac{\partial \bm \eta}{\partial \bm \beta} \\
  &=  (\bm y - \bm \mu)^T \text{diag}(\phi_0/w_i)^{-1} \cdot \text{diag}(\ddot \psi(\theta_i))^{-1} \cdot \text{diag}\left(\frac{\partial\mu_i}{\partial \eta_i}\right) \cdot \bm X\\
  &= \frac{1}{\phi_0}(\bm y - \bm \mu)^T \text{diag}\left(\frac{w_i}{V(\mu_i)(d\eta_i/d\mu_i)^2}\right)\cdot \text{diag}\left(\frac{\partial\eta_i}{\partial \mu_i}\right) \cdot \bm X \\
  &\equiv \frac{1}{\phi_0}(\bm y - \bm \mu)^T \bm W \bm M \bm X.
\end{split}
\end{equation}
Here, $\bm W \equiv \text{diag}(W_i)$ is a diagonal matrix of \textit{working weights} and $\bm M \equiv \text{diag}\left(\frac{\partial\eta_i}{\partial \mu_i}\right) = \text{diag}(g'(\mu_i))$ is a diagonal matrix of link derivatives. Transposing, we get the score vector
\begin{equation}
\bm U(\bm \beta) = \frac{1}{\phi_0}\bm X^T \bm M \bm W (\bm y - \bm \mu).
\label{eq:glm-score}
\end{equation}
To get the Fisher information matrix, note first that
\begin{equation}
\text{Var}[\bm y] = \text{diag}\left(\phi_0\frac{V(\mu_i)}{w_i}\right) = \phi_0 \bm W^{-1} \bm M^{-2}
\label{eq:glm-variance-y}
\end{equation}
we can compute the covariance matrix of the score vector:
\begin{equation}
  \begin{split}
\bm I(\bm \beta) = \text{Var}[\bm U(\bm \beta)] &= \frac{1}{\phi^2_0}\bm X^T \bm M \bm W \text{Var}[\bm y] \bm M \bm W \bm X \\
&= \frac{1}{\phi^2_0}\bm X^T \bm M \bm W \phi_0 \bm W^{-1}\bm M^{-2} \bm M \bm W \bm X \\
&= \frac{1}{\phi_0}\bm X^T \bm W \bm X.
\label{eq:glm-fisher-info}
  \end{split}
\end{equation}

\subsection{Maximum likelihood estimation of $\bm \beta$}

To estimate $\bm \beta$, we can set the score vector to zero:
\begin{equation}
  \frac{1}{\phi_0}\bm X^T \widehat{\bm M} \widehat{\bm W} (\bm y - \widehat{\bm \mu}) = 0 \quad \Longleftrightarrow \quad \bm X^T \text{diag}\left(\frac{w_i}{V(\widehat \mu_i)g'(\widehat \mu_i)}\right)(\bm y - \widehat{\bm \mu}) = 0.
\end{equation}
These equations are called the \textit{normal equations}. Unfortunately, unlike least squares, the normal equations cannot be solved analytically for $\widehat{\bm \beta}$. They are solved numerically instead; see Section~\ref{sec:irls}. Note that $\phi_0$ cancels from the normal equations, and therefore the coefficients $\bm{\beta}$ can be estimated without estimating the dispersion. Recall that we have seen this phenomenon for least squares. Also note that the normal equations simplify when the canonical link function is used, so that $\eta_i = \theta_i$. Assuming additionally that $w_i = 1$, we get
\begin{equation}
\bm {\widehat M} \bm {\widehat W} = \text{diag}\left(\frac{\widehat{\partial \mu_i/\partial \theta_i}}{V(\widehat \mu_i)}\right) = \frac{\ddot \psi(\widehat \theta_i)}{\ddot \psi(\widehat \theta_i)} = 1,
\end{equation}
so the normal equations reduce to
\begin{equation}
\bm X^T (\bm y - \widehat{\bm \mu}) = 0.
\label{eq:canonical-normal-equations}
\end{equation}
We recognize these as the normal equation for linear regression. Since both ungrouped logistic regression and Poisson regression also use canonical links and have unit weights, the simplified normal equations~\eqref{eq:canonical-normal-equations} apply to the latter regressions as well.

In the linear regression case, we interpreted the normal equations~\eqref{eq:canonical-normal-equations} as an orthogonality statement: $\bm y - \widehat{\bm \mu} \perp C(\bm X)$. In the case of GLMs, the $C(\bm X) \equiv \{\bm \mu = \mathbb E[\bm y]: \bm \beta \in \mathbb R^p\}$ is no longer a linear space. In fact, it is a nonlinear transformation of the column space of $\bm X$ (a $p$-dimensional manifold in $\mathbb R^n$):
\begin{equation}
C(\bm X) \equiv \{\bm \mu = \mathbb E[\bm y]: \bm \beta \in \mathbb R^p\} = \{g^{-1}(\bm X \bm \beta): \bm \beta \in \mathbb R^p\}.
\end{equation}
Therefore, we cannot view the mapping $\bm y \mapsto \bm{\widehat \mu}$ as a linear projection. Nevertheless, it is possible to interpret $\bm{\widehat \mu}$ as the ``closest'' point (in some sense) to $\bm y$ in $C(\bm X)$. To see this, recall the deviance form of the EDM density~\eqref{eq:deviance-form}. Taking a logarithm and summing over $i = 1, \dots, n$, we find the following expression for the negative log likelihood:
\begin{equation}
-\log \mathcal L(\bm \beta) = \sum_{i = 1}^n \frac{d(y_i, \mu_i)}{2\phi_i} + C = \frac{\sum_{i = 1}^n w_id(y_i, \mu_i)}{2\phi_0} + C  \equiv \frac{D(\bm y, \bm \mu)}{2\phi_0} + C \equiv \frac12 D^*(\bm y, \bm \mu) + C.
\label{eq:glm-likelihood-via-deviance}
\end{equation}
$D(\bm y, \bm \mu)$ is called the \textit{deviance} or the \textit{total deviance}, and it can be interpreted as a kind of distance between the mean vector $\bm \mu$ and the observation vector $\bm y$. For example, in the linear model case, $D(\bm y, \bm \mu) = \|\bm y - \bm \mu\|^2$. The quantity $D^*(\bm y, \bm \mu)$ is called the \textit{scaled deviance.} In the linear model case, $D(\bm y, \bm \mu) = \frac{\|\bm y - \bm \mu\|^2}{\sigma^2}$. Therefore, maximizing the GLM log likelihood is equivalent to minimizing the deviance:
\begin{equation}
\bm{\widehat \beta} = \underset{\bm \beta}{\arg \min}\ D(\bm y, \bm \mu(\bm \beta)), \quad \text{so that} \quad \bm{\widehat \mu} = \underset{\bm \mu \in C(\bm X)}{\arg \min}\ D(\bm y, \bm \mu).
\end{equation}



\subsection{Iteratively reweighted least squares} \label{sec:irls}

\paragraph{Log-concavity of GLM likelihood.} Before talking about maximizing the GLM log-likelihood, we investigate the concavity of this function. We claim that, in the case when the canonical link is used, $\log \mathcal L(\bm \beta)$ is a concave function of $\bm \beta$, which implies that this function is ``easy to optimize'', i.e. has no local maxima.

\begin{proposition} \label{prop:log-concavity}
If $g$ is the canonical link function, then the function $\log \mathcal L(\bm \beta)$ defined in~\eqref{eq:glm-log-likelihood} is concave in $\bm \beta$.
\end{proposition}
\begin{proof}
It suffices to show that $\psi$ is a convex function, since then $\log \mathcal L(\bm \beta)$ would be the sum of a linear function of $\bm \beta$ and the composition of a concave function with a linear function. To verify that $\psi$ is convex, it suffices to recall that $\ddot \psi(\theta) = \frac{1}{\phi}\text{Var}_\theta[y] > 0$.
\end{proof}
\noindent Proposition~\eqref{prop:log-concavity} gives us confidence that an iterative algorithm will converge to the global maximum of the likelihood. We present such an iterative algorithm next.

\paragraph{Newton-Raphson.}

We can maximize the log-likelihood~\eqref{eq:glm-log-likelihood} via the Newton Raphson algorithm, which involves the gradient and Hessian of the function we'd like to maximize. The gradient is the score vector~\eqref{eq:glm-score}, while the Hessian is the Fisher information~\eqref{eq:glm-fisher-info}. The Newton-Raphson iteration is therefore
\begin{equation}
  \begin{split}
    \bm{\widehat \beta}^{(t+1)} &= \bm{\widehat \beta}^{(t)} - (\nabla^2_{\bm \beta} \log \mathcal L(\bm{\widehat \beta}^{(t)}))^{-1} \nabla_{\bm \beta} \log \mathcal L(\bm{\widehat \beta}^{(t)}) \\
    &= \bm{\widehat \beta}^{(t)} + (\bm X^T \bm{\widehat W}^{(t)}\bm X)^{-1}\bm X^T\bm{\widehat W}^{(t)}\bm{\widehat M}^{(t)}(\bm y - \bm{\widehat \mu}^{(t)}).
  \end{split}
\label{eq:NR-iteration}
\end{equation}
See Figure~\ref{fig:newton-raphson}.

\begin{figure}[h!]
  \centering
  \includegraphics[width = 0.6\textwidth]{figures/newton-raphson.jpeg}
  \caption{Newton-Raphson iteratively approximates the log likelihood via a quadratic function and maximizing that function.}
  \label{fig:newton-raphson}
\end{figure}

\paragraph{Iteratively reweighted least squares (IRLS).}

A nice interpretation of the Newton-Raphson algorithm is as a sequence of weighted least squares fits, known as the iteratively reweighted least squares (IRLS) algorithm. Suppose that we have a current estimate $\bm{\widehat \beta}^{(t)}$, and suppose we are looking for a vector $\bm \beta$ near $\bm{\widehat \beta}^{(t)}$ that fits the model even better. We have
\begin{equation*}
\mathbb E_{\bm \beta}[\bm y] = g^{-1}(\bm X \bm \beta) \approx g^{-1}(\bm X \bm{\widehat \beta}^{(t)}) + \text{diag}(\partial \mu_i/\partial \eta_i)(\bm X \bm \beta - \bm X \bm{\widehat \beta}^{(t)}) = \bm{\widehat \mu}^{(t)} + (\bm{\widehat M}^{(t)})^{-1}(\bm X \bm \beta - \bm X \bm{\widehat \beta}^{(t)})
\end{equation*}
and
\begin{equation*}
\text{Var}_{\bm \beta}[\bm y] \approx \phi_0 (\bm{\widehat W}^{(t)})^{-1}(\bm{\widehat M}^{(t)})^{-2},
\end{equation*}
recalling equation~\eqref{eq:glm-variance-y}. Thus, up to the first two moments, near $\bm \beta = \bm{\widehat \beta}^{(t)}$ the distribution of $\bm y$ is approximately
\begin{equation}
\bm y = \bm{\widehat \mu}^{(t)} + (\bm{\widehat M}^{(t)})^{-1}(\bm X \bm \beta - \bm X \bm{\widehat \beta}^{(t)}) + \bm \epsilon, \quad \bm \epsilon \sim N\left(\bm 0, \phi_0 (\bm{\widehat W}^{(t)})^{-1}(\bm{\widehat M}^{(t)})^{-2}\right),
\end{equation}
or, equivalently,
\begin{equation}
\bm z^{(t)} \equiv \bm{\widehat M}^{(t)}(\bm y - \bm{\widehat \mu}^{(t)}) + \bm X \bm{\widehat \beta}^{(t)} = \bm X \bm \beta + \bm \epsilon', \quad \bm \epsilon' \sim N(\bm 0, \phi_0 (\bm{\widehat W}^{(t)})^{-1}).
\label{eq:second-order-approximation}
\end{equation}
The regression of the \textit{adjusted response variable} $\bm z^{(t)}$ on $\bm X$ leaves us with a weighted linear regression (hence the name \textit{working weights} for $W_i$), whose maximum likelihood estimate is
\begin{equation}
\bm{\widehat \beta}^{(t+1)} = (\bm X^T \bm{\widehat W}^{(t)} \bm X)^{-1} \bm X^T \bm{\widehat W}^{(t)} \bm z^{(t)},
\label{eq:IRLS-iteration}
\end{equation}
which we define as our next iterate. It's easy to verify that the IRLS iteration~\eqref{eq:IRLS-iteration} is equivalent to the Newton-Raphson iteration~\eqref{eq:NR-iteration}. Note that we have derived these algorithms for canonical links; they each can be derived for non-canonical links but need not be equivalent in this more general case.


\paragraph{Estimation of $\phi_0$ and GLM residuals.}

While sometimes the parameter $\phi_0$ is known (e.g. for binomial or Poisson GLMs), in other cases $\phi_0$ must be estimated (e.g. for the normal linear model). Recall from the linear model that we estimated $\sigma^2 = \phi_0$ by taking the sum of the squares of the residuals: $\widehat \sigma^2 = \frac{1}{n-p}\|\bm y - \bm{\widehat \mu}\|^2$. However, it's unclear in the GLM context exactly how to define a residual. In fact, there are two common ways of doing so, called \textit{deviance residuals} and \textit{Pearson residuals}. Deviance residuals are defined in terms of the unit deviance:
\begin{equation}
  r^D_i \equiv \text{sign}(y_i - \widehat \mu_i)\sqrt{w_i d(y_i, \widehat \mu_i)}.
\end{equation}
On the other hand, Pearson residuals are defined as variance-normalized residuals:
\begin{equation}
  r^P_i \equiv \frac{y_i - \widehat \mu_i}{\sqrt{V(\widehat \mu_i)/w_i}}.
\end{equation}
These residuals can be viewed as residuals from the (converged) weighted linear regression model~\eqref{eq:second-order-approximation}. In the normal case, these residuals coincide, but in the general case they do not. Based on these two notions of GLM residuals, we can define two estimators of $\phi_0$. One, based on the deviance residuals, is the \textit{mean deviance estimator of dispersion}
\begin{equation}
\widetilde \phi^D_0 \equiv \frac{1}{n-p}\|r^D\|^2 \equiv \frac{1}{n-p}\sum_{i = 1}^n w_i d(y_i, \widehat \mu_i) \equiv \frac{1}{n-p}D(\bm y; \bm{\widehat \mu});
\end{equation}
recall that the total deviance $D(\bm y; \bm{\widehat \mu})$ is a generalization of the residual sum of squares. The other, based on the Pearson residuals, is called the \textit{Pearson estimator of dispersion}:
\begin{equation}
\widetilde \phi^P_0 \equiv \frac{1}{n-p}X^2 \equiv \frac{1}{n-p}\|r^P\|^2 \equiv \frac{1}{n-p}\sum_{i = 1}^n w_i \frac{(y_i - \widehat \mu_i)^2}{V(\mu_i)}.
\label{eq:pearson-estimator-dispersion}
\end{equation}
$X^2$ is known as the Pearson $X^2$ statistic. The deviance-based estimator can be more accurate than the Pearson estimator under small-dispersion asymptotics. However, the Pearson estimator is more robust when only the first two moments of the EDM model are correct and in the absence of small-dispersion asymptotics. For these reasons, the Pearson estimator is generally preferred.

\section{Inference in GLMs} \label{sec:glm-inf}

\subsection{Preliminaries}

\subsubsection{Inferential goals}

There are two types of inferential goals: hypothesis testing and confidence interval/region construction.

\paragraph{Hypothesis testing}

\begin{enumerate}
  \item \textbf{Single coefficient}: $H_0: \beta_j = \beta_j^0$ versus $H_1: \beta_j \neq \beta_j^0$ for some $\beta_j^0 \in \mathbb R$.
  \item \textbf{Group of coefficients}: $H_0: \bm \beta_S = \bm \beta_S^0$ versus $H_1: \bm \beta_S \neq \bm \beta_S^0$ for some $S \subset \{0,\dots,p-1\}$ and some $\bm \beta_S^0 \in \mathbb R^{|S|}$.
  \item \textbf{Goodness of fit}: The goodness of fit null hypothesis is that the GLM~\eqref{eq:glm-def} is correctly specified. Consider the \textit{saturated model}   
  \begin{equation}
  y_i \overset{\text{ind}} \sim \text{EDM}(\mu_i, \phi_0/w_i) \quad \text{for} \quad i = 1,\dots,n. 
  \label{eq:saturated-model}
  \end{equation}
  Let 
  \begin{equation*}
  \mathcal M^{\text{GLM}} \equiv \{\bm \mu: \mu_i = \bm x_{i*}^T \bm \beta + o_i \text{ for some } \bm \beta \in \mathbb R^p\}
  \end{equation*}
  be the set of mean vectors consistent with the GLM. Then, the goodness of fit testing problem is $H_0: \bm \mu \in \mathcal M^{\text{GLM}}$ versus $H_1: \bm \mu \not \in \mathcal M^{\text{GLM}}$.
\end{enumerate}

\paragraph{Confidence interval/region construction}

\begin{enumerate}
  \item \textbf{Confidence interval for a single coefficient}: Here, the goal is to produce a confidence interval $\text{CI}(\beta_j)$ for a coefficient $\beta_j$.
  \item \textbf{Confidence region for a group of coefficients}: Here, the goal is to produce a confidence region $\text{CR}(\bm \beta_S)$ for a group of coefficients $\bm \beta_S$.
  \item \textbf{Confidence interval for a fitted value}: In GLMs, fitted values can either be considered for parameters on the linear scale ($\eta_i = \bm x_{i*}^T \bm \beta + o_i$) or the mean scale ($\mu_i = g^{-1}(\bm x_{i*}^T \bm \beta + o_i)$). The goal, then, is to produce confidence intervals $\text{CI}(\eta_i)$ or $\text{CI}(\mu_i)$ for $\eta_i$ or $\mu_i$, respectively.
  \end{enumerate}

\subsubsection{Inferential tools}

Inference in GLMs is based on asymptotic likelihood theory. These asymptotics can be based on \textit{large-sample asymptotics} or \textit{small-dispersion asymptotics}. Large-sample asymptotics are applicable for testing hypotheses and estimating parameters within models where the number of parameters is fixed while the sample size grows. Small-dispersion asymptotics ar applicable for testing hypotheses and estimating parameters within models where the dispersion is small, regardless of the sample size. Large-sample asymptotics are applicable to testing and estimating coefficients in GLMs~\eqref{eq:glm-def} with a fixed number of parameters when as the sample size grows, but not to testing goodness of fit. Indeed, goodness-of-fit tests refer to the saturated model~\eqref{eq:saturated-model}, whose number of parameters grows with $n$. Small-dispersion asymptotics, on the other hand, apply to goodness-of-fit testing.

Hypothesis tests (and, by inversion, confidence intervals) can be constructed in three asymptotically equivalent ways: Wald tests, likelihood ratio tests (LRT), and score tests. These tests can be justified using either large-sample or small-dispersion asymptotics, depending on the context. Despite their asymptotic equivalence, in finite samples some tests may be preferable to others (though for normal linear models, these tests are equivalent in finite samples as well). See Figure~\ref{fig:trinity-comparison}.

\begin{figure}[h!]
\includegraphics[width = \textwidth]{figures/trinity-comparison.png}
\caption{A comparison of the three asymptotic methods for GLM inference.}
\label{fig:trinity-comparison}
\end{figure}

\subsection{Wald inference}

Wald inference is based on the following asymptotic normality statement:
\begin{equation}
  \bm{\widehat \beta} \overset \cdot \sim N(\bm \beta, \bm I^{-1}(\bm \beta)) = N(\bm \beta, \phi_0(\bm X^T \bm W(\bm \beta) \bm X)^{-1}),
  \label{eq:wald-approximation}
\end{equation}
recalling our derivation of the Fisher information from equation~\eqref{eq:glm-fisher-info}. This approximation can be justified via \textit{large-sample asymptotics} or \textit{small-dispersion asymptotics}. Wald inference is easy to carry out, and for this reason is considered the default type of inference. However, as we'll see in Unit 5, it also tends to be the least accurate in small samples. Furthermore, Wald tests are usually not applied for testing goodness of fit.

\paragraph{Wald test for $\beta_j = \beta_j^0$ (known $\phi_0$).} Based on the Wald approximation~\eqref{eq:wald-approximation}, under the null hypothesis, we have
\begin{equation}
\widehat \beta_j \overset \cdot \sim N(\beta_j^0, \phi_0[(\bm X^T \bm W(\bm \beta) \bm X)^{-1}]_{jj}) \approx N(\beta_j^0, \phi_0[(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{jj}) \equiv N(0, \text{SE}(\beta_j)^2),
\end{equation}
where we have used a plug-in estimator of the variance. This leaves us with the Wald $z$-test
\begin{equation}
\phi(\bm X, \bm y) \equiv \mathbbm 1\left(\left|\frac{\widehat \beta_j - \beta_j^0}{\text{SE}(\beta_j)}\right| > z_{1-\alpha/2}\right).
\end{equation}
Note that since a one-dimensional parameter is being tested, we can make the test one-sided if desired.

\paragraph{Wald test for $\bm \beta_S = \bm \beta_S^0$ (known $\phi_0$).}

Extending the reasoning above, we have under the null hypothesis that
\begin{equation}
  \bm{\widehat \beta}_S \overset \cdot \sim N(\bm \beta_S^0, \phi_0[(\bm X^T \bm W(\bm \beta) \bm X)^{-1}]_{S,S}) \approx N(\bm \beta_S^0, \phi_0[(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{S,S}),
\end{equation}
and therefore
\begin{equation}
\frac{1}{\phi_0} (\bm{\widehat \beta}_S - \bm \beta_S^0)^T \left([(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{S,S}\right)^{-1}(\bm{\widehat \beta}_S - \bm \beta_S^0) \overset \cdot \sim \chi^2_{|S|}.
\end{equation}
Hence we have the Wald $\chi^2$ test
\begin{equation}
  \phi(\bm X, \bm y) \equiv \mathbbm 1\left(\frac{1}{\phi_0} (\bm{\widehat \beta}_S - \bm \beta_S^0)^T \left([(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{S,S}\right)^{-1}(\bm{\widehat \beta}_S - \bm \beta_S^0) > \chi^2_{|S|}(1-\alpha)\right).
\end{equation}

\paragraph{Wald confidence interval for $\beta_j$ (known $\phi_0$).} Inverting the Wald test for $\beta_j$, we get a Wald confidence interval:
\begin{equation}
  \text{CI}(\beta_j) \equiv \widehat \beta_j \pm z_{1-\alpha/2} \cdot \text{SE}(\beta_j), \quad \text{where} \quad \text{SE}(\beta_j) \equiv \sqrt{\phi_0[(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{jj}}.
  \label{eq:conf-int-beta}
\end{equation}

\paragraph{Wald confidence region for $\bm \beta_S$ (known $\phi_0$).}

By inverting the test of $H_0: \bm \beta_S = \bm \beta_S^0$, we get the Wald confidence region
\begin{equation}
  \text{CR}(\bm{\beta}_S) \equiv \left\{\bm \beta_S: \frac{1}{\phi_0} (\bm{\widehat \beta}_S - \bm \beta_S)^T \left([(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{S,S}\right)^{-1}(\bm{\widehat \beta}_S - \bm \beta_S) \leq \chi^2_{|S|}(1-\alpha)\right\}.
\end{equation}
Note that if $S = \{0, 1, \dots, p-1\}$, we are left with
\begin{equation}
  \text{CR}(\bm{\beta}_S) \equiv \left\{\bm \beta: \frac{1}{\phi_0} (\bm{\widehat \beta} - \bm \beta)^T \bm X^T \bm W(\bm{\widehat \beta}) \bm X (\bm{\widehat \beta} - \bm \beta) \leq \chi^2_{p}(1-\alpha)\right\}.
\end{equation}


\paragraph{Wald confidence intervals for $\eta_i$ and $\mu_i$ (known $\phi_0$).}

Given the Wald approximation~\eqref{eq:wald-approximation}, we have
\begin{equation*}
\widehat \eta_i \equiv o_i + \bm x_{i*}^T \bm{\widehat \beta} \overset \cdot \sim N(\eta_i, \phi_0 \cdot \bm x_{i*}^T (\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1} \bm x_{i*}) \equiv N(\eta_i, \text{SE}(\eta_i)^2).
\end{equation*}
Hence, the Wald interval for $\eta_i$ is
\begin{equation}
  \text{CI}(\eta_i) \equiv o_i + \bm x_{i*}^T \bm{\widehat\beta} \pm z_{1-\alpha/2} \cdot \text{SE}(\eta_i), \quad \text{where} \quad \text{SE}(\eta_i) \equiv \sqrt{\phi_0\bm x_{i*}^T(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}\bm x_{i*}}.
\end{equation}
A confidence interval for $\mu_i \equiv \mathbb E_{\bm \beta}[y_i] = g^{-1}(\eta_i)$ can be obtained by applying the monotonic function $g^{-1}$ to the endpoints of the confidence interval for $\eta_i$. Note that the resulting confidence interval may be asymmetric. We can get a symmetric interval by applying the delta method, but this interval would be less accurate because it involves the delta method approximation in addition to the Wald approximation.

\paragraph{Wald inference when $\phi_0$ is unknown.} When $\phi_0$ is unknown, we need to plug in an estimate $\widetilde \phi_0$ (e.g. the deviance-based or Pearson-based estimate). Now our standard errors are $\text{SE}(\beta_j) \equiv \sqrt{\widetilde \phi_0 \cdot [(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{jj}}$, and our test statistic for $H_0: \beta_j = \beta_j^0$ is
\begin{equation}
\frac{\widehat \beta_j - \beta_j^0}{\sqrt{\widetilde \phi_0}\sqrt{[(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{jj}}}.
\end{equation}
Unlike linear regression, it is not the case in general that $\bm{\widehat \beta}$ and $\widetilde \phi_0$ are independent. Nevertheless, they are \textit{asymptotically independent}. Therefore, the above statistic is \textit{approximately} distributed as $t_{n-p}$. Hence the test for $H_0: \beta_j = \beta_j^0$ is
\begin{equation}
\phi(\bm X, \bm y) \equiv \mathbbm 1\left(\left|\frac{\widehat \beta_j - \beta_j^0}{\text{SE}(\beta_j)}\right| > t_{n-p}(1-\alpha/2)\right).
\end{equation}
Likewise, we would replace $z_{1-\alpha}$ by $t_{n-p}(1-\alpha/2)$ for all tests and confidence intervals concerning univariate quantities. For multivariate quantities, we will get approximate $F$ distributions instead of approximate $\chi^2$ distributions. For example,
\begin{equation*}
  \frac{\frac{1}{|S|}(\bm{\widehat \beta}_S - \bm \beta_S^0)^T \left([(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}]_{S,S}\right)^{-1}(\bm{\widehat \beta}_S - \bm \beta_S^0)}{\widetilde \phi_0} \overset \cdot \sim F_{|S|, n-p}.
\end{equation*}

\subsection{Likelihood ratio inference}

\paragraph{Testing one or more coefficients ($\phi_0$ known).}

Let $\ell(\bm y, \bm \mu) = -\frac{D(\bm y, \bm \mu)}{2\phi_0} + C$ be the GLM log-likelihood (recall equation~\eqref{eq:glm-likelihood-via-deviance}). Let $H_0: \bm \beta_S = \bm \beta_S^0$ be a null hypothesis about some subset of variables $S \subset \{0, 1, \dots, p-1\}$, and $\bm{\widehat \mu}_{\text{-}S}$ be the maximum likelihood estimate under the null hypothesis. Likelihood ratio inference is based on the following asymptotic chi square distribution:
\begin{equation}
2(\ell(\bm y, \bm{\widehat \mu}) - \ell(\bm y, \bm{\widehat \mu}_{\text{-}S})) = \frac{D(\bm y, \bm{\widehat \mu}_{\text{-}S}) - D(\bm y, \bm {\widehat \mu})}{\phi_0} \overset \cdot \sim \chi^2_{|S|}.
\label{eq:likelihood-ratio-approximation}
\end{equation}
This approximation holds either in large samples (large sample asymptotics), or in small samples but with small dispersion (small-dispersion asymptotics). The latter has to do with the fact that under small-dispersion asymptotics,
\begin{equation}
  \frac{d(y_i, \mu_i)}{\phi_0/w_i} \overset \cdot \sim \chi^2_1,
\end{equation}
so
\begin{equation}
\frac{D(\bm y, \bm \mu)}{\phi_0} = \sum_{i = 1}^n \frac{d(y_i, \mu_i)}{\phi_0/w_i} \overset \cdot \sim \chi^2_n.
\end{equation}

Suppose we wish to test the null hypothesis $H_0: \bm \beta_S = \bm \beta_S^0$. Then, based on the approximation~\eqref{eq:likelihood-ratio-approximation} we can define the likelihood ratio test
\begin{equation}
\phi(\bm X, \bm y) \equiv \mathbbm 1\left(\frac{D(\bm y, \bm{\widehat \mu}_{\text{-}S}) - D(\bm y, \bm {\widehat \mu})}{\phi_0} > \chi^2_{|S|}(1-\alpha)\right).
\end{equation}

\paragraph{Confidence interval for a single coefficient.}

We can obtain a confidence interval for $\beta_j$ by inverting the likelihood ratio test. Let $\bm{\widehat \mu}_{\text{-}j}(\beta^0_j)$ be the fitted mean vector under the constraint $\beta_j = \beta_j^0$. Then, inverting the likelihood ratio test gives us the confidence interval
\begin{equation}
\text{CI}(\beta_j) \equiv \left\{\beta_j: \frac{D(\bm y, \bm{\widehat \mu}_{\text{-}j}(\beta_j)) - D(\bm y, \bm {\widehat \mu})}{\phi_0} \leq \chi^2_{|S|}(1-\alpha)\right\}.
\end{equation}
Likelihood ratio based confidence intervals tend to be more accurate than Wald intervals (especially when the parameter is near the edge of the parameter space), but they require more computation because $\bm{\widehat \mu}_{\text{-}j}(\beta_j)$ must be computed on a large grid of $\beta_j$ values. If we wanted to create \textit{confidence regions} for groups of parameters, this would get computationally out of hand due to the curse of dimensionality.

\paragraph{Goodness of fit testing ($\phi_0$ known).}

For $\phi_0$ known, we can also construct a goodness of fit test. To this end, we compare the deviances of the GLM and saturated model:
\begin{equation}
  \frac{D(\bm y, \bm{\widehat \mu}) - D(\bm y, \bm y)}{\phi_0} = \frac{D(\bm y; \bm{\widehat \mu})}{\phi_0} \overset \cdot \sim \chi^2_{n-p}.
\end{equation}
Note that the goodness of fit test is a significance test with respect to the saturated model~\eqref{eq:saturated-model}, which has $n$ free parameters. Therefore, the number of free parameters increases with the sample size, so large-sample asymptotics cannot justify this test. Instead, we must rely on small-dispersion asymptotics.

\paragraph{Likelihood ratio inference for $\phi_0$ unknown.}

If $\phi_0$ is unknown, we can estimate it as discussed above, and construct an $F$-statistic as follows:
  \begin{equation}
F \equiv \frac{(D(\bm y; \bm{\widehat \mu}_{\text{-}S})-D(\bm y; \bm{\widehat \mu}))/|S|}{\widetilde \phi_0}.
\end{equation}
In normal linear model theory, the null distribution of $F$ is \textit{exactly} $F_{|S|, n-p}$. For GLMs, the null distribution of $F$ is \textit{approximately} $F_{|S|, n-p}$. We can use this $F$ distribution to construction hypothesis tests for groups of coefficients, or invert it to get a confidence interval for a single coefficient. We cannot construct a goodness of fit test in the case that $\phi_0$ is unknown, because the residual degrees of freedom would be used up to estimate $\phi_0$ rather than to carry out inference.

\subsection{Score-based inference} \label{sec:score-tests-1}

Score-based inference can be used for the same set of inferential tasks as likelihood ratio inference. 

\paragraph{Testing multiple coefficients ($\phi_0$ known).} Let $H_0: \bm \beta_S = \bm \beta_S^0$ be a null hypothesis about a subset of variables $S$, and let $\bm{\widehat \beta}^0$ be the maximum likelihood estimate under this null hypothesis. Then, score test inference is based on the asymptotic approximation
\begin{equation}
U(\bm{\widehat \beta}^0)^T \bm I(\bm{\widehat \beta}^0)^{-1} U(\bm{\widehat \beta}^0) \overset \cdot \sim \chi^2_{|S|},
\end{equation}
recalling that $U(\bm \beta) = \partial \ell(\bm \beta)/\partial \bm \beta$ is the score vector. This approximation can justified either by small-dispersion asymptotics or based on large-sample asymptotics (both based on the central limit theorem). Since 
\begin{equation}
\bm{\widehat \beta}^0_{\text{-}S} = \underset{\bm \beta_{\text{-}S}}{\arg \max}\ \ell(\bm \beta^0_{S}, \bm \beta_{\text{-}S}), 
\end{equation}
it follows that
\begin{equation}
[U(\bm{\widehat \beta}^0)]_{\text{-}S} \equiv \frac{\partial \ell}{\partial \bm \beta_{\text{-}S}}(\bm{\widehat \beta}^0) = \frac{\partial \ell}{\partial \bm \beta_{\text{-}S}}(\bm \beta^0_{S}, \bm{\widehat \beta}_{\text{-}S}) = 0.
\end{equation}
Hence, we have
\begin{equation}
U(\bm{\widehat \beta}^0)^T \bm I(\bm{\widehat \beta}^0)^{-1} U(\bm{\widehat \beta}^0) = [U(\bm{\widehat \beta}^0)]_S^T [\bm I(\bm{\widehat \beta}^0)^{-1}]_{S, S} [U(\bm{\widehat \beta}^0)]_S.
\end{equation}
Recalling the expressions~\eqref{eq:glm-score} and~\eqref{eq:glm-fisher-info} for the score vector and information matrix, we have that
\begin{equation}
U(\bm{\widehat \beta}^0)^T \bm I(\bm{\widehat \beta}^0)^{-1} U(\bm{\widehat \beta}^0) = \frac{1}{\phi_0}(\bm y - \bm{\widehat \mu^0})^T \bm{\widehat W}^0 \bm{\widehat M}^0 \bm X_{S,*}[(\bm X^T \bm{\widehat W}^0 \bm X)^{-1}]_{S, S} \bm X_{S,*}^T \bm{\widehat M}^0 \bm{\widehat W}^0 (\bm y - \bm{\widehat \mu^0}).
\end{equation}
Therefore, we arrive at the score test for $H_0: \bm \beta_S = \bm \beta_S^0$:
\begin{equation}
\small
\phi(\bm X, \bm y) = \mathbbm 1\left((\bm y - \bm{\widehat \mu^0})^T \bm{\widehat W}^0 \bm{\widehat M}^0 \bm X_{S,*}[(\bm X^T \bm{\widehat W}^0 \bm X)^{-1}]_{S, S} \bm X_{S,*}^T \bm{\widehat M}^0 \bm{\widehat W}^0 (\bm y - \bm{\widehat \mu^0}) > \phi_0 \chi^2_{|S|}(1-\alpha)\right).
\end{equation}
The nice thing about a score test is that the model need only be fit under the null hypothesis. If there are several variables one is considering adding to a model, then the model need not be refit upon the addition of each variable.


\paragraph{Testing a single coefficient ($\phi_0$ known).}

If $S = \{j\}$, the score test becomes univariate and can be based on the following normal approximation:
\begin{equation}
\frac{\bm x_{j,*}^T \bm{\widehat M}^0 \bm{\widehat W}^0 (\bm y - \bm{\widehat \mu^0})}{\sqrt{\phi_0 ([(\bm X^T \bm{\widehat W}^0 \bm X)^{-1}]_{j, j})^{-1}}} \overset \cdot \sim N(0, 1).
\label{eq:score-test-univariate}
\end{equation}
Unlike its multivariate counterparts, we can construct not just a two-sided test but also one-sided tests based on this normal approximation. For example, below is a right-sided score test for $H_0: \beta_j = \beta_j^0$:
\begin{equation}
\phi(\bm X, \bm y) = \mathbbm 1 \left(\frac{\bm x_{j,*}^T \bm{\widehat M}^0 \bm{\widehat W}^0 (\bm y - \bm{\widehat \mu^0})}{\sqrt{\phi_0 ([(\bm X^T \bm{\widehat W}^0 \bm X)^{-1}]_{j, j})^{-1}}} > z_{1-\alpha}\right). 
\end{equation}

\paragraph{Confidence interval for a single coefficient ($\phi_0$ known).}

Just as with the likelihood ratio test, it is possible to invert a score test for a single coefficient to obtain a confidence interval. It is uncommon to invert a multivariate test to obtain a confidence region for multiple coordinates of $\bm \beta$, given the computationally expensive search across a grid of possible $\bm \beta$ values.

\paragraph{Goodness of fit testing ($\phi_0$ known).}

We can view goodness of fit testing as testing a hypothesis about the coefficients in a GLM with $\bm X = \bm I_{n \times n}$, which amounts to the saturated model that allows unrestricted means for each observation. Furthermore, the coefficient vector fitted under the null hypothesis amounts to fitting the GLM as usual. Therefore, we have
\begin{equation}
\begin{split}
U(\bm{\widehat \beta})^T \bm I(\bm{\widehat \beta})^{-1} U(\bm{\widehat \beta}) &= \frac{1}{\phi_0}(\bm y - \bm{\widehat \mu})^T \bm{\widehat W} \bm{\widehat M} \bm X (\bm X^T \bm{\widehat W} \bm X)^{-1} \bm X^T \bm{\widehat M} \bm{\widehat W} (\bm y - \bm{\widehat \mu}) \\
&= \frac{1}{\phi_0}(\bm y - \bm{\widehat \mu})^T \bm{\widehat W} \bm{\widehat M}^2 (\bm y - \bm{\widehat \mu}) \\
&= \frac{1}{\phi_0}(\bm y - \bm{\widehat \mu})^T \text{diag}\left( \frac{w_i}{V(\hat \mu_i)} \right) (\bm y - \bm{\widehat \mu}) \\
&= \frac{1}{\phi_0}\sum_{i = 1}^n \frac{w_i (y_i - \widehat \mu_i)^2}{V(\widehat \mu_i)} \equiv \frac{1}{\phi_0} X^2,
\end{split}
\end{equation}
where $X^2$ is the Pearson chi-square statistic. Therefore, the score test for goodness of fit is
\begin{equation}
\phi(\bm X, \bm y) \equiv \mathbbm 1\left(X^2 \phi_0 > \chi^2_{n-p}(1-\alpha)\right).
\end{equation}
In the context of contingency table analysis (see the next chapter), this test reduces to the Pearson chi-square test of independence between two categorical variables. This test was proposed in 1900; it was only pointed out about a century later that this is a score test (Smyth 2003).

\paragraph{Score test inference for $\phi_0$ unknown.}

Score test inference for one of more coefficients $\bm \beta_S$ can be achieved by replacing $\phi_0$ with one of its estimators, and replacing the normal and chi-square distributions with $t$ and $F$ distributions, respectively. For example, the score test for a single coefficient $\beta_j$ is
\begin{equation}
\phi(\bm X, \bm y) = \mathbbm 1 \left(\frac{\bm x_{j,*}^T \bm{\widehat M}^0 \bm{\widehat W}^0 (\bm y - \bm{\widehat \mu^0})}{\sqrt{\widetilde \phi ([(\bm X^T \bm{\widehat W}^0 \bm X)^{-1}]_{j, j})^{-1}}} > t_{n-p}(1-\alpha)\right).
\end{equation}
The $t$ and $F$ distributions are not exact in finite samples, but are better approximations than the normal and chi-square distributions. The score test for goodness of fit is not applicable in the case when $\phi_0$ is unknown, similarly to the likelihood ratio test. Indeed, note the relationship between the Pearson goodness of fit test, which rejects when $\frac{1}{\phi_0}X^2 > \chi^2_{n-p}(1-\alpha)$ and the Pearson estimator of the dispersion parameter: $\widetilde \phi_0 \equiv \frac{X^2}{n-p}$. If we try to plug in the Pearson estimator for the dispersion into the Pearson goodness of fit test, we end up with a test statistic deterministically equal to $n-p$. This reflects the fact that the residual degrees of freedom can either be used to estimate the dispersion or to test goodness of fit; they cannot be used for both.

\section{R demo}

\subsection{Crime data}

Let's revisit the crime data from Homework 2, this time fitting a logistic regression to it.
<<message = FALSE>>=
# read crime data
crime_data <- read_tsv("data/Statewide_crime.dat")

# read and transform population data
population_data <- read_csv("data/state-populations.csv")
population_data <- population_data |>
  filter(State != "Puerto Rico") |>
  select(State, Pop) |>
  rename(state_name = State, state_pop = Pop)

# collate state abbreviations
state_abbreviations <- tibble(
  state_name = state.name,
  state_abbrev = state.abb
) |>
  add_row(state_name = "District of Columbia", state_abbrev = "DC")

# add CrimeRate to crime_data
crime_data <- crime_data |>
  mutate(STATE = ifelse(STATE == "IO", "IA", STATE)) |>
  rename(state_abbrev = STATE) |>
  filter(state_abbrev != "DC") |> # remove outlier
  left_join(state_abbreviations, by = "state_abbrev") |>
  left_join(population_data, by = "state_name") |>
  mutate(CrimeRate = Violent / state_pop) |>
  select(state_abbrev, CrimeRate, Metro, HighSchool, Poverty, state_pop)

crime_data
@

We can fit a GLM using the \verb|glm| command, specifying as additional arguments the observation weights as well as the exponential dispersion model. In this case, the weights are the state populations and the family is binomial:
<<>>=
glm_fit <- glm(CrimeRate ~ Metro + HighSchool + Poverty,
  weights = state_pop,
  family = "binomial",
  data = crime_data
)
@

We can print the summary table as usual:
<<>>=
summary(glm_fit)
@
Amazingly, everything is very significant! This is because the weights for each observation (the state populations) are very high, effectively making the sample size very high. But frankly this is a bit suspicious. Glancing at the bottom of the regression summary, we see a residual deviance of 11742 on 46 degrees of freedom. This part of the summary refers to the deviance-based goodness of fit test. Under the null hypothesis that the model fits well, we expect that the residual deviance has a distribution of $\chi^2_{46}$, which has mean 46.

Let's formally check the goodness of fit. We can extract the residual deviance and residual degrees of freedom from the GLM fit:
<<>>=
glm_fit$deviance
glm_fit$df.residual
@
\noindent We can then compute the chi-square $p$-value:
<<>>=
# compute based on residual deviance from fit object
pchisq(glm_fit$deviance,
  df = glm_fit$df.residual,
  lower.tail = FALSE
)

# compute residual deviance as sum of squares of residuals
pchisq(sum(resid(glm_fit, "deviance")^2),
  df = glm_fit$df.residual,
  lower.tail = FALSE
)
@
\noindent Wow, we get a $p$-value of zero! Let's try doing a score-based (i.e. Pearson) goodness of fit test:
<<>>=
pchisq(sum(resid(glm_fit, "pearson")^2),
  df = glm_fit$df.residual,
  lower.tail = FALSE
)
@
\noindent Also zero. So we need to immediately stop using this model for inference about these data, since it fits the data very poorly. We will discuss how to build a better model for the crime data in the next unit. For now, we turn to analyzing a different data set.

\subsection{Noisy miner data}

\textit{Credit: Generalized Linear Models With Examples in R textbook.}

Let's consider the noisy miner dataset. Noisy miners are a small but aggressive native Australian bird. We want to know how the number of these birds observed in a patch of land depends on various factors of that patch of land.
<<>>=
library(GLMsData)
data("nminer")
nminer |> as_tibble()
@

Since the response is a count, we can model it as a Poisson random variable. Let's fit that GLM:
<<>>=
glm_fit <- glm(Minerab ~ . - Miners, family = "poisson", data = nminer)
summary(glm_fit)
@
\noindent We exclude \verb|Miners| because this is just a binarized version of the response variable. Things look a bit better on the GOF front:
<<>>=
pchisq(sum(resid(glm_fit, "deviance")^2),
  df = glm_fit$df.residual,
  lower.tail = FALSE
)

pchisq(sum(resid(glm_fit, "pearson")^2),
  df = glm_fit$df.residual,
  lower.tail = FALSE
)
@
Still there is some model misspecification, but for now we still proceed with the rest of the analysis.

The standard errors shown in the summary are based on the Wald test. We can get Wald confidence intervals based on these standard errors by using the formula:
<<>>=
glm_fit |>
  summary() |>
  coef() |>
  as.data.frame() |>
  transmute(`2.5 %` = Estimate + qnorm(0.025)*`Std. Error`,
            `97.5 %` = Estimate + qnorm(0.025)*`Std. Error`)
@
Or, we can simply use \verb|confint.default()|:
<<>>=
confint.default(glm_fit)
@
Or, we might want LRT-based confidence intervals, which are given by \verb|confint()|:
<<>>=
confint(glm_fit)
@
In this case, the two sets of confidence intervals seem fairly similar.

Now, we can get prediction intervals, either on the linear predictor scale or on the mean scale:
<<>>=
pred_linear <- predict(glm_fit, newdata = nminer[31,], se.fit = TRUE)
pred_mean <- predict(glm_fit, newdata = nminer[31,], type = "response", se.fit = TRUE)

pred_linear
pred_mean
log(pred_mean$fit)
@
We see that the prediction on the linear predictor scale is exactly the logarithm of the prediction on the mean scale. However, the standard error given on the mean scale uses the delta method. We prefer to directly transform the confidence interval from the linear scale using the inverse link function (in this case, the exponential):
<<>>=
# using delta method
c(pred_mean$fit + qnorm(0.025)*pred_mean$se.fit,
  pred_mean$fit + qnorm(0.975)*pred_mean$se.fit)

# using transformation
exp(c(pred_linear$fit + qnorm(0.025)*pred_linear$se.fit,
      pred_linear$fit + qnorm(0.975)*pred_linear$se.fit))
@
In this case the intervals obtained are somewhat different. We can plot confidence intervals for the fit in a univariate case (e.g. regressing \verb|Minerab| on \verb|Eucs|) using \verb|geom_smooth()|:
<<fig.width = 4, fig.height = 3, fig.align ='center'>>=
nminer |>
  ggplot(aes(x = Eucs, y = Minerab)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "glm",
              method.args = list(family = "poisson"))
@

We can also test the coefficients in the model. The Wald tests for individual coefficients were already given by the regression summary above. We might want to carry out likelihood ratio tests for individual coefficients instead. For example, let's do this for \verb|Eucs|
<<>>=
glm_fit_partial <- glm(Minerab ~ . - Miners - Eucs, family = "poisson", data = nminer)
anova(glm_fit_partial, glm_fit, test = "LRT")
@
The \verb|Eucs| variable is quite significant! We can manually carry out the LRT as a sanity check:
<<>>=
deviance_partial <- glm_fit_partial$deviance
deviance_full <- glm_fit$deviance
lrt_stat <- deviance_partial - deviance_full
p_value <- pchisq(lrt_stat, df = 1, lower.tail = FALSE)
tibble(lrt_stat, p_value)
@

We can test groups of variables using the likelihood ratio test as well.


\chapter{Generalized linear models: Special cases}

Chapter 4 developed a general theory for GLMs. In Chapter 5, we specialize this theory to several important cases, including logistic regression and Poisson regression.

\section{Logistic regression} \label{sec:logistic-regression}

\subsection{Model definition and interpretation}

\paragraph{Model definition.} Recall from Chapter 4 that the logistic regression model is
\begin{equation}
m_i y_i \overset{\text{ind}}\sim \text{Bin}(m_i, \pi_i); \quad \text{logit}(\pi_i) = \log\frac{\pi_i}{1-\pi_i} = \bm x^T_{i*}\bm \beta.
\end{equation}
Here we use the canonical logit link function, although other link functions are possible. We also set the offsets to 0. The interpretation of the parameter $\beta_j$ is that a unit increase in $x_j$---other predictors held constant---is associated with an (additive) increase of $\beta_j$ on the log-odds scale or a multiplicative increase of $e^{\beta_j}$ on the odds scale. Note that logistic regression data come in two formats: \textit{ungrouped} and \textit{grouped}. For ungrouped data, we have $m_1 = \dots = m_n = 1$, so $y_i \in \{0,1\}$ are Bernoulli random variables. For grouped data, we can have several independent Bernoulli observations per predictor $\bm x_{i*}$, which give rise to binomial proportions $y_i \in [0,1]$. This happens most often when all the predictors are discrete. You can always convert grouped data into ungrouped data, but not necessarily vice versa. We'll discuss below that the grouped and ungrouped formulations of logistic regression have the same MLE and standard errors but different deviances.

\paragraph{Generative model equivalent.} Consider the following generative model for $(\bm x, y) \in \mathbb R^{p-1} \times \{0,1\}$:
\begin{equation}
y \sim \text{Ber}(\pi); \quad \bm x|y \sim \begin{cases}N(\bm \mu_0, \bm V) \quad \text{if } y = 0 \\ N(\bm \mu_1, \bm V) \quad \text{if } y = 1\end{cases}.
\end{equation}
Then, we can derive that $y|\bm x$ follows a logistic regression model (called a \textit{discriminative} model because it conditions on $\bm x$). Indeed,
\begin{equation}
\begin{split}
\text{logit}(p(y = 1|\bm x)) &= \log\frac{p(y = 1)p(\bm x|y = 1)}{p(y = 0)p(\bm x|y = 0)} \\
&= \log\frac{\pi \exp\left(-\frac12(\bm x - \bm \mu_1)^T\bm V^{-1}(\bm x - \bm \mu_1)\right)}{(1-\pi) \exp\left(-\frac12(\bm x - \bm \mu_0)^T\bm V^{-1}(\bm x - \bm \mu_0)\right)} \\
&= \beta_0 + \bm x^T \bm V^{-1}(\bm \mu_1 - \bm \mu_0) \\
&\equiv \beta_0 + \bm x^T \bm \beta_{\text{-}0}.
\end{split}
\end{equation}
This is another natural route to motivating the logistic regression model.

\paragraph{Special case: $2 \times 2$ contingency table.}

Suppose that $x \in \{0,1\}$, and consider the logistic regression model $\text{logit}(\pi_i) = \beta_0 + \beta_1 x_i$. For example, suppose that $x \in \{0,1\}$ encodes treatment (1) and control (0) in a clinical trial, and $y_i \in \{0,1\}$ encodes success (1) and failure (0). We make $n$ observations of $(x_i, y_i)$ in this ungrouped setup. The parameter $e^{\beta_1}$ can be interpreted as the \textit{odds ratio}:
\begin{equation}
e^{\beta_1} = \frac{\mathbb P[y = 1|x=1]/\mathbb P[y = 0|x=1]}{\mathbb P[y = 1|x=0]/\mathbb P[y = 0|x=0]}.
\end{equation}
This parameter is the multiple by which the odds of success increase when going from control to treatment. We can summarize such data via the $2 \times 2$ \textit{contingency table} (Table~\ref{tab:2-by-2-table}). A grouped version of this data would be $\{(x_1, y_1) = (0, 7/24), (x_2, y_2) = (1, 9/21)\}$. The null hypothesis $H_0: \beta_1 = 0 \Longleftrightarrow H_0: e^{\beta_1} = 1$ states that the success probability in both rows of the table is the same.
\begin{table}[h!]
\centering
\begin{tabular}{c|cc|c}
 & Success & Failure & Total \\
 \hline
 Treatment & 9 & 12 & 21 \\
 Control & 7 & 17 & 24 \\
 \hline
 Total & 16 & 29 & 45
\end{tabular}
\caption{An example of a $2 \times 2$ contingency table.}
\label{tab:2-by-2-table}
\end{table}

\paragraph{Logistic regression with case-control studies.}

In a prospective study (e.g. a clinical trial), we assign treatment or control (i.e., $x$) to individuals, and then observe a binary outcome (i.e., $y$). Sometimes, the outcome $y$ takes a long time to measure or has highly imbalanced distribution in the population (e.g. the development of lung cancer). In this case, an appealing study design is the \textit{retrospective study}, where individuals are sampled based on their \textit{response values} (e.g. presence of lung cancer) rather than their treatment/exposure status (e.g. smoking). It turns out that a logistic regression model is appropriate for such retrospective study designs as well. Indeed, suppose that $y|\bm x$ follows a logistic regression model. Let's try to figure out the distribution of $y|\bm x$ in the retrospectively gathered sample. Letting $z \in \{0,1\}$ denote the indicator that an observation is sampled, define $\rho_1 \equiv \mathbb P[z = 1|y = 1]$ and $\rho_0 \equiv \mathbb P[z = 1|y = 0]$, and assume that $\mathbb P[z = 1, y, \bm x] = \mathbb P[z = 1 | y]$. The latter assumption states that the predictors $\bm x$ were not used in the retrospective sampling process. Then,
\begin{equation*}
\text{logit}(\mathbb P[y = 1|z = 1, \bm x]) = \log \frac{\rho_1 \mathbb P[y = 1|\bm x]}{\rho_0 \mathbb P[y = 0|\bm x]} = \log \frac{\rho_1}{\rho_0} + \text{logit}(\mathbb P[y = 1|\bm x]) = \left(\log \frac{\rho_1}{\rho_0} + \beta_0\right) + \bm x^T \bm \beta_{\text{-}0}.
\end{equation*}
Thus, conditioning on retrospective sampling changes only the intercept term, but preserves the coefficients of $\bm x$. Therefore, we can carry out inference for $\bm \beta_{\text{-}0}$ in the same way regardless of whether the study design is prospective or retrospective.

\subsection{Estimation and inference}

\subsubsection{Score and Fisher information}

Recall from Chapter 4 that
\begin{equation}
\bm U(\bm \beta) = \frac{1}{\phi_0}\bm X^T \bm M \bm W (\bm y - \bm \mu) \quad \text{and} \quad \bm I(\bm \beta) = \frac{1}{\phi_0}\bm X^T \bm W \bm X,
\end{equation}
where
\begin{equation}
  \begin{split}
\bm W \equiv \text{diag}\left(\frac{w_i}{V(\mu_i)(d\eta_i/d\mu_i)^2}\right) \quad \text{and} \quad \bm M \equiv \text{diag}\left(\frac{\partial\eta_i}{\partial \mu_i}\right).
  \end{split}
\end{equation}
Since logistic regression uses a canonical link function, we get the following simplifications:
\begin{equation}
  \begin{split}
\bm W = \text{diag}\left(w_i V(\mu_i)\right) = \text{diag}\left(m_i \pi_i(1-\pi_i)\right) \quad \text{and} \quad \bm M = \text{diag}\left(\frac{1}{\pi_i(1-\pi_i)}\right).
  \end{split}
\end{equation}
Here we have substituted the notation $\bm \pi$ for $\bm \mu$, and recall that for logistic regression, $\phi_0 = 1$, $w_i = m_i$ and $V(\pi_i) = \pi_i(1-\pi_i)$. Therefore, the score equations for logistic regression are
\begin{equation}
0 = \bm X^T \text{diag}\left(m_i\right)(\bm y - \bm{\widehat\mu}) \quad \Longleftrightarrow \quad \sum_{i = 1}^n m_i x_{ij}(y_i-\widehat \pi_i) = 0, \quad j = 0, \dots, p-1.
\label{eq:logistic-score-equations}
\end{equation}
We can solve these equations using IRLS. The Fisher information is
\begin{equation}
\bm I(\bm \beta) = \bm X^T \text{diag}\left(m_i \pi_i(1-\pi_i)\right) \bm X.
\end{equation}

\subsubsection{Wald inference}

Using the results in the previous paragraph, we can carry out Wald inference based on the normal approximation
\begin{equation}
\bm{\widehat \beta} \overset \cdot \sim N\left(\bm \beta, \left(\bm X^T\text{diag}(m_i \widehat \pi_i(1-\widehat \pi_i))\bm X\right)^{-1}\right).
\end{equation}
This approximation holds for $\sum_{i = 1}^n m_i \rightarrow \infty$. 

\paragraph{Example: $2 \times 2$ contingency table.}

Suppose we have a $2 \times 2$ contingency table. The grouped logistic regression formulation of these data is
\begin{equation}
y_0 \sim \frac{1}{m_0}\text{Bin}(m_0, \pi_0); \quad y_1 \sim \frac{1}{m_1}\text{Bin}(m_1, \pi_1); \quad \text{logit}(\pi_i) = \beta_0 + \beta_1 x_i.
\end{equation}
In this case, we have $n = p = 2$, so the grouped logistic regression model is saturated. Therefore, we have
\begin{equation}
\hat \pi_0 = y_0, \quad \text{and} \quad \hat \pi_1 = y_1, \quad \text{so} \quad \hat \beta_1 = \log \frac{\hat \pi_1 / (1 - \hat \pi_1)}{\hat \pi_0 / (1 - \hat \pi_0)} = \log \frac{y_1 / (1 - y_1)}{y_0 / (1 - y_0)}.
\end{equation}
The squared Wald standard error for $\hat \beta_1$ is
\begin{equation}
\begin{split}
\text{SE}^2(\widehat \beta_1) &\equiv \left[\left(\bm X^T\text{diag}(m_i \widehat \pi_i(1-\widehat \pi_i))\bm X\right)^{-1}\right]_{22} \\
&= \left[\left(\begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}^T\begin{pmatrix} m_0y_0(1-y_0) & 0 \\ 0 & m_1y_1(1-y_1) \end{pmatrix}\begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}\right)^{-1}\right]_{22} \\
&= \left[\left(\begin{pmatrix} m_0 y_0 (1-y_0) + m_1 y_1 (1-y_1) & m_1 y_1(1-y_1) \\ m_1 y_1(1-y_1) & m_1 y_1(1-y_1) \end{pmatrix}\right)^{-1}\right]_{22} \\
&= \frac{m_0 y_0 (1-y_0) + m_1 y_1 (1-y_1)}{m_0y_0(1-y_0) \cdot m_1y_1(1-y_1)} \\
&= \frac{1}{m_0y_0(1-y_0)} + \frac{1}{m_1y_1(1-y_1)}.
\end{split}
\end{equation}
Therefore, the Wald test for $H_0: \beta_1 = 0$ rejects if
\begin{equation}
\left|\frac{\hat \beta_1}{\text{SE}(\hat \beta_1)}\right| = \left|\frac{\log \frac{y_1 / (1 - y_1)}{y_0 / (1 - y_0)}}{\sqrt{\frac{1}{m_0y_0(1-y_0)} + \frac{1}{m_1y_1(1-y_1)}}}\right| > z_{1-\alpha/2}.
\end{equation}

\paragraph{Hauck-Donner effect.} Unfortunately, Wald inference in finite samples does not always perform very well. The Wald test above is known to be conservative if one or more of the mean parameters (in this case, $\pi_i$) tends to the edge of the parameter space (in this case, $\pi_i \rightarrow 0$ or $\pi_i \rightarrow 1$). This is called the \textit{Hauck-Donner effect}. As an example, consider testing $H_0: \beta_0 = 0$ in the intercept-only model
\begin{equation}
my \sim \text{Bin}(m, \pi); \quad \text{logit}(\pi) = \beta_0.
\end{equation}
The Wald test statistic is $z \equiv \widehat \beta/\text{SE} = \text{logit}(y)\sqrt{my(1-y)}$. This test statistic actually tends to \textit{decrease} as $y \rightarrow 1$ (Figure~\ref{fig:hauck-donner}), since the standard error grows faster than the estimate itself. So the test statistic becomes less significant as we go further away from the null! A similar situation arises in the $2 \times 2$ contingency table example above, where the Wald test for $H_0: \beta_1 = 0$ becomes less significant as $y_0 \rightarrow 0$ and $y_1 \rightarrow 1$. As a limiting case of this, the Wald test is undefined if $y_0 = 0$ and $y_1 = 1$. This situation is a special case of \textit{perfect separability} in logistic regression: when a hyperplane in covariate space separates observations with $y_i = 0$ from those with $y_i = 1$. Some of the maximum likelihood coefficient estimates are infinite in this case, causing the Wald test be undefined since it uses these coefficient estimates as test statistics.

\begin{figure}
\centering
\includegraphics{figures/hauck-donner.pdf}
\caption{The Hauck-Donner effect: The Wald statistic for testing $H_0: \pi = 0.5$ within the model $my \sim \text{Bin}(m, \pi)$ decreases as the proportion $y$ approaches 1. Here, $m = 25$.}
\label{fig:hauck-donner}
\end{figure}

\subsubsection{Likelihood ratio inference}

\paragraph{The Bernoulli and binomial deviance.}

Let's first compute the deviance of a Bernoulli or binomial model. These deviances are the same because these two models have the same natural parameter and log-partition function. The unit deviance is
\begin{equation}
t(y, \pi) = y \log \pi + (1-y)\log(1-\pi).
\end{equation}
Hence, we have
\begin{equation}
t(y, y) = y \log y + (1-y) \log(1-y).
\end{equation}
Hence, the unit deviance is
\begin{equation}
d(y, \mu) \equiv 2(t(y,y)-t(y,\pi)) = 2\left(y \log \frac{y}{\pi} + (1-y)\log \frac{1-y}{1-\pi}\right).
\end{equation}
The total deviance, therefore, is
\begin{equation}
D(\bm y, \hat{\bm \pi}) \equiv \sum_{i = 1}^n w_i d(y_i, \widehat \pi_i) = 2\sum_{i = 1}^n \left(m_i y_i \log \frac{y_i}{\widehat \pi_i} + m_i(1-y_i) \log\frac{1-y_i}{1-\widehat \pi_i}\right).
\label{eq:logistic-deviance}
\end{equation}

\paragraph{Comparing the deviances of grouped and ungrouped logistic regression models.}

Let us pause to compare the total deviances of grouped and ungrouped logistic regression models. Consider the following grouped and ungrouped models:
\begin{equation}
y^{\text{grp}}_i \overset{\text{ind}} \sim \frac{1}{m_i}\text{Bin}(m_i, \pi_i) \quad \text{and} \quad y^{\text{ungrp}}_{ik} \overset{\text{ind}} \sim \text{Ber}(\pi_i), \quad k = 1, \dots, m_i, \quad \text{where} \quad \text{logit}(\pi_i) = \bm x_{i*}^T \bm \beta.
\end{equation}
The relationship between the grouped and ungrouped observations is that
\begin{equation}
y^{\text{grp}}_i = \frac{1}{m_i}\sum_{k = 1}^{m_i} y^{\text{ungrp}}_{ik} \equiv \bar y^{\text{ungrp}}_i.
\end{equation}
Since the grouped and ungrouped logistic regression models have the same likelihoods, it follows that they have the same maximum likelihood estimates $\widehat{\bm \beta}$ and $\widehat{\bm \pi}$. However, the total deviances of the two models are different. The total deviance of the grouped model is given by Equation~\eqref{eq:logistic-deviance}:
\begin{equation}
D(\bm y^{\text{grp}}, \hat{\bm \pi}) = 2\sum_{i = 1}^n \left(m_i y^{\text{grp}}_i \log \frac{y^{\text{grp}}_i}{\widehat \pi_i} + m_i(1-y^{\text{grp}}_i) \log\frac{1-y^{\text{grp}}_i}{1-\widehat \pi_i}\right).
\label{eq:total-deviance-grouped}
\end{equation}
On the other hand, the total deviance of the ungrouped model is
\begin{equation}
\begin{split}
D(\bm y^{\text{ungrp}}, \hat{\bm \pi}) &= 2\sum_{i = 1}^n \sum_{k = 1}^{m_i} \left(y^{\text{ungrp}}_{ik} \log \frac{y^{\text{ungrp}}_{ik}}{\widehat \pi_i} + (1-y^{\text{ungrp}}_{ik}) \log\frac{1-y^{\text{ungrp}}_{ik}}{1-\widehat \pi_i}\right) \\
&= 2\sum_{i = 1}^n \sum_{k = 1}^{m_i} \left(y^{\text{ungrp}}_{ik} \log \frac{1}{\widehat \pi_i} + (1-y^{\text{ungrp}}_{ik}) \log\frac{1}{1-\widehat \pi_i}\right) \\
&= 2\sum_{i = 1}^n \left(m_i y^{\text{grp}}_i \log \frac{1}{\widehat \pi_i} + m_i(1-y^{\text{grp}}_i) \log\frac{1}{1-\widehat \pi_i}\right).
\end{split}
\label{eq:total-deviance-ungrouped}
\end{equation}
In the second line, we used the fact that $y \log y \rightarrow 0$ and $(1-y)\log(1-y) \rightarrow 0$ as $y \rightarrow 0$ or $y \rightarrow 1$. Comparing the grouped~\eqref{eq:total-deviance-grouped} and ungrouped~\eqref{eq:total-deviance-ungrouped} total deviances, we see that these are given by related, but different expressions. Because small dispersion asymptotics applies to the grouped model but not the ungrouped model, we have that
\begin{equation}
\text{under small-dispersion asymptotics,} \quad D(\bm y^{\text{grp}}, \hat{\bm \pi}) \overset \cdot \sim \chi^2_{n-p} \quad \text{but} \quad D(\bm y^{\text{ungrp}}, \hat{\bm \pi}) \not \sim \chi^2_{n-p}.
\end{equation}

\paragraph{Likelihood ratio inference for one or more coefficients.}

Letting $\bm{\widehat \pi}_0$ and $\bm{\widehat \pi}_1$ be the MLEs from two nested models, we can then express the likelihood ratio statistic as
\begin{equation}
D(\bm y, \bm{\widehat \pi}_0) - D(\bm y, \bm{\widehat \pi}_1) = 2\sum_{i = 1}^n \left(m_i y_i \log \frac{\widehat \pi_{i1}}{\widehat \pi_{i0}} + m_i(1-y_i) \log\frac{1-\widehat \pi_{i1}}{1-\widehat \pi_{i0}}\right).
\end{equation}
Note that this expression holds for grouped or ungrouped logistic regression models. We can then construct a likelihood ratio test in the usual way. Likelihood ratio inference can be justified by either large-sample or small-dispersion asymptotics.

\paragraph{Goodness of fit testing.}

In grouped logistic regression, we can also use the likelihood ratio test to test goodness of fit. To do so, we compare the total deviance of the fitted model~\eqref{eq:logistic-deviance} to a chi-squared quantile. In particular, the deviance-based goodness of test rejects when
\begin{equation}
D(\bm y, \hat{\bm \pi}) = 2\sum_{i = 1}^n \left(m_i y_i \log \frac{y_i}{\widehat \pi_i} + m_i(1-y_i) \log\frac{1-y_i}{1-\widehat \pi_i}\right) > \chi^2_{n-p}(1-\alpha).
\end{equation}
This test is justified by small-dispersion asymptotics based on the saddlepoint approximation, which is decent when $\min(m_i \pi_i, (1-m_i)\pi_i) \geq 3$ for each $i$.

\paragraph{Example: $2 \times 2$ table.}

Let us revisit the example of the $2 \times 2$ table model, within which we would like to test $H_0: \beta_1 = 0$. Note that we can view this as a goodness of fit test of the intercept-only model in a grouped logistic regression model, since the alternative model is the saturated model (it has two observations and two parameters). To compute the likelihood ratio statistic, we first need to fit the intercept-only model. The score equations~\eqref{eq:logistic-score-equations} reduce to
\begin{equation}
m_0 (y_0 - \hat \pi) + m_1 (y_1 - \hat \pi) = 0 \quad \Longrightarrow \quad \hat \pi_0 = \hat \pi_1 = \hat \pi = \frac{m_0 y_0 + m_1 y_1}{m_0 + m_1}.
\end{equation}
Therefore, the deviance-based test of $H_0: \beta_1 = 0$ rejects when
\begin{equation*}
\begin{split}
D(\bm y, \bm {\widehat \pi}) &= 2\sum_{i = 1}^n \left(m_i y_i \log \frac{y_i}{\widehat \pi_i} + m_i(1-y_i) \log\frac{1-y_i}{1-\widehat \pi_i}\right) \\
&= \left(m_0 y_0 \log\frac{y_0}{\hat \pi} + m_0(1-y_0)\log\frac{1-y_0}{1-\hat \pi}\right) + \left(m_1 y_1 \log\frac{y_1}{\hat \pi} + m_1(1-y_1)\log\frac{1-y_1}{1-\hat \pi}\right) \\
&> \chi^2_{1}(1-\alpha).
\end{split}
\end{equation*}
Likelihood ratio inference can give nontrivial conclusions in cases when Wald inference cannot, e.g. in the case of perfect separability. In the above example, suppose $y_0 = 0$ and $y_1 = 1$, giving perfect separability. Then, we can use the fact that $y \log y \rightarrow 0$ and $(1-y)\log(1-y) \rightarrow 0$ as $y \rightarrow 0$ or $y \rightarrow 1$ to see that
\begin{equation}
D(\bm y, \bm {\widehat \pi}) = 2\left(m_0 \log\frac{1}{1-\hat \pi} + m_1 \log\frac{1}{\hat \pi}\right) = 2\left(m_0 \log \frac{m_0 + m_1}{m_0} + m_1 \log \frac{m_0 + m_1}{m_1}\right).
\end{equation}
This gives us a finite value, which we can compare to $\chi^2_{1}(1-\alpha)$ to test $H_0: \beta_1 = 0$. Even though the likelihood ratio statistic is still defined, we do still have to be careful because the data may suggest that the parameters are too close to the boundary of the parameter space. However, the rate at which the test breaks down as the parameters approach this boundary is slower than the rate at which the Wald test breaks down.

\subsubsection{Score-based inference}

Here we present only the score-based goodness-of-fit test. Recalling Section~\ref{sec:score-tests-1}, the score statistic for goodness of fit is Pearson's $X^2$ statistic:
\begin{equation}
X^2 = \sum_{i = 1}^n \frac{w_i (y_i - \widehat \mu_i)^2}{V(\widehat \mu_i)} = \sum_{i = 1}^n \frac{m_i(y_i - \widehat \pi_i)^2}{\widehat \pi_i(1-\widehat \pi_i)}.
\end{equation}
This test is justified by small-dispersion asymptotics based on the central limit theorem, which is decent when $\min(m_i \pi_i, (1-m_i)\pi_i) \geq 5$ for each $i$.

\subsubsection{Fisher's exact test}

As an alternative to asymptotic tests for logistic regression, in the case of $2 \times 2$ tables there is an \textit{exact} test of $H_0: \beta_1 = 0$. Suppose we have
\begin{equation}
s_1 = m_1y_1 \sim \text{Bin}(m_1, \pi_1) \quad and \quad s_2 = m_2y_2 \sim \text{Bin}(m_2, \pi_2).
\end{equation}
The trick is to conduct inference \textit{conditional on} $s_1 + s_2$. Note that under $H_0: \pi_1 = \pi_2$, we have
\begin{equation}
\begin{split}
\mathbb P[s_1 = t | s_1+s_2 = v] &= \mathbb P[s_1 = t | s_1 + s_2 = v] \\
&= \frac{\mathbb P[s_1 = t, s_2 = v-t]}{\mathbb P[s_1 + s_2 = v]} \\
&= \frac{{m_1 \choose t}\pi^{t}(1-\pi)^{m_1 - t}{m_2 \choose v-t}\pi^{v-t}(1-\pi)^{m_2 - (v-t)}}{{m_1 + m_2 \choose v}\pi^v (1-\pi)^{m_1 + m_2 - v}} \\
&= \frac{{m_1 \choose t}{m_2 \choose v-t}}{{m_1 + m_2 \choose v}}.
\end{split}
\end{equation}
Therefore, a finite-sample $p$-value to test $H_0: \pi_1 = \pi_2$ versus $H_1: \pi_1 > \pi_2$ is $\mathbb P[s_1 \geq t | s_1 + s_2]$, which can be computed exactly based on the formula above.

\section{Poisson regression} \label{sec:poisson-regression}

\subsection{Model definition and interpretation}

The Poisson regression model (with offsets) is
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \log \mu_i = o_i + \bm x_{i*}^T \bm \beta.
\label{eq:poisson-with-offsets}
\end{equation}
Because the log of the mean is linear in the predictors, Poisson regression models are often called \textit{loglinear models}. To interpret the coefficients, note that a unit increase in $x_j$ (while keeping the other variables fixed) is associated with an increase in the predicted mean by a factor of $e^{\beta_j}$.

\subsection{Example: Modeling rates}

One cool feature of the Poisson model is that rates can be easily modeled with the help of offsets. Let's say that the count $y_i$ is collected over the course of a time interval of length $t_i$, or a spatial region with area $t_i$, or a population of size $t_i$, etc. Then, it is meaningful to model
\begin{equation}
y_i \overset{\text{ind}} \sim \text{Poi}(\pi_i t_i), \quad \log \pi_i = \bm x^T_{i*}\bm \beta,
\end{equation}
where $\pi_i$ represents the rate of events per day / per square mile / per capita, etc. In other words,
\begin{equation}
y_i \overset{\text{ind}} \sim \text{Poi}(\mu_i), \quad \log \mu_i = \log t_i + \bm x^T_{i*}\bm \beta,
\end{equation}
which is exactly equation~\eqref{eq:poisson-with-offsets} with offsets $o_i = \log t_i$. For example, in single cell RNA-sequencing, $y_i$ is the number of reads aligning to a gene in cell $i$ and $t_i$ is the total number of reads measured in the cell, a quantity called the \textit{sequencing depth}. We might use a Poisson regression model to carry out a \textit{differential expression analysis} between two cell types.

\subsection{Estimation and inference}

\paragraph{Score, Fisher information, and Wald inference.}

We found in Chapter~\ref{ch:glm-theory} that the score and Fisher information for Poisson regression are
\begin{equation}
\bm U(\bm{ \beta}) = \bm X^T(\bm y - \bm{\mu}).
\end{equation}
and 
\begin{equation}
\bm I(\bm \beta) = \bm X^T \bm W \bm X = \bm X^T \text{diag}(V(\mu_i))\bm X = \bm X^T \text{diag}(\mu_i)\bm X,
\end{equation}
respectively. Hence, the normal equations for the MLE are
\begin{equation}
\bm X^T(\bm y - \bm{\hat \mu}).
\end{equation}
Wald inference is based on the approximation
\begin{equation}
\bm{\hat \beta} \overset \cdot \sim N(\bm \beta, (\bm X^T \text{diag}(\hat \mu_i)\bm X)^{-1}).
\end{equation}

\paragraph{Likelihood ratio inference.}

For likelihood ratio inference, we first derive the total deviance. The unit deviance of a Poisson distribution is
\begin{equation}
d(y, \mu) = y \log \frac{y}{\mu} - (y - \mu).
\end{equation}
Hence, the total deviance is
\begin{equation}
D(\bm y, \bm \mu) = \sum_{i = 1}^n d(y_i, \mu_i) = \sum_{i = 1}^n \left(y_i \log \frac{y_i}{\mu_i} - (y_i - \mu_i)\right).
\end{equation}
The residual deviance is then
\begin{equation}
D(\bm y, \bm{\hat\mu}) = \sum_{i = 1}^n \left(y_i \log \frac{y_i}{\hat \mu_i} - (y_i - \hat \mu_i)\right) = \sum_{i = 1}^n y_i \log \frac{y_i}{\hat \mu_i}.
\end{equation}
The last equality holds for any model containing the intercept, since by the normal equations we have $\sum_{i = 1}^n (y_i - \hat \mu_i) = \bm 1^T (\bm y - \bm{\hat \mu}) = 0$. We can carry out a likelihood ratio test for $H_0: \bm \beta_S = \bm 0$ via
\begin{equation}
D(\bm y, \bm{\hat \mu^0}) - D(\bm y, \bm{\hat \mu}) = \sum_{i = 1}^n y_i \log \frac{\hat \mu_i}{\hat \mu^0_{i}} \overset{\cdot}\sim \chi^2_{|S|}.
\end{equation}
We can carry out a goodness of fit test via
\begin{equation}
D(\bm y, \bm{\hat\mu})= \sum_{i = 1}^n y_i \log \frac{y_i}{\hat \mu_i} \overset{\cdot}\sim \chi^2_{n - p}.
\end{equation}

\paragraph{Score-based inference.}

Recalling equation~\eqref{eq:score-test-univariate}, the score test for $H_0: \beta_j = 0$ is based on
\begin{equation}
\frac{\bm x_{j*}^T (\bm y - \bm{\widehat \mu}^0)}{\sqrt{\left([(\bm X^T \text{diag}(\hat \mu^0_{i})\bm X)^{-1}]_{jj}\right)^{-1}}} \overset{\cdot}\sim N(0, 1).
\end{equation}
On the other hand, the score test for goodness of fit is based on the Pearson $X^2$ statistic:
\begin{equation}
X^2 \equiv \sum_{i = 1}^n \frac{(y_i - \hat \mu_i)^2}{\hat \mu_i} \overset{\cdot}\sim \chi^2_{n - p}.
\end{equation}

\subsection{Relationship between Poisson and multinomial distributions}

Suppose that $y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i)$ for $i = 1, \dots, n$. Then,
\begin{equation}
\begin{split}
\mathbb P\left[y_1 = m_1, \dots, y_n = m_n \left| \sum_{i}y_i = m\right.\right] &= \frac{\mathbb P[y_1 = m_1, \dots, y_n = m_n]}{\mathbb P[\sum_{i}y_i = m]} \\
&= \frac{\prod_{i = 1}^n e^{-\mu_i}\frac{\mu_i^{m_i}}{m_i!}}{e^{-\sum_i \mu_i}\frac{(\sum_i \mu_i)^m}{m!}} \\
&= {m \choose m_1, \dots, m_n}\prod_{i = 1}^n \pi_i^{m_i}; \quad \pi_i \equiv \frac{\mu_i}{\sum_{i' = 1}^n \mu_{i'}}.
\end{split}
\end{equation}
We recognize the last expression as the probability mass function of the multinomial distribution with parameters $(\pi_1, \dots, \pi_n)$ summing to one. In words, the joint distribution of a set of independent Poisson distributions conditional on their sum is a multinomial distribution.

\subsection{Example: $2 \times 2$ contingency tables}

\subsubsection{Poisson model for $2 \times 2$ contingency tables}

Let's say that we have two binary random variables $x_1, x_2 \in \{0,1\}$ with joint distribution $\mathbb P(x_1 = j, x_2 = k) = \pi_{jk}$ for $j,k \in \{0,1\}$. We collect a total of $n$ samples from this joint distribution and summarize the counts in a $2 \times 2$ table, where $y_{jk}$ is the number of times we observed $(x_1, x_2) = (j,k)$, so that
\begin{equation}
(y_{00}, y_{01}, y_{10}, y_{11})|n \sim \text{Mult}(n, (\pi_{00}, \pi_{01}, \pi_{10}, \pi_{11})).
\end{equation}
Our primary question is whether these two random variables are independent, i.e.
\begin{equation}
\pi_{jk} = \pi_{j+}\pi_{+k}, \quad \text{where} \quad \pi_{j+} \equiv \mathbb P[x_1 = j] = \pi_{j1} + \pi_{j2}; \quad \pi_{+k} \equiv \mathbb P[x_2 = k] = \pi_{1k} + \pi_{2k}.
\label{eq:null-product-formulation}
\end{equation}
We can express this equivalently as
\begin{equation}
\pi_{00}(\pi_{00} + \pi_{01} + \pi_{10} + \pi_{11}) = \pi_{00} = \pi_{0+}\pi_{+0} = (\pi_{00} + \pi_{01})(\pi_{00} + \pi_{10}) \quad \Longleftrightarrow \quad \pi_{00}\pi_{11} = \pi_{01}\pi_{10}.
\end{equation}
In other words, we can express the independence hypothesis concisely as
\begin{equation}
H_0: \frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}} = 1.
\label{eq:independence-2}
\end{equation}
Let's arbitrarily assume that, additionally, $n \sim \text{Poi}(\mu_{++})$. Then, by the relationship between Poisson and multinomial distributions, we have
\begin{equation}
y_{jk} \overset{\text{ind}} \sim \text{Poi}(\mu_{++}\pi_{jk}).
\end{equation}
Let $i \in 1,2,3,4$ index the four pairs $(x_1, x_2) \in \{(0,0), (0,1), (1,0), (1,1)\}$, so that
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \log \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_{12}x_{i1} x_{i2}, \quad i = 1, \dots, 4,
\label{eq:2-by-2-Poisson-reg}
\end{equation}
where
\begin{equation}
\beta_0 = \log \mu_{++} + \log \pi_{00}; \quad \beta_1 = \log \frac{\pi_{10}}{\pi_{00}}; \quad \beta_2 = \log \frac{\pi_{01}}{\pi_{00}}; \quad \beta_{12} = \log\frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}}.
\label{eq:2-by-2-Poisson-reg-coefs}
\end{equation}
Note that the independence hypothesis~\eqref{eq:independence-2} reduces to the hypothesis $H_0: \beta_{12} = 0$:
\begin{equation}
H_0: \frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}} = 1 \quad \Longleftrightarrow \quad H_0: \beta_{12} = 0.
\end{equation}
So the presence of an interaction in the Poisson regression is equivalent to a lack of independence between $x_1$ and $x_2$. We can test the latter hypothesis using our standard tools for Poisson regression. 

For example, we can use the Pearson $X^2$ goodness of fit test. To apply this test, we must find the fitted means under the null hypothesis model:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \log \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}, \quad i = 1, \dots, 4.
\label{eq:2-by-2-Poisson-reg-null}
\end{equation}
The normal equations give us the following:
\begin{equation*}
y_{++} \equiv \sum_{j, k = 0}^1 y_{jk} = \sum_{j, k = 0}^1 \hat \mu_{jk} \equiv \hat \mu_{++}; \quad y_{+1} \equiv \sum_{j = 0}^1 y_{j1} = \sum_{j = 0}^1 \hat \mu_{j1} \equiv \hat \mu_{+1}; \quad y_{+1} \equiv \sum_{k = 0}^1 y_{1k} = \sum_{k = 0}^1 \hat \mu_{1k} \equiv \hat \mu_{+1}.
\end{equation*}
By combining these equations, we arrive at
\begin{equation*}
\hat \mu_{++} = y_{++}; \quad \hat \mu_{j+} = y_{j+} \text{ for all } j \in \{0, 1\}; \quad \hat \mu_{+k} = y_{+k} \text{ for all } k \in \{0, 1\}.
\end{equation*}
Therefore, the fitted means under the null hypothesis model~\eqref{eq:2-by-2-Poisson-reg-null} are
\begin{equation}
\hat \mu_{jk} = \hat \mu_{++}\hat \pi_{jk} = \hat \mu_{++}\hat \pi_{j+}\hat \pi_{+k} = y_{++}\frac{y_{j+}}{y_{++}}\frac{y_{+k}}{y_{++}} = \frac{y_{j+}y_{+k}}{y_{++}}.
\end{equation}
Hence, we have
\begin{equation}
X^2 = \sum_{j,k = 0}^1 \frac{(y_{jk} - \widehat \mu_{jk})^2}{\widehat \mu_{jk}} = \sum_{j, k = 1}^1 \frac{(y_{jk} - y_{j+}y_{+k}/y_{++})^2}{y_{j+}y_{+k}/y_{++}}.
\end{equation}
Alternatively, we can use the likelihood ratio test, which gives
\begin{equation}
G^2 = 2\sum_{j,k = 0}^1 y_{jk}\log\frac{y_{jk}}{\widehat \mu_{jk}} = 2\sum_{j, k = 1}^1 y_{jk}\log\frac{y_{jk}}{y_{j+}y_{+k}/y_{++}}.
\end{equation}
We would compare both $X^2$ and $G^2$ to a $\chi^2_1$ distribution.

\subsubsection{Inference is the same regardless of conditioning on margins}

Now, our data might actually have been collected such that $n \sim \text{Poi}(\mu_{++})$, or maybe $n$ was fixed in advance. Is the Poisson inference proposed above actually valid in the latter case? In fact, it is! To see this, let us consider the log likelihoods of the two models:
\begin{equation}
p_{\bm \mu}(\bm y) = p_{\mu_{++}}(y_{++} = n)p_{\bm \pi}(\bm y|y_{++} = n), 
\end{equation}
so
\begin{equation}
\log p_{\bm \mu}(\bm y) = \log p_{\mu_{++}}(y_{++} = n) + \log p_{\bm \pi}(\bm y|y_{++} = n) = C + \log p_{\bm \pi}(\bm y|y_{++} = n).
\end{equation}
In other words, the log-likelihoods of the Poisson and multinomial models, as a function of $\bm \pi$, differ from each other by a constant. Therefore, any likelihood-based inference in these models is equivalent. The same argument shows that conditioning on the row or column totals (as opposed to the overall total) also yields the same exact inference. Therefore, regardless of the sampling mechanism, we can always conduct an independence test in a $2 \times 2$ table via a Poisson regression.

\subsubsection{Equivalence among Poisson and logistic regressions}

We've talked above two ways to view a $2 \times 2$ contingency table. In the logistic regression view, we thought about one variable as a predictor and the other as a response, seeking to test whether the predictor has an impact on the response. In the Poisson regression view, we thought about the two variables symmetrically, seeking to test independence. It turns out that these two perspectives are equivalent. Recall that we have derived in equations~\eqref{eq:2-by-2-Poisson-reg} and~\eqref{eq:2-by-2-Poisson-reg-coefs} that $x_1 \perp \! \! \! \perp x_2$ if and only if $\beta_{12} = 0$ in the Poisson regression
\begin{equation}
\log y_i \overset{\text{ind}}{\sim} \text{Poi}(\mu_i), \quad \log \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_{12} x_{i1}x_{i2}, \quad i = 1, \dots, 4.
\end{equation}
However, we have
\begin{equation}
\beta_{12} = \log \frac{\pi_{11}\pi_{00}}{\pi_{01}\pi_{10}} = \log \frac{\pi_{11}/\pi_{01}}{\pi_{01}/\pi_{00}} = \log \frac{\mathbb P[x_2 = 1 \mid x_2 = 1] / \mathbb P[x_2 = 0 \mid x_2 = 1]}{\mathbb P[x_2 = 1 \mid x_2 = 0] / \mathbb P[x_2 = 0 \mid x_2 = 0]}.
\end{equation}
Recalling the logistic regression of \(x_2\) on \(x_1\):
\begin{equation}
\text{logit }\mathbb P[x_2 = 1|x_1] = \tilde \beta_0 + \tilde \beta_{1} x_1,
\label{eq:2-by-2-logistic-reg}
\end{equation}
and that $\tilde \beta_{1}$ is the log odds ratio, we conclude that
\begin{equation}
\beta_{12} = \log \frac{\mathbb P[x_2 = 1 \mid x_2 = 1] / \mathbb P[x_2 = 0 \mid x_2 = 1]}{\mathbb P[x_2 = 1 \mid x_2 = 0] / \mathbb P[x_2 = 0 \mid x_2 = 0]} = \tilde \beta_{1},
\end{equation}
so $x_1 \perp \! \! \! \perp x_2$ if and only if $\tilde \beta_1 = 0$. Due to the equivalence between Poisson and multinomial distributions, the hypothesis tests and confidence intervals for the log odds ratio $\beta_{12}$ (or $\tilde \beta_1$) obtained from Poisson and logistic regressions will be the same.

\subsection{Example: Poisson models for $J \times K$ contingency tables}

Suppose now that $x_1 \in \{1, \dots, J\}$ and $x_2 \in \{1, \dots, K\}$. Then, we denote $\mathbb P[x_1 = j, x_2 = k] = \pi_{jk}$. We still are interested in testing for independence between $j$ and $k$, which amounts to a goodness-of-fit test for the Poisson model
\begin{equation}
y_{jk} \overset{\text{ind}}\sim\text{Poi}(\mu_{jk}); \quad \log \mu_{jk} = \beta_0 + \beta^1_j + \beta^2_k.
\end{equation}
The score (Pearson) and deviance-based goodness of fit statistics for this test are
\begin{equation}
X^2 = \sum_{j = 1}^J \sum_{k = 1}^K \frac{(y_{ij} - \widehat \mu_{ij})^2}{\widehat \mu_{ij}} \quad \text{and} \quad G^2 = 2\sum_{j = 1}^J \sum_{k = 1}^K y_{ij}\log \frac{y_{ij}}{\widehat \mu_{ij}}, \quad \text{where } \widehat \mu_{ij} = \widehat y_{++}\frac{y_{i+}}{y_{++}}\frac{y_{+j}}{y_{++}}. 
\end{equation}
Like with the $2 \times 2$ case, the test is the same regardless if we condition on the row sums, column sums, total count, or if we do not condition at all. The degrees of freedom in the full model is $JK$, while the degrees of freedom in the partial model is $J+K-1$, so the degrees of freedom for the goodness-of-fit test is $JK - J - K + 1 = (J-1)(K-1)$. Pearson erroneously concluded that the test had $JK-1$ degrees of freedom, which when Fisher corrected created a lot of animosity between these two statisticians.

\subsection{Example: Poisson models for $J \times K \times L$ contingency tables}

These ideas can be extended to multi-way tables, for example three-way tables. If we have $x_1 \in \{1, \dots, J\}, x_2 \in \{1, \dots, K\}, x_3 \in \{1, \dots, L\}$, then we might be interested in testing several kinds of null hypotheses:
\begin{itemize}
\item Mutual independence: $H_0: x_1 \perp \!\!\! \perp x_2 \perp \!\!\! \perp x_3$.
\item Joint independence: $H_0: x_1 \perp \!\!\! \perp (x_2, x_3)$.
\item Conditional independence: $H_0: x_1 \perp \!\!\! \perp x_2 \mid x_3$.
\end{itemize}
These three null hypotheses can be shown to be equivalent to the Poisson regression model
\begin{equation}
y_{jkl} \overset{\text{ind}}\sim \text{Poi}(\mu_{jkl}),
\end{equation}
where
\begin{equation}
\log \mu_{jkl} = \beta_0 + \beta^1_{j} + \beta^2_k + \beta^3_l \quad \text{(mutual independence)};
\end{equation}
\begin{equation}
\log \mu_{jkl} = \beta_0 + \beta^1_{j} + \beta^2_k + \beta^3_l + \beta^{2,3}_{kl} \quad \text{(joint independence)};
\end{equation}
\begin{equation}
\log \mu_{jkl} = \beta_0 + \beta^1_{j} + \beta^2_k + \beta^3_l + \beta^{1,3}_{jl} + \beta^{2,3}_{l} \quad \text{(conditional independence)}.
\end{equation}

\section{Negative binomial regression} \label{sec:nb-regression}

\paragraph{Overdispersion.} A pervasive issue with Poisson regression is \textit{overdispersion}: that the variances of observations are greater than the corresponding means. A common cause of overdispersion is omitted variable bias. Suppose that $y \sim \text{Poi}(\mu)$, where $\log \mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2$. However, we omitted variable $x_2$ and are considering the GLM based on $\log \mu = \beta_0 + \beta_1 x_1$. If $\beta_2 \neq 0$ and $x_2$ is correlated with $x_1$, then we have a confounding issue. Let's consider the more benign situation that $x_2$ is independent of $x_1$. Then, we have
\begin{equation}
\mathbb E[y|x_1] = \mathbb E[\mathbb E[y|x_1, x_2]|x_1] = \mathbb E[e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2}|x_1] = e^{\beta_0 + \beta_1 x_1}\mathbb E[e^{\beta_2 x_2}] = e^{\beta'_0 + \beta_1 x_1}.
\end{equation}
So in the model for the mean of $y$, the impact of omitted variable $x_2$ seems only to have impacted the intercept. Let's consider the variance of $y$:
\begin{equation}
\text{Var}[y|x_1] = \mathbb E[\text{Var}[y|x_1, x_2]|x_1] + \text{Var}[\mathbb E[y|x_1, x_2]|x_1] = e^{\beta'_0 + \beta_1 x_1} + e^{2(\beta'_0 + \beta_1 x_1)}\text{Var}[e^{\beta_2 x_2}] > e^{\beta'_0 + \beta_1 x_1} = \mathbb E[y|x_1].
\end{equation}
So indeed, the variance is larger than what we would have expected under the Poisson model.

\paragraph{Hierarchical Poisson regression.} Let's say that $y|\bm x \sim \text{Poi}(\lambda)$, where $\lambda|\bm x$ is random due to the fluctuations of the omitted variables. A common distribution used to model nonnegative random variables is the \textit{gamma} distribution $\Gamma(\mu, k)$, parameterized by a mean $\mu > 0$ and a \textit{shape} $k > 0$. This distribution has probability density function
\begin{equation}
f(\lambda; k, \mu) = \frac{(k/\mu)^k}{\Gamma(k)}e^{-k\lambda/\mu}\lambda^{k-1},
\end{equation}
with mean and variance given by
\begin{equation}
\mathbb E[\lambda] = \mu; \quad \text{Var}[\lambda] = \mu^2/k.
\end{equation}
Therefore, it makes sense to augment the Poisson regression model as follows:
\begin{equation}
\lambda|\bm x \sim \Gamma(\mu, k), \quad \log \mu = \bm x^T \bm \beta, \quad y | \lambda \sim \text{Poi}(\lambda).
\label{eq:nb-hierarchical}
\end{equation}

\paragraph{Negative binomial distribution.}

A simpler way to write the hierarchical model~\eqref{eq:nb-hierarchical} would be to marginalize out $\lambda$. Doing so leaves us with a count distribution called the \textit{negative binomial distribution}:
\begin{equation}
\lambda \sim \Gamma(\mu, k),\  y | \lambda \sim \text{Poi}(\lambda) \quad \Longrightarrow \quad y \sim \text{NegBin}(\mu, k).
\end{equation}
The negative binomial probability mass function is
\begin{equation}
p(y; \mu, k) = \int_0^\infty \frac{(k/\mu)^k}{\Gamma(k)}e^{-k\lambda/\mu}\lambda^{k-1}e^{-\lambda}\frac{\lambda^y}{y!}d\lambda = \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)}\left(\frac{\mu}{\mu + k}\right)^{y}\left(\frac{k}{\mu + k}\right)^{k}.
\end{equation}
This random variable has mean and variance given by
\begin{equation}
\mathbb E[y] = \mathbb E[\lambda] = \mu \quad \text{and} \quad \text{Var}[y] = \mathbb E[\lambda] + \text{Var}[\lambda] = \mu + \frac{\mu^2}{k}.
\end{equation}
As we send $k \rightarrow \infty$, the distribution of $\lambda$ tends to a point mass and the negative binomial distribution tends to $\text{Poi}(\mu)$.

\paragraph{Negative binomial as exponential dispersion model.}

Let us see whether we can express the negative binomial model as an exponential dispersion model. First, let us write out the probability mass function:
\begin{equation}
p(y; \mu, k) = \exp\left(y \log \frac{\mu}{\mu + k} - k \log \frac{\mu + k}{k}\right)\frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)}.
\end{equation}
Unfortunately, we run into difficulties expressing this probability mass function in EDM form, because there is not a neat decoupling between the natural parameter and the dispersion parameter. Indeed, for unknown $k$, the negative binomial model is \textit{not} an EDM. However, we can still express the negative binomial model as an EDM (in fact, a one-parameter exponential family) if we treat $k$ as known. In particular, we can read off that
\begin{equation}
\theta = \log \frac{\mu}{\mu + k}, \quad \psi(\theta) = k\log \frac{\mu + k}{k} = -k\log(1-e^{\theta}).
\label{eq:neg-bin-exp-fam}
\end{equation}
An alternate parameterization of the negative binomial model is via $\gamma = 1/k$. With this parameterization, we have
\begin{equation}
\mathbb E[y] = \mu \quad \text{and} \quad \text{Var}[y] = \mu + \gamma \mu^2.
\end{equation}
Here, $\gamma$ acts as a kind of dispersion parameter, as the variance of $y$ grows with $\gamma$. Note that the relationship between $\text{Var}[y]$ and $\gamma$ is not exactly proportional, as it is in EDMs. Nevertheless, the $\gamma$ parameter is often called the negative binomial \textit{dispersion}. Note that setting $\gamma = 0$ recovers the Poisson distribution.

\paragraph{Negative binomial regression.}

Let's revisit the hierarchical model~\eqref{eq:nb-hierarchical}, writing it more succinctly in terms of the negative binomial distribution:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{NegBin}(\mu_i, \gamma), \quad \log \mu_i = \bm x^T \bm \beta.
\end{equation}
Notice that we typically assume that all observations share the same dispersion parameter $\gamma$. Reading off from equation~\eqref{eq:neg-bin-exp-fam}, we see that the canonical link function for the negative binomial distribution is $\mu \mapsto \log \frac{\mu}{\mu + k}$. However, typically for negative binomial regression we use the log link $g(\mu) = \log \mu$ instead. This is the link of Poisson regression, and leads to more interpretable coefficient estimates. This is our first example of a non-canonical link!

\paragraph{Score and Fisher information.}

Recall from Chapter 4 that
\begin{equation}
\bm U(\bm \beta) = \frac{1}{\phi_0}\bm X^T \bm M \bm W (\bm y - \bm \mu) \quad \text{and} \quad \bm I(\bm \beta) = \frac{1}{\phi_0}\bm X^T \bm W \bm X,
\end{equation}
where
\begin{equation}
  \begin{split}
\bm W \equiv \text{diag}\left(\frac{w_i}{V(\mu_i)(d\eta_i/d\mu_i)^2}\right) \quad \text{and} \quad \bm M \equiv \text{diag}\left(\frac{\partial\eta_i}{\partial \mu_i}\right).
  \end{split}
\end{equation}
In our case, we have
\begin{equation}
w_i = 1; \quad V(\mu_i) = \mu_i + \gamma \mu_i^2; \quad \frac{\partial\eta_i}{\partial \mu_i} = \frac{1}{\mu_i}.
\end{equation}
Putting this together, we find that
\begin{equation}
\bm W = \text{diag}\left(\frac{\mu_i}{1 + \gamma \mu_i}\right); \quad \bm M = \text{diag}\left(\frac{1}{1 + \gamma \mu_i}\right).
\end{equation}

\paragraph{Estimation in negative binomial regression.}

Negative binomial regression is an EDM when $\gamma$ is known, but typically the dispersion parameter is unknown. Note that there is a dependency in $\psi$ on $k$ (i.e. on $\gamma$), which complicates things. It means that the estimate $\bm{\widehat \beta}$ depends on the parameter $\gamma$ (this does not happen, for example, in the normal linear model case).\footnote{Having said that, the dependency between $\bm{\widehat \beta}$ and $\widehat \gamma$ is weak, as the two are asymptotically independent parameters.} Therefore, estimation in negative binomial regression is typically an iterative procedure, where at each step $\bm \beta$ is estimated for the current value of $\gamma$ and then $\gamma$ is estimated based on the updated value of $\bm \beta$. Let's discuss each of these tasks in turn. Given a value of $\widehat \gamma$, we have the normal equations
\begin{equation}
\bm X^T \text{diag}\left(\frac{1}{1 + \widehat \gamma \widehat \mu_i}\right)(\bm y - \bm{\widehat \mu}) = 0.
\end{equation}
This reduces to the Poisson normal equations when $\widehat \gamma = 0$. Solving these equations for a fixed value of $\widehat \gamma$ can be done via IRLS, as usual. Estimating $\gamma$ for a fixed value of $\bm{\widehat \beta}$ can be done in several ways, including setting to zero the derivative of the likelihood with respect to $\gamma$. This results in a nonlinear equation (not given here) that can be solved iteratively.

\paragraph{Wald inference.}

Wald inference is based on
\begin{equation}
\widehat{\text{Var}}[\bm{\widehat \beta}] = (\bm X^T \bm{\widehat W} \bm X)^{-1}, \quad \text{where} \quad \bm{\widehat W} = \text{diag}\left(\frac{\widehat \mu_i}{1 + \widehat\gamma \widehat\mu_i}\right).
\end{equation}

\paragraph{Likelihood ratio test inference.}

The negative binomial deviance is
\begin{equation}
D(\bm y; \bm{\widehat \mu}) = 2\sum_{i = 1}^n \left(y_i \log \frac{y_i}{\widehat \mu_i} - \left(y_i + \frac{1}{\widehat \gamma}\right)\log \frac{1 + \widehat \gamma y_i}{1 + \widehat \gamma \widehat \mu_i}\right).
\end{equation}
We can use this for comparing nested models, \textbf{but not for goodness of fit testing!} The issue is that we have estimated the parameter $\gamma$, whereas goodness of fit tests are applicable only when the dispersion parameter is known.

\paragraph{Testing for overdispersion.}

It is reasonable to want to test for overdispersion, i.e. to test the null hypothesis $H_0: \gamma = 0$. This is somewhat of a tricky task, because $\gamma = 0$ is at the edge of the parameter space. We can do so using a likelihood ratio test. As it turns out, the likelihood ratio statistic $T^{\text{LRT}}$ has asymptotic null distribution
\begin{equation*}
T^{\text{LRT}} \equiv 2(\ell^{\text{NB}} - \ell^{\text{Poi}}) \overset \cdot \sim \frac{1}{2}\delta_0 + \frac{1}{2}\chi^2_1.
\end{equation*}
Here, $\delta_0$ is the delta mass at zero. The reason for this is that, under the null, we can view the estimated dispersion parameter as being symmetrically distributed around 0. However, since the dispersion parameter is nonnegative, this means it gets rounded up to 0 with probability 1/2. Therefore, the likelihood ratio test for $H_0: \gamma = 0$ rejects when
\begin{equation}
T^{\text{LRT}} > \chi^2_1(1-2\alpha).
\end{equation}
Note that the above test for overdispersion can be viewed as a goodness of fit test for the Poisson GLM. It is different from the usual GLM goodness of fit tests, because the saturated model against which the latter tests stays in the Poisson family. Nevertheless, significant results in standard goodness of fit tests for Poisson GLMs are often an indication of overdispersion. Or, they may indicate omitted variable bias (e.g. you forgot to include an interaction), so it's somewhat tricky.

\paragraph{Overdispersion in logistic regression.}

Note that overdispersion is potentially an issue not only in Poisson regression models, but in logistic regression models as well. Dealing with overdispersion in the latter case is more tricky, because the analog of the negative binomial model (the beta-binomial model) is not an exponential family. An alternate route to dealing with overdispersion is quasi-likelihood modeling, but this topic is beyond the scope of the course.

\section{R demo} \label{sec:r-demo}

\subsection{Contingency table analysis}

Let's take a look at the UC Berkeley admissions data:
<<>>=
ucb_data <- UCBAdmissions |> as_tibble()
ucb_data
@

It contains data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex. Let's see whether there is an association between \verb|Gender| and \verb|Admit|. Let's first aggregate over department:
<<>>=
ucb_data_agg <- ucb_data |>
  group_by(Admit, Gender) |>
  summarise(n = sum(n), .groups = "drop")
ucb_data_agg
@

Let's see what the admissions rates are by gender:
<<>>=
ucb_data_agg |>
  group_by(Gender) |>
  summarise(`Admission rate` = sum(n*(Admit == "Admitted"))/sum(n))
@
This suggests that men have substantially higher admission rates than women. Let's see if we can confirm this using either a Fisher's exact test or a Pearson chi-square test.
<<>>=
# first convert to 2x2 table format
admit_vs_gender <- ucb_data_agg |>
  pivot_wider(names_from = Gender, values_from = n) |>
  column_to_rownames(var = "Admit")
admit_vs_gender

# Fisher exact test (note that the direction of the effect can be deduced)
fisher.test(admit_vs_gender)

# Chi-square test
chisq.test(admit_vs_gender)
@

As a sanity check, let's run the Poisson regression underlying the chi-square test above.
<<>>=
pois_fit <- glm(n ~ Admit + Gender + Admit*Gender,
                family = "poisson",
                data = ucb_data_agg)
summary(pois_fit)
@

Based on all of these tests, there seems to be a very substantial difference in admissions rates based on gender. That is not good.

But perhaps, women tend to apply to more selective departments? Let's look into this:
<<fig.width=3, fig.height = 3, fig.align='center'>>=
ucb_data |>
  group_by(Dept) |>
  summarise(admissions_rate = sum(n*(Admit == "Admitted"))/sum(n),
            prop_female_applicants = sum(n*(Gender == "Female"))/sum(n)) |>
  ggplot(aes(x = admissions_rate, y = prop_female_applicants)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "Admissions rate",
       y = "Proportion female applicants")
@

Indeed, it does seem that female applicants typically applied to more selective departments! This suggests that it is very important to control for department when evaluating the association between admissions and gender. To do this, we can run a test for conditional independence in the $J \times K \times L$ table:

<<>>=
pois_fit <- glm(n ~ Admit + Dept + Gender + Admit:Dept + Gender:Dept ,
                family = "poisson",
                data = ucb_data)
pchisq(sum(resid(pois_fit, "pearson")^2),
  df = pois_fit$df.residual,
  lower.tail = FALSE
)
@
Still we find a significant effect! But what is the direction of the effect? The chi square test does not tell us. We can simply compute the admissions rates by department and plot them:
<<fig.width=3, fig.height = 3, fig.align='center'>>=
ucb_data |>
  group_by(Dept, Gender) |>
  summarise(`Admission rate` = sum(n*(Admit == "Admitted"))/sum(n),
            .groups = "drop") |>
  pivot_wider(names_from = Gender, values_from = `Admission rate`) |>
  ggplot(aes(x = Female, y = Male, label = Dept)) +
  geom_point() +
  ggrepel::geom_text_repel() +
  geom_abline(color = "red", linetype = "dashed") +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = "Female admission rate",
       y = "Male admission rate")
@
Now the difference doesn't seem so huge, with most departments close to even and with department A heavily skewed towards admitting women!

\subsection{Revisiting the crime data, again}

<<message = FALSE>>=
library(tidyverse)
@
\noindent Here we are again, face to face with the crime data, with one last chance to get the analysis right. Let's load and preprocess it, as before.
<<message = FALSE>>=
# read crime data
crime_data <- read_tsv("data/Statewide_crime.dat")

# read and transform population data
population_data <- read_csv("data/state-populations.csv")
population_data <- population_data |>
  filter(State != "Puerto Rico") |>
  select(State, Pop) |>
  rename(state_name = State, state_pop = Pop)

# collate state abbreviations
state_abbreviations <- tibble(
  state_name = state.name,
  state_abbrev = state.abb
) |>
  add_row(state_name = "District of Columbia", state_abbrev = "DC")

# add CrimeRate to crime_data
crime_data <- crime_data |>
  mutate(STATE = ifelse(STATE == "IO", "IA", STATE)) |>
  rename(state_abbrev = STATE) |>
  filter(state_abbrev != "DC") |> # remove outlier
  left_join(state_abbreviations, by = "state_abbrev") |>
  left_join(population_data, by = "state_name") |>
  select(state_abbrev, Violent, Metro, HighSchool, Poverty, state_pop)

crime_data
@
\noindent Let's recall the logistic regression we ran on these data in Chapter 4:
<<>>=
bin_fit <- glm(Violent / state_pop ~ Metro + HighSchool + Poverty,
  weights = state_pop,
  family = "binomial",
  data = crime_data
)
@
\noindent We had found very poor results from the goodness of fit test for this model. We have therefore omitted some important variables and/or we have serious overdispersion on our hands.

\noindent We haven't discussed in any detail how to deal with overdispersion in logistic regression models, so let's try a Poisson model instead. The natural way to model rates using Poisson distributions is via offsets:
<<>>=
pois_fit <- glm(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),
  family = "poisson",
  data = crime_data
)
summary(pois_fit)
@
\noindent Again, everything is significant, and again, the regression summary shows that we have a huge residual deviance. This was to be expected, given that $\text{Bin}(m, \pi) \approx \text{Poi}(m\pi)$ for large $m$ and small $\pi$. So, the natural thing to try is a negative binomial regression! Negative binomial regression is not implemented in the regular \verb|glm| package, but \verb|glm.nb()| from the \verb|MASS| package is a dedicated function for this task. Let's see what we get:
<<>>=
nb_fit <- MASS::glm.nb(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),
  data = crime_data
)
summary(nb_fit)
@
\noindent Aha! Things are not looking so significant anymore! And the residual deviance is not as huge! Although, we must be careful! The residual deviance no longer has the usual $\chi^2$ distribution because of the estimated dispersion parameter. So we don't really have an easy goodness of fit test. The estimated value of $\gamma$ (confusingly called $\theta$ in the summary) is significantly different from zero, indicating overdispersion. Let's formally test for overdispersion using the nonstandard likelihood ratio test discussed above:
<<>>=
T_LRT <- 2 * (as.numeric(logLik(nb_fit)) - as.numeric(logLik(pois_fit)))
p_LRT <- pchisq(T_LRT, df = 1, lower.tail = FALSE)/2
p_LRT
@
So at the very least the NB model fits much better than the Poisson model. Let's do some inference based on this model. For example, we can get Wald confidence intervals:
<<>>=
confint.default(nb_fit)
@
\noindent Or we can get LRT-based (i.e. profile) confidence intervals:
<<>>=
confint(nb_fit)
@
\noindent Or we can get confidence intervals for the predicted means:
<<>>=
predict(nb_fit,
  newdata = crime_data |> column_to_rownames(var = "state_abbrev"),
  type = "response",
  se.fit = TRUE
)
@
\noindent We can carry out some hypothesis tests as well, e.g. to test $H_0: \beta_{\text{Metro}} = 0$:
<<>>=
nb_fit_partial <- MASS::glm.nb(Violent ~ HighSchool + Poverty + offset(log(state_pop)),
  data = crime_data
)
anova_fit <- anova(nb_fit_partial, nb_fit)
anova_fit
@

\chapter{Multiple testing} \label{sec:multiple-testing}

\section{Introduction to multiple testing}

Consider the problem of assessing which variables in a GLM have nonzero coefficients. In the preceding chapters, we have described a variety of tests for obtaining $p$-values for each coefficient. Given this set of $p$-values (call them $p_1$, \dots, $p_m$), we must determine which variables to deem significant. As it turns out, this task is a nontrivial one for several reasons, the first of which is the \textit{multiplicity problem}.

\subsection{The multiplicity problem}

When R prints a regression summary, it adds stars to variables based on their $p$-values. Variables with $p$-values below 0.05 get one star, those with $p$-values below 0.01 get two stars, and those with $p$-values below 0.001 get three stars. A natural strategy for selecting significant variables is to choose those with one or more stars. However, the issue with this strategy is that even null variables (those with coefficients of zero) will sometimes have small $p$-values by chance (Figure~\ref{fig:spurious-correlation}). The more total variables we are testing, the more of them will have small $p$-values by chance. This is the \textit{multiplicity problem}.

\begin{figure}[h!]
\centering
\includegraphics[width = 0.75\textwidth]{figures/spurious-correlation.png}
\caption{A spurious correlation resulting from data snooping.}
\label{fig:spurious-correlation}
\end{figure}

To quantify this issue, consider the case when all $m$ variables under consideration are null. Then, the chance that any one of them has a $p$-value below 0.05 is 0.05. So, the expected number of variables with one or more stars is $0.05m$. For example, if we have 100 variables, then we expect to see 5 variables with stars on average, even though none of the variables are actually relevant to the response! The growth of the quantity $0.05m$ with $m$ confirms that the multiplicity problem grows more severe as the number of hypotheses tested increases.

Another way of thinking about the multiplicity problem is in the context of \textit{selection bias}. The process of scanning across all variables and selecting those with small $p$-values is a \textit{selection event}. Once the selection event has occurred, one must consider the null distribution of a $p$-value \textit{conditionally on the fact that it was selected}. Since the selection event favors small $p$-values, the null distribution of a $p$-value conditional on selection is no longer uniform; it becomes skewed toward zero. Interpreting $p$-values (and their accompanying stars) ``at face value'' is misleading because it ignores the crucial selection step. Other terms for this include ``data snooping'' and ``p-hacking.''

The multiplicity problem is not limited to regression. In the next two sections, we develop some definitions to describe the multiplicity problem more formally and generally.

\subsection{Global testing and multiple testing}

Suppose we have $m$ null hypotheses $H_{01}, \dots, H_{0m}$. Let $p_1, \dots, p_m$ be the corresponding $p$-values.
\begin{definition}
A $p$-value $p_j$ for a null hypothesis $H_{0j}$ is \textit{valid} if
\begin{equation}
\mathbb P_{H_{0j}}[p_j \leq t] \leq t \quad \text{for all } t \in [0, 1].
\label{eq:valid-p-value}
\end{equation}
\end{definition}
\noindent This definition covers the uniform distribution, as well as distributions that are stochastically larger than uniform. Distributions of the latter kind are often obtained from resampling-based tests, such as permutation tests. In the remainder of this chapter, we will assume that all $p$-values are valid.

Given a set of $p$-values, there are several inferential goals potentially of interest. These can be subdivided first into \textit{global testing} and \textit{multiple testing}.
\begin{definition}
A \textit{global testing procedure} is a test of the \textit{global null hypothesis}
\begin{equation}
H_0 \equiv \bigcap_{j = 1}^m H_{0j}.
\end{equation}
In other words, it is a function $\phi: (p_1, \dots, p_m) \mapsto [0,1]$. A global test has level $\alpha$ if it controls the Type-I error at this level:
\begin{equation}
\mathbb E_{H_0}[\phi(p_1, \dots, p_m)] \leq \alpha.
\label{eq:global-test-type-1-error}
\end{equation}
\end{definition}
\noindent A global testing procedure determines whether \textit{any} of the null hypotheses can be rejected. In regression modeling, a global test would be a test of the hypothesis $H_0: \beta_1 = \cdots = \beta_m = 0$.

\begin{definition}
A \textit{multiple testing procedure} is a mapping from the set of $p$-values to a set of hypotheses to reject:
\begin{equation}
\mathcal M: (p_1, \dots, p_m) \mapsto \hat S \subseteq \{1, \dots, m\}.
\end{equation}
\end{definition}
\noindent A multiple testing procedure determines \textit{which} of the null hypotheses can be rejected. In regression modeling, a multiple testing procedure would be a method for selecting which variables have nonzero coefficients, the problem we discussed in the beginning of this section.

While evaluating a global testing procedure is straightforward, evaluating a multiple testing procedure is more nuanced. We define two potential inferential goals for a multiple testing procedure in the next section. 

\subsection{Multiple testing goals} \label{subsec:multiple-testing-goals}

Let us define
\begin{equation}
\mathcal H_0 \equiv \{j \in \{1, \dots, m\}: H_{0j} \text{ is true}\} \quad \text{and} \quad \mathcal H_1 \equiv \{j \in \{1, \dots, m\}: H_{0j} \text{ is false}\}.
\end{equation}
In other words, $\mathcal H_0$ is the set of indices of the true null hypotheses, and $\mathcal H_1$ is the set of indices of the false null hypotheses. There a two primary notions of Type-I error that multiple testing procedures seek to control: the \textit{family-wise error rate} (FWER) and the \textit{false discovery rate} (FDR).

\paragraph{Definitions of Type-I error rate and power.}

\begin{definition}[Tukey, 1953]
The family-wise error rate (FWER) of a multiple testing procedure $\mathcal M: (p_1, \dots, p_m) \mapsto \hat S$ is the probability that it makes any false rejections:
\begin{equation}
\text{FWER}(\mathcal M) \equiv \mathbb P[\hat S \cap \mathcal H_0 \neq \varnothing].
\end{equation}
A multiple testing procedure controls the FWER at level $\alpha$ if
\begin{equation}
\text{FWER}(\mathcal M) \leq \alpha.
\end{equation}
\end{definition}

\begin{definition}[Benjamini and Hochberg, 1995]
The false discovery proportion (FDP) of a rejection set $\hat S$ is the proportion of these rejections that are false:
\begin{equation}
\text{FDP}(\hat S) \equiv \frac{|\hat S \cap \mathcal H_0|}{|\hat S|}, \quad \text{where} \quad \frac{0}{0} \equiv 0.
\end{equation}
The false discovery rate (FDR) a multiple testing procedure $\mathcal M: (p_1, \dots, p_m) \mapsto \hat S$ is its expected false discovery proportion:
\begin{equation}
\text{FDR}(\mathcal M) \equiv \mathbb E[\text{FDP}(\hat S)] = \mathbb E\left[\frac{|\hat S \cap \mathcal H_0|}{|\hat S|}\right].
\label{eq:fdr-def}
\end{equation}
A multiple testing procedure controls the FDR at level $q$ if
\begin{equation}
\text{FDR}(\mathcal M) \leq q.
\end{equation}
\end{definition}
Regardless of what error rate a multiple testing procedure is intended to control, we would like it to have high \textit{power}:
\begin{equation}
\text{power}(\mathcal M) \equiv \mathbb E\left[\frac{|\hat S \cap \mathcal H_1|}{|\mathcal H_1|}\right].
\end{equation}

\paragraph{Relationship between the FWER and FDR.} Note that the FWER is a probability, while the FDR is an expected proportion. This distinction is highlighted by using the different symbols $\alpha$ and $q$ for the nominal FWER and FDR levels, respectively. The FWER is a more stringent error rate than the FDR, because it can only be low if \textit{no} false discoveries are made most of the time; the FDR can be low if false discoveries are a small proportion of the total number of discoveries most of the time. 
\begin{proposition}
For any multiple testing procedure $\mathcal M$, we have $\text{FDR}(\mathcal M) \leq \text{FWER}(\mathcal M)$. Therefore, a multiple testing procedure controlling the FWER at level $\alpha$ also controls the FDR at level $\alpha$.
\end{proposition}
\begin{proof}
\begin{equation}
\text{FDR} \equiv \mathbb E\left[\frac{|\hat S \cap \mathcal H_0|}{|\hat S|}\right] \leq \mathbb E\left[\mathbbm 1(|\hat S \cap \mathcal H_0| > 0)\right] \equiv \text{FWER}.
\end{equation}
\end{proof}
\noindent The FWER was the error rate of choice in the 20th century, when limitations on data collection permitted only small handfuls of hypotheses to be tested. In the 21st century, the internet and other new technologies permitted much larger-scale collection of data, leading to much larger sets of hypotheses being tested (e.g. tens of thousands). In this context, the less stringent FDR rate became more popular. In many cases, an initial large-scale FDR-controlling procedure is viewed as an \textit{exploratory analysis}, whose goal is to nominate a smaller number of hypotheses for confirmation with follow-up experiments. The purpose of controlling the FDR in this context is to limit resources wasted on following up false leads. 

\section{Global testing} \label{sec:global-testing}

Recall that a global test is a test of the intersection null hypothesis $H_0 \equiv \cap_{j = 1}^m H_{0j}$. Let us first examine the naive global test, which rejects if any of the $p$-values are below $\alpha$:
\begin{equation}
\phi_{\text{naive}}(p_1, \dots, p_m) = \mathbbm 1\left(p_j \leq \alpha \text{ for some } j = 1, \dots, m\right).
\label{eq:naive-global-test}
\end{equation}
This test does not control the Type-I error. In fact, assuming the input $p$-values are independent, we have
\begin{equation}
\mathbb E_{H_0}[\phi_{\text{naive}}(p_1, \dots, p_m)] = 1-(1-\alpha)^m \rightarrow 1 \quad \text{as} \quad m \rightarrow \infty.
\end{equation}
This is a manifestation of the multiplicity problem discussed before. In this section, we will discuss two ways of adjusting for multiplicity in the context of global testing:
\begin{itemize}
\item Bonferroni test: Powerful against few strong signals.
\item Fisher combination test: Powerful against many weak signals.
\end{itemize}
Each test is listed with the alternative against which it is powerful. Note that in the context of global testing and multiple testing, the alternative is a multivariate object. The main difference between the Bonferroni test and the Fisher combination test is how the signal (i.e. deviation from the null) is spread across the $m$ hypotheses being tested. If the signal is highly concentrated in a few non-null hypotheses, then the Bonferroni test is better. If the signal is spread out over many non-null hypotheses, then the Fisher combination test is better.

\subsection{Bonferroni global test (Bonferroni, 1936 and Dunn, 1961)}

\paragraph{Test definition and validity.} The motivation for the Bonferroni global test is to find the strongest signal among the $p$-values, and reject the global null if this signal is strong enough. It makes sense that such a strategy would be powerful against sparse alternatives. We define the Bonferroni test via
\begin{equation}
\phi(p_1, \dots, p_m) \equiv \mathbbm 1\left(\min_{1 \leq j \leq m} p_j \leq \alpha/m\right).
\end{equation}
The Bonferroni global test rejects if any of the $p$-values crosses the \textit{multiplicity-adjusted} or \textit{Bonferroni-adjusted} significance threshold of $\alpha/m$. This test can be viewed as a modified version of the naive test~\eqref{eq:naive-global-test}, but with the significance threshold $\alpha$ adjusted downward to $\alpha/m$. The more hypotheses we test, the more stringent the significance threshold must be. 

\begin{proposition}
The Bonferroni test controls the FWER at level $\alpha$ for any joint dependence structure among the $p$-values.
\end{proposition}
\begin{proof}
We can verify the Type-I error control of the Bonferroni test via a union bound:
\begin{equation}
\mathbb P_{H_0}\left[\min_{1 \leq j \leq m} p_j \leq \alpha/m\right] \leq \sum_{j = 1}^m \mathbb P_{H_{0j}}\left[p_j \leq \alpha/m\right] = m \cdot \alpha/m = \alpha.
\end{equation}
\end{proof}

\paragraph{The impact of $p$-value dependence.} While the Bonferroni global test is valid for arbitrary $p$-value dependence structures, the underlying union bound may be loose for certain dependence structures. In particular, the Bonferroni bound derived above is tightest for independent $p$-values. Intuitively, the smallest $p$-value has the highest chance of being small if each $p$-value has its own independent source of randomness. Mathematically, let us compute the Type-I error of the Bonferroni global test under independence:
\begin{equation}
\mathbb P_{H_0}\left[\min_{1 \leq j \leq m} p_j \leq \alpha/m\right] = 1 - (1-\alpha/m)^m \approx \alpha.
\end{equation}
Therefore, the Bonferroni test exhausts essentially its entire level under independence. On the other hand, under perfect dependence (i.e. $p_1 = \cdots = p_m$ almost surely), the Bonferroni test is quite conservative:
\begin{equation}
\mathbb P_{H_0}\left[\min_{1 \leq j \leq m} p_j \leq \alpha/m\right] = \mathbb P_{H_{01}}\left[p_1 \leq \alpha/m\right] = \alpha/m.
\end{equation}
In this special case, the level is $m$ times lower than it should be, because no multiplicity adjustment is needed if the $p$-values are perfectly dependent.

\subsection{Fisher combination test (Fisher, 1925)}

If, on the other hand, we expect the signal to be spread out over many non-null hypotheses, the valuable evidence against the alternative is missed if only the minimum $p$-value is considered. In such circumstances, the Fisher combination test may be more powerful than the Bonferroni global test.

\paragraph{Test definition and validity.}

The Fisher combination test is based on the observation that
\begin{equation}
\text{if } p \sim U[0,1], \quad \text{then} \quad -2\log p \sim \chi^2_2.
\end{equation}
Therefore, if $p_1, \dots, p_m$ are independent uniform random variables, then we have
\begin{equation}
-2\sum_{j = 1}^m \log p_j \sim \chi^2_{2m}.
\end{equation}
This leads to the Fisher combination test:
\begin{equation}
\phi(p_1, \dots, p_m) \equiv \mathbbm 1\left(-2\sum_{j = 1}^m \log p_j \geq \chi^2_{2m}(1-\alpha)\right).
\label{eq:fishers-formula}
\end{equation}
\begin{proposition}
The Fisher combination test controls Type-I error at level $\alpha$~\eqref{eq:global-test-type-1-error} if the $p$-values are independent.
\end{proposition}
\begin{proof}
Under the null, the $p$-values are stochastically larger than uniform~\eqref{eq:fishers-formula}. Therefore, $-2\sum_{j = 1}^m \log p_j$ is stochastically larger than $\chi^2_{2m}$, from which the conclusion follows.
\end{proof}

\paragraph{Discussion.}

The Fisher exact test has a similar flavor to another chi-squared test. Suppose $X_j \sim N(\mu_j, 1)$, and we would like to test $H_j: \mu_j = 0$. Under the global null, we have
\begin{equation}
\text{if } X_1, \dots, X_m \overset{\text{i.i.d.}}\sim N(0,1), \text{ then } \sum_{j = 1}^m X_j^2 \sim \chi^2_m.
\label{eq:chisquare-formula}
\end{equation}
It turns out that the tests based on equation~\eqref{eq:fishers-formula} and~\eqref{eq:chisquare-formula} are quite similar. This helps us build intuition for what the Fisher combination test is doing. It's averaging the strengths of the signal across hypotheses.

The independence assumption of the Fisher combination test makes it significantly less broadly applicable than the Bonferroni global test. However, one common application of the Fisher combination test is \textit{meta-analysis}: The combination of information across multiple studies of the same hypothesis (or very related hypotheses). In this setting, the $p$-values are independent across studies, and the Fisher combination test is a natural choice because the strength of the signal is roughly the same across studies because they are studying very related hypotheses.

\section{Multiple testing} \label{subsec:multiple-testing}

Here we present one method each for FWER and FDR control. 

\subsection{The Bonferroni procedure for FWER control}

We discussed the Bonferroni test for the global null. This test can be extended to an FWER-controlling procedure:
\begin{equation}
\hat S \equiv \{j: p_j \leq \alpha/m\}.
\end{equation}
\begin{proposition}
The Bonferroni procedure controls the FWER at level $\alpha$ for arbitrary $p$-value dependence structures.
\end{proposition}
\begin{proof}
We have
\begin{equation}
\mathbb P[\hat S \cap \mathcal H_0 \neq \varnothing] = \mathbb P\left[\min_{j \in \mathcal H_0} p_j \leq \alpha/m\right] \leq \sum_{j \in \mathcal H_0} \mathbb P[p_j \leq \alpha/m] = \frac{|\mathcal H_0|}{m}\alpha \leq \alpha.
\end{equation}
\end{proof}

Note that the FWER is actually controlled at the level $\frac{|\mathcal H_0|}{m}\alpha \leq \alpha$, making the Bonferroni test conservative to the extent that $|\mathcal H_0| < m$. The null proportion $\frac{|\mathcal H_0|}{m}$ has such an effect on the performance of many multiple testing procedures. Not all global tests can be extended to FWER-controlling procedures in this way. For example, the Fisher combination test does not single out any of the hypotheses, as it only aggregates the $p$-values. By contrast, the Bonferroni test searches for $p$-values that are individually very small, allowing for it to double as an FWER-controlling procedure.

\subsection{The Benjamini-Hochberg procedure for FDR control}

Designing procedures with FDR control, as well as verifying the latter property, is typically harder than for FWER control. It is harder to decouple the effects of the individual hypotheses, as the denominator $|S|$ in the FDR definition~\eqref{eq:fdr-def} couples them together. Both the FDR criterion and the most popular FDR-controlling procedure were proposed by Benjamini and Hochberg in 1995.

\paragraph{Procedure.} To define the BH procedure, consider thresholding the $p$-values at $t \in [0,1]$. We would expect $\mathbb E[|\{j: p_j \leq t\} \cap \mathcal H_0|] = |\mathcal H_0|t$ false discoveries among $\{j: p_j \leq t\}$. Since $|\mathcal H_0|$ is unknown, we can bound it from above by $mt$. This leads to the FDP estimate
\begin{equation}
\widehat{\text{FDP}}(t) \equiv \frac{mt}{|\{j: p_j \leq t\}|}.
\end{equation}
The BH procedure is then defined via
\begin{equation}
\hat S \equiv \{j: p_j \leq \widehat t\}, \quad \text{where} \quad \widehat t = \max\{t \in [0,1]: \widehat{\text{FDP}}(t) \leq q\}.
\end{equation}
In words, we choose the most liberal $p$-value threshold for which the estimated FDP is below the nominal level $q$. Note that the set over which the above maximum is taken is always nonempty because it at least contains 0: $\widehat{\text{FDP}}(0) = \frac{0}{0} \equiv 0$.

\paragraph{FDR control under independence.} Benjamini and Hochberg established that their procedure controls the FDR if the $p$-values are independent. Here we present an alternative argument due to Storey, Taylor, and Siegmund (2004).

\begin{proposition}
The BH procedure controls the FDR at level $q$ assuming that the $p$-values are independent.
\end{proposition}
\begin{proof}
We have
\begin{equation}
\begin{split}
\text{FDR} = \mathbb E\left[\text{FDP}(\widehat t)\right] &= \mathbb E\left[\frac{|\{j \in \mathcal H_0: p_j \leq \widehat t\}|}{|\{j: p_j \leq \widehat t\}|}\right] \\
&= \mathbb E\left[\frac{|\{j \in \mathcal H_0: p_j \leq \widehat t\}|}{m\widehat t}\widehat{\text{FDP}}(\widehat t)\right] \leq q \cdot \mathbb E\left[\frac{|\{j \in \mathcal H_0: p_j \leq \widehat t\}|}{m\widehat t}\right].
\end{split}
\end{equation}
To prove that the last expectation is bounded above by 1, note that
\begin{equation}
M(t) \equiv \frac{|\{j \in \mathcal H_0: p_j \leq t\}|}{m t}
\end{equation}
is a backwards martingale with respect to the filtration
\begin{equation}
\mathcal F_t = \sigma(\{p_j: j \in \mathcal H_1\}, |\{j \in \mathcal H_0: p_j \leq t'\}| \text{ for } t' \geq t),
\end{equation}
with $t$ running backwards from 1 to 0. Indeed, for $s < t$ we have
\begin{equation}
\mathbb E[M(s)|\mathcal F_t] = \mathbb E\left[\left.\frac{|\{j \in \mathcal H_0: p_j \leq s\}|}{m s} \right| \mathcal F_t\right] = \frac{\frac{s}{t}|\{j \in \mathcal H_0: p_j \leq t\}|}{m s} = \frac{|\{j \in \mathcal H_0: p_j \leq t\}|}{m t} = M(t).
\end{equation}
The threshold $\widehat t$ is a stopping time with respect to this filtration, so by the optional stopping theorem, we have
\begin{equation}
\mathbb E\left[\frac{|\{j \in \mathcal H_0: p_j \leq \widehat t\}|}{m\widehat t}\right] = \mathbb E[M(\widehat t)] \leq \mathbb E[M(1)] = \frac{|\mathcal H_0|}{m} \leq 1.
\end{equation}
This completes the proof.
\end{proof}

\paragraph{FDR control under dependence.}

Under dependence, the BH procedure's FDR can be bounded by a multiple of the nominal FDR level.
\begin{proposition}[Benjamini and Yekutieli, 2001]
The BH procedure controls the FDR at level $q(1 + \frac{1}{2} + \cdots + \frac{1}{m})$ regardless of the $p$-value dependency structure.
\end{proposition}
\begin{proof}
We have
\begin{equation*}
\begin{split}
\text{FDP}(\hat S) &= \sum_{k = 1}^m \frac{|\hat S \cap \mathcal H_0|}{k}\mathbbm 1(|\hat S| = k) \\
&= \sum_{k = 1}^m \sum_{j \in \mathcal H_0} \frac{1}{k}\mathbbm 1(j \in \hat S, |\hat S| = k) \\
&= \sum_{k = 1}^m \sum_{j \in \mathcal H_0} \frac{1}{k}\mathbbm 1\left(p_j \leq \frac{qk}{m}, |\hat S| = k\right) \\
&= \sum_{k = 1}^m \sum_{j \in \mathcal H_0} \sum_{l = 1}^k \frac{1}{k} \mathbbm 1\left(p_j \in \left[\frac{q(l-1)}{m}, \frac{ql}{m}\right], |\hat S| = k\right) \\
&\leq \sum_{k = 1}^m \sum_{j \in \mathcal H_0} \sum_{l = 1}^k \frac{1}{l} \mathbbm 1\left(p_j \in \left[\frac{q(l-1)}{m}, \frac{ql}{m}\right], |\hat S| = k\right) \\
&= \sum_{j \in \mathcal H_0} \sum_{l = 1}^m \frac 1 l \sum_{k = l}^m \mathbbm 1\left(p_j \in \left[\frac{q(l-1)}{m}, \frac{ql}{m}\right], |\hat S| = k\right) \\
&\leq \sum_{j \in \mathcal H_0} \sum_{l = 1}^m \frac 1 l \mathbbm 1\left(p_j \in \left[\frac{q(l-1)}{m}, \frac{ql}{m}\right]\right).
\end{split}
\end{equation*}
It follows that
\begin{equation*}
\begin{split}
\text{FDR} = \mathbb E[\text{FDP}(\hat S)] &\leq \sum_{j \in \mathcal H_0} \sum_{l = 1}^m \frac 1 l \mathbb P\left[p_j \in \left[\frac{q(l-1)}{m}, \frac{ql}{m}\right]\right] \leq \sum_{j \in \mathcal H_0} \sum_{l = 1}^m \frac 1 l \frac{q}{m} = \frac{|\mathcal H_0|}{m}q\left(1 + \frac{1}{2} + \cdots + \frac{1}{m}\right).
\end{split}
\end{equation*}

\end{proof}

It turns out that this upper bound is tight in the sense that there exist $p$-value distributions for which the BH procedure's FDR is equal to this upper bound. However, such $p$-value distributions are highly contrived. In practice, the BH procedure has empirically been shown to control the FDR for a wide variety of dependency structures besides independence. However, theoretical FDR control results for the BH procedure are available only for a few dependency structures. A notable example is a type of positive dependency called \textit{positive regression dependence on a subset}, or PRDS. Benjamini and Yekutieli proved FDR control for BH under PRDS in 2001. This theoretical condition is somewhat hard to verify in practice, however. The simplest example of a set of PRDS $p$-values is when $\bm x \sim N(\bm \mu, \bm \Sigma) \in \mathbb R^m$ where $\bm \Sigma$ has all positive entries and the $p$-values are derived based on one-sided tests. Outside of this special case, there are few known instances of PRDS $p$-values.

\subsection{Additional topics}

\paragraph{Weighted multiple testing procedures.}

Sometimes, we may have more prior evidence against certain null hypotheses than others, which we wish to incorporate in the global testing or multiple testing procedure to boost power. A common approach to doing so is to \textit{weight} the $p$-values. Letting $w_1, \dots, w_m$ be $p$-value weights averaging to 1, define \textit{weighted $p$-values} $\tilde p_j$ via
\begin{equation}
\tilde p_j \equiv \frac{p_j}{w_j}.
\end{equation}
Note that $p$-values corresponding to hypotheses with large (small) weights will be made more (less) significant. We can then attempt to apply the above global testing and multiple testing procedures on the weighted $p$-values $\tilde p_j$ rather than the original $p$-values $p_j$. As it turns out, in many cases these weighted procedures retain the Type-I error guarantees of their unweighted counterparts.

\begin{proposition}
The weighted variants of the Bonferroni global test, the Bonferroni FWER procedure, and the BH FDR procedure all control their respective Type-I error rates under the same conditions as their unweighted counterparts (arbitrary dependence for the Bonferroni procedures and independence for BH).
\end{proposition}
\begin{proof}
Here, we prove the statement just for the Bonferroni global test; the remaining proofs are left as exercises. The weighted Bonferroni global test is as follows:
\begin{equation}
\phi(p_1, \dots, p_m) \equiv \mathbbm 1\left(\min_{1 \leq j \leq m} \frac{p_j}{w_j} \leq \frac{\alpha}{m}\right).
\end{equation}
It follows that
\begin{equation*}
\begin{split}
\mathbb E_{H_0}[\phi(p_1, \dots, p_m)] &= \mathbb E_{H_0}\left[\mathbbm 1\left(\min_{1 \leq j \leq m} \frac{p_j}{w_j} \leq \frac{\alpha}{m}\right)\right] \leq \sum_{j = 1}^m \mathbb P\left[p_j \leq \frac{\alpha}{m} w_j\right] \leq \sum_{j = 1}^m \frac{\alpha}{m} w_j = \alpha.
\end{split}
\end{equation*}
The last equality follows from the fact that the weights $w_j$ average to 1 by assumption.
\end{proof}

\paragraph{Adaptive multiple testing procedures.}

Aside from weighting $p$-values, another route to improving the power of multiple testing procedures is \textit{null proportion adaptivity}. Null proportion adaptivity adjusts the stringency of the multiplicity correction based on an estimate of the null proportion 
\begin{equation*}
\pi_0 \equiv \frac{|\mathcal H_0|}{m}.
\end{equation*}
The smaller the null proportion, the less stringent a multiplicity correction is needed. To exemplify this, we consider the Storey-BH procedure, which estimates the null proportion as follows:
\begin{equation}
\hat \pi_0 \equiv \frac{1 + |\{j \in \{1, \dots, m\} : p_j > \lambda\}|}{(1-\lambda)m}, \quad \text{for some } \lambda \in (0, 1).
\end{equation}
The intuition is that, hopefully, most of the non-null $p$-values will be below $\lambda$. The number of $p$-values above $\lambda$ then mostly captures the number of null $p$-values above $\lambda$, which one expects to be $(1-\lambda)$ times the number of total null $p$-values. The ``1+'' in the numerator is for theoretical purposes. Then, the Storey-BH procedure is defined as similarly to the BH procedure, except with FDP estimate
\begin{equation}
\widehat{\text{FDP}}(t) \equiv \frac{\hat \pi_0 m t}{|\{j: p_j \leq t\}|}
\end{equation}
and restricted threshold estimate
\begin{equation}
\widehat t = \max\{t \in [0,\lambda]: \widehat{\text{FDP}}(t) \leq q\}.
\end{equation}

\begin{proposition}[Storey, Taylor, and Siegmund, 2004]
Under independent $p$-values, the Storey-BH procedure controls the FDR at level $\alpha$ for any $\lambda \in (0, 1)$.
\end{proposition}

\paragraph{High-probability FDP bounds.}

Some have criticized FDR control as a Type-I error criterion, as it guarantees control over only the mean of the FDP. The realized FDP on a given problem instance may be much larger than the nominal FDR, and this is not captured by the FDR control guarantee. To address this, one can instead consider high-probability FDP bounds, which guarantee that the FDP will not exceed a certain threshold with high probability.


\chapter{High-dimensional inference under Model-X} \label{sec:model-x}

All of the statistical inference done so far in this class was \textit{low-dimensional}: we assumed that the number of predictors $p$ was fixed and at most equal to the sample size $n$. However, some modern applications fall outside of this regime and therefore require new statistical methodology. We discuss here a line of work initiated by Candès, E., Fan, Y., Janson, L., \& Lv, J. (2018). Panning for gold: `model-X’ knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3), 551–577.

\section{Motivation and problem statement}

All statistical inference requires assumptions, and inherently difficult problems like high-dimensional inference require strong assumptions. One such assumption is the
\begin{equation}
\textit{Model-X assumption:} \text{ That the joint distribution of } (x_1, \dots, x_p) \text{ is known}.
\end{equation}
This assumption is in some sense the opposite of what we have been considering in this class so far: Usually we assume nothing about the joint distribution of covariates (we treat these as fixed anyways), and assume instead that $y|(x_1, \dots, x_p)$ follows a generalized linear model. Notably, this assumption is stronger than correct specification of a parametric model for $(x_1, \dots, x_p)$; it states that we know not only a model for this distribution but all of its parameters as well. Below we discuss the motivation for this assumption, and the inference problem that grows out of it.
\paragraph{Motivation: Genome-wide association studies (GWAS).}

In GWAS, $x_1, \dots, x_p \in \{0,1,2\}$ represent \textit{genotypes} of an individual at $p$ genomic locations. Suppose that humans typically have either an A or a T at genomic location $j$, where A is more common. Since we have two sets of chromosomes (one maternal and one paternal), the \textit{genotype} at this location can either be AA, AT, or TT. The allele T is called the \textit{minor allele} because it is less common, and $x_j$ is defined as the number of minor alleles an individual has at location $j$: AA implies $x_j = 0$, AT implies $x_j = 1$, TT implies $x_j = 2$. We collect this genotype information at $p$ genomic locations from each individual, as well as a response variable $y$, like disease status. The goal is to find the genomic locations whose genotypes are associated with the response. The nice thing in this application is that the joint distribution $(x_1, \dots, x_p)$ has been studied extensively in the field of population genetics, and is well-approximated by a \textit{hidden Markov model}. This motivates the model-X assumption.

\paragraph{Problem statement.}

It turns out that if we have a model for the joint distribution of the predictors, we need not make any assumptions on the distribution of the response given the predictors. But this leaves us with the following awkward question: If we have no parametric model for the response, then what even are the hypotheses we are testing? Well, for each genomic location $j$, we are trying to test whether the genotype at that location is associated with the response, controlling for the genotypes at other genomic locations. Probabilistically, this may be written as:
\begin{equation}
H_{0j}: x_j \perp \! \! \! \perp y \mid \bm x_{\text{-}j}.
\label{eq:conditional-independence}
\end{equation}
Under mild assumptions, this hypothesis turns out to coincide with the usual $H_{0j}: \beta_j = 0$ in the case when $y$ does follow a GLM. The problem statement, then, is to test the hypotheses $H_{0j}$ based on data
\begin{equation}
(x_{i1}, \dots, x_{ip}, y_i) \overset{\text{i.i.d.}}\sim F_{\bm x, y}, \quad i = 1, \dots, n,
\end{equation}
given knowledge of the distribution $F_{\bm x}$. Note that \textit{regularized regression} methods such as the LASSO have been developed to get estimates of regression coefficients in high dimensions. However, the issue with these shrinkage-based estimation methodologies is that they do not come with inferential guarantees and therefore cannot provide valid tests of the conditional independence hypothesis~\eqref{eq:conditional-independence}. Under the model-X assumption, we can get around this roadblock.

\section{Conditional randomization test}

One idea is to view $x_j$ as a treatment (though not necessarily binary) and $\bm x_{\text{-}j}$ as a set of covariates. The model-X assumption gives us knowledge of the \textit{propensity function} $p(x_j|\bm x_{\text{-}j})$, i.e. the distribution of treatment assignments given the covariates. In the spirit of Fisher's randomization test (see Homework 5 Problem 1), we can build a null distribution for any test statistic $T(\bm X, \bm y)$---e.g. a lasso coefficient---by \textit{randomly reassigning the treatment $x_j$ to each individual based on its covariates $\bm x_{\text{-}j}$}. More explicitly, let
\begin{equation}
\widetilde x_{ij} | \bm X, \bm y \overset{\text{ind}}\sim F_{x_j|\bm x_{\text{-}j} = \bm x_{i,\text{-}j}}.
\end{equation}
Let $\bm{\widetilde X}$ be the matrix obtained by replacing the $j$th column in $\bm X$ with $\widetilde{\widehat x}_{*j}$ as defined above. For a test statistic $T$, we then define the CRT $p$-value by comparing the test statistic's value on the original data with its distribution under resampling:
\begin{equation}
p_j^{\text{CRT}} \equiv \mathbb P[T(\bm{\widetilde X}, \bm y) \geq T(\bm X, \bm y)|\bm X, \bm y].
\end{equation}
In practice, we approximate this $p$-value by resampling a finite number $B$ of instances $\bm{\widetilde X}^b$ and setting
\begin{equation}
\widehat p_j^{\text{CRT}} \equiv \frac{1}{B+1}\sum_{b = 1}^B \mathbbm 1(T(\bm{\widetilde X}^b, \bm y) \geq T(\bm X, \bm y)).
\end{equation}
The CRT is a simple and elegant inferential framework that gives finite-sample valid $p$-values for high-dimensional inference. However, its adoption has been slowed by the computational cost of resampling.

\section{Model-X knockoffs}

An alternative to the CRT for model-X inference is \textit{model-X knockoffs}. This methodology requires constructing a set of $p$ new \textit{knockoff} variables $(\widetilde x_1, \dots, \widetilde x_p)$, whose joint distribution with the original variables satisfies the following exchangeability criterion:
\begin{equation}
\text{for each } j, \quad (x_j, \widetilde x_{j}) \overset d = (\widetilde x_{j}, x_j) \mid \bm x_{\text{-}j}, \bm{\widetilde x}_{\text{-}j}.
\end{equation}
Knockoff variables are meant to serve as valid \textit{negative controls} for the original variables: they have the same dependency structure but they have no association with the response $y$. Constructing such knockoff variables is a nontrivial endeavor that depends on the joint distribution of the original variables. If this can be done, then we can sample an entire knockoff matrix $\bm{\widetilde X}$, row by row. We then assess the significant of all $2p$ variables using test statistics $Z_1(\bm X, \bm{\widetilde X}, \bm y), \dots, Z_p(\bm X, \bm{\widetilde X}, \bm y), \widetilde Z_1(\bm X, \bm{\widetilde X}, \bm y), \dots, \widetilde Z_p(\bm X, \bm{\widetilde X}, \bm y)$, constructed to ensure the following swap-equivariance property: swapping $\bm X_{*j}$ with $\bm{\widetilde X}_{*j}$ results in $Z_j(\bm X, \bm{\widetilde X}, \bm y)$ swapping with $\widetilde Z_j(\bm X, \bm{\widetilde X}, \bm y)$, while all the other test statistics stay the same. For example, consider running the LASSO of $\bm y$ on the \textit{augmented design matrix} $[\bm X, \bm{\widetilde X}]$, and defining the $Z_j$'s as the fitted coefficients for the corresponding variables. With these $Z_j$'s in hand, the idea is to define the significance of the $j$th original variable by comparing the test statistics for itself and for its knockoff:
\begin{equation}
T_j(\bm X, \bm{\widetilde X}, \bm y) \equiv Z_j(\bm X, \bm{\widetilde X}, \bm y) - \widetilde Z_j(\bm X, \bm{\widetilde X}, \bm y).
\end{equation}
Large values of $T_j$ are evidence against $H_{0j}$. If the knockoffs are constructed correctly, then the test statistics $T_j$ for null $j$ can be shown to have symmetric distributions around zero. In other words, the original variable and its knockoff are equally likely to be more significant. Using this observation, a clever multiple testing algorithm called \textit{Selective SeqStep} can be used to choose a cutoff $\widehat t$ for the test statistics in a way that provably controls the FDR at a pre-specified level $q$. Remarkably, this entire construction bypasses the construction of $p$-values!

\section{Comparing CRT to MX knockoffs} \label{sec:crt-knockoffs-comparison}

There are pros and cons to both the CRT and MX knockoffs. Both procedures offer valid, finite-sample inference in high dimensions, which sets them apart from many other inferential methodologies. Both procedures require the model-X assumption, however, which may or may not be reasonable in a given application. MX knockoffs is the more popular methodology at this time, due to its computational speed. It can be used to carry out inference for all $p$ hypotheses in ``one shot'', by running one big regularized regression on the augmented design matrix. It has been applied successfully to genome-wide association studies, using a hidden Markov model as the model for X. On the other hand, MX knockoffs is a randomized procedure, giving different results for different realizations of $\bm{\widetilde X}$. Furthermore, it does not provide $p$-values quantifying the significance of individual predictors, which hinders the interpretability of its results. On the other hand, the CRT requires more computation than knockoffs, so it has been slower to be adopted in practice. But this procedure is not randomized in the same way that knockoffs is; with more computation its results can be arbitrarily ``de-randomized.'' Furthermore, the CRT does have a $p$-value output, which facilitates easy interpretation and more flexibility for downstream multiple testing.


\end{document}
