% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  letterpaper,
  oneside]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{lmodern}
\usepackage{bm, bbm}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{fancyhdr}
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT 9610 Lecture Notes},
  pdfauthor={Eugene Katsevich},
  pdflang={english},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{STAT 9610Lecture Notes}
\author{Eugene Katsevich}
\date{}

\begin{document}
\frontmatter
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, frame hidden, breakable, interior hidden, borderline west={3pt}{0pt}{shadecolor}, sharp corners, enhanced]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\mainmatter
\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

\hypertarget{welcome}{%
\section*{Welcome}\label{welcome}}
\addcontentsline{toc}{section}{Welcome}

\markright{Welcome}

This is a set of lecture notes developed for the PhD statistics course
``STAT 9610: Statistical Methodology'' at the University of
Pennsylvania. Much of the content is adapted from Alan Agresti's book
\emph{Foundations of Linear and Generalized Linear Models} (2015). These
notes may contain typos and errors, and will be updated in subsequent
iterations of STAT 9610.

\hypertarget{preview-linear-and-generalized-linear-models}{%
\section*{Preview: Linear and generalized linear
models}\label{preview-linear-and-generalized-linear-models}}
\addcontentsline{toc}{section}{Preview: Linear and generalized linear
models}

\markright{Preview: Linear and generalized linear models}

\emph{See also Agresti 1.1, Dunn and Smyth 1.1-1.2, 1.5-1.6, 1.8-1.12}

The overarching statistical goal addressed in this class is to learn
about relationships between a response \(y\) and predictors
\(x_0, x_1, \dots, x_{p-1}\). This abstract formulation encompasses an
extremely wide variety of applications. The most widely used set of
statistical models to address such problems are \emph{generalized linear
models}, which are the focus of this class.

Let's start by recalling the \emph{linear model}, the most fundamental
of the generalized linear models. In this case, the response is
continuous (\(y \in \mathbb{R}\)) and modeled as:

\begin{equation}\protect\hypertarget{eq-lm1}{}{
y = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} + \epsilon,
}\label{eq-lm1}\end{equation}

where

\begin{equation}\protect\hypertarget{eq-lm2}{}{
\epsilon \sim (0, \sigma^2), \quad \text{i.e.} \ \mathbb{E}[\epsilon] = 0 \ \text{and} \ \text{Var}[\epsilon] = \sigma^2.
}\label{eq-lm2}\end{equation}

We view the predictors \(x_0, \dots, x_{p-1}\) as fixed, so the only
source of randomness in \(y\) is \(\epsilon\). Another way of writing
the linear model is:

\[
\mu \equiv \mathbb{E}[y] = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} \equiv \eta.
\]

Not all responses are continuous, however. In some cases, we have binary
responses (\(y \in \{0,1\}\)) or count responses (\(y \in \mathbb{Z}\)).
In these cases, there is a mismatch between the:

\[
\textit{linear predictor } \eta \equiv \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1}
\]

and the

\[
\textit{mean response } \mu \equiv \mathbb{E}[y].
\]

The linear predictor can take arbitrary real values
(\(\eta \in \mathbb{R}\)), but the mean response can lie in a restricted
range, depending on the response type. For example, \(\mu \in [0,1]\)
for binary \(y\) and \(\mu \in [0, \infty)\) for count \(y\).

For these kinds of responses, it makes sense to model a
\emph{transformation} of the mean as linear, rather than the mean
itself:

\[
g(\mu) = g(\mathbb{E}[y]) = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} = \eta.
\]

This transformation \(g\) is called the link function. For binary \(y\),
a common choice of link function is the \emph{logit link}, which
transforms a probability into a log-odds:

\[
\text{logit}(\pi) \equiv \log \frac{\pi}{1-\pi}.
\]

So the predictors contribute linearly on the log-odds scale rather than
on the probability scale. For count \(y\), a common choice of link
function is the \emph{log link}.

Models of the form

\[
g(\mu) = \eta
\]

are called \emph{generalized linear models} (GLMs). They specialize to
linear models for the identity link function, i.e., \(g(\mu) = \mu\).
The focus of this course is methodologies to learn about the
coefficients
\(\boldsymbol{\beta} \equiv (\beta_0, \dots, \beta_{p-1})^T\) of a GLM
based on a sample
\((\boldsymbol{X}, \boldsymbol{y}) \equiv \{(x_{i,0}, \dots, x_{i,p-1}, y_i)\}_{i = 1}^n\)
drawn from this distribution. Learning about the coefficient vector
helps us learn about the relationship between the response and the
predictors.

\hypertarget{course-outline}{%
\section*{Course outline}\label{course-outline}}
\addcontentsline{toc}{section}{Course outline}

\markright{Course outline}

This course is broken up into five units:

\begin{itemize}
\tightlist
\item
  \textbf{Chapter 1. Linear model: Estimation.} The \emph{least squares}
  point estimate \(\boldsymbol{\widehat \beta}\) of
  \(\boldsymbol{\beta}\) based on a dataset
  \((\boldsymbol{X}, \boldsymbol{y})\) under the linear model
  assumptions.
\item
  \textbf{Chapter 2. Linear model: Inference.} Under the additional
  assumption that \(\epsilon \sim N(0,\sigma^2)\), how to carry out
  statistical inference (hypothesis testing and confidence intervals)
  for the coefficients.
\item
  \textbf{Chapter 3. Linear model: Misspecification.} What to do when
  the linear model assumptions are not correct: What issues can arise,
  how to diagnose them, and how to fix them.
\item
  \textbf{Chapter 4. GLMs: General theory.} Estimation and inference for
  GLMs (generalizing Chapters 1 and 2). GLMs fit neatly into a unified
  theory based on \emph{exponential families}.
\item
  \textbf{Chapter 5. GLMs: Special cases.} Looking more closely at the
  most important special cases of GLMs, including logistic regression
  and Poisson regression.
\end{itemize}

\hypertarget{notation}{%
\section*{Notation}\label{notation}}
\addcontentsline{toc}{section}{Notation}

\markright{Notation}

We will use the following notations in this course. Vector and matrix
quantities will be bolded, whereas scalar quantities will not be.
Capital letters will be used for matrices, and lowercase for vectors and
scalars. No notational distinction will be made between random
quantities and their realizations. The letters \(i = 1, \dots, n\) and
\(j = 0, \dots, p-1\) will index samples and predictors, respectively.
The predictors \(\{x_{ij}\}_{i,j}\) will be gathered into an
\(n \times p\) matrix \(\boldsymbol{X}\). The rows of \(\boldsymbol{X}\)
correspond to samples, with the \(i\)th row denoted
\(\boldsymbol{x}_{i*}\). The columns of \(\boldsymbol{X}\) correspond to
predictors, with the \(j\)th column denoted \(\boldsymbol{x}_{*j}\). The
responses \(\{y_i\}_i\) will be gathered into an \(n \times 1\) response
vector \(\boldsymbol{y}\). The notation \(\equiv\) will be used for
definitions.

\bookmarksetup{startatroot}

\hypertarget{linear-models-estimation}{%
\chapter{Linear models: Estimation}\label{linear-models-estimation}}

\hypertarget{types-of-predictors-interpreting-linear-model-coefficients}{%
\section{Types of predictors; interpreting linear model
coefficients}\label{types-of-predictors-interpreting-linear-model-coefficients}}

\emph{See also Agresti 1.2, Dunn and Smyth 1.4, 1.7, 2.7}

The types of predictors \(x_j\) (e.g.~binary or continuous) has less of
an effect on the regression than the type of response, but it is still
important to pay attention to the former.

\textbf{Intercepts.} It is common to include an \emph{intercept} in a
linear regression model, a predictor \(x_0\) such that \(x_{i0} = 1\)
for all \(i\). When an intercept is present, we index it as the 0th
predictor. The simplest kind of linear model is the \emph{intercept-only
model} or the \emph{one-sample model}:
\begin{equation}\protect\hypertarget{eq-one-sample-model}{}{
y = \beta_0 + \epsilon.
}\label{eq-one-sample-model}\end{equation} The parameter \(\beta_0\) is
the mean of the response.

\textbf{Binary predictors.} In addition to an intercept, suppose we have
a binary predictor \(x_1 \in \{0,1\}\) (e.g.~\(x_1 = 1\) for patients
who took blood pressure medication and \(x_1 = 0\) for those who
didn't). This leads to the following linear model:
\begin{equation}\protect\hypertarget{eq-two-sample-model}{}{
y = \beta_0 + \beta_1 x_1 + \epsilon.
}\label{eq-two-sample-model}\end{equation} Here, \(\beta_0\) is the mean
response (say blood pressure) for observations with \(x_1 = 0\) and
\(\beta_0 + \beta_1\) is the mean response for observations with
\(x_1 = 1\). Therefore, the parameter \(\beta_1\) is the difference in
mean response between observations with \(x_1 = 1\) and \(x_1 = 0\).
This parameter is sometimes called the \emph{effect} or \emph{effect
size} of \(x_1\), though a causal relationship might or might not be
present. The model (\ref{eq-two-sample-model}) is sometimes called the
\emph{two-sample model}, because the response data can be split into two
``samples'': those corresponding to \(x_1 = 0\) and those corresponding
to \(x_1 = 1\).

\textbf{Categorical predictors.} A binary predictor is a special case of
a categorical predictor: A predictor taking two or more discrete values.
Suppose we have a predictor \(w \in \{w_0, w_1, \dots, w_{C-1}\}\),
where \(C \geq 2\) is the number of categories and
\(w_0, \dots, w_{C-1}\) are the \emph{levels} of \(w\). E.g. suppose
\(\{w_0, \dots, w_{C-1}\}\) is the collection of U.S. states, so that
\(C = 50\). If we want to regress a response on the categorical
predictor \(w\), we cannot simply set \(x_1 = w\) in the context of the
linear regression (\ref{eq-two-sample-model}). Indeed, \(w\) does not
necessarily take numerical values. Instead, we need to add a predictor
\(x_j\) for each of the levels of \(w\). In particular, define
\(x_j \equiv \mathbbm 1(w = w_j)\) for \(j = 1, \dots, C-1\) and
consider the regression
\begin{equation}\protect\hypertarget{eq-C-sample-model}{}{
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_{C-1}x_{C-1} + \epsilon.
}\label{eq-C-sample-model}\end{equation} Here, category 0 is the
\emph{base category}, and \(\beta_0\) represents the mean response in
the base category. The coefficient \(\beta_j\) represents the difference
in mean response between the \(j\)th category and the base category.

\textbf{Quantitative predictors.} A quantitative predictor is one that
can take on any real value. For example, suppose that
\(x_1 \in \mathbb{R}\), and consider the linear model
\begin{equation}\protect\hypertarget{eq-simple-linear-regression}{}{
y = \beta_0 + \beta_1 x_1 + \epsilon.
}\label{eq-simple-linear-regression}\end{equation} Now, the
interpretation of \(\beta_1\) is that an increase in \(x_1\) by 1 is
associated with an increase in \(y\) by \(\beta_1\). We must be careful
to avoid saying ``an increase in \(x_1\) by 1 \emph{causes} \(y\) to
increase by \(\beta_1\)'' unless we make additional causal assumptions.
Note that the units of \(x_1\) matter. If \(x_1\) is the height of a
person, then the value and the interpretation of \(\beta_1\) changes
depending on whether that height is measured in feet or in meters.

\textbf{Ordinal predictors.} There is an awkward category of predictor
in between categorical and continuous called \emph{ordinal}. An ordinal
predictor is one that takes a discrete number of values, but these
values have an intrinsic ordering,
e.g.~\(x_1 \in \{\texttt{small}, \texttt{medium}, \texttt{large}\}\). It
can be treated as categorical at the cost of losing the ordering
information, or as continuous if one is willing to assign quantitative
values to each category.

\textbf{Multiple predictors.} A linear regression need not contain just
one predictor (aside from an intercept). For example, let's say \(x_1\)
and \(x_2\) are two predictors. Then, a linear model with both
predictors is
\begin{equation}\protect\hypertarget{eq-multiple-regression}{}{
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon.
}\label{eq-multiple-regression}\end{equation} When there are multiple
predictors, the interpretation of coefficients must be revised somewhat.
For example, \(\beta_1\) in the above regression is the effect of an
increase in \(x_1\) by 1 \emph{while holding \(x_2\) constant} or
\emph{while adjusting for \(x_2\)} or \emph{while controlling for
\(x_2\)}. If \(y\) is blood pressure, \(x_1\) is a binary predictor
indicating blood pressure medication taken and \(x_2\) is sex, then
\(\beta_1\) is the effect of the medication on blood pressure while
controlling for sex. In general, the coefficient of a predictor depends
on what other predictors are in the model. As an extreme case, suppose
the medication has no actual effect, but that men generally have higher
blood pressure and higher rates of taking the medication. Then, the
coefficient \(\beta_1\) in the single regression model
(\ref{eq-two-sample-model}) would be nonzero but the coefficient in the
multiple regression model (\ref{eq-multiple-regression}) would be equal
to zero. In this case, sex acts as a \emph{confounder}.

\textbf{Interactions.} Note that the multiple regression model
(\ref{eq-multiple-regression}) has the built-in assumption that the
effect of \(x_1\) on \(y\) is the same for any fixed value of \(x_2\)
(and vice versa). In some cases, the effect of one variable on the
response may depend on the value of another variable. In this case, it's
appropriate to add another predictor called an \emph{interaction}.
Suppose \(x_2\) is quantitative (e.g.~years of job experience) and
\(x_2\) is binary (e.g.~sex, with \(x_2 = 1\) meaning male). Then, we
can define a third predictor \(x_3\) as the product of the first two,
i.e.~\(x_3 = x_1x_2\). This gives the regression model
\begin{equation}\protect\hypertarget{eq-interaction}{}{
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon.
}\label{eq-interaction}\end{equation} Now, the effect of adding another
year of job experience is \(\beta_1\) for females and
\(\beta_1 + \beta_3\) for males. The coefficient \(\beta_3\) is the
difference in the effect of job experience between males and females.

\hypertarget{model-matrices-model-vector-spaces-and-identifiability}{%
\section{Model matrices, model vector spaces, and
identifiability}\label{model-matrices-model-vector-spaces-and-identifiability}}

\emph{See also Agresti 1.3-1.4, Dunn and Smyth 2.1, 2.2, 2.5.1}

The matrix \(\boldsymbol{X}\) is called the \emph{model matrix} or the
\emph{design matrix}. Concatenating the linear model equations across
observations gives us an equivalent formulation: \[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}; \quad \mathbb{E}[\boldsymbol{\epsilon}] = \boldsymbol{0}, \ \text{Var}[\boldsymbol{\epsilon}] = \sigma^2 \boldsymbol{I_n}
\] or \[
\mathbb{E}[\boldsymbol{y}] = \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{\mu}.
\] As \(\boldsymbol{\beta}\) varies in \(\mathbb{R}^p\), the set of
possible vectors \(\boldsymbol{\mu} \in \mathbb{R}^n\) is defined \[
C(\boldsymbol{X}) \equiv \{\boldsymbol{\mu} = \boldsymbol{X} \boldsymbol{\beta}: \boldsymbol{\beta} \in \mathbb{R}^p\}.
\] \(C(\boldsymbol{X})\), called the \emph{model vector space}, is a
subspace of \(\mathbb{R}^n\):
\(C(\boldsymbol{X}) \subseteq \mathbb{R}^n\). Since \[
\boldsymbol{X} \boldsymbol{\beta} = \beta_0 \boldsymbol{x_{*0}} + \cdots + \beta_{p-1} \boldsymbol{x_{*p-1}},
\] the model vector space is the column space of the matrix
\(\boldsymbol{X}\) (Figure~\ref{fig-model-vector-space}).

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/model-vector-space.png}

}

\caption{\label{fig-model-vector-space}The model vector space.}

\end{figure}

The \emph{dimension} of \(C(\boldsymbol{X})\) is the rank of
\(\boldsymbol{X}\), i.e.~the number of linearly independent columns of
\(\boldsymbol{X}\). If \(\text{rank}(\boldsymbol{X}) < p\), this means
that there are two different vectors \(\boldsymbol{\beta}\) and
\(\boldsymbol{\beta'}\) such that
\(\boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} \boldsymbol{\beta'}\).
Therefore, we have two values of the parameter vector that give the same
model for \(\boldsymbol{y}\). This makes \(\boldsymbol{\beta}\)
\emph{not identifiable}, and makes it impossible to reliably determine
\(\boldsymbol{\beta}\) based on the data. For this reason, we will
generally assume that \(\boldsymbol{\beta}\) is \emph{identifiable},
i.e.~\(\boldsymbol{X} \boldsymbol{\beta} \neq \boldsymbol{X} \boldsymbol{\beta'}\)
if \(\boldsymbol{\beta} \neq \boldsymbol{\beta'}\). This is equivalent
to the assumption that \(\text{rank}(\boldsymbol{X}) = p\). Note that
this cannot hold when \(p > n\), so for the majority of the course we
will assume that \(p \leq n\). In this case,
\(\text{rank}(\boldsymbol{X}) = p\) if and only if \(\boldsymbol{X}\)
has \emph{full-rank}.

As an example when \(p \leq n\) but when \(\boldsymbol{\beta}\) is still
not identifiable, consider the case of a categorical predictor. Suppose
the categories of \(w\) were \(\{w_1, \dots, w_{C-1}\}\), i.e.~the
baseline category \(w_0\) did not exist. In this case, the model
(\ref{eq-C-sample-model}) would not be identifiable because
\(x_0 = 1 = x_1 + \cdots + x_{C-1}\) and thus
\(x_{*0} = 1 = x_{*1} + \cdots + x_{*,C-1}\). Indeed, this means that
one of the predictors can be expressed as a linear combination of the
others, so \(\boldsymbol{X}\) cannot have full rank. A simpler way of
phrasing the problem is that we are describing \(C-1\) intrinsic
parameters (the means in each of the \(C-1\) groups) with \(C\) model
parameters. There must therefore be some redundancy. For this reason, if
we include an intercept term in the model then we must designate one of
our categories as the baseline and exclude its indicator from the model.

\hypertarget{least-squares-estimation}{%
\section{Least squares estimation}\label{least-squares-estimation}}

\hypertarget{algebraic-perspective}{%
\subsection{Algebraic perspective}\label{algebraic-perspective}}

\emph{See also Agresti 2.1.1, Dunn and Smyth 2.4.1, 2.5.2}

Now, suppose that we are given a dataset
\((\boldsymbol{X}, \boldsymbol{y})\). How do we go about estimating
\(\boldsymbol{\beta}\) based on this data? The canonical approach is the
\emph{method of least squares}: \[
\boldsymbol{\widehat{\beta}} \equiv \underset{\boldsymbol{\beta}}{\arg \min}\ \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}\|^2.
\] The quantity \[
\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2 = \|\boldsymbol{y} - \boldsymbol{\widehat{\mu}}\|^2 = \sum_{i = 1}^n (y_i - \widehat{\mu}_i)^2
\] is called the \emph{residual sum of squares (RSS)}, and it measures
the lack of fit of the linear regression model. We therefore want to
choose \(\boldsymbol{\widehat{\beta}}\) to minimize this lack of fit.
Letting
\(L(\boldsymbol{\beta}) = \frac{1}{2}\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}\|^2\),
we can do some calculus to derive that \[
\frac{\partial}{\partial \boldsymbol{\beta}}L(\boldsymbol{\beta}) = -\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}).
\] Setting this vector of partial derivatives equal to zero, we arrive
at the \emph{normal equations}:
\begin{equation}\protect\hypertarget{eq-normal-equations}{}{
-\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}) = 0 \quad \Longleftrightarrow \quad \boldsymbol{X}^T \boldsymbol{X} \boldsymbol{\widehat{\beta}} = \boldsymbol{X}^T \boldsymbol{y}.
}\label{eq-normal-equations}\end{equation} If \(\boldsymbol{X}\) is full
rank, the matrix \(\boldsymbol{X}^T \boldsymbol{X}\) is invertible and
we can therefore conclude that
\begin{equation}\protect\hypertarget{eq-beta-hat}{}{
\boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y}.
}\label{eq-beta-hat}\end{equation}

\hypertarget{probabilistic-perspective}{%
\subsection{Probabilistic perspective}\label{probabilistic-perspective}}

\emph{See also Agresti 2.7.1}

\hypertarget{least-squares-as-maximum-likelihood-estimation.}{%
\subsubsection{Least squares as maximum likelihood
estimation.}\label{least-squares-as-maximum-likelihood-estimation.}}

Note that if \(\boldsymbol{\epsilon}\) is assumed to be
\(N(0,\sigma^2 \boldsymbol{I_n})\), then the least squares solution
would also be the maximum likelihood solution. Indeed, for
\(y_i \sim N(\mu_i, \sigma^2)\), the log-likelihood is:

\[
\log \left[\prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)\right] = \text{constant} - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \mu_i)^2.
\]

\hypertarget{gauss-markov-theorem}{%
\subsubsection{Gauss-Markov theorem}\label{gauss-markov-theorem}}

Now that we have derived the least squares estimator, we can compute its
bias and variance. To obtain the bias, we first calculate that:

\[
\mathbb{E}[\widehat{\boldsymbol{\beta}}] = \mathbb{E}[(\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y}] = (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \mathbb{E}[\boldsymbol{y}] = (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{\beta}.
\]

Therefore, the least squares estimator is unbiased. To obtain the
variance, we compute:

\begin{equation}\protect\hypertarget{eq-var-of-beta-hat}{}{
\begin{split}
\text{Var}[\boldsymbol{\widehat{\beta}}] &= \text{Var}[(\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y}] \\
&= (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T\text{Var}[\boldsymbol{y}]\boldsymbol{X} (\boldsymbol{X}^T \boldsymbol{X})^{-1} \\
&= (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T(\sigma^2 \boldsymbol{I_n})\boldsymbol{X} (\boldsymbol{X}^T \boldsymbol{X})^{-1} \\
&= \sigma^2 (\boldsymbol{X}^T \boldsymbol{X})^{-1}.
\end{split}
}\label{eq-var-of-beta-hat}\end{equation}

\begin{theorem}[Gauss-Markov
theorem]\protect\hypertarget{thm-gauss-markov}{}\label{thm-gauss-markov}

For homoskedastic linear models (eqs. (\ref{eq-lm1}) and
(\ref{eq-lm2})), the least squares coefficient estimates have the
smallest covariance matrix (in the sense of positive semidefinite
matrices) among all linear unbiased estimates of \(\boldsymbol{\beta}\).

\end{theorem}

\hypertarget{geometric-perspective}{%
\subsection{Geometric perspective}\label{geometric-perspective}}

\emph{See also Agresti 2.2.1-2.2.3}

The following is the key geometric property of least squares
(Figure~\ref{fig-least-squares-as-projection}).

\begin{proposition}[]\protect\hypertarget{prp-orthogonal-projection}{}\label{prp-orthogonal-projection}

The mapping
\(\boldsymbol{y} \mapsto \boldsymbol{\widehat{\mu}} = \boldsymbol{X}\boldsymbol{\widehat{\beta}} \in C(\boldsymbol{X})\)
is an \emph{orthogonal projection} onto \(C(\boldsymbol{X})\), with
projection matrix

\begin{equation}\protect\hypertarget{eq-hat-matrix}{}{
\boldsymbol{H} \equiv  \boldsymbol{X}(\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \quad (\textit{the hat matrix}).
}\label{eq-hat-matrix}\end{equation}

\end{proposition}

Geometrically, this makes sense since we define
\(\boldsymbol{\widehat{\beta}}\) so that
\(\boldsymbol{\widehat{\mu}} \in C(\boldsymbol{X})\) is as close to
\(\boldsymbol{y}\) as possible. The shortest path between a point and a
plane is the perpendicular. A simple example of \(\boldsymbol{H}\) can
be obtained by considering the intercept-only regression.

\begin{proof}

To prove that \(\boldsymbol{y} \mapsto \boldsymbol{\widehat{\mu}}\) is
an orthogonal projection onto \(C(\boldsymbol{X})\), it suffices to show
that:

\[
\boldsymbol{v}^T (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}) = 0 \text{ for each } \boldsymbol{v} \in C(\boldsymbol{X}).
\]

Since the columns
\(\{\boldsymbol{x_{*0}}, \dots, \boldsymbol{x_{*p-1}}\}\) of
\(\boldsymbol{X}\) form a basis for \(C(\boldsymbol{X})\), it suffices
to show that
\(\boldsymbol{x_{*j}}^T (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}) = 0\)
for each \(j = 0, \dots, p-1\). This is a consequence of the normal
equations
\(\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat{\beta}}) = 0\)
derived in (\ref{eq-normal-equations}).

To show that the projection matrix is \(\boldsymbol{H}\)
(\ref{eq-hat-matrix}), it suffices to check that:

\[
\boldsymbol{\widehat{\mu}} = \boldsymbol{X}\boldsymbol{\widehat{\beta}} = \boldsymbol{X}(\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y} \equiv \boldsymbol{H} \boldsymbol{y}.
\]

\end{proof}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/least-squares-as-projection.png}

}

\caption{\label{fig-least-squares-as-projection}Least squares as
orthogonal projection.}

\end{figure}

\begin{proposition}[]\protect\hypertarget{prp-projection-matrices}{}\label{prp-projection-matrices}

If \(\boldsymbol{P}\) is an orthogonal projection onto a subspace
\(\boldsymbol{W}\), then:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\boldsymbol{P}\) is idempotent, i.e.,
  \(\boldsymbol{P}^2 = \boldsymbol{P}\).
\item
  For all \(\boldsymbol{v} \in \boldsymbol{W}\), we have
  \(\boldsymbol{P}\boldsymbol{v} = \boldsymbol{v}\), and for all
  \(\boldsymbol{v} \in \boldsymbol{W}^{\perp}\), we have
  \(\boldsymbol{P} \boldsymbol{v} = 0\).
\item
  \(\text{trace}(\boldsymbol{P}) = \text{dim}(\boldsymbol{W})\).
\end{enumerate}

\end{proposition}

One consequence of the geometric interpretation of least squares is that
the fitted values \(\boldsymbol{\widehat{\mu}}\) depend on
\(\boldsymbol{X}\) only through \(C(\boldsymbol{X})\). As we will see in
Homework 1, there are many different model matrices \(\boldsymbol{X}\)
leading to the same model space. Essentially, this reflects the fact
that there are many different bases for the same vector space. Consider,
for example, changing the units on the columns of \(\boldsymbol{X}\). It
can be verified that not just the fitted values
\(\boldsymbol{\widehat{\mu}}\) but also the predictions on a new set of
features remain invariant to reparametrization (this follows from parts
(a) and (b) of Homework 1 Problem 1). Therefore, while reparametrization
can have a huge impact on the fitted coefficients, it has no impact on
the predictions of linear regression.

\hypertarget{analysis-of-variance-and-r2}{%
\section{\texorpdfstring{Analysis of variance and
\(R^2\)}{Analysis of variance and R\^{}2}}\label{analysis-of-variance-and-r2}}

\emph{See also Agresti 2.4.2, 2.4.3, 2.4.6, Dunn and Smyth 2.9}

\hypertarget{analysis-of-variance}{%
\subsection{Analysis of variance}\label{analysis-of-variance}}

The orthogonality property of least squares, together with the
Pythagorean theorem, leads to a fundamental relationship called
\emph{the analysis of variance}.

Let's say that \(S \subset \{0, 1, \dots, p-1\}\) is a subset of the
predictors we wish to exclude from the model. First regress
\(\boldsymbol{y}\) on \(\boldsymbol{X}\) to get
\(\boldsymbol{\widehat{\beta}}\) as usual. Then, we consider the
\emph{partial model matrix} \(\boldsymbol{X_{*,\text{-}S}}\) obtained by
selecting all predictors except those in \(S\). Regressing
\(\boldsymbol{y}\) on \(\boldsymbol{X_{*, \text{-}S}}\) results in
\(\boldsymbol{\widehat{\beta}_{\text{-}S}}\) (note:
\(\boldsymbol{\widehat{\beta}_{\text{-}S}}\) is not necessarily obtained
from \(\boldsymbol{\widehat{\beta}}\) by extracting the coefficients
corresponding to \(\text{-}S\)).

\begin{theorem}[]\protect\hypertarget{thm-analysis-of-variance}{}\label{thm-analysis-of-variance}

\begin{equation}\protect\hypertarget{eq-pythagorean-theorem}{}{
\|\boldsymbol{y} -  \boldsymbol{X_{*, \text{-}S}}\boldsymbol{\widehat{\beta}_{\text{-}S}}\|^2 = \|\boldsymbol{X}\boldsymbol{\widehat{\beta}} - \boldsymbol{X_{*, \text{-}S}}\boldsymbol{\widehat{\beta}_{\text{-}S}}\|^2 + \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat{\beta}}\|^2.
}\label{eq-pythagorean-theorem}\end{equation}

\end{theorem}

\begin{proof}

Consider the three points \(\boldsymbol{y}\),
\(\boldsymbol{X}\boldsymbol{\widehat{\beta}}\),
\(\boldsymbol{X_{*, \text{-}S}}\boldsymbol{\widehat{\beta}_{\text{-}S}} \in \mathbb{R}^n\).
Since \(\boldsymbol{X}\boldsymbol{\widehat{\beta}}\) and
\(\boldsymbol{X_{*, \text{-}S}}\boldsymbol{\widehat{\beta}_{\text{-}S}}\)
are both in \(C(\boldsymbol{X})\), it follows by the orthogonal
projection property that
\(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat{\beta}}\) is
orthogonal to
\(\boldsymbol{X}\boldsymbol{\widehat{\beta}} - \boldsymbol{X_{*, \text{-}S}}\boldsymbol{\widehat{\beta}_{\text{-}S}}\).
In other words, these three points form a right triangle
(Figure~\ref{fig-sum-of-squares}). The relationship
(\ref{eq-pythagorean-theorem}) is then a consequence of the Pythagorean
theorem.

\end{proof}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/sum-of-squares.jpeg}

}

\caption{\label{fig-sum-of-squares}Pythagorean theorem for regression on
a subset of predictors.}

\end{figure}

We will rely on this fundamental relationship throughout this course.
One important special case is when \(S = \{1, \dots, p-1\}\), i.e., the
model without \(S\) is the intercept-only model. In this case,
\(\boldsymbol{X_{*, \text{-}S}} = \boldsymbol{1_n}\) and
\(\boldsymbol{\widehat{\beta}_{\text{-}S}} = \bar{y}\). Therefore,
equation (\ref{eq-pythagorean-theorem}) implies the following.

\begin{proposition}[]\protect\hypertarget{prp-sum-of-squares}{}\label{prp-sum-of-squares}

\[
\|\boldsymbol{y} -  \bar{y} \boldsymbol{1_n}\|^2 = \|\boldsymbol{X}\boldsymbol{\widehat{\beta}} - \bar{y} \boldsymbol{1_n}\|^2 + \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat{\beta}}\|^2.
\]

Equivalently, we can rewrite this equation as follows:

\begin{equation}\protect\hypertarget{eq-anova}{}{
\textnormal{SST} \equiv \sum_{i = 1}^n (y_i - \bar{y})^2 = \sum_{i = 1}^n (\widehat{\mu}_i - \bar{y})^2 + \sum_{i = 1}^n (y_i - \widehat{\mu}_i)^2 \equiv \textnormal{SSR} + \textnormal{SSE}.
}\label{eq-anova}\end{equation}

\end{proposition}

\hypertarget{r2-and-multiple-correlation}{%
\subsection{\texorpdfstring{\(R^2\) and multiple
correlation}{R\^{}2 and multiple correlation}}\label{r2-and-multiple-correlation}}

The ANOVA decomposition (\ref{eq-anova}) of the variation in
\(\boldsymbol{y}\) into that explained by the linear regression model
(SSR) and that left over (SSE) leads naturally to the definition of
\(R^2\) as the fraction of variation in \(\boldsymbol{y}\) explained by
the linear regression model:

\[
R^2 \equiv \frac{\text{SSR}}{\text{SST}} = \frac{\sum_{i = 1}^n (\widehat{\mu}_i - \bar{y})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} = \frac{\|\boldsymbol{X}\boldsymbol{\widehat{\beta}} - \bar{y} \boldsymbol{1_n}\|^2}{\|\boldsymbol{y} -  \bar{y} \boldsymbol{1_n}\|^2}.
\]

By the decomposition (\ref{eq-anova}), we have \(R^2 \in [0,1]\). The
closer \(R^2\) is to 1, the more closely the data follow the fitted
linear regression model. This intuition is formalized in the following
result.

\begin{proposition}[]\protect\hypertarget{prp-multiple-correlation}{}\label{prp-multiple-correlation}

\(R^2\) is the squared sample correlation between
\(\boldsymbol{X} \boldsymbol{\widehat{\beta}}\) and \(\boldsymbol{y}\).

\end{proposition}

For this reason, the positive square root of \(R^2\) is called the
\emph{multiple correlation coefficient}.

\begin{proof}

The first step is to observe that the mean of
\(\boldsymbol{X} \boldsymbol{\widehat{\beta}}\) is \(\bar{y}\) (this
follows from the normal equations). Therefore, the sample correlation
between \(\boldsymbol{X} \boldsymbol{\widehat{\beta}}\) and
\(\boldsymbol{y}\) is the inner product of the unit-normalized vectors
\(\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \bar{y} \boldsymbol{1}\)
and \(\boldsymbol{y} - \bar{y} \boldsymbol{1}\), which is the cosine of
the angle between them. From the geometry of
Figure~\ref{fig-sum-of-squares}, we find that the cosine of the angle
between
\(\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \bar{y} \boldsymbol{1}\)
and \(\boldsymbol{y} - \bar{y} \boldsymbol{1}\) is
\(\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \bar{y} \boldsymbol{1}\|/\|\boldsymbol{y} - \bar{y} \boldsymbol{1}\|\).
Squaring this relation gives the desired conclusion.

\end{proof}

\hypertarget{r2-increases-as-predictors-are-added}{%
\subsection{\texorpdfstring{\(R^2\) increases as predictors are
added}{R\^{}2 increases as predictors are added}}\label{r2-increases-as-predictors-are-added}}

The \(R^2\) is an \emph{in-sample} measure, i.e., it uses the same data
to fit the model and to assess the quality of the fit. Therefore, it is
generally an optimistic measure of the (out-of-sample) prediction error.
One manifestation of this is that the \(R^2\) increases if any
predictors are added to the model (even if these predictors are
``junk''). To see this, it suffices to show that SSE decreases as we add
predictors. Without loss of generality, suppose that we start with a
model with all predictors except those in
\(S \subset \{0, 1, \dots, p-1\}\) and compare it to the model including
all the predictors \(\{0,1,\dots,p-1\}\). We can read off from the
Pythagorean theorem (\ref{eq-pythagorean-theorem}) that:

\[
\text{SSE}(\boldsymbol{X_{*, \text{-}S}}, \boldsymbol{y}) \equiv \|\boldsymbol{y} -  \boldsymbol{X_{*, \text{-}S}}\boldsymbol{\widehat{\beta}_{\text{-}S}}\|^2 \geq  \|\boldsymbol{y} -  \boldsymbol{X}\boldsymbol{\widehat{\beta}}\|^2 \equiv \text{SSE}(\boldsymbol{X}, \boldsymbol{y}).
\]

Adding many junk predictors will have the effect of degrading predictive
performance but will nevertheless increase \(R^2\).

\hypertarget{special-cases}{%
\section{Special cases}\label{special-cases}}

\hypertarget{the-c-groups-model}{%
\subsection{\texorpdfstring{The \(C\)-groups
model}{The C-groups model}}\label{the-c-groups-model}}

\emph{See also Agresti 2.3.2-2.3.3}

\hypertarget{anova-decomposition-for-c-groups-model}{%
\subsubsection{\texorpdfstring{ANOVA decomposition for \(C\) groups
model}{ANOVA decomposition for C groups model}}\label{anova-decomposition-for-c-groups-model}}

Let's consider the special case of the ANOVA decomposition
(\ref{eq-anova}) when the model matrix \(\boldsymbol{X}\) represents a
single categorical predictor \(w\). In this case, each observation \(i\)
is associated with one of the \(C\) classes of \(w\), which we denote
\(c(i) \in \{1, \dots, C\}\). Let's consider the \(C\) groups of
observations \(\{i: c(i) = c\}\) for \(c \in \{1, \dots, C\}\). For
example, \(w\) may be the type of a car (compact, midsize, minivan,
etc.) and \(y\) might be its fuel efficiency in miles per gallon.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{mpg }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fct\_reorder}\NormalTok{(class, hwy), }\AttributeTok{y =}\NormalTok{ hwy)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Car class"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Gas mileage (mpg)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-estimation_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\end{figure}

It is easy to check that the least squares fitted values
\(\widehat{\mu}_i\) are simply the means of the corresponding groups:

\[
\widehat{\mu}_i = \bar{y}_{c(i)}, \quad \text{where}\ \bar{y}_{c(i)} \equiv \frac{\sum_{i: c(i) = c} y_i}{|\{i: c(i) = c\}|}.
\]

Therefore, we have:

\[
\text{SSR} = \sum_{i = 1}^n (\widehat{\mu}_i - \bar{y})^2 = \sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2 \equiv \text{between-groups sum of squares (SSB)}.
\]

and

\[
\text{SSE} = \sum_{i = 1}^n (y_i - \widehat{\mu}_i)^2 = \sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2 \equiv \text{within-groups sum of squares (SSW)}.
\]

We therefore obtain the following corollary of the ANOVA decomposition
(\ref{eq-anova}):

\begin{equation}\protect\hypertarget{eq-anova-C-groups}{}{
\text{SST} = \text{SSB} + \text{SSW}.
}\label{eq-anova-C-groups}\end{equation}

\hypertarget{simple-linear-regression}{%
\subsection{Simple linear regression}\label{simple-linear-regression}}

\emph{See also Agresti 2.1.3}

Consider a linear regression model with an intercept and one
quantitative predictor, \(x\):

\begin{equation}\protect\hypertarget{eq-simple-regression}{}{
y = \beta_0 + \beta_1 x + \epsilon.
}\label{eq-simple-regression}\end{equation}

This is the simple linear regression model.

\hypertarget{anova-decomposition-for-simple-linear-regression}{%
\subsubsection{ANOVA decomposition for simple linear
regression}\label{anova-decomposition-for-simple-linear-regression}}

Figure Figure~\ref{fig-anova-simple-linear-regression} gives an
interpretation of the ANOVA decomposition (\ref{eq-anova}) in the case
of the simple linear regression model (\ref{eq-simple-regression}).

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{figures/anova-simple-linear-regression.png}

}

\caption{\label{fig-anova-simple-linear-regression}ANOVA decomposition
for simple linear regression.}

\end{figure}

\hypertarget{connection-between-r2-and-correlation}{%
\subsubsection{\texorpdfstring{Connection between \(R^2\) and
correlation}{Connection between R\^{}2 and correlation}}\label{connection-between-r2-and-correlation}}

There is a connection between \(R^2\) and correlation in simple linear
regression.

\begin{proposition}[]\protect\hypertarget{prp-R2-correlation}{}\label{prp-R2-correlation}

\[
R^2 = \rho_{xy}^2.
\]

Let \(\rho_{xy}\) denote the sample correlation between \(x\) and \(y\),
and let \(R^2_{xy}\) be the \(R^2\) from the simple linear regression
(\ref{eq-simple-regression}). Then, we have:

\end{proposition}

\begin{proof}

This fact is a consequence of Proposition
\ref{prp-multiple-correlation}.

\end{proof}

\hypertarget{regression-to-the-mean}{%
\subsubsection{Regression to the mean}\label{regression-to-the-mean}}

Simple linear regression can be used to study the relationship between
the same quantity across time (or generations). For example, let \(x\)
and \(y\) be the height of a parent and child. This example motivated
Sir Francis Galton to study linear regression in the first place.
Alternatively, \(x\) and \(y\) can be a student's score on a
standardized test in two consecutive years, or the number of games won
by a given sports team in two consecutive seasons. In this situation, it
is reasonable to assume that the sample standard deviations of \(x\) and
\(y\) are the same (or to normalize these variables to achieve this). In
this case, one can show that:

\begin{equation}\protect\hypertarget{eq-coefficient-as-correlation}{}{
\widehat{\beta}_0 = \bar{y} - \rho_{xy} \bar{x} \quad \text{and} \quad \widehat{\beta}_1 = \rho_{xy}.
}\label{eq-coefficient-as-correlation}\end{equation}

It follows that:

\[
|\widehat{\mu}_i - \bar{y}| = |\widehat{\beta}_0 + \widehat{\beta}_1 x_i - \bar{y}| = |\rho_{xy}(x_i - \bar{x})| = |\rho_{xy}| \cdot |x_i - \bar{x}|.
\]

Since \(|\rho_{xy}| < 1\) unless \(\boldsymbol{x}\) and
\(\boldsymbol{y}\) are perfectly correlated (by the Cauchy-Schwarz
inequality), this means that:

\begin{equation}\protect\hypertarget{eq-regression-to-the-mean}{}{
|\widehat{\mu}_i - \bar{y}| < |x_i - \bar{x}| \quad \text{for each } i.
}\label{eq-regression-to-the-mean}\end{equation}

Therefore, we expect \(y_i\) to be closer to its mean than \(x_i\) is to
its mean. This phenomenon is called \emph{regression to the mean} (and
is in fact the origin of the term ``regression''). Many mistakenly
attribute a causal mechanism to this phenomenon, when in reality it is
simply a statistical artifact. For example, suppose \(x_i\) is the
number of games a sports team won last season and \(y_i\) is the number
of games it won this season. It is widely observed that teams with
exceptional performance in a given season suffer a ``winner's curse,''
performing worse in the next season. The reason for the winner's curse
is simple: teams perform exceptionally well due to a combination of
skill and luck. While skill stays roughly constant from year to year,
the team which performed exceptionally well in a given season is
unlikely to get as lucky as it did the next season.

\hypertarget{collinearity-adjustment-and-partial-correlation}{%
\section{Collinearity, adjustment, and partial
correlation}\label{collinearity-adjustment-and-partial-correlation}}

\emph{See also Agresti 2.2.4, 2.5.6, 2.5.7, 4.6.5}

An important part of linear regression analysis is the dependence of the
least squares coefficient for a predictor on what other predictors are
in the model. This relationship is dictated by the extent to which the
given predictor is correlated with the other predictors. In this
section, we'll use some additional notation. Let
\(S \subset \{0, \dots, p-1\}\) be a group of predictors (we can assume
without loss of generality that \(S = \{0, \dots, s-1\}\) for some
\(1 \leq s < p\)). Then, denote
\(\text{-}S \equiv \{0, \dots, p-1\} \setminus S\). Let
\(\boldsymbol{\widehat{\beta}}_S\) denote the least squares coefficients
when regressing \(\boldsymbol{y}\) on \(\boldsymbol{X}_{*S}\) and let
\(\boldsymbol{\widehat{\beta}}_{S|\text{-}S}\) denote the least squares
coefficients corresponding to \(S\) when regressing \(\boldsymbol{y}\)
on
\(\boldsymbol{X} = (\boldsymbol{X}_{*S}, \boldsymbol{X}_{*,\text{-}S})\).

\hypertarget{least-squares-estimates-in-the-orthogonal-case}{%
\subsection{Least squares estimates in the orthogonal
case}\label{least-squares-estimates-in-the-orthogonal-case}}

The simplest case to analyze is when a group of predictors
\(\boldsymbol{X}_{*S}\) is orthogonal to the rest of the predictors
\(\boldsymbol{X}_{*,\text{-}S}\) in the sense that

\[
\boldsymbol{X}_{*S}^T \boldsymbol{X}_{*,\text{-}S} = \boldsymbol{0}.
\]

In this case, we can derive the least squares coefficient vector
\(\boldsymbol{\widehat{\beta}} = (\boldsymbol{\widehat{\beta}}_{S|\text{-}S}, \boldsymbol{\widehat{\beta}}_{\text{-}S|S})\)
from the normal equations:

\begin{equation}\protect\hypertarget{eq-orthogonality}{}{
\begin{pmatrix}
\boldsymbol{\widehat{\beta}}_{S|\text{-}S} \\
\boldsymbol{\widehat{\beta}}_{\text{-}S|S}
\end{pmatrix} = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y} =
\begin{pmatrix}
\boldsymbol{X}_S^T \boldsymbol{X}_S & \boldsymbol{0} \\
\boldsymbol{0} & \boldsymbol{X}_{\text{-}S}^T \boldsymbol{X}_{\text{-}S}
\end{pmatrix}^{-1}
\begin{pmatrix}
\boldsymbol{X}_S^T \\
\boldsymbol{X}_{\text{-}S}^T
\end{pmatrix} \boldsymbol{y} =
\begin{pmatrix}
(\boldsymbol{X}_S^T \boldsymbol{X}_S)^{-1} \boldsymbol{X}_S^T \boldsymbol{y} \\
(\boldsymbol{X}_{\text{-}S}^T \boldsymbol{X}_{\text{-}S})^{-1} \boldsymbol{X}_{\text{-}S}^T \boldsymbol{y}
\end{pmatrix} = 
\begin{pmatrix}
\boldsymbol{\widehat{\beta}}_{S} \\
\boldsymbol{\widehat{\beta}}_{\text{-}S}
\end{pmatrix}.
}\label{eq-orthogonality}\end{equation}

Therefore, the least squares coefficients when regressing
\(\boldsymbol{y}\) on \((\boldsymbol{X}_S, \boldsymbol{X}_{\text{-}S})\)
are the same as those obtained from regressing \(\boldsymbol{y}\)
separately on \(\boldsymbol{X}_S\) and \(\boldsymbol{X}_{\text{-}S}\),
i.e.

\begin{equation}\protect\hypertarget{eq-orthogonality-consequence}{}{
\boldsymbol{\widehat{\beta}}_{S|\text{-}S} = \boldsymbol{\widehat{\beta}}_{S}.
}\label{eq-orthogonality-consequence}\end{equation}

\hypertarget{least-squares-estimates-via-orthogonalization}{%
\subsection{Least squares estimates via
orthogonalization}\label{least-squares-estimates-via-orthogonalization}}

Let's now focus our attention on a single predictor \(x_j\). If this
predictor is orthogonal to the remaining predictors, then the result
(\ref{eq-orthogonality-consequence}) states that
\(\widehat{\beta}_{j|\text{-}j}\) can be obtained from simply regressing
\(y\) on \(x_j\). However, this is usually not the case. Usually,
\(\boldsymbol{x}_{*j}\) has a nonzero projection
\(\boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}}\) onto
\(C(\boldsymbol{X}_{*,\text{-}j})\):

\[
\boldsymbol{x}_{*j} = \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}} + \boldsymbol{x}^{\perp}_{*j},
\]

where \(\boldsymbol{x}^{\perp}_{*j}\) is the residual from regressing
\(\boldsymbol{x}_{*j}\) onto \(\boldsymbol{X}_{*,\text{-}j}\) and is
therefore orthogonal to \(C(\boldsymbol{X}_{*,\text{-}j})\). In other
words, \(\boldsymbol{x}^{\perp}_{*j}\) is the projection of
\(\boldsymbol{x}_{*j}\) onto the orthogonal complement of
\(C(\boldsymbol{X}_{*,\text{-}j})\).

With this decomposition, let us change basis from
\((\boldsymbol{x}_{*j}, \boldsymbol{X}_{*,\text{-}j})\) to
\((\boldsymbol{x}^{\perp}_{*j}, \boldsymbol{X}_{*,\text{-}j})\) by the
process explored in Homework 1 Question 1. Let us write:

\[
\begin{aligned}
\boldsymbol{y} &= \boldsymbol{x}_{*j} \beta_{j|\text{-}j} + \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\beta}_{\text{-}j|j} + \boldsymbol{\epsilon} \\
&\Longleftrightarrow \ \boldsymbol{y} = (\boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}} + \boldsymbol{x}^{\perp}_{*j})\beta_{j|\text{-}j} + \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\beta}_{\text{-}j|j} + \boldsymbol{\epsilon} \\
&\Longleftrightarrow \ \boldsymbol{y} = \boldsymbol{x}^{\perp}_{*j}\beta_{j|\text{-}j} + \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\beta}'_{\text{-}j|j} + \boldsymbol{\epsilon}.
\end{aligned}
\]

What this means is that \(\widehat{\beta}_{j|\text{-}j}\), the least
squares coefficient of \(\boldsymbol{x}_{*j}\) in the regression of
\(\boldsymbol{y}\) on
\((\boldsymbol{x}_{*j}, \boldsymbol{X}_{*,\text{-}j})\), is also the
least squares coefficient of \(\boldsymbol{x}^{\perp}_{*j}\) in the
regression of \(\boldsymbol{y}\) on
\((\boldsymbol{x}^{\perp}_{*j}, \boldsymbol{X}_{*,\text{-}j})\).
However, since \(\boldsymbol{x}^{\perp}_{*j}\) is orthogonal to
\(\boldsymbol{X}_{*,\text{-}j}\) by construction, we can use the result
(\ref{eq-orthogonality}) to conclude that:

\[
\widehat{\beta}_{j|\text{-}j} \text{ is the least squares coefficient of } \boldsymbol{x}^{\perp}_{*j} \text{ in the *univariate* regression of } \boldsymbol{y} \text{ on } \boldsymbol{x}^{\perp}_{*j} \text{ (without intercept).}
\]

We can solve this univariate regression explicitly to obtain:

\begin{equation}\protect\hypertarget{eq-orthogonal-univariate}{}{
\widehat{\beta}_{j|\text{-}j} = \frac{(\boldsymbol{x}^{\perp}_{*j})^T \boldsymbol{y}}{\|\boldsymbol{x}^{\perp}_{*j}\|^2}.
}\label{eq-orthogonal-univariate}\end{equation}

\hypertarget{adjustment-and-partial-correlation}{%
\subsection{Adjustment and partial
correlation}\label{adjustment-and-partial-correlation}}

Equivalently, letting \(\boldsymbol{\widehat{\beta}}_{\text{-}j}\) be
the least squares estimate in the regression of \(\boldsymbol{y}\) on
\(\boldsymbol{X}_{*,\text{-}j}\) (note that this is \emph{not} the same
as \(\boldsymbol{\widehat{\beta}}_{\text{-}j|j}\)), we can write:

\[
\widehat{\beta}_{j|\text{-}j} = \frac{(\boldsymbol{x}^{\perp}_{*j})^T(\boldsymbol{y} - \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\beta}}_{\text{-}j})}{\|\boldsymbol{x}^{\perp}_{*j}\|^2} = \frac{(\boldsymbol{x}_{*j} - \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}})^T(\boldsymbol{y} - \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\beta}}_{\text{-}j})}{\|\boldsymbol{x}_{*j} -\boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}}\|^2}.
\]

We can interpret this result as follows:

\begin{theorem}[]\protect\hypertarget{thm-frisch-waugh-lovell}{}\label{thm-frisch-waugh-lovell}

The linear regression coefficient \(\widehat{\beta}_{j|\text{-}j}\)
results from first adjusting \(\boldsymbol{y}\) and
\(\boldsymbol{x}_{*j}\) for the effects of all other variables, and then
regressing the residuals from \(\boldsymbol{y}\) onto the residuals from
\(\boldsymbol{x}_{*j}\).

\end{theorem}

In this sense, \emph{the least squares coefficient for a predictor in a
multiple linear regression reflects the effect of the predictor on the
response after controlling for the effects of all other predictors.} A
related quantity is the \emph{partial correlation} between
\(\boldsymbol{x}_{*j}\) and \(\boldsymbol{y}\) after controlling for
\(\boldsymbol{X}_{*,\text{-}j}\), defined as the correlation between
\(\boldsymbol{x}_{*j} - \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}}\)
and
\(\boldsymbol{y} - \boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\beta}}_{\text{-}j}\).
We can then connect the least squares coefficient \(\widehat{\beta}_j\)
to this partial correlation in a similar spirit to equation
(\ref{eq-coefficient-as-correlation}).

\hypertarget{effects-of-collinearity}{%
\subsection{Effects of collinearity}\label{effects-of-collinearity}}

Collinearity between a predictor \(x_j\) and the other predictors tends
to make the estimate \(\widehat{\beta}_{j|\text{-}j}\) unstable.
Intuitively, this makes sense because it becomes harder to distinguish
between the effects of predictor \(x_j\) and those of the other
predictors on the response. To find the variance of
\(\widehat{\beta}_{j|\text{-}j}\) for a model matrix \(\boldsymbol{X}\),
we could in principle use the formula (\ref{eq-var-of-beta-hat}).
However, this formula involves the inverse of the matrix
\(\boldsymbol{X}^T \boldsymbol{X}\), which is hard to reason about.
Instead, we can employ the formula (\ref{eq-orthogonal-univariate}) to
calculate directly that:

\begin{equation}\protect\hypertarget{eq-conditional-variance}{}{
\text{Var}[\widehat{\beta}_{j|\text{-}j}] = \frac{\sigma^2}{\|\boldsymbol{x}_{*j}^\perp\|^2}.
}\label{eq-conditional-variance}\end{equation}

We see that the variance of \(\widehat{\beta}_{j|\text{-}j}\) is
inversely proportional to \(\|\boldsymbol{x}_{*j}^\perp\|^2\). This
means that the greater the collinearity, the less of
\(\boldsymbol{x}_{*j}\) is left over after adjusting for
\(\boldsymbol{X}_{*,\text{-}j}\), and the greater the variance of
\(\widehat{\beta}_{j|\text{-}j}\). To quantify the effect of this
adjustment, suppose there were no other predictors other than the
intercept term. Then, we would have:

\[
\text{Var}[\widehat{\beta}_j] = \frac{\sigma^2}{\|\boldsymbol{x}_{*j}-\bar{x}_j \boldsymbol{1}_n\|^2}.
\]

Therefore, we can rewrite the variance (\ref{eq-conditional-variance})
as:

\begin{equation}\protect\hypertarget{eq-vif}{}{
\text{Var}[\widehat{\beta}_{j|\text{-}j}] = \frac{\|\boldsymbol{x}_{*j}-\bar{x}_j \boldsymbol{1}_n\|^2}{\|\boldsymbol{x}_{*j}-\boldsymbol{X}_{*,\text{-}j}\boldsymbol{\widehat{\gamma}}\|^2} \cdot \text{Var}[\widehat{\beta}_j] = \frac{1}{1-R_j^2} \cdot \text{Var}[\widehat{\beta}_j] \equiv \text{VIF}_j \cdot \text{Var}[\widehat{\beta}_j],
}\label{eq-vif}\end{equation}

where \(R_j^2\) is the \(R^2\) value when regressing
\(\boldsymbol{x}_{*j}\) on \(\boldsymbol{X}_{*,\text{-}j}\) and VIF
stands for \emph{variance inflation factor}. The higher \(R_j^2\), the
more of the variance in \(\boldsymbol{x}_{*j}\) is explained by other
predictors, the higher the variance in
\(\widehat{\beta}_{j|\text{-}j}\).

\hypertarget{remark-average-treatment-effect-estimation-in-causal-inference}{%
\subsection{Remark: Average treatment effect estimation in causal
inference}\label{remark-average-treatment-effect-estimation-in-causal-inference}}

Suppose we'd like to study the effect of an exposure or treatment
(e.g.~taking a blood pressure medication) on a response \(y\)
(e.g.~blood pressure). In the Neyman-Rubin causal model, for a given
individual \(i\) we denote by \(y_i(1)\) and \(y_i(0)\) the outcomes
that would have occurred had the individual received the treatment and
the control, respectively. These are called \emph{potential outcomes}.
Let \(t_i \in \{0,1\}\) indicate whether the \(i\)th individual actually
received treatment or control. Therefore, the observed outcome is
\(y_i^{\text{obs}} = y_i(t_i)\).\footnote{This requires the *stable unit treatment value assumption* (SUTVA), which prevents issues like *interference*, where the treatment of individual $i$ can be affected by the assigned treatments of other individuals.}
Based on the data \(\{(t_i, y_i)\}_{i = 1, \dots, n}\), the most basic
goal is to estimate the:

\[
\textit{average treatment effect} \  \tau \equiv \mathbb{E}[y(1) - y(0)],
\]

where averaging is done over the population of individuals (often called
\emph{units} in causal inference). Of course, we do not observe both
\(y(1)\) and \(y(0)\) for any unit. Additionally, usually in
observational studies we have \emph{confounding variables}
\(z_2, \dots, z_{p-1}\): variables that influence both the treatment
assignment and the response (e.g.~degree of health-seeking activity). It
is important to control for these confounders in order to get an
unbiased estimate of the treatment effect. Suppose the following linear
model holds:

\[
y(t) = \beta_0 + \beta_1 t + \beta_2 z_2 + \cdots + \beta_{p-1} z_{p-1} + \epsilon \quad \text{for } t \in \{0, 1\}, \quad \text{where} \ \epsilon \perp\!\!\!\!\perp t.
\]

This assumption implies that the treatment effect is constant, and the
response is a linear function of the treatment and observed confounders,
and there is no unmeasured confounding. Note that:

\[
\tau \equiv \mathbb{E}[y(1) - y(0)] = \beta_1.
\]

Furthermore:

\[
y^{\text{obs}} = \beta_0 + \beta_1 t + \beta_2 z_2 + \cdots + \beta_{p-1} z_{p-1} + \epsilon \quad \text{for } t \in \{0, 1\}.
\]

In this case, the average treatment effect \(\tau\) is \emph{identified}
as the coefficient \(\beta_1\) in the above regression,
i.e.~\(\tau = \beta\). Therefore, the least squares estimate
\(\widehat{\beta}_1\) is an unbiased estimate of the average treatment
effect. (Causal inference is beyond the scope of STAT 9610; see STAT
9210 instead.)

\hypertarget{r-demo}{%
\section{R demo}\label{r-demo}}

\emph{See also Agresti 2.6, Dunn and Smyth 2.6}

The R demo will be based on the \texttt{ScotsRaces} data from the
Agresti textbook. Data description (quoted from the textbook):

\begin{quote}
``Each year the Scottish Hill Runners Association publishes a list of
hill races in Scotland for the year. The table below shows data on the
record time for some of the races (in minutes). Explanatory variables
listed are the distance of the race (in miles) and the cumulative climb
(in thousands of feet).''
\end{quote}

We will also familiarize ourselves with several important functions from
the \texttt{tidyverse} packages, including the \texttt{ggplot2} package
for data visualization and \texttt{dplyr} package for data manipulation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# for data import, manipulation, and plotting}
\FunctionTok{library}\NormalTok{(GGally)    }\CommentTok{\# for ggpairs() function}
\FunctionTok{library}\NormalTok{(ggrepel)   }\CommentTok{\# for geom\_text\_repel() function}
\FunctionTok{library}\NormalTok{(car)       }\CommentTok{\# for vif() function}
\FunctionTok{library}\NormalTok{(conflicted)}
\FunctionTok{conflicts\_prefer}\NormalTok{(dplyr}\SpecialCharTok{::}\NormalTok{filter)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read the data into R}
\NormalTok{scots\_races }\OtherTok{\textless{}{-}} \FunctionTok{read\_tsv}\NormalTok{(}\StringTok{"data/ScotsRaces.dat"}\NormalTok{) }\CommentTok{\# read\_tsv from readr for data import}
\NormalTok{scots\_races}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 35 x 4
   race                   distance climb  time
   <chr>                     <dbl> <dbl> <dbl>
 1 GreenmantleNewYearDash      2.5  0.65  16.1
 2 Carnethy5HillRace           6    2.5   48.4
 3 CraigDunainHillRace         6    0.9   33.6
 4 BenRhaHillRace              7.5  0.8   45.6
 5 BenLomondHillRace           8    3.07  62.3
 6 GoatfellHillRace            8    2.87  73.2
 7 BensofJuraFellRace         16    7.5  205. 
 8 CairnpappleHillRace         6    0.8   36.4
 9 ScoltyHillRace              5    0.8   29.8
10 TraprainLawRace             6    0.65  39.8
# i 25 more rows
\end{verbatim}

\hypertarget{exploration}{%
\subsection{Exploration}\label{exploration}}

Before modeling our data, let's first explore it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pairs plot}

\CommentTok{\# Q: What are the typical ranges of the variables?}
\CommentTok{\# Q: What are the relationships among the variables?}

\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{race) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select() from dplyr for selecting columns}
  \FunctionTok{ggpairs}\NormalTok{() }\CommentTok{\# ggpairs() from GGally to create pairs plot}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-estimation_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# mile time versus distance}

\CommentTok{\# Q: How does mile time vary with distance?}
\CommentTok{\# Q: What races deviate from this trend?}
\CommentTok{\# Q: How does climb play into it?}

\CommentTok{\# add mile time variable to scots\_races}
\NormalTok{scots\_races }\OtherTok{\textless{}{-}}\NormalTok{ scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mile\_time =}\NormalTok{ time }\SpecialCharTok{/}\NormalTok{ distance) }\CommentTok{\# mutate() from dplyr to add column}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot mile time versus distance}
\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ distance, }\AttributeTok{y =}\NormalTok{ mile\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-estimation_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add climb information as point color}
\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ distance, }\AttributeTok{y =}\NormalTok{ mile\_time, }\AttributeTok{colour =}\NormalTok{ climb)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-estimation_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# highlight extreme points}
\NormalTok{scots\_races\_extreme }\OtherTok{\textless{}{-}}\NormalTok{ scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(distance }\SpecialCharTok{\textgreater{}} \DecValTok{15} \SpecialCharTok{|}\NormalTok{ mile\_time }\SpecialCharTok{\textgreater{}} \DecValTok{9}\NormalTok{) }\CommentTok{\# filter() from dplyr to subset rows}

\CommentTok{\# plot mile time versus distance}
\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ distance, }\AttributeTok{y =}\NormalTok{ mile\_time, }\AttributeTok{label =}\NormalTok{ race, }\AttributeTok{colour =}\NormalTok{ climb)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ race), }\AttributeTok{data =}\NormalTok{ scots\_races\_extreme)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-estimation_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# clean up plot}
\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ distance, }\AttributeTok{y =}\NormalTok{ mile\_time, }\AttributeTok{label =}\NormalTok{ race, }\AttributeTok{color =}\NormalTok{ climb)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ race), }\AttributeTok{data =}\NormalTok{ scots\_races\_extreme) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Distance (miles)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Mile Time (minutes per mile)"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Climb}\SpecialCharTok{\textbackslash{}n}\StringTok{(thousands of ft)"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-estimation_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

\hypertarget{linear-model-coefficient-interpretation}{%
\subsection{Linear model coefficient
interpretation}\label{linear-model-coefficient-interpretation}}

Let's fit some linear models and interpret the coefficients.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Q: What is the effect of an extra mile of distance on time?}

\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance }\SpecialCharTok{+}\NormalTok{ climb, }\AttributeTok{data =}\NormalTok{ scots\_races)}
\FunctionTok{coef}\NormalTok{(lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)    distance       climb 
 -13.108551    6.350955   11.780133 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Linear model with interaction}

\CommentTok{\# Q: What is the effect of an extra mile of distance on time}
\CommentTok{\#  for a run with low climb?}

\CommentTok{\# Q: What is the effect of an extra mile of distance on time}
\CommentTok{\#  for a run with high climb?}

\NormalTok{lm\_fit\_int }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance }\SpecialCharTok{*}\NormalTok{ climb, }\AttributeTok{data =}\NormalTok{ scots\_races)}
\FunctionTok{coef}\NormalTok{(lm\_fit\_int)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   (Intercept)       distance          climb distance:climb 
    -0.7671925      4.9622542      3.7132519      0.6598256 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{min\_climb =} \FunctionTok{min}\NormalTok{(climb), }\AttributeTok{max\_climb =} \FunctionTok{max}\NormalTok{(climb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  min_climb max_climb
      <dbl>     <dbl>
1       0.3       7.5
\end{verbatim}

Let's take a look at the regression summary for \verb|lm_fit|:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance }\SpecialCharTok{+}\NormalTok{ climb, }\AttributeTok{data =}\NormalTok{ scots\_races)}
\FunctionTok{summary}\NormalTok{(lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = time ~ distance + climb, data = scots_races)

Residuals:
    Min      1Q  Median      3Q     Max 
-16.654  -4.842   1.110   4.667  27.762 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -13.1086     2.5608  -5.119 1.41e-05 ***
distance      6.3510     0.3578  17.751  < 2e-16 ***
climb        11.7801     1.2206   9.651 5.37e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.734 on 32 degrees of freedom
Multiple R-squared:  0.9717,    Adjusted R-squared:   0.97 
F-statistic: 549.9 on 2 and 32 DF,  p-value: < 2.2e-16
\end{verbatim}

We get a coefficient of 6.35 with standard error 0.36 for
\texttt{distance}, where the standard error is an estimate of the
quantity (\ref{eq-conditional-variance}).

\hypertarget{r2-and-sum-of-squared-decompositions.}{%
\subsection{\texorpdfstring{\(R^2\) and sum-of-squared
decompositions.}{R\^{}2 and sum-of-squared decompositions.}}\label{r2-and-sum-of-squared-decompositions.}}

We can extract the \(R^2\) from this fit by reading it off from the
bottom of the summary, or by typing

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm\_fit)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.971725
\end{verbatim}

We can construct sum-of-squares decompositions
(\ref{eq-pythagorean-theorem}) using the \texttt{anova} function. This
function takes as arguments the partial model and the full model. For
example, consider the partial model
\texttt{time\ \textasciitilde{}\ distance}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_partial }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance, }\AttributeTok{data =}\NormalTok{ scots\_races)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_partial, lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: time ~ distance
Model 2: time ~ distance + climb
  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    
1     33 9546.9                                 
2     32 2441.3  1    7105.6 93.14 5.369e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We find that adding the predictor \texttt{climb} reduces the RSS by
7106, from 9547 to 2441. As another example, we can compute the \(R^2\)
by comparing the full model with the null model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_null }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ scots\_races)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_null, lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: time ~ 1
Model 2: time ~ distance + climb
  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    
1     34 86340                                  
2     32  2441  2     83899 549.87 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Therefore, the \(R^2\) is 83899/86340 = 0.972, consistent with the above
regression summary.

\hypertarget{adjustment-and-collinearity.}{%
\subsection{Adjustment and
collinearity.}\label{adjustment-and-collinearity.}}

We can also test the adjustment formula (\ref{eq-orthogonal-univariate})
numerically. Let's consider the coefficient of \verb|distance| in the
regression \verb|time ~ distance + climb|. We can obtain this
coefficient by first regressing \verb|climb| out of \verb|distance| and
\verb|time|:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_dist\_on\_climb }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(distance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ climb, }\AttributeTok{data =}\NormalTok{ scots\_races)}
\NormalTok{lm\_time\_on\_climb }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ climb, }\AttributeTok{data =}\NormalTok{ scots\_races)}

\NormalTok{scots\_races\_resid }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{dist\_residuals =} \FunctionTok{residuals}\NormalTok{(lm\_dist\_on\_climb),}
  \AttributeTok{time\_residuals =} \FunctionTok{residuals}\NormalTok{(lm\_time\_on\_climb)}
\NormalTok{)}

\NormalTok{lm\_adjusted }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(time\_residuals }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dist\_residuals }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ scots\_races\_resid}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm\_adjusted)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = time_residuals ~ dist_residuals - 1, data = scots_races_resid)

Residuals:
    Min      1Q  Median      3Q     Max 
-16.654  -4.842   1.110   4.667  27.762 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
dist_residuals   6.3510     0.3471    18.3   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.474 on 34 degrees of freedom
Multiple R-squared:  0.9078,    Adjusted R-squared:  0.9051 
F-statistic: 334.8 on 1 and 34 DF,  p-value: < 2.2e-16
\end{verbatim}

We find a coefficient of 6.35 with standard error 0.35, which matches
that obtained in the original regression.

We can get the partial correlation between \texttt{distance} and
\texttt{time} by taking the empirical correlation between the residuals.
We can compare this quantity to the usual correlation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scots\_races\_resid }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{cor}\NormalTok{(dist\_residuals, time\_residuals)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9527881
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scots\_races }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{cor}\NormalTok{(distance, time)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9430944
\end{verbatim}

In this case, the two correlation quantities are similar.

To obtain the variance inflation factors defined in equation
(\ref{eq-vif}), we can use the \texttt{vif} function from the
\texttt{car} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vif}\NormalTok{(lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
distance    climb 
1.740812 1.740812 
\end{verbatim}

Why are these two VIF values the same?

\bookmarksetup{startatroot}

\hypertarget{linear-models-inference}{%
\chapter{Linear models: Inference}\label{linear-models-inference}}

We now understand the least squares estimator
\(\boldsymbol{\widehat{\beta}}\) from geometric and algebraic points of
view. In Chapter 2, we will switch to a probabilistic perspective to
derive inferential statements for linear models, in the form of
hypothesis tests and confidence intervals. In order to facilitate this,
we will assume that the error terms are normally distributed:

\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \text{where} \ \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_n).
\]

\hypertarget{building-blocks-for-linear-model-inference}{%
\section{Building blocks for linear model
inference}\label{building-blocks-for-linear-model-inference}}

\emph{See also Agresti 3.1.1, 3.1.2, 3.1.4}

First we put in place some building blocks: The multivariate normal
distribution (Section \ref{sec-mvrnorm}), the distributions of linear
regression estimates and residuals (Section \ref{sec-lin-reg-dist}), and
estimation of the noise variance \(\sigma^2\) (Section
\ref{sec-noise-estimation}).

\hypertarget{sec-mvrnorm}{%
\subsection{The multivariate normal distribution}\label{sec-mvrnorm}}

Recall that a random vector \(\boldsymbol{w} \in \mathbb{R}^d\) has a
multivariate normal distribution with mean \(\boldsymbol{\mu}\) and
covariance matrix \(\boldsymbol{\Sigma}\) if it has probability density

\[
p(\boldsymbol{w}) = \frac{1}{\sqrt{(2\pi)^{d}\text{det}(\boldsymbol{\Sigma})}}\exp\left(-\frac{1}{2}(\boldsymbol{w} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{w} - \boldsymbol{\mu})\right).
\]

These random vectors have lots of special properties, including:

\begin{itemize}
\tightlist
\item
  \textbf{Linear transformation}: If
  \(\boldsymbol{w} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})\), then
  \(\boldsymbol{A} \boldsymbol{w} + \boldsymbol{b} \sim N(\boldsymbol{A} \boldsymbol{\mu} + \boldsymbol{b}, \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^\top)\).
\item
  \textbf{Independence}: If \[
  \begin{pmatrix}\boldsymbol{w}_1 \\ \boldsymbol{w}_2 \end{pmatrix} \sim N\left(\begin{pmatrix}\boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix} , \begin{pmatrix}\boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{12}^\top & \boldsymbol{\Sigma}_{22}\end{pmatrix}\right),
  \] then \(\boldsymbol{w}_1 \perp\!\!\!\perp \boldsymbol{w}_2\) if and
  only if \(\boldsymbol{\Sigma}_{12} = \boldsymbol{0}\).
\end{itemize}

An important distribution related to the multivariate normal is the
\(\chi^2_d\) (chi-squared with \(d\) degrees of freedom) distribution,
defined as

\[
\chi^2_d \equiv \sum_{j = 1}^d w_j^2 \quad \text{for} \quad w_1, \dots, w_d \overset{\text{i.i.d.}}{\sim} N(0, 1).
\]

\hypertarget{sec-lin-reg-dist}{%
\subsection{The distributions of linear regression estimates and
residuals}\label{sec-lin-reg-dist}}

\emph{See also Dunn and Smyth 2.8.2}

The most important distributional result in linear regression is that

\begin{equation}\protect\hypertarget{eq-beta-hat-dist}{}{
\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}).
}\label{eq-beta-hat-dist}\end{equation}

Indeed, by the linear transformation property of the multivariate normal
distribution,

\[
\begin{split}
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}_n) &\Longrightarrow \boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{y} \sim N((\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{\beta}, (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \sigma^2 \boldsymbol{I}_n \boldsymbol{X}(\boldsymbol{X}^\top \boldsymbol{X})^{-1}) \\
&= N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}).
\end{split}
\]

Next, let's consider the joint distribution of
\(\boldsymbol{\widehat{\mu}} = \boldsymbol{X} \boldsymbol{\widehat{\beta}}\)
and
\(\boldsymbol{\widehat{\epsilon}} = \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\).
We have

\[
\begin{split}
\begin{pmatrix} \boldsymbol{\widehat{\mu}} \\ \boldsymbol{\widehat{\epsilon}} \end{pmatrix} = \begin{pmatrix} \boldsymbol{H} \boldsymbol{y} \\ (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y} \end{pmatrix} = \begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\boldsymbol{y} &\sim N\left(\begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\boldsymbol{X} \boldsymbol{\beta}, \begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\cdot \sigma^2 \boldsymbol{I} \begin{pmatrix} \boldsymbol{H} & \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\right) \\
&= N\left(\begin{pmatrix} \boldsymbol{X} \boldsymbol{\beta} \\ \boldsymbol{0} \end{pmatrix}, \begin{pmatrix} \sigma^2 \boldsymbol{H} & \boldsymbol{0} \\ \boldsymbol{0} & \sigma^2(\boldsymbol{I} - \boldsymbol{H}) \end{pmatrix} \right).
\end{split}
\]

In other words,

\begin{equation}\protect\hypertarget{eq-fit-and-error-dist}{}{
\boldsymbol{\widehat{\mu}} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{H}) \quad \text{and} \quad \boldsymbol{\widehat{\epsilon}} \sim N(\boldsymbol{0}, \sigma^2(\boldsymbol{I} - \boldsymbol{H})), \quad \text{with} \quad \boldsymbol{\widehat{\mu}} \perp\!\!\!\perp \boldsymbol{\widehat{\epsilon}}.
}\label{eq-fit-and-error-dist}\end{equation}

The statistical independence between \(\boldsymbol{\widehat{\mu}}\) and
\(\boldsymbol{\widehat{\epsilon}}\) is a result of the fact that these
two quantities are projections of \(\boldsymbol{y}\) onto two orthogonal
subspaces: \(C(\boldsymbol{X})\) and \(C(\boldsymbol{X})^\perp\) (Figure
\ref{fig-orthogonality-fit-residuals}).

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/orthogonality-fit-residuals.jpg}

}

\caption{\label{fig-orthogonality-fit-residuals}The fitted vector
\(\boldsymbol{\widehat{\mu}}\) and the residual vector
\(\boldsymbol{\widehat{\epsilon}}\) are projections of
\(\boldsymbol{y}\) onto orthogonal subspaces.}

\end{figure}

Since \(\boldsymbol{\widehat{\beta}}\) is a deterministic function of
\(\boldsymbol{\widehat{\mu}}\) (in particular,
\(\boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{\widehat{\mu}}\)),
it also follows that

\begin{equation}\protect\hypertarget{eq-beta-ind-eps}{}{
\boldsymbol{\widehat{\beta}} \perp\!\!\!\perp \boldsymbol{\widehat{\epsilon}}.
}\label{eq-beta-ind-eps}\end{equation}

\hypertarget{sec-noise-estimation}{%
\subsection{\texorpdfstring{Estimation of the noise variance
\(\sigma^2\)}{Estimation of the noise variance \textbackslash sigma\^{}2}}\label{sec-noise-estimation}}

\emph{See also Dunn and Smyth 2.4.2, 2.5.3}

We can't quite do inference for \(\boldsymbol{\beta}\) based on the
distributional result \ref{eq-beta-hat-dist} because the noise variance
\(\sigma^2\) is unknown to us. Intuitively, since
\(\sigma^2 = \mathbb{E}[\epsilon_i^2]\), we can get an estimate of
\(\sigma^2\) by looking at the quantity
\(\|\boldsymbol{\widehat{\epsilon}}\|^2\). To get the distribution of
this quantity, we need the following lemma:

\begin{lemma}[]\protect\hypertarget{lem-normal-projection}{}\label{lem-normal-projection}

Let \(\boldsymbol{w} \sim N(\boldsymbol{0}, \boldsymbol{P})\) for some
projection matrix \(\boldsymbol{P}\). Then,
\(\|\boldsymbol{w}\|^2 \sim \chi^2_d\), where
\(d = \text{trace}(\boldsymbol{P})\) is the dimension of the subspace
onto which \(\boldsymbol{P}\) projects.

\end{lemma}

\begin{proof}

Let
\(\boldsymbol{P} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{U}^\top\)
be an eigenvalue decomposition of \(\boldsymbol{P}\), where
\(\boldsymbol{U}\) is orthogonal and \(\boldsymbol{D}\) is a diagonal
matrix with \(D_{ii} \in \{0,1\}\). We have
\(\boldsymbol{w} \overset{d}{=} \boldsymbol{U} \boldsymbol{D} \boldsymbol{z}\)
for \(\boldsymbol{z} \sim N(0, \boldsymbol{I}_n)\). Therefore,

\[
\|\boldsymbol{w}\|^2 = \|\boldsymbol{D} \boldsymbol{z}\|^2 = \sum_{i: D_{ii} = 1} z_i^2 \sim \chi^2_d, \quad \text{where } d = |\{i: D_{ii} = 1\}| = \text{trace}(D) = \text{trace}(\boldsymbol{P}).
\]

\end{proof}

Recall that \(\boldsymbol{I} - \boldsymbol{H}\) is a projection onto the
\((n-p)\)-dimensional space \(C(\boldsymbol{X})^\perp\), so by Lemma
\ref{lem-normal-projection} and equation \ref{eq-fit-and-error-dist} we
have

\begin{equation}\protect\hypertarget{eq-eps-norm-dist}{}{
\|\boldsymbol{\widehat{\epsilon}}\|^2 \sim \sigma^2 \chi^2_{n-p}.
}\label{eq-eps-norm-dist}\end{equation}

From this result, it follows that
\(\mathbb{E}[\|\boldsymbol{\widehat{\epsilon}}\|^2] = n-p\), so

\begin{equation}\protect\hypertarget{eq-unbiased-noise-estimate}{}{
\widehat{\sigma}^2 \equiv \frac{1}{n-p}\|\boldsymbol{\widehat{\epsilon}}\|^2
}\label{eq-unbiased-noise-estimate}\end{equation}

is an unbiased estimate for \(\sigma^2\). Why does the denominator need
to be \(n-p\) rather than \(n\) for the estimator above to be unbiased?
The reason for this is that the residuals
\(\boldsymbol{\widehat{\epsilon}}\) are the projection of the true noise
vector \(\boldsymbol{\epsilon} \in \mathbb{R}^n\) onto the
\((n-p)\)-dimensional subspace \(C(\boldsymbol{X})^\perp\) (Figure
\ref{fig-residuals-as-noise-projection}). To see this, note that

\[
\boldsymbol{\widehat{\epsilon}} = (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y} = (\boldsymbol{I} - \boldsymbol{H})(\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) = (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{\epsilon}.
\]

Therefore, the norm of the residual vector will be smaller than that of
the noise vector, especially to the extent that \(p\) is close to \(n\).

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/residuals-as-noise-projection.jpg}

}

\caption{\label{fig-residuals-as-noise-projection}The residual vector
\(\boldsymbol{\widehat{\epsilon}}\) is the projection of the noise
vector \(\boldsymbol{\epsilon}\) onto \(C(\boldsymbol{X})^\perp\).}

\end{figure}

\hypertarget{hypothesis-testing}{%
\section{Hypothesis Testing}\label{hypothesis-testing}}

\emph{See also Agresti 3.2.1, 3.2.2, 3.2.4, 3.2.8}

Typically, two types of null hypotheses are tested in a regression
setting: those involving one-dimensional parameters and those involving
multi-dimensional parameters. For example, consider the null hypotheses
\(H_0: \beta_j = 0\) and \(H_0: \boldsymbol{\beta}_S = \boldsymbol{0}\)
for \(S \subseteq \{0, 1, \dots, p-1\}\), respectively. We discuss tests
of these two kinds of hypotheses in Sections \ref{sec-one-dim-testing}
and \ref{sec-multi-dim-testing}, and then discuss the power of these
tests in Section \ref{sec-power}.

\hypertarget{sec-one-dim-testing}{%
\subsection{Testing a One-Dimensional
Parameter}\label{sec-one-dim-testing}}

\emph{See also Dunn and Smyth 2.8.3}

\hypertarget{t-test-for-a-single-coefficient}{%
\subsubsection{\texorpdfstring{\(t\)-test for a Single
Coefficient}{t-test for a Single Coefficient}}\label{t-test-for-a-single-coefficient}}

The most common question to ask in a linear regression context is: Is
the \(j\)th predictor associated with the response when controlling for
the other predictors? In the language of hypothesis testing, this
corresponds to the null hypothesis:

\begin{equation}\protect\hypertarget{eq-one-dim-null}{}{
H_0: \beta_j = 0
}\label{eq-one-dim-null}\end{equation}

According to \ref{eq-beta-hat-dist}, we have
\(\widehat{\beta}_j \sim N(0, \sigma^2/s_j^2)\), where, as we learned in
Chapter 1:

\[
s_j^{2} \equiv [(\boldsymbol{X}^T \boldsymbol{X})^{-1}_{jj}]^{-1} = \|\boldsymbol{x}_{*j}^\perp\|^2.
\]

Therefore,

\begin{equation}\protect\hypertarget{eq-oracle-z-stat}{}{
\frac{\widehat{\beta}_j}{\sigma/s_j} \sim N(0,1),
}\label{eq-oracle-z-stat}\end{equation}

and we are tempted to define a level \(\alpha\) test of the null
hypothesis \ref{eq-one-dim-null} based on this normal distribution.
While this is infeasible since we don't know \(\sigma^2\), we can
substitute in the unbiased estimate \ref{eq-unbiased-noise-estimate}
derived in Section \ref{sec-noise-estimation}. Then,

\[
\text{SE}(\widehat{\beta}_j) \equiv \frac{\widehat{\sigma}}{s_j}
\]

is the standard error of \(\widehat{\beta}_j\), which is an
approximation to the standard deviation of \(\widehat{\beta}_j\).
Dividing \(\widehat{\beta}_j\) by its standard error gives us the
\(t\)-statistic:

\[
t_j \equiv \frac{\widehat{\beta}_j}{\text{SE}(\widehat{\beta}_j)} = \frac{\widehat{\beta}_j}{\sqrt{\frac{1}{n-p}\|\boldsymbol{\widehat{\epsilon}}\|^2}/s_j}.
\]

This statistic is \emph{pivotal}, in the sense that it has the same
distribution for any \(\boldsymbol{\beta}\) such that \(\beta_j = 0\).
Indeed, we can rewrite it as:

\[
t_j = \frac{\frac{\widehat{\beta}}{\sigma/s_j}}{\sqrt{\frac{\sigma^{-2}\|\boldsymbol{\widehat{\epsilon}}\|^2}{n-p}}}.
\]

Recalling the independence of \(\boldsymbol{\widehat{\beta}}\) and
\(\boldsymbol{\widehat{\epsilon}}\) \ref{eq-beta-ind-eps}, the scaled
chi-square distribution of \(\|\boldsymbol{\widehat{\epsilon}}\|^2\)
\ref{eq-eps-norm-dist}, and the standard normal distribution of
\(\frac{\widehat{\beta}}{\sigma/s_j}\) \ref{eq-oracle-z-stat}, we find
that:

\[
\text{Under } H_0:\beta_j = 0, \quad t_j \sim \frac{N(0,1)}{\sqrt{\frac{1}{n-p}\chi^2_{n-p}}}, \quad \text{with numerator and denominator independent.}
\]

The latter distribution is called the \emph{\(t\) distribution with
\(n-p\) degrees of freedom} and is denoted \(t_{n-p}\). This paves the
way for the two-sided \(t\)-test:

\[
\phi_t(\boldsymbol{X}, \boldsymbol{y}) = \mathbbm{1}(|t_j| > t_{n-p}(1-\alpha/2)),
\]

where \(t_{n-p}(1-\alpha/2)\) denotes the \(1-\alpha/2\) quantile of
\(t_{n-p}\). Note that, by the law of large numbers,

\[
\frac{1}{n-p}\chi^2_{n-p} \overset{P}{\rightarrow} 1 \quad \text{as} \quad n - p \rightarrow \infty,
\]

so for large \(n-p\) we have \(t_{j} \sim t_{n-p} \approx N(0,1)\).
Hence, the \(t\)-test is approximately equal to the following
\(z\)-test:

\[
\phi_t(\boldsymbol{X}, \boldsymbol{y}) \approx \phi_z(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}(|t_j| > z(1-\alpha/2)),
\]

where \(z(1-\alpha/2)\) is the \(1-\alpha/2\) quantile of \(N(0,1)\).
The \(t\)-test can also be defined in a one-sided fashion if power
against one-sided alternatives is desired.

\hypertarget{example-one-sample-model}{%
\subsubsection{Example: One-Sample
Model}\label{example-one-sample-model}}

Consider the intercept-only linear regression model
\(y = \beta_0 + \epsilon\), and let's apply the \(t\)-test derived above
to test the null hypothesis \(H_0: \beta_0 = 0\). We have
\(\widehat{\beta}_0 = \bar{y}\). Furthermore, we have

\[
\text{SE}^2(\widehat{\beta}_0) = \frac{\widehat{\sigma}^2}{n}, \quad \text{where} \quad \widehat{\sigma}^2 = \frac{1}{n-1}\|\boldsymbol{y} - \bar{y} \boldsymbol{1}_n\|^2.
\]

Hence, we obtain the \(t\) statistic:

\[
t = \frac{\widehat{\beta}_0}{\text{SE}(\widehat{\beta}_0)} = \frac{\sqrt{n} \bar{y}}{\sqrt{\frac{1}{n-1}\|\boldsymbol{y} - \bar{y} \boldsymbol{1}_n\|^2}}.
\]

According to the theory above, this test statistic has a null
distribution of \(t_{n-1}\).

\hypertarget{example-two-sample-model}{%
\subsubsection{Example: Two-Sample
Model}\label{example-two-sample-model}}

Suppose we have \(x_1 \in \{0,1\}\), in which case the linear regression
\(y = \beta_0 + \beta_1 x_1 + \epsilon\) becomes a two-sample model. We
can rewrite this model as:

\[
y_i \sim \begin{cases}
N(\beta_0, \sigma^2) \quad &\text{for } x_i = 0; \\
N(\beta_0 + \beta_1, \sigma^2) \quad &\text{for } x_i = 1.
\end{cases}
\]

It is often of interest to test the null hypothesis
\(H_0: \beta_1 = 0\), i.e., that the two groups have equal means. Let's
define:

\[
\bar{y}_0 \equiv \frac{1}{n_0}\sum_{i: x_i = 0} y_i, \quad \bar{y}_1 \equiv \frac{1}{n_1}\sum_{i: x_i = 1} y_i, \quad \text{where} \quad n_0 = |\{i: x_i = 0\}| \text{ and } n_1 = |\{i: x_i = 1\}|.
\]

Then, we have seen before that \(\widehat{\beta}_0 = \bar{y}_0\) and
\(\widehat{\beta}_1 = \bar{y}_1 - \bar{y}_0\). We can compute that:

\[
s_1^2 \equiv \|\boldsymbol{x}_{*1}^{\perp}\|^2 = \|\boldsymbol{x}_{*1} - \frac{n_1}{n}\boldsymbol{1}\|^2 = n_1\frac{n_0^2}{n^2} + n_0\frac{n_1^2}{n^2} = \frac{n_0 n_1}{n} = \frac{1}{\frac{1}{n_0} + \frac{1}{n_1}}
\]

and

\[
\widehat{\sigma}^2 = \frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar{y}_0)^2 + \sum_{i: x_i = 1}(y_i - \bar{y}_1)^2\right).
\]

Therefore, we arrive at a \(t\)-statistic of:

\[
t = \frac{\sqrt{\frac{1}{\frac{1}{n_0} + \frac{1}{n_1}}}(\bar{y}_1 - \bar{y}_0)}{\sqrt{\frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar{y}_0)^2 + \sum_{i: x_i = 1}(y_i - \bar{y}_1)^2\right)}}.
\]

Under the null hypothesis, this statistic has a distribution of
\(t_{n-2}\).

\hypertarget{t-test-for-a-contrast-among-coefficients}{%
\subsubsection{\texorpdfstring{\(t\)-test for a Contrast Among
Coefficients}{t-test for a Contrast Among Coefficients}}\label{t-test-for-a-contrast-among-coefficients}}

Given a vector \(\boldsymbol{c} \in \mathbb{R}^p\), the quantity
\(\boldsymbol{c}^T \boldsymbol{\beta}\) is sometimes called a
\emph{contrast}. For example, suppose
\(\boldsymbol{c} = (1,-1, 0, \dots, 0)\). Then,
\(\boldsymbol{c}^T \boldsymbol{\beta} = \beta_1 - \beta_2\) is the
difference in effects of the first and second predictors. We are
sometimes interested in testing whether such a contrast is equal to
zero, i.e., \(H_0: \boldsymbol{c}^T \boldsymbol{\beta} = 0\). While this
hypothesis can involve two or more of the predictors, the parameter
\(\boldsymbol{c}^T \boldsymbol{\beta}\) is still one-dimensional, and
therefore we can still apply a \(t\)-test. Going back to the
distribution
\(\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2(\boldsymbol{X}^T \boldsymbol{X})^{-1})\),
we find that:

\[
\boldsymbol{c}^T\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{c}^T\boldsymbol{\beta}, \sigma^2\boldsymbol{c}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{c}).
\]

Therefore, under the null hypothesis that
\(\boldsymbol{c}^T \boldsymbol{\beta} = 0\), we can derive that:

\begin{equation}\protect\hypertarget{eq-contrasts-t-dist}{}{
\frac{\boldsymbol{c}^T \boldsymbol{\widehat{\beta}}}{\widehat{\sigma} \sqrt{\boldsymbol{c}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{c}}} \sim t_{n-p},
}\label{eq-contrasts-t-dist}\end{equation}

giving us another \(t\)-test. Note that the \(t\)-tests described above
can be recovered from this more general formulation by setting
\(\boldsymbol{c} = \boldsymbol{e}_j\), the indicator vector with the
\(j\)th coordinate equal to 1 and all others equal to zero.

\hypertarget{sec-multi-dim-testing}{%
\subsection{Testing a Multi-Dimensional
Parameter}\label{sec-multi-dim-testing}}

\emph{See also Dunn and Smyth 2.10.1}

\hypertarget{f-test-for-a-group-of-coefficients}{%
\subsubsection{\texorpdfstring{\(F\)-Test for a Group of
Coefficients}{F-Test for a Group of Coefficients}}\label{f-test-for-a-group-of-coefficients}}

Now we move on to the case of testing a multi-dimensional parameter:
\(H_0: \boldsymbol{\beta}_S = \boldsymbol{0}\) for some
\(S \subseteq \{0, 1, \dots, p-1\}\). In other words, we would like to
test

\[
H_0: \boldsymbol{y} = \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} + \boldsymbol{\epsilon} \quad \text{versus} \quad H_1: \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}.
\]

To test this hypothesis, let us fit least squares coefficients
\(\boldsymbol{\widehat{\beta}}_{-S}\) and
\(\boldsymbol{\widehat{\beta}}\) for the partial model as well as the
full model. If the partial model fits well, then the residuals
\(\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\)
from this model will not be much larger than the residuals
\(\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\) from
the full model. To quantify this intuition, let us recall our analysis
of variance decomposition from Chapter 1:

\[
\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 = \|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 + \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2.
\]

Let's consider the ratio

\[
\frac{\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2} = \frac{\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2},
\]

which is the relative increase in the residual sum of squares when going
from the full model to the partial model. Let us rewrite this ratio in
terms of projection matrices. Let \(\boldsymbol{H}\) be the projection
matrix for the full model, and let \(\boldsymbol{H}_{\text{-}S}\) be the
projection matrix for the partial model. Note that
\(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}\) is the projection matrix
onto the \(|S|\)-dimensional space
\(C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{\text{-}S})^\perp\)
(Figure~\ref{fig-f-test-geometry}).

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{figures/F-test-geometry.png}

}

\caption{\label{fig-f-test-geometry}Geometry of the \(F\)-test.
Orthogonality relationships stem from
\(C(\boldsymbol{X}_{*,\text{-}S}) \perp C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^\perp \perp C(\boldsymbol{X})^\perp\).}

\end{figure}

We have

\[
\frac{\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2} = \frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}\|^2},
\]

so the numerator and denominator are the squared norms of the
projections of \(\boldsymbol{y}\) onto
\(C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^\perp\) and
\(C(\boldsymbol{X})^\perp\), respectively
(Figure~\ref{fig-f-test-geometry}). Under the null hypothesis, we have
\(\boldsymbol{y} = \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} + \boldsymbol{\epsilon}\),
and

\[
(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{X}_{*,\text{-}S} \boldsymbol{\beta}_{-S} = (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{X}_{*,\text{-}S} \boldsymbol{\beta}_{-S} = 0
\]

because
\(\boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} \in C(\boldsymbol{X}_{*, \text{-}S}) \perp C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^T \perp C(\boldsymbol{X})^\perp\).
It follows that

\[
\frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}\|^2} = \frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\|^2}.
\]

Since the projection matrices in the numerator and denominator project
onto orthogonal subspaces, we have
\((\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon} \perp\!\!\!\perp (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\),
with
\(\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon}\|^2 \sim \sigma^2 \chi^2_{|S|}\)
and
\(\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\|^2 \sim \sigma^2 \chi^2_{n-p}\).
Renormalizing numerator and denominator to have expectation 1 under the
null, we arrive at the \(F\)-statistic

\[
F \equiv \frac{(\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2)/|S|}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2/(n-p)}.
\]

We have derived that under the null hypothesis,

\[
F \sim \frac{\chi^2_{|S|}/|S|}{\chi^2_{n-p}/(n-p)}, \quad \text{with numerator and denominator independent.}
\]

This distribution is called the \(F\)-distribution with \(|S|\) and
\(n-p\) degrees of freedom, and is denoted \(F_{|S|, n-p}\). Denoting by
\(F_{|S|, n-p}(1-\alpha)\) the \(1-\alpha\) quantile of this
distribution, we arrive at the \(F\)-test

\[
\phi_F(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}(F > F_{|S|, n-p}(1-\alpha)).
\]

Note that the \(F\)-test searches for deviations of
\(\boldsymbol{\beta}_{S}\) in all directions, and does not have
one-sided variants like the \(t\)-test.

\hypertarget{example-testing-for-any-significant-coefficients-except-the-intercept}{%
\subsubsection{Example: Testing for Any Significant Coefficients Except
the
Intercept}\label{example-testing-for-any-significant-coefficients-except-the-intercept}}

Suppose \(\boldsymbol{x}_{*,0} = \boldsymbol{1}_n\) is an intercept
term. Then, consider the null hypothesis
\(H_0: \beta_1 = \cdots = \beta_{p-1} = 0\). In other words, the null
hypothesis is the intercept-only model, and the alternative hypothesis
is the regression model with an intercept and \(p-1\) additional
predictors. In this case, \(S = \{1, \dots, p-1\}\) and \(-S = \{0\}\).
The corresponding \(F\) statistic is

\[
F \equiv \frac{(\|\boldsymbol{y} - \bar{y} \boldsymbol{1}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2)/(p-1)}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2/(n-p)},
\]

with null distribution \(F_{p-1, n-p}\).

\hypertarget{example-testing-for-equality-of-group-means-in-c-groups-model}{%
\subsubsection{\texorpdfstring{Example: Testing for Equality of Group
Means in \(C\)-Groups
Model}{Example: Testing for Equality of Group Means in C-Groups Model}}\label{example-testing-for-equality-of-group-means-in-c-groups-model}}

As a further special case, consider the \(C\)-groups model from Chapter
1. Recall the ANOVA decomposition

\[
\sum_{i = 1}^n (y_i - \bar{y})^2 = \sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2 + \sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2 = \text{SSB} + \text{SSW}.
\]

The \(F\)-statistic in this case becomes

\[
F = \frac{\sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2/(C-1)}{\sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2/(n-C)} = \frac{\text{SSB}/(C-1)}{\text{SSW}/(n-C)},
\]

with null distribution \(F_{C-1, n-C}\).

\hypertarget{sec-power}{%
\section{Power}\label{sec-power}}

\emph{See also Agresti 3.2.5}

So far we've been focused on finding the null distributions of various
test statistics in order to construct tests with Type-I error control.
Now let's shift our attention to examining the power of these tests.

\hypertarget{the-power-of-a-t-test}{%
\subsection{\texorpdfstring{The power of a
\(t\)-test}{The power of a t-test}}\label{the-power-of-a-t-test}}

Consider the \(t\)-test of the null hypothesis \(H_0: \beta_j = 0\).
Suppose that, in reality, \(\beta_j \neq 0\). What is the probability
the \(t\)-test will reject the null hypothesis? To answer this question,
recall that \(\widehat \beta_j \sim N(\beta_j, \sigma^2/s_j^2)\).
Therefore,

\begin{equation}\protect\hypertarget{eq-t-alt-dist-1}{}{
t = \frac{\widehat \beta_j}{\text{SE}(\widehat \beta_j)} = \frac{\beta_j}{\text{SE}(\widehat \beta_j)} + \frac{\widehat \beta_j - \beta_j}{\text{SE}(\widehat \beta_j)} \overset{\cdot}{\sim} N\left(\frac{\beta_j s_j}{\sigma}, 1\right)
}\label{eq-t-alt-dist-1}\end{equation}

Here we have made the approximation
\(\text{SE}(\widehat \beta_j) \approx \frac{\sigma}{s_j}\), which is
pretty good when \(n-p\) is large. Therefore, the power of the two-sided
\(t\)-test is

\[
\mathbb{E}[\phi_t] = \mathbb{P}[\phi_t = 1] \approx \mathbb{P}[|t| > z_{1-\alpha/2}] \approx \mathbb{P}\left[\left|N\left(\frac{\beta_j s_j}{\sigma}, 1\right)\right| > z_{1-\alpha/2}\right]
\]

Therefore, the quantity \(\frac{\beta_j s_j}{\sigma}\) determines the
power of the \(t\)-test. To understand \(s_j\) a little better, let's
assume that the rows \(\boldsymbol{x}_{i*}\) of the model matrix are
drawn i.i.d. from some distribution \((x_0, \dots, x_{p-1})\). Then we
have roughly

\[
\boldsymbol{x}_{*j}^\perp \approx \boldsymbol{x}_{*j} - \mathbb{E}[\boldsymbol{x}_{*j}|\boldsymbol{X}_{*, \text{-}j}],
\]

so
\(x_{ij}^\perp \approx x_{ij} - \mathbb{E}[x_{ij}|\boldsymbol{x}_{i,\text{-}j}]\).
Hence,

\[
s_j^2 \equiv \|\boldsymbol{x}_{*j}^\perp\|^2 \approx n\mathbb{E}[(x_j-\mathbb{E}[x_j|\boldsymbol{x}_{\text{-}j}])^2] = n\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]].
\]

Hence, we can rewrite the alternative distribution
(\ref{eq-t-alt-dist-1}) as

\begin{equation}\protect\hypertarget{eq-t-alt-dist-2}{}{
t \overset{\cdot}{\sim} N\left(\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]}}{\sigma}, 1\right)
}\label{eq-t-alt-dist-2}\end{equation}

We can see clearly now how the power of the \(t\)-test varies with the
effect size \(\beta_j\), the sample size \(n\), the degree of
collinearity \(\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]\),
and the noise standard deviation \(\sigma\).

\hypertarget{the-power-of-an-f-test}{%
\subsection{\texorpdfstring{The power of an
\(F\)-test}{The power of an F-test}}\label{the-power-of-an-f-test}}

Now let's turn our attention to computing the power of the \(F\)-test.
We have

\[
F = \frac{\|\boldsymbol{X}\boldsymbol{\widehat \beta} - \boldsymbol{X}_{*, \text{-}S}\boldsymbol{\widehat \beta}_{-S}\|^2/|S|}{\|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat \beta}\|^2/|n-p|} = \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\|(\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y}\|^2/|n-p|} \approx \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\sigma^2}.
\]

To calculate the distribution of the numerator, we need to introduce the
notion of a non-central chi-squared random variable.

\begin{definition}[]\protect\hypertarget{def-noncentral-chi-square}{}\label{def-noncentral-chi-square}

For some vector \(\boldsymbol{\mu} \in \mathbb{R}^d\), suppose
\(\boldsymbol{z} \sim N(\boldsymbol{\mu}, \boldsymbol{I}_d)\). Then, we
define the distribution of \(\|\boldsymbol{z}\|^2\) as the noncentral
chi-square random variable with \(d\) degrees of freedom and
noncentrality parameter \(\|\boldsymbol{\mu}\|^2\) and denote this
distribution by \(\chi^2_d(\|\boldsymbol{\mu}\|^2)\).

\end{definition}

The following proposition states two useful facts about noncentral
chi-square distributions.

\begin{proposition}[]\protect\hypertarget{prp-noncentral-chi-square}{}\label{prp-noncentral-chi-square}

The following two relations hold: 1. The mean of a
\(\chi^2_d(\|\boldsymbol{\mu}\|^2)\) random variable is
\(d + \|\boldsymbol{\mu}\|^2\). 2. If \(\boldsymbol{P}\) is a projection
matrix and
\(\boldsymbol{y} = \boldsymbol{\mu} + \boldsymbol{\epsilon}\), then
\(\frac{1}{\sigma^2}\|\boldsymbol{P} \boldsymbol{y}\|^2 \sim \chi^2_{\text{tr}(\boldsymbol{P})}\left(\frac{1}{\sigma^2}\|\boldsymbol{P} \boldsymbol{\mu}\|^2\right).\)

\end{proposition}

It therefore follows that

\[
F \approx \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\sigma^2} \sim \frac{1}{|S|}\chi^2_{|S|}\left(\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S})\boldsymbol{X} \boldsymbol{\beta}\|^2\right) = \frac{1}{|S|}\chi^2_{|S|}\left(\frac{1}{\sigma^2}\|\boldsymbol{X}^\perp_{*, S}\boldsymbol{\beta}_S\|^2\right).
\]

Assuming as before that the rows of \(\boldsymbol{X}\) are samples from
a joint distribution, we can write

\[
\|\boldsymbol{X}^\perp_{*, S}\boldsymbol{\beta}_S\|^2 \approx n\boldsymbol{\beta}_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S.
\]

Therefore,

\[
F \overset{\cdot}{\sim} \frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{\sigma^2}\right)
\]

which is similar in spirit to equation (\ref{eq-t-alt-dist-2}). To get a
better sense of what this relationship implies for the power of the
\(F\)-test, we find from the first part of
Proposition~\ref{prp-noncentral-chi-square} that, under the alternative,

\[
\mathbb{E}[F] \approx \mathbb{E}\left[\frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{\sigma^2}\right)\right] = 1 + \frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{|S| \cdot \sigma^2}.
\]

By contrast, under the null, the mean of the \(F\)-statistic is 1. The
\(|S|\) term in the denominator above suggests that testing larger sets
of variables explaining the same amount of variation in
\(\boldsymbol{y}\) will hurt power. The test must accommodate for the
fact that larger sets of variables will explain more of the variability
in \(y\) even under the null hypothesis.

\hypertarget{power-of-the-t-test-when-predictors-are-added-to-the-model}{%
\subsection{\texorpdfstring{Power of the \(t\)-test when predictors are
added to the
model}{Power of the t-test when predictors are added to the model}}\label{power-of-the-t-test-when-predictors-are-added-to-the-model}}

As we know, the outcome of a regression is a function of the predictors
that are used. What happens to the \(t\)-test \(p\)-value for
\(H_0: \beta_j = 0\) when a predictor is added to the model? To keep
things simple, let's consider the

\[
\text{true underlying model:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
\]

Let's consider the power of testing \(H_0: \beta_0 = 0\) in the
regression models

\[
\text{model 0:}\ y = \beta_0 x_0 + \epsilon \quad \text{versus} \quad \text{model 1:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
\]

There are four cases based on
\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}]\) and the value
of \(\beta_1\) in the true model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] \neq 0\) and
  \(\beta_1 \neq 0\). In this case, in model 0 we have omitted an
  important variable that is correlated with \(\boldsymbol{x}_{*0}\).
  Therefore, the meaning of \(\beta_0\) differs between model 0 and
  model 1, so it may not be meaningful to compare the \(p\)-values
  arising from these two models.
\item
  \(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] \neq 0\) and
  \(\beta_1 = 0\). In this case, we are adding a null predictor that is
  correlated with \(x_{*0}\). Recall that the power of the \(t\)-test
  hinges on the quantity
  \(\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]}}{\sigma}\).
  Adding the predictor \(x_1\) has the effect of reducing the
  conditional predictor variance
  \(\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]\), therefore
  reducing the power. This is a case of \emph{predictor competition}.
\item
  \(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0\) and
  \(\beta_1 \neq 0\). In this case, we are adding a non-null predictor
  that is orthogonal to \(\boldsymbol{x}_{*0}\). While the conditional
  predictor variance
  \(\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]\) remains the
  same due to orthogonality, the residual variance \(\sigma^2\) is
  reduced when going from model 0 to model
  1.\footnote{If $\beta_1$ is small enough, then the unbiased estimate of the residual variance may actually increase due to a reduction in the residual degrees of freedom in the denominator.}
  Therefore, in this case adding \(x_1\) to the model increases the
  power for testing \(H_0: \beta_0 = 0\). This is a case of
  \emph{predictor collaboration}.
\item
  \(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0\) and
  \(\beta_1 = 0\). In this case, we are adding an orthogonal null
  variable, which does not change the conditional predictor variance or
  the residual variance, and therefore keeps the power of the test the
  same.
\end{enumerate}

In conclusion, adding a predictor can either increase or decrease the
power of a \(t\)-test. Similar reasoning can be applied to the
\(F\)-test.

\textbf{Remark: Adjusting for covariates in randomized experiments.}
Case 3 above, i.e.,
\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0\) and
\(\beta_1 \neq 0\), arises in the context of randomized experiments in
causal inference. In this case, \(y\) represents the outcome, \(x_0\)
represents the treatment, and \(x_1\) represents a covariate. Because
the treatment is randomized, there is no correlation between \(x_0\) and
\(x_1\). Therefore, it is not necessary to adjust for \(x_1\) in order
to get an unbiased estimate of the average treatment effect. However, it
is known that adjusting for covariates can lead to more \emph{precise}
estimates of the treatment effect due to the phenomenon discussed in
case 3 above. This point is also related to the discussion in Chapter 1
about the fact that if \(x_0\) and \(x_1\) are orthogonal, then the
least squares coefficient \(\widehat \beta_0\) is the same regardless of
whether \(x_1\) is included in the model. As we see here, either
including \(x_1\) in the model or adjusting \(y\) for \(x_1\) is
necessary to get better power.

\hypertarget{confidence-and-prediction-intervals}{%
\section{Confidence and prediction
intervals}\label{confidence-and-prediction-intervals}}

\emph{See also Agresti 3.3, Dunn and Smyth 2.8.4-2.8.5}

In addition to hypothesis testing, we often want to construct confidence
intervals for the coefficients.

\hypertarget{confidence-interval-for-a-coefficient}{%
\subsection{Confidence interval for a
coefficient}\label{confidence-interval-for-a-coefficient}}

Under \(H_0: \beta_j = 0\), we showed that
\(\frac{\widehat{\beta_j}}{\widehat{\sigma}/s_j} \sim t_{n-p}\). The
same argument shows that for arbitrary \(\beta_j\), we have

\[
\frac{\widehat{\beta_j} - \beta_j}{\widehat{\sigma}/s_j} \sim t_{n-p}.
\]

We can use this relationship to construct a confidence interval for
\(\beta_j\) as follows:

\begin{equation}\protect\hypertarget{eq-pointwise-interval-beta}{}{
\begin{split}
1-\alpha = \mathbb{P}[|t_{n-p}| \leq t_{n-p}(1-\alpha/2)] &= \mathbb{P}\left[\left|\frac{\widehat{\beta_j} - \beta_j}{\widehat{\sigma}/s_j}\right| \leq t_{n-p}(1-\alpha/2) \right] \\
&= \mathbb{P}\left[\beta_j \in \left[\widehat{\beta_j} - \frac{\widehat{\sigma}}{s_j}t_{n-p}(1-\alpha/2), \widehat{\beta_j} + \frac{\widehat{\sigma}}{s_j}t_{n-p}(1-\alpha/2) \right]\right] \\
&\equiv \mathbb{P}\left[\beta_j \in \left[\widehat{\beta_j} - \text{SE}(\widehat{\beta_j})t_{n-p}(1-\alpha/2), \widehat{\beta_j} + \text{SE}(\widehat{\beta_j})t_{n-p}(1-\alpha/2) \right]\right] \\
&\equiv \mathbb{P}[\beta_j \in \text{CI}(\beta_j)].
\end{split}
}\label{eq-pointwise-interval-beta}\end{equation}

The confidence interval \(\text{CI}(\beta_j)\) defined above therefore
has \(1-\alpha\) coverage. Because of the duality between confidence
intervals and hypothesis tests, the factors contributing to powerful
tests (\ref{sec-power}) also lead to shorter confidence intervals.

\hypertarget{confidence-interval-for-mathbbeyboldsymbolx_0}{%
\subsection{\texorpdfstring{Confidence interval for
\(\mathbb{E}[y|\boldsymbol{x_0}]\)}{Confidence interval for \textbackslash mathbb\{E\}{[}y\textbar\textbackslash boldsymbol\{x\_0\}{]}}}\label{confidence-interval-for-mathbbeyboldsymbolx_0}}

Suppose now that we have a new predictor vector
\(\boldsymbol{x_0} \in \mathbb{R}^p\). The mean of the response for this
predictor vector is
\(\mathbb{E}[y|\boldsymbol{x_0}] = \boldsymbol{x_0}^T \boldsymbol{\beta}\).
Plugging in \(\boldsymbol{x_0}\) for \(\boldsymbol{c}\) in the relation,
we obtain

\[
\frac{\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} - \boldsymbol{x_0}^T \boldsymbol{\beta}}{\widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}}} \sim t_{n-p}.
\]

From this, we can derive that

\[
\text{CI}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}) \cdot t_{n-p}(1-\alpha/2) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}} \cdot t_{n-p}(1-\alpha/2)
\]

is a \(1-\alpha\) confidence interval for
\(\boldsymbol{x_0}^T \boldsymbol{\beta}\). We see that the width of this
confidence interval depends on \(\boldsymbol{x_0}\) through the quantity
\(\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}\).
Let's give this quantity a closer look, in the case when the regression
contains an intercept, i.e., \(\boldsymbol{x_{*,0}} = \boldsymbol{1}\).
Then, we have
\(\boldsymbol{x_0} = (1, \boldsymbol{x^T_{0,\text{-}0}})\). Then,
defining \(\bar{x} \in \mathbb{R}^{p-1}\) as the vector of column-wise
means of \(\boldsymbol{X_{*,\text{-}0}}\), we can rewrite the regression
as

\[
y = \beta_0 + \boldsymbol{x_{\text{-}0}}^T \boldsymbol{\beta_{\text{-}0}} + \epsilon \equiv \beta'_0 + (\boldsymbol{x_{\text{-}0}}-\bar{x})^T  \boldsymbol{\beta_{\text{-}0}} + \epsilon.
\]

Therefore, we seek a prediction interval for
\(\boldsymbol{x_{0}}^T \boldsymbol{\beta} = \beta'_0 + (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T \boldsymbol{\beta_{\text{-}0}}\).
With this reformulation, we can compute

\[
\begin{split}
\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0} &= (1 \ (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T)\begin{pmatrix}\boldsymbol{1}^T \boldsymbol{1} & 0 \\ 0 &\boldsymbol{X_{*,\text{-}0}}^T \boldsymbol{X_{*,\text{-}0}} \end{pmatrix}^{-1}{1 \choose \boldsymbol{x_{0, \text{-}0}}-\bar{x}} \\
&= \frac{1}{n} + (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T (\boldsymbol{X_{*,\text{-}0}}^T \boldsymbol{X_{*,\text{-}0}})^{-1}(\boldsymbol{x_{0, \text{-}0}}-\bar{x}).
\end{split}
\]

Hence, we see that this quantity grows larger as
\(\boldsymbol{x_{0, \text{-}0}}-\bar{x}\) grows larger, and achieves its
minimum when \(\boldsymbol{x_{0, \text{-}0}}=\bar{x}\). Let's look at
the special case when \(p = 2\), so there is just one predictor except
the intercept. Then, we have
\(\boldsymbol{X_{*,\text{-}0}} = \boldsymbol{x_{*,1}}-\bar{x_1}\), so

\[
\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0} = \frac{1}{n} + \frac{(x_{01}-\bar{x_1})^2}{\|\boldsymbol{x_{*,1}}-\bar{x_1}\|^2}.
\]

\hypertarget{prediction-interval-for-yboldsymbolx_0}{%
\subsection{\texorpdfstring{Prediction interval for
\(y|\boldsymbol{x_0}\)}{Prediction interval for y\textbar\textbackslash boldsymbol\{x\_0\}}}\label{prediction-interval-for-yboldsymbolx_0}}

Instead of creating a confidence interval for a point on the regression
line, we may want to create a confidence interval for a new draw \(y_0\)
of \(y\) for \(\boldsymbol{x} = \boldsymbol{x_0}\), i.e., a
\emph{prediction interval}. Note that

\[
y_0 - \boldsymbol{x_0}^T \widehat{\beta} = \boldsymbol{x_0}^T \beta + \epsilon_0 - \boldsymbol{x_0}^T \widehat{\beta} = \epsilon_0 + \boldsymbol{x_0}^T (\beta-\widehat{\beta}) \sim N(0, \sigma^2 + \sigma^2 \boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}).
\]

Therefore, we have

\[
\frac{y_0 - \boldsymbol{x_0}^T \widehat{\beta}}{\widehat{\sigma}\sqrt{1 + \boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}}} \sim t_{n-p},
\]

which leads to the \(1-\alpha\) prediction interval

\begin{equation}\protect\hypertarget{eq-pointwise-contrast-interval}{}{
\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{1+\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}} \cdot t_{n-p}(1-\alpha/2) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}) \cdot t_{n-p}(1-\alpha/2).
}\label{eq-pointwise-contrast-interval}\end{equation}

\textbf{Remark: Prediction with confidence in machine learning.}

The entire field of supervised machine learning is focused on accurately
predicting \(y_0\) from \(\boldsymbol{x_0}\), usually using nonlinear
functions \(\widehat{f}(\boldsymbol{x_0})\). In addition to providing a
guess \(\widehat{y_0}\) for \(y_0\), it is often useful to quantify the
uncertainty in this guess. In other words, it is useful to come up with
a prediction interval (or prediction region) \(\text{PI}(y_0)\) such
that

\begin{equation}\protect\hypertarget{eq-conditional-prediction-interval}{}{
\mathbb{P}[y_0 \in \text{PI}(y_0) \mid \boldsymbol{x_0}] \geq 1-\alpha.
}\label{eq-conditional-prediction-interval}\end{equation}

For example, in safety-critical applications of machine learning like
self-driving cars, it is essential to have confidence in predictions.
Unfortunately, beyond the realm of linear regression, it is hard to come
up with intervals satisfying (\ref{eq-conditional-prediction-interval})
for each point \(\boldsymbol{x_0}\). However, the emerging field of
\emph{conformal inference} provides guarantees on average over possible
values of \(\boldsymbol{x}\):

\begin{equation}\protect\hypertarget{eq-unconditional-prediction-interval}{}{
\mathbb{P}[y \in \text{PI}(y)] = \mathbb{E}[\mathbb{P}[y \in \text{PI}(y) \mid \boldsymbol{x}]] \geq 1-\alpha.
}\label{eq-unconditional-prediction-interval}\end{equation}

Remarkably, these guarantees place no assumption on the machine learning
method used and require only that the data points on which
\(\widehat{f}\) is trained are exchangeable (an even weaker condition
than i.i.d.). While the unconditional guarantee
(\ref{eq-unconditional-prediction-interval}) is weaker than the
conditional one (\ref{eq-conditional-prediction-interval}), it can be
obtained for modern machine learning and deep learning models.

\hypertarget{simultaneous-intervals}{%
\subsection{Simultaneous intervals}\label{simultaneous-intervals}}

Note that the intervals in the preceding sections have \emph{pointwise
coverage}. For example, we have

\[
\mathbb{P}[\beta_j \in \text{CI}(\beta_j)] \geq 1-\alpha \quad \text{for each } j.
\]

or

\[
\mathbb{P}[\boldsymbol{x_0}^T \boldsymbol{\beta} \in \text{CI}(\boldsymbol{x_0}^T \boldsymbol{\beta})] \geq 1-\alpha \quad \text{for each } \boldsymbol{x_0}.
\]

Sometimes a stronger \emph{simultaneous coverage} guarantee is desired,
e.g.,

\begin{equation}\protect\hypertarget{eq-simultaneous-coordinatewise}{}{
\mathbb{P}[\beta_j \in \text{CI}^{\text{sim}}(\beta_j) \ \text{for each } j] \geq 1-\alpha
}\label{eq-simultaneous-coordinatewise}\end{equation}

or

\begin{equation}\protect\hypertarget{eq-simultaneous-contrasts}{}{
\mathbb{P}[\boldsymbol{x_0}^T \boldsymbol{\beta} \in \text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \ \text{for each } \boldsymbol{x_0}] \geq 1-\alpha.
}\label{eq-simultaneous-contrasts}\end{equation}

Simultaneous confidence intervals are possible to construct as well. As
a starting point, note that

\[
\frac{\frac{1}{p}\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X} \boldsymbol{\beta}\|^2}{\widehat{\sigma}^2} \sim F_{p, n-p}.
\]

Hence, we have

\[
\mathbb{P}[\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X} \boldsymbol{\beta}\|^2 \leq p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha)] \geq 1-\alpha.
\]

Hence, the region

\[
\text{CR}(\boldsymbol{\beta}) \equiv \{\boldsymbol{\beta}: (\boldsymbol{\widehat{\beta}} - \boldsymbol{\beta})^T \boldsymbol{X}^T \boldsymbol{X} (\boldsymbol{\widehat{\beta}} - \boldsymbol{\beta})  \leq p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha)\} \subseteq \mathbb{R}^p
\]

is a \(1-\alpha\) confidence region for the vector
\(\boldsymbol{\beta}\):

\[
\mathbb{P}[\boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})] \geq 1-\alpha.
\]

It's easy to see that \(\text{CR}(\boldsymbol{\beta})\) is an ellipse
centered at \(\boldsymbol{\widehat{\beta}}\).

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{figures/confidence-regions.jpg}

}

\caption{\label{fig-confidence-region}Confidence region and simultaneous
and pointwise confidence intervals.}

\end{figure}

Since the confidence region is for the entire vector
\(\boldsymbol{\beta}\), we can define simultaneous confidence intervals
for each coordinate as follows:

\[
\text{CI}^{\text{sim}}(\beta_j) \equiv \{\beta_j: \boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\}.
\]

Then, these confidence intervals will satisfy the simultaneous coverage
property (\ref{eq-simultaneous-coordinatewise}). We will obtain a more
explicit expression for \(\text{CI}^{\text{sim}}(\beta_j)\) shortly.

Similarly, we may define the simultaneous confidence regions

\[
\text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \equiv \{\boldsymbol{x_0}^T \boldsymbol{\beta}: \boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\}.
\]

Let us find a more explicit expression for the latter interval. For
notational ease, let us define
\(\boldsymbol{\Sigma} \equiv \boldsymbol{X}^T \boldsymbol{X}\). Then,
note that if \(\boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\),
then by the Cauchy-Schwarz inequality we have

\[
\begin{split}
(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}-\boldsymbol{x_0}^T \boldsymbol{\beta})^2 = \|\boldsymbol{x_0}^T (\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 &= \|(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{x_0})^T \boldsymbol{\Sigma}^{1/2}(\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 \\
&\leq \|(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{x_0})\|^2\|\boldsymbol{\Sigma}^{1/2}(\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 \leq \boldsymbol{x_0}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{x_0} p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha),
\end{split}
\]

i.e.,

\begin{equation}\protect\hypertarget{eq-simultaneous-fit-se}{}{
\boldsymbol{x_0}^T \boldsymbol{\beta} \in \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{x_0}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}})\cdot\sqrt{pF_{p, n-p}(1-\alpha)}.
}\label{eq-simultaneous-fit-se}\end{equation}

Defining the above interval as
\(\text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta})\) gives
us the simultaneous coverage property (\ref{eq-simultaneous-contrasts}).
Comparing to equation (\ref{eq-pointwise-contrast-interval}), we see
that the simultaneous interval is the pointwise interval expanded by a
factor of \(\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)\).
Specializing to the case \(\boldsymbol{x_0} \equiv \boldsymbol{e_j}\),
we get an expression for the simultaneous intervals for each coordinate:

\begin{equation}\protect\hypertarget{eq-simultaneous-coordinatewise-se}{}{
\text{CI}^{\text{sim}}(\beta_j) \equiv \widehat{\beta_j} \pm \widehat{\sigma} \sqrt{(\boldsymbol{X}^T \boldsymbol{X})^{-1}_{jj}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \text{SE}(\widehat{\beta_j})\sqrt{pF_{p, n-p}(1-\alpha)},
}\label{eq-simultaneous-coordinatewise-se}\end{equation}

which again is the pointwise interval (\ref{eq-pointwise-interval-beta})
expanded by a factor of
\(\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)\). These simultaneous
intervals are called \emph{Working-Hotelling intervals}.

\hypertarget{practical-considerations}{%
\section{Practical considerations}\label{practical-considerations}}

\hypertarget{practical-versus-statistical-significance}{%
\subsection{Practical versus statistical
significance}\label{practical-versus-statistical-significance}}

You can have a statistically significant effect that is not practically
significant. The hypothesis testing framework is most useful in the case
when the signal-to-noise ratio is relatively small. Otherwise,
constructing a confidence interval for the effect size is a more
meaningful approach.

\hypertarget{correlation-versus-causation-and-simpsons-paradox}{%
\subsection{Correlation versus causation, and Simpson's
paradox}\label{correlation-versus-causation-and-simpsons-paradox}}

Causation can be elusive for several reasons. One is reverse causation,
where it is not clear whether \(X\) causes \(Y\) or \(Y\) causes \(X\).
Another is confounding, where there is a third variable \(Z\) that
causes both \(X\) and \(Y\). For the latter reason, linear regression
coefficients can be sensitive to the choice of other predictors to
include and can be misleading if you omit important variables from the
regression. A special and sometimes overlooked case of this is
\emph{Simpson's paradox}, where an important discrete variable is
omitted. Consider the example in Figure \ref{fig-simpson-paradox}.
Sometimes this discrete variable may seem benign, such as the year in
which the data was collected. Such variables might or might not be
measured.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures/kidney-stones.png}

}

\caption{\label{fig-simpson-paradox}An example of Simpson's paradox
(source: Wikipedia).}

\end{figure}

\hypertarget{dealing-with-correlated-predictors}{%
\subsection{Dealing with correlated
predictors}\label{dealing-with-correlated-predictors}}

It depends on the goal. If we're trying to tease apart effects of
correlated predictors, then we have no choice but to proceed as usual
despite lower power. Otherwise, we can test predictors in groups via the
\(F\)-test to get higher power at the cost of lower ``resolution.''
Sometimes, it is recommended to simply remove predictors that are
correlated with other predictors. This practice, however, is somewhat
arbitrary and not recommended.

\hypertarget{model-selection}{%
\subsection{Model selection}\label{model-selection}}

We need to ask ourselves: Why do we want to do model selection? It can
either be for prediction purposes or for inferential purposes. If it is
for prediction purposes, then we can apply cross-validation to select a
model and we don't need to think very hard about statistical
significance. If it is for inference, then we need to be more careful.
There are various classical model selection criteria (e.g., AIC, BIC),
but it is not entirely clear what statistical guarantee we are getting
for the resulting models. A simpler approach is to apply a \(t\)-test
for each variable in the model, apply a multiple testing correction to
the resulting \(p\)-values, and report the set of significant variables
and the associated guarantee. Re-fitting the linear regression after
model selection leads us into some dicey inferential territory due to
selection bias. This is the subject of ongoing research, and the jury is
still out on the best way of doing this.

\hypertarget{r-demo-1}{%
\section{R demo}\label{r-demo-1}}

\emph{See also Agresti 3.4.1, 3.4.3, Dunn and Smyth 2.6, 2.14}

Let's put into practice what we've learned in this chapter by analyzing
data about house prices.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(GGally)}

\NormalTok{houses\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_tsv}\NormalTok{(}\StringTok{"data/Houses.dat"}\NormalTok{)}
\NormalTok{houses\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 7
    case taxes  beds baths   new price  size
   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
 1     1  3104     4     2     0  280.  2048
 2     2  1173     2     1     0  146.   912
 3     3  3076     4     2     0  238.  1654
 4     4  1608     3     2     0  200   2068
 5     5  1454     3     3     0  160.  1477
 6     6  2997     3     2     1  500.  3153
 7     7  4054     3     2     0  266.  1355
 8     8  3002     3     2     1  290.  2075
 9     9  6627     5     4     0  587   3990
10    10   320     3     2     0   70   1160
# i 90 more rows
\end{verbatim}

\hypertarget{exploration-1}{%
\subsection{Exploration}\label{exploration-1}}

Let's first do a bit of exploration:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# visualize distribution of housing prices, superimposing the mean}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{bins =} \DecValTok{30}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(price)),}
    \AttributeTok{colour =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{linetype =} \StringTok{"dashed"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compare median and mean price}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean\_price =} \FunctionTok{mean}\NormalTok{(price),}
    \AttributeTok{median\_price =} \FunctionTok{median}\NormalTok{(price)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  mean_price median_price
       <dbl>        <dbl>
1       155.         133.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a pairs plot of continuous variables}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(price, size, taxes) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# see how price relates to beds}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(beds), }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# see how price relates to baths}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(baths), }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# see how price relates to new}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(new), }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

\hypertarget{hypothesis-testing-1}{%
\subsection{Hypothesis testing}\label{hypothesis-testing-1}}

Let's run a linear regression and interpret the summary. But first, we
must decide whether to model beds/baths as categorical or continuous? We
should probably model these as categorical, given the potentially
nonlinear trend observed in the box plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(beds) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(baths) }\SpecialCharTok{+}\NormalTok{ new }\SpecialCharTok{+}\NormalTok{ size,}
  \AttributeTok{data =}\NormalTok{ houses\_data}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = price ~ factor(beds) + factor(baths) + new + size, 
    data = houses_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-179.306  -32.037   -2.899   19.115  152.718 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)     -19.26307   18.01344  -1.069 0.287730    
factor(beds)3   -16.46430   15.04669  -1.094 0.276749    
factor(beds)4   -12.48561   21.12357  -0.591 0.555936    
factor(beds)5  -101.14581   55.83607  -1.811 0.073366 .  
factor(baths)2    2.39872   15.44014   0.155 0.876885    
factor(baths)3   -0.70410   26.45512  -0.027 0.978825    
factor(baths)4  273.20079   83.65764   3.266 0.001540 ** 
new              66.94940   18.50445   3.618 0.000487 ***
size              0.10882    0.01234   8.822 7.46e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 51.17 on 91 degrees of freedom
Multiple R-squared:  0.7653,    Adjusted R-squared:  0.7446 
F-statistic: 37.08 on 8 and 91 DF,  p-value: < 2.2e-16
\end{verbatim}

We can read off the test statistics and \(p\)-values for each variable
from the regression summary, as well as for the \(F\)-test against the
constant model from the bottom of the summary.

Let's use an \(F\)-test to assess whether the categorical \texttt{baths}
variable is important.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_partial }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(beds) }\SpecialCharTok{+}\NormalTok{ new }\SpecialCharTok{+}\NormalTok{ size,}
  \AttributeTok{data =}\NormalTok{ houses\_data}
\NormalTok{)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_partial, lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: price ~ factor(beds) + new + size
Model 2: price ~ factor(beds) + factor(baths) + new + size
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     94 273722                                
2     91 238289  3     35433 4.5104 0.005374 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

What if we had not coded \texttt{baths} as a factor?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_not\_factor }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(beds) }\SpecialCharTok{+}\NormalTok{ baths }\SpecialCharTok{+}\NormalTok{ new }\SpecialCharTok{+}\NormalTok{ size,}
  \AttributeTok{data =}\NormalTok{ houses\_data}
\NormalTok{)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_partial, lm\_fit\_not\_factor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: price ~ factor(beds) + new + size
Model 2: price ~ factor(beds) + baths + new + size
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     94 273722                           
2     93 273628  1     94.33 0.0321 0.8583
\end{verbatim}

If we want to test for the equality of means across groups of a
categorical predictor, without adjusting for other variables, we can use
the ANOVA \(F\)-test. There are several equivalent ways of doing so:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# just use the summary function}
\NormalTok{lm\_fit\_baths }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(baths), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{summary}\NormalTok{(lm\_fit\_baths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = price ~ factor(baths), data = houses_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-146.44  -45.88   -7.89   22.22  352.01 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)       90.21      19.51   4.624 1.17e-05 ***
factor(baths)2    57.68      21.72   2.656  0.00927 ** 
factor(baths)3   174.52      31.13   5.607 1.97e-07 ***
factor(baths)4   496.79      82.77   6.002 3.45e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 80.44 on 96 degrees of freedom
Multiple R-squared:  0.3881,    Adjusted R-squared:  0.369 
F-statistic:  20.3 on 3 and 96 DF,  p-value: 2.865e-10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use the anova function as before}
\NormalTok{lm\_fit\_const }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_const, lm\_fit\_baths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: price ~ 1
Model 2: price ~ factor(baths)
  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    
1     99 1015150                                  
2     96  621130  3    394020 20.299 2.865e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use the aov function}
\NormalTok{aov\_fit }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(baths), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{summary}\NormalTok{(aov\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Df Sum Sq Mean Sq F value   Pr(>F)    
factor(baths)  3 394020  131340    20.3 2.86e-10 ***
Residuals     96 621130    6470                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We can also use an \(F\)-test to test for the presence of an interaction
with a multi-class categorical predictor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_interaction }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ size }\SpecialCharTok{*} \FunctionTok{factor}\NormalTok{(beds), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{summary}\NormalTok{(lm\_fit\_interaction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = price ~ size * factor(beds), data = houses_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-232.643  -25.938   -0.942   19.172  155.517 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)          50.12619   48.22282   1.039 0.301310    
size                  0.05037    0.04210   1.197 0.234565    
factor(beds)3      -103.85734   52.20373  -1.989 0.049620 *  
factor(beds)4      -143.90213   67.31359  -2.138 0.035185 *  
factor(beds)5      -507.88205  144.10191  -3.524 0.000663 ***
size:factor(beds)3    0.07589    0.04368   1.738 0.085633 .  
size:factor(beds)4    0.09234    0.04704   1.963 0.052638 .  
size:factor(beds)5    0.21147    0.05957   3.550 0.000609 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 53.35 on 92 degrees of freedom
Multiple R-squared:  0.7421,    Adjusted R-squared:  0.7225 
F-statistic: 37.81 on 7 and 92 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_size }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ size }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(beds), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_size, lm\_fit\_interaction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: price ~ size + factor(beds)
Model 2: price ~ size * factor(beds)
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     95 300953                                
2     92 261832  3     39121 4.5819 0.004905 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Contrasts of regression coefficients can be tested using the
\texttt{glht()} function from the \texttt{multcomp} package.

\hypertarget{confidence-intervals}{%
\subsection{Confidence intervals}\label{confidence-intervals}}

We can construct pointwise confidence intervals for each coefficient
using \texttt{confint()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                       2.5 %      97.5 %
(Intercept)     -55.04455734  16.5184161
factor(beds)3   -46.35270691  13.4241025
factor(beds)4   -54.44498235  29.4737689
factor(beds)5  -212.05730801   9.7656895
factor(baths)2  -28.27123130  33.0686620
factor(baths)3  -53.25394742  51.8457394
factor(baths)4  107.02516067 439.3764122
new              30.19258305 103.7062177
size              0.08431972   0.1333284
\end{verbatim}

To create simultaneous confidence intervals, we need a somewhat more
manual approach. We start with the coefficients and standard errors:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(lm\_fit))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   Estimate  Std. Error     t value     Pr(>|t|)
(Intercept)     -19.2630706 18.01344052 -1.06937209 2.877304e-01
factor(beds)3   -16.4643022 15.04669172 -1.09421410 2.767490e-01
factor(beds)4   -12.4856067 21.12356937 -0.59107467 5.559357e-01
factor(beds)5  -101.1458092 55.83607248 -1.81147786 7.336590e-02
factor(baths)2    2.3987153 15.44014266  0.15535578 8.768849e-01
factor(baths)3   -0.7041040 26.45511871 -0.02661504 9.788251e-01
factor(baths)4  273.2007864 83.65764044  3.26570036 1.540093e-03
new              66.9494004 18.50445029  3.61801617 4.872475e-04
size              0.1088241  0.01233621  8.82151661 7.460814e-14
\end{verbatim}

Then we add lower and upper confidence interval endpoints based on the
formula (\ref{eq-simultaneous-coordinatewise-se}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(houses\_data)}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(lm\_fit))}
\NormalTok{f\_quantile }\OtherTok{\textless{}{-}} \FunctionTok{qf}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha, }\AttributeTok{df1 =}\NormalTok{ p, }\AttributeTok{df2 =}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ p)}
\FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(lm\_fit)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rownames\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"Variable"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(Variable, Estimate, }\StringTok{\textasciigrave{}}\AttributeTok{Std. Error}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{CI\_lower =}\NormalTok{ Estimate }\SpecialCharTok{{-}} \StringTok{\textasciigrave{}}\AttributeTok{Std. Error}\StringTok{\textasciigrave{}} \SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(p }\SpecialCharTok{*}\NormalTok{ f\_quantile),}
    \AttributeTok{CI\_upper =}\NormalTok{ Estimate }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{Std. Error}\StringTok{\textasciigrave{}} \SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(p }\SpecialCharTok{*}\NormalTok{ f\_quantile)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Variable     Estimate  Std. Error      CI_lower    CI_upper
1    (Intercept)  -19.2630706 18.01344052  -95.38917389  56.8630327
2  factor(beds)3  -16.4643022 15.04669172  -80.05271036  47.1241059
3  factor(beds)4  -12.4856067 21.12356937 -101.75533960  76.7841262
4  factor(beds)5 -101.1458092 55.83607248 -337.11309238 134.8214739
5 factor(baths)2    2.3987153 15.44014266  -62.85244495  67.6498756
6 factor(baths)3   -0.7041040 26.45511871 -112.50535022 111.0971422
7 factor(baths)4  273.2007864 83.65764044  -80.34245635 626.7440292
8            new   66.9494004 18.50445029  -11.25174573 145.1505465
9           size    0.1088241  0.01233621    0.05669037   0.1609578
\end{verbatim}

Note that the simultaneous intervals are substantially larger.

To construct pointwise confidence intervals for the fit, we can use the
\texttt{predict()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(lm\_fit, }\AttributeTok{newdata =}\NormalTok{ houses\_data, }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        fit       lwr      upr
1 193.52176 165.22213 221.8214
2  79.98449  51.91430 108.0547
3 150.64507 122.28397 179.0062
4 191.71955 172.27396 211.1651
5 124.30169  81.34488 167.2585
6 376.74308 333.44559 420.0406
\end{verbatim}

To get pointwise prediction intervals, we switch \texttt{"confidence"}
to \texttt{"prediction"}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(lm\_fit, }\AttributeTok{newdata =}\NormalTok{ houses\_data, }\AttributeTok{interval =} \StringTok{"prediction"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        fit       lwr      upr
1 193.52176  88.00908 299.0344
2  79.98449 -25.46688 185.4359
3 150.64507  45.11589 256.1743
4 191.71955  88.22951 295.2096
5 124.30169  13.95069 234.6527
6 376.74308 266.25901 487.2271
\end{verbatim}

To construct simultaneous confidence intervals for the fit or
predictions, we again need a slightly more manual approach. We call
\texttt{predict()} again, but this time asking it for the standard
errors rather than the confidence intervals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lm\_fit, }\AttributeTok{newdata =}\NormalTok{ houses\_data, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(predictions}\SpecialCharTok{$}\NormalTok{fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        1         2         3         4         5         6 
193.52176  79.98449 150.64507 191.71955 124.30169 376.74308 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(predictions}\SpecialCharTok{$}\NormalTok{se.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        1         2         3         4         5         6 
14.246855 14.131352 14.277804  9.789472 21.625709 21.797212 
\end{verbatim}

Now we can construct the simultaneous confidence intervals via the
formula (\ref{eq-simultaneous-fit-se}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\_quantile }\OtherTok{\textless{}{-}} \FunctionTok{qf}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha, }\AttributeTok{df1 =}\NormalTok{ p, }\AttributeTok{df2 =}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ p)}
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{lower =}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{fit }\SpecialCharTok{{-}}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{se.fit }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(p }\SpecialCharTok{*}\NormalTok{ f\_quantile),}
  \AttributeTok{upper =}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{fit }\SpecialCharTok{+}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{se.fit }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(p }\SpecialCharTok{*}\NormalTok{ f\_quantile)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
   lower upper
   <dbl> <dbl>
 1 133.   254.
 2  20.3  140.
 3  90.3  211.
 4 150.   233.
 5  32.9  216.
 6 285.   469.
 7  82.8  145.
 8 188.   331.
 9 371.   803.
10  57.3  128.
# i 90 more rows
\end{verbatim}

In the case of simple linear regression, we can plot these pointwise and
simultaneous confidence intervals as bands:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to produce confidence intervals for fits in general, use the predict() function}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(houses\_data)}
\NormalTok{p }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ size, }\AttributeTok{data =}\NormalTok{ houses\_data)}
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lm\_fit, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{t\_quantile }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, }\AttributeTok{df =}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ p)}
\NormalTok{f\_quantile }\OtherTok{\textless{}{-}} \FunctionTok{qf}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha, }\AttributeTok{df1 =}\NormalTok{ p, }\AttributeTok{df2 =}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ p)}
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{fit =}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{fit,}
    \AttributeTok{se =}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{se.fit,}
    \AttributeTok{ptwise\_width =}\NormalTok{ t\_quantile }\SpecialCharTok{*}\NormalTok{ se,}
    \AttributeTok{simultaneous\_width =} \FunctionTok{sqrt}\NormalTok{(p }\SpecialCharTok{*}\NormalTok{ f\_quantile) }\SpecialCharTok{*}\NormalTok{ se}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ size)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fit), }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fit }\SpecialCharTok{+}\NormalTok{ ptwise\_width, }\AttributeTok{color =} \StringTok{"Pointwise"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fit }\SpecialCharTok{{-}}\NormalTok{ ptwise\_width, }\AttributeTok{color =} \StringTok{"Pointwise"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fit }\SpecialCharTok{+}\NormalTok{ simultaneous\_width, }\AttributeTok{color =} \StringTok{"Simultaneous"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fit }\SpecialCharTok{{-}}\NormalTok{ simultaneous\_width, }\AttributeTok{color =} \StringTok{"Simultaneous"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

\hypertarget{predictor-competition-and-collaboration}{%
\subsection{Predictor competition and
collaboration}\label{predictor-competition-and-collaboration}}

Let's look at the power of detecting the association between
\texttt{price} and \texttt{beds}. We can imagine that \texttt{beds} and
\texttt{baths} are correlated:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{houses\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ beds, }\AttributeTok{y =}\NormalTok{ baths)) }\SpecialCharTok{+}
  \FunctionTok{geom\_count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{linear-models-inference_files/figure-pdf/unnamed-chunk-21-1.pdf}

}

\end{figure}

So let's see how significant \texttt{beds} is, with and without
\texttt{baths} in the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_only\_beds }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(beds), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{summary}\NormalTok{(lm\_fit\_only\_beds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = price ~ factor(beds), data = houses_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-234.35  -50.63  -15.69   24.56  365.86 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)     105.94      21.48   4.931 3.43e-06 ***
factor(beds)3    44.69      24.47   1.827 0.070849 .  
factor(beds)4   105.70      32.35   3.268 0.001504 ** 
factor(beds)5   246.71      69.62   3.544 0.000611 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 93.65 on 96 degrees of freedom
Multiple R-squared:  0.1706,    Adjusted R-squared:  0.1447 
F-statistic: 6.583 on 3 and 96 DF,  p-value: 0.0004294
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_only\_baths }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(baths), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\NormalTok{lm\_fit\_beds\_baths }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(beds) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(baths), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_only\_baths, lm\_fit\_beds\_baths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: price ~ factor(baths)
Model 2: price ~ factor(beds) + factor(baths)
  Res.Df    RSS Df Sum of Sq     F  Pr(>F)  
1     96 621130                             
2     93 572436  3     48693 2.637 0.05424 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that the significance of \texttt{beds} dropped by two orders of
magnitude. This is an example of predictor competition.

On the other hand, note that the variable \texttt{new} is not very
correlated with \texttt{beds}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(new }\SpecialCharTok{\textasciitilde{}}\NormalTok{ beds, }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{summary}\NormalTok{(lm\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = new ~ beds, data = houses_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.15762 -0.11000 -0.11000 -0.08619  0.91381 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  0.03857    0.14950   0.258    0.797
beds         0.02381    0.04871   0.489    0.626

Residual standard error: 0.3157 on 98 degrees of freedom
Multiple R-squared:  0.002432,  Adjusted R-squared:  -0.007747 
F-statistic: 0.2389 on 1 and 98 DF,  p-value: 0.6261
\end{verbatim}

but we know it has a substantial impact on \texttt{price}. Let's look at
the significance of the test that \texttt{beds} is not important when we
add \texttt{new} to the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit\_only\_new }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ new, }\AttributeTok{data =}\NormalTok{ houses\_data)}
\NormalTok{lm\_fit\_beds\_new }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ new }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(beds), }\AttributeTok{data =}\NormalTok{ houses\_data)}
\FunctionTok{anova}\NormalTok{(lm\_fit\_only\_new, lm\_fit\_beds\_new)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: price ~ new
Model 2: price ~ new + factor(beds)
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     98 787781                                  
2     95 619845  3    167936 8.5795 4.251e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Adding \texttt{new} to the model made the \(p\)-value more significant
by a factor of 10. This is an example of predictor collaboration.

\bookmarksetup{startatroot}

\hypertarget{linear-models-misspecification}{%
\chapter{Linear models:
Misspecification}\label{linear-models-misspecification}}

In our discussion of linear model inference in Chapter 2, we assumed the
normal linear model throughout:

\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \text{where} \ \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_n).
\]

In this unit, we will discuss what happens when this model is
misspecified:

\begin{itemize}
\tightlist
\item
  \textbf{Non-normality} (Section~\ref{sec-non-normality}):
  \(\boldsymbol{\epsilon} \sim (0, \sigma^2 \boldsymbol{I}_n)\) but not
  \(N(0, \sigma^2 \boldsymbol{I}_n)\).
\item
  \textbf{Heteroskedastic and/or correlated errors}
  (Section~\ref{sec-heteroskedasticity}):
  \(\boldsymbol{\epsilon} \sim (0, \boldsymbol{\Sigma})\), where
  \(\boldsymbol{\Sigma} \neq \sigma^2 \boldsymbol{I}\). This includes
  the case of heteroskedastic errors (\(\boldsymbol{\Sigma}\) is
  diagonal but not a constant multiple of the identity) and correlated
  errors (\(\boldsymbol{\Sigma}\) is not diagonal).
\item
  \textbf{Model bias} (Section~\ref{sec-model-bias}): It is not the case
  that
  \(\mathbb{E}[\boldsymbol{y}] = \boldsymbol{X} \boldsymbol{\beta}\) for
  some \(\boldsymbol{\beta} \in \mathbb{R}^p\).
\item
  \textbf{Outliers} (Section~\ref{sec-outliers}): For one or more \(i\),
  it is not the case that
  \(y_i \sim N(\boldsymbol{x}_{i*}^T \boldsymbol{\beta}, \sigma^2)\).
\end{itemize}

For each type of misspecification, we will discuss its origins,
consequences, detection, and fixes
(Section~\ref{sec-non-normality}-Section~\ref{sec-outliers}). We
conclude with an R demo (\textbf{?@sec-R-demo-misspecification}).

\hypertarget{origins-consequences-diagnostics-and-overview-of-fixes}{%
\section{Origins, consequences, diagnostics, and overview of
fixes}\label{origins-consequences-diagnostics-and-overview-of-fixes}}

\hypertarget{sec-non-normality}{%
\subsection{Non-normality}\label{sec-non-normality}}

\hypertarget{origin}{%
\subsubsection{Origin}\label{origin}}

Non-normality occurs when the distribution of \(y|\boldsymbol{x}\) is
either skewed or has heavier tails than the normal distribution. This
may happen, for example, if there is some discreteness in \(y\).

\hypertarget{consequences}{%
\subsubsection{Consequences}\label{consequences}}

Non-normality is the most benign of linear model misspecifications.
While we derived linear model inferences under the normality assumption,
all the corresponding statements hold asymptotically without this
assumption. Recall Homework 2 Question 1, or take for example the
simpler problem of estimating the mean \(\mu\) of a distribution based
on \(n\) samples from it: We can test \(H_0: \mu = 0\) and build a
confidence interval for \(\mu\) even if the underlying distribution is
not normal. So if \(n\) is relatively large and \(p\) is relatively
small, you need not worry too much. If \(n\) is small and the errors are
highly skewed or heavy-tailed, we may have issues with incorrect
standard errors.

\hypertarget{detection}{%
\subsubsection{Detection}\label{detection}}

Non-normality is a property of the error terms \(\epsilon_i\). We do not
observe these directly, but we can approximate them using the residuals:

\[
\widehat{\epsilon}_i = y_i - \boldsymbol{x}_{i*}^T \boldsymbol{\widehat{\beta}}.
\]

Recall from Chapter 2 that
\(\text{Var}[\boldsymbol{\widehat{\epsilon}}] = \sigma^2(\boldsymbol{I} - \boldsymbol{H})\).
Letting \(h_i\) be the \(i\)th diagonal entry of \(\boldsymbol{H}\), it
follows that \(\widehat{\epsilon}_i \sim (0, \sigma^2(1-h_i))\). The
\emph{standardized residuals} are defined as:

\begin{equation}\protect\hypertarget{eq-standardized-residuals}{}{
r_i = \frac{\widehat{\epsilon}_i}{\widehat{\sigma} \sqrt{1-h_i}}.
}\label{eq-standardized-residuals}\end{equation}

Under normality, we would expect \(r_i \overset{\cdot}{\sim} N(0,1)\).
We can therefore assess normality by producing a histogram or normal
QQ-plot of these residuals (see Figure {[}-@fig:qqplot{]}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{figures/qqplot.png}

}

\caption{\label{fig-qqplot}Histogram and normal QQ plot of standardized
residuals.}

\end{figure}

\hypertarget{fixes}{%
\subsubsection{Fixes}\label{fixes}}

As mentioned above, non-normality is not necessarily a problem that
needs to be fixed, except in small samples. In small samples (but not
too small!), we can apply the residual bootstrap for robust standard
error computation and/or robust hypothesis testing.

\hypertarget{sec-heteroskedasticity}{%
\subsection{Heteroskedastic and correlated
errors}\label{sec-heteroskedasticity}}

\hypertarget{origin-1}{%
\subsubsection{Origin}\label{origin-1}}

\textbf{Heteroskedasticity} can arise as follows. Suppose each
observation \(y_i\) is actually the average of \(n_i\) underlying
observations, each with variance \(\sigma^2\). Then, the variance of
\(y_i\) is \(\sigma^2/n_i\), which will differ across \(i\) if \(n_i\)
differ. It is also common to see the variance of a distribution increase
as the mean increases (as in Figure {[}-@fig:heteroskedasticity{]}),
whereas for a linear model the variance of \(y\) stays constant as the
mean of \(y\) varies.

\textbf{Correlated errors} can arise when observations have group,
spatial, or temporal structure. Below are examples:

\begin{itemize}
\tightlist
\item
  \textbf{Group/clustered structure}: We have 10 samples
  \((\boldsymbol{x}_{i*}, y_i)\) each from 100 schools.
\item
  \textbf{Spatial structure}: We have 100 soil samples from a
  \(10\times10\) grid on a 1km \(\times\) 1km field.
\item
  \textbf{Temporal structure}: We have 366 COVID positivity rate
  measurements, one from each day of the year 2020.
\end{itemize}

The issue arises because there are common sources of variation among
samples that are in the same group or spatially/temporally close to one
another.

\hypertarget{consequences-1}{%
\subsubsection{Consequences}\label{consequences-1}}

All normal linear model inference from Chapter 2 hinges on the
assumption that
\(\boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I})\).
If instead of \(\sigma^2 \boldsymbol{I}\) we have
\(\text{Var}[\boldsymbol{\epsilon}] = \boldsymbol{\Sigma}\) for some
matrix \(\boldsymbol{\Sigma}\), then we may suffer two consequences:
wrong inference (in terms of confidence interval coverage and hypothesis
test levels) and inefficient inference (in terms of confidence interval
width and hypothesis test power). One way of seeing the consequence of
heteroskedasticity for confidence interval coverage is the width of
prediction intervals; see Figure {[}-@fig:heteroskedasticity{]} for
intuition.

\begin{figure}

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{figures/heteroskedasticity.png}

}

\caption{\label{fig-heteroskedasticity}Heteroskedasticity in a simple
bivariate linear model (image source:
\href{http://www3.wabash.edu/econometrics/EconometricsBook/chap19.htm}{source}).}

\end{figure}

Like with heteroskedastic errors, correlated errors can cause invalid
standard errors. In particular, positively correlated errors typically
cause standard errors to be smaller than they should be, leading to
inflated Type-I error rates. For intuition, consider estimating the mean
of a distribution based on \(n\) samples. Consider the cases when these
samples are independent, compared to when they are perfectly correlated.
The effective sample size in the former case is \(n\) and in the latter
case is 1.

\hypertarget{detection-1}{%
\subsubsection{Detection}\label{detection-1}}

Heteroskedasticity is usually assessed via the \emph{residual plot}
(Figure {[}-@fig:residual-plots{]}). In this plot, the standardized
residuals \(r_i\) (\ref{eq-standardized-residuals}) are plotted against
the fitted values \(\widehat{\mu}_i\). In the absence of
heteroskedasticity, the spread of the points around the origin should be
roughly constant as a function of \(\widehat{\mu}\) (Figure
\href{a}{-@fig:residual-plots}). A common sign of heteroskedasticity is
the fan shape where variance increases as a function of
\(\widehat{\mu}\) (Figure \href{c}{-@fig:residual-plots}).

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures/residual-plots.png}

}

\caption{\label{fig-residual-plots}Residuals plotted against
linear-model fitted values that reflect (a) model adequacy, (b)
quadratic rather than linear relationship, and (c) nonconstant variance
(image source: Agresti Figure 2.8).}

\end{figure}

Residual plots once again come in handy to detect correlated errors.
Instead of plotting the standardized residuals against the fitted
values, we should plot the residuals against whatever variables we think
might explain variation in the response that the regression does not
account for. In the presence of group structures, we can plot residuals
versus group (via a boxplot); in the presence of spatial or temporal
structure, we can plot residuals as a function of space or time. If the
residuals show a dependency on these variables, this suggests they are
correlated. This dependency can be checked via formal means as well,
e.g., via an ANOVA test in the case of groups or by estimating the
autocorrelation function in the case of temporal structure.

\hypertarget{sec-model-bias}{%
\subsection{Model bias}\label{sec-model-bias}}

\hypertarget{origin-2}{%
\subsubsection{Origin}\label{origin-2}}

Model bias arises when predictors are left out of the regression model:

\begin{equation}\protect\hypertarget{eq-confounding}{}{
\text{assumed model: } \boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}; \quad \text{actual model: } \boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{Z} \boldsymbol{\gamma} + \boldsymbol{\epsilon}.
}\label{eq-confounding}\end{equation}

We may not always know about or measure all the variables that impact a
response \(\boldsymbol{y}\).

Model bias can also arise when the predictors do not impact the response
on the linear scale. For example:

\begin{equation}\protect\hypertarget{eq-wrong-scale}{}{
\text{assumed model: } \mathbb{E}[\boldsymbol{y}] = \boldsymbol{X} \boldsymbol{\beta}; \quad \text{actual model: } g(\mathbb{E}[\boldsymbol{y}]) = \boldsymbol{X} \boldsymbol{\beta}.
}\label{eq-wrong-scale}\end{equation}

\hypertarget{consequences-2}{%
\subsubsection{Consequences}\label{consequences-2}}

In cases of model bias, the parameters \(\boldsymbol{\beta}\) in the
assumed linear model lose their meanings. The least squares estimate
\(\boldsymbol{\widehat{\beta}}\) will be a biased estimate for the
parameter we probably actually want to estimate. In the case
(\ref{eq-confounding}) when predictors are left out of the regression
model, these additional predictors \(\boldsymbol{Z}\) will act as
confounders and create bias in \(\boldsymbol{\widehat{\beta}}\) as an
estimate of the \(\boldsymbol{\beta}\) parameters in the true model,
unless \(\boldsymbol{X}^T \boldsymbol{Z} = 0\). As discussed in Chapter
2, this can lead to misleading conclusions.

\hypertarget{detection-2}{%
\subsubsection{Detection}\label{detection-2}}

Similarly to the detection of correlated errors, we can try to identify
model bias by plotting the standardized residuals against predictors
that may have been left out of the model. A good place to start is to
plot standardized residuals against the predictors \(\boldsymbol{X}\)
(one at a time) that are in the model, since nonlinear transformations
of these might have been left out. In this case, you would see something
like Figure \href{b}{-Figure~\ref{fig-residual-plots}}.

It is possible to formally test for model bias in cases when we have
repeated observations of the response for each value of the predictor
vector. In particular, suppose that
\(\boldsymbol{x}_{i*} = \boldsymbol{x}_c\) for \(c = c(i)\) and
predictor vectors
\(\boldsymbol{x}_1, \dots, \boldsymbol{x}_C \in \mathbb{R}^p\). Then,
consider testing the following hypothesis:

\[
H_0: y_i = \boldsymbol{x}_{i*}^T \boldsymbol{\beta} + \epsilon_i \quad \text{versus} \quad H_1: y_i = \beta_{c(i)} + \epsilon_i.
\]

The model under \(H_0\) (the linear model) is nested in the model for
\(H_1\) (the saturated model), and we can test this hypothesis using an
\(F\)-test called the \emph{lack of fit \(F\)-test}.

\hypertarget{overview-of-fixes}{%
\subsubsection{Overview of fixes}\label{overview-of-fixes}}

To fix model bias in the case (\ref{eq-confounding}), ideally we would
identify the missing predictors \(\boldsymbol{Z}\) and add them to the
regression model. This may not always be feasible or possible. To fix
model bias in the case (\ref{eq-wrong-scale}), it is sometimes advocated
to find a transformation \(g\) (e.g., a square root or a logarithm) of
\(\boldsymbol{y}\) such that
\(\mathbb{E}[g(\boldsymbol{y})] = \boldsymbol{X} \boldsymbol{\beta}\).
However, a better solution is to use a \emph{generalized linear model},
which we will discuss starting in Chapter 4.

\hypertarget{sec-outliers}{%
\subsection{Outliers}\label{sec-outliers}}

\hypertarget{origin-3}{%
\subsubsection{Origin}\label{origin-3}}

Outliers often arise due to measurement or data entry errors. An
observation can be an outlier in \(\boldsymbol{x}\), in \(y\), or both.

\hypertarget{consequences-3}{%
\subsubsection{Consequences}\label{consequences-3}}

An outlier can have the effect of biasing the estimate
\(\boldsymbol{\widehat{\beta}}\). This occurs when an observation has
outlying \(\boldsymbol{x}\) as well as outlying \(y\).

\hypertarget{detection-3}{%
\subsubsection{Detection}\label{detection-3}}

There are a few measures associated with an observation that can be used
to detect outliers, though none are perfect. The first quantity is
called the \emph{leverage}, defined as:

\[
\text{leverage of observation } i \equiv \text{corr}^2(y_i, \widehat{\mu}_i)^2.
\]

This quantity measures the extent to which the fitted value
\(\widehat{\mu}_i\) is sensitive to the (noise in the) observation
\(y_i\). It can be derived that:

\[
\text{leverage of observation } i = h_i,
\]

which is the \(i\)th diagonal element of the hat matrix
\(\boldsymbol{H}\). This is related to the fact that
\(\text{Var}[\widehat{\epsilon}_i] = \sigma^2(1-h_i)\). The larger the
leverage, the smaller the variance of the residual, so the closer the
line passes to the \(i\)th observation. The leverage of an observation
is larger to the extent that \(\boldsymbol{x}_{i*}\) is far from
\(\boldsymbol{\bar{x}}\). For example, in the bivariate linear model
\(y_i = \beta_0 + \beta_1 x_i + \epsilon_i\),

\[
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i' = 1}^n (x_{i'} - \bar{x})^2}.
\]

Note that the average of the leverages is:

\[
\frac{1}{n}\sum_{i = 1}^n h_i = \frac{1}{n}\text{trace}(\boldsymbol{H}) = \frac{p}{n}.
\]

An observation's leverage is considered large if it is significantly
larger than this, e.g., three times larger.

Note that the leverage is not a function of \(y_i\), so a high-leverage
point might or might not be an outlier in \(y_i\) and therefore might or
might not have a strong impact on the regression. To assess more
directly whether an observation is \emph{influential}, we can compare
the least squares fits with and without that observation. To this end,
we define the \emph{Cook's distance}:

\[
D_i = \frac{\sum_{i' = 1}^n (\widehat{\mu}_{i'} - \widehat{\mu}^{\text{-}i}_{i'})^2}{p\widehat{\sigma}^2},
\]

where
\(\widehat{\mu}^{\text{-}i}_{i'} = \boldsymbol{x}_{i*}^T \boldsymbol{\widehat{\beta}}^{\text{-}i}\)
and \(\boldsymbol{\widehat{\beta}}^{\text{-}i}\) is the least squares
estimate based on
\((\boldsymbol{X}_{\text{-}i,*}, \boldsymbol{y}_{\text{-}i})\). An
observation is considered influential if it has Cook's distance greater
than one.

There is a connection between Cook's distance and leverage:

\[
D_i = \left(\frac{y_i - \widehat{\mu}_i}{\widehat{\sigma} \sqrt{1-h_{ii}}}\right)^2 \cdot \frac{h_{ii}}{p(1-h_{ii})}.
\]

We recognize the first term as the standardized residual; therefore a
point is influential if its residual and leverage are large.

Note that Cook's distance may not successfully identify outliers. For
example, if there are groups of outliers, then they will \emph{mask}
each other in the calculation of Cook's distance.

\hypertarget{overview-of-fixes-1}{%
\subsubsection{Overview of fixes}\label{overview-of-fixes-1}}

If outliers can be detected, then the fix is to remove them from the
regression. But, we need to be careful. Definitively determining whether
observations are outliers can be tricky. Outlier detection can even be
used as a way to commit fraud with data, as now-defunct blood testing
start-up
\href{https://arstechnica.com/tech-policy/2021/09/cherry-picking-data-was-routine-practice-at-theranos-former-lab-worker-says/}{Theranos}
is alleged to have done. As an alternative to removing outliers, we can
fit estimators \(\boldsymbol{\widehat{\beta}}\) that are less sensitive
to outliers; see Section \textbf{?@sec-robust-estimation}.

\hypertarget{sec-asymptotic-methods}{%
\section{Asymptotic methods for heteroskedastic and correlated
errors}\label{sec-asymptotic-methods}}

Broadly speaking, approaches to fixing heteroskedastic or correlated
errors can be divided into (1) those based on estimating
\(\boldsymbol{\Sigma}\) and (2) those based on resampling. Methods based
on estimating \(\boldsymbol{\Sigma}\) can use this estimate to either
(i) build a better estimate \(\boldsymbol{\widehat{\beta}}\) or (ii)
build better standard errors for the least squares estimate. Resampling
methods include the bootstrap (for estimation) and the permutation test
(for testing).

\hypertarget{sec-better-estimate}{%
\subsection{\texorpdfstring{Methods that build a better estimate of
\(\boldsymbol{\widehat{\beta}}\)}{Methods that build a better estimate of \textbackslash boldsymbol\{\textbackslash widehat\{\textbackslash beta\}\}}}\label{sec-better-estimate}}

Suppose
\(\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{\Sigma})\).
This is a \emph{generalized least squares} problem for which inference
can be carried out. The generalized least squares estimate is
\(\boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{y}\),
which is distributed as
\(\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, (\boldsymbol{X}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{X})^{-1})\).
This is the best linear unbiased estimate of \(\boldsymbol{\beta}\),
recovering efficiency. We can carry out inference based on the latter
distributional result analogously to how we did so in Chapter 2. The
issue, of course, is that we usually do not know
\(\boldsymbol{\Sigma}\). Therefore, we can consider the following
approach: (1) estimate \(\boldsymbol{\widehat{\beta}}\) using OLS, (2)
use this estimate to get an estimate \(\boldsymbol{\widehat{\Sigma}}\)
of \(\boldsymbol{\Sigma}\), (3) use \(\boldsymbol{\widehat{\Sigma}}\) to
get a (hopefully) more efficient estimator

\begin{equation}\protect\hypertarget{eq-fgls-estimate}{}{
\boldsymbol{\widehat{\beta}}^{\text{FGLS}} \equiv (\boldsymbol{X}^T \boldsymbol{\widehat{\Sigma}}^{-1}\boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{\widehat{\Sigma}}^{-1}\boldsymbol{y}.
}\label{eq-fgls-estimate}\end{equation}

This is called the \emph{feasible generalized least squares estimate}
(FGLS), to contrast it with the infeasible estimate that assumes
\(\boldsymbol{\Sigma}\) is known exactly. The procedure above can be
iterated until convergence. To estimate
\(\boldsymbol{\widehat{\Sigma}}\), we usually need to make some
parametric assumptions. For example, in the case of grouped structure,
we might assume a \emph{random effects model}. In the case of a temporal
structure, we might assume an \emph{AR(1) model}.

\hypertarget{sec-better-standard-errors}{%
\subsection{Methods that build better standard errors for OLS
estimate}\label{sec-better-standard-errors}}

Sometimes we don't feel comfortable enough with our estimate of
\(\boldsymbol{\Sigma}\) to actually modify the least squares estimator.
So we want to keep using our least squares estimator, but still get
standard errors robust to heteroskedastic or correlated errors. There
are several strategies to computing valid standard errors in such
situations.

\hypertarget{sec-sandwich-errors}{%
\subsubsection{Sandwich standard errors}\label{sec-sandwich-errors}}

Let's say that
\(\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\),
where
\(\boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \boldsymbol{\Sigma})\).
Then, we can compute that the covariance matrix of the least squares
estimate \(\boldsymbol{\widehat{\beta}}\) is

\begin{equation}\protect\hypertarget{eq-sandwich}{}{
\text{Var}[\boldsymbol{\widehat{\beta}}] = (\boldsymbol{X}^T \boldsymbol{X})^{-1}(\boldsymbol{X}^T \boldsymbol{\Sigma} \boldsymbol{X})(\boldsymbol{X}^T \boldsymbol{X})^{-1}.
}\label{eq-sandwich}\end{equation}

Note that this expression reduces to the usual
\(\sigma^2(\boldsymbol{X}^T \boldsymbol{X})^{-1}\) when
\(\boldsymbol{\Sigma} = \sigma^2 \boldsymbol{I}\). It is called the
sandwich variance because we have the
\((\boldsymbol{X}^T \boldsymbol{\Sigma} \boldsymbol{X})\) term
sandwiched between two \((\boldsymbol{X}^T \boldsymbol{X})^{-1}\) terms.
If we have some estimate \(\boldsymbol{\widehat{\Sigma}}\) of the
covariance matrix, we can construct

\[
\widehat{\text{Var}}[\boldsymbol{\widehat{\beta}}] \equiv (\boldsymbol{X}^T \boldsymbol{X})^{-1}(\boldsymbol{X}^T \boldsymbol{\widehat{\Sigma}} \boldsymbol{X})(\boldsymbol{X}^T \boldsymbol{X})^{-1}.
\]

Different estimates \(\boldsymbol{\widehat{\Sigma}}\) are appropriate in
different situations. Below we consider three of the most common
choices: one for heteroskedasticity (due to Huber-White), one for
group-correlated errors (due to Liang-Zeger), and one for
temporally-correlated errors (due to Newey-West).

\hypertarget{sec-specific-sandwich-errors}{%
\subsubsection{Specific instances of sandwich standard
errors}\label{sec-specific-sandwich-errors}}

\textbf{Huber-White standard errors.}

Suppose
\(\boldsymbol{\Sigma} = \text{diag}(\sigma_1^2, \dots, \sigma_n^2)\) for
some variances \(\sigma_1^2, \dots, \sigma_n^2 > 0\). The Huber-White
sandwich estimator is defined by (\ref{eq-sandwich}), with

\[
\boldsymbol{\widehat{\Sigma}} \equiv \text{diag}(\widehat{\sigma}_1^2, \dots, \widehat{\sigma}_n^2), \quad \text{where} \quad \widehat{\sigma}_i^2 = (y_i - \boldsymbol{x}_{i*}^T \boldsymbol{\widehat{\beta}})^2.
\]

While each estimator \(\widehat{\sigma}_i^2\) is very poor, Huber and
White's insight was that the resulting estimate of the (averaged)
quantity
\(\boldsymbol{X}^T \boldsymbol{\widehat{\Sigma}}\boldsymbol{X}\) is not
bad. To see why, assume that
\((\boldsymbol{x}_{i*}, y_i) \overset{\text{i.i.d.}}{\sim} F\) for some
joint distribution \(F\). Then, we have that

\[
\begin{split}
\frac{1}{n}(\boldsymbol{X}^T \widehat{\boldsymbol{\Sigma}} \boldsymbol{X} - \boldsymbol{X}^T \boldsymbol{\Sigma} \boldsymbol{X}) &= \frac{1}{n} \sum_{i=1}^n (\widehat{\sigma}_i^2 - \sigma_i^2) \boldsymbol{x}_{i*} \boldsymbol{x}_{i*}^T \\
&= \frac{1}{n} \sum_{i=1}^n ((\epsilon_i + \boldsymbol{x}_{i*}^T(\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}))^2 - \sigma_i^2) \boldsymbol{x}_{i*} \boldsymbol{x}_{i*}^T \\
&= \frac{1}{n} \sum_{i=1}^n \epsilon_i^2 \boldsymbol{x}_{i*} \boldsymbol{x}_{i*}^T + o_p(1) \\
&\to_p 0.
\end{split}
\]

The last step holds by the law of large numbers, since
\(\mathbb{E}[\epsilon_i^2 \boldsymbol{x}_{i*} \boldsymbol{x}_{i*}^T] = 0\)
for each \(i\).

\textbf{Liang-Zeger standard errors.}

Next, let's consider the case of group-correlated errors. Suppose that
the observations are \emph{clustered}, with correlated errors among
clusters but not between clusters. Suppose there are \(C\) clusters of
observations, with the \(i\)th observation belonging to cluster
\(c(i) \in \{1, \dots, C\}\). Suppose for the sake of simplicity that
the observations are ordered so that clusters are contiguous. Let
\(\boldsymbol{\widehat{\epsilon}}_c\) be the vector of residuals in
cluster \(c\), so that
\(\boldsymbol{\widehat{\epsilon}} = (\boldsymbol{\widehat{\epsilon}}_1, \dots, \boldsymbol{\widehat{\epsilon}}_C)\).
Then, the true covariance matrix is
\(\boldsymbol{\Sigma} = \text{block-diag}(\boldsymbol{\Sigma}_1, \dots, \boldsymbol{\Sigma}_C)\)
for some positive definite
\(\boldsymbol{\Sigma}_1, \dots, \boldsymbol{\Sigma}_C\). The Liang-Zeger
estimator is then defined by (\ref{eq-sandwich}), with

\[
\boldsymbol{\widehat{\Sigma}} \equiv \text{block-diag}(\boldsymbol{\widehat{\Sigma}_1}, \dots, \boldsymbol{\widehat{\Sigma}_C}), \quad \text{where} \quad  \boldsymbol{\widehat{\Sigma}_c} \equiv \boldsymbol{\widehat{\epsilon}}_c \boldsymbol{\widehat{\epsilon}}_c^T.
\]

Note that the Liang-Zeger estimator is a generalization of the
Huber-White estimator. Its justification is similar as well: while each
\(\boldsymbol{\widehat{\Sigma}_c}\) is a poor estimator, the resulting
estimate of the (averaged) quantity
\(\boldsymbol{X}^T \boldsymbol{\widehat{\Sigma}}\boldsymbol{X}\) is not
bad as long as the number of clusters is large. Liang-Zeger standard
errors are referred to as ``clustered standard errors'' in the
econometrics community.

\textbf{Newey-West standard errors.}

Finally, consider the case when our observations \(i\) have a temporal
structure, and we believe there to be nontrivial correlations between
\(\epsilon_{i1}\) and \(\epsilon_{i2}\) for \(|i1 - i2| \leq L\). Then,
a natural extension of the Huber-White estimate of
\(\boldsymbol{\Sigma}\) is
\(\boldsymbol{\widehat{\Sigma}}_{i1,i2} = \widehat{\epsilon}_{i1}\widehat{\epsilon}_{i2}\)
for each pair \((i1, i2)\) such that \(|i1 - i2| \leq L\).
Unfortunately, this is not guaranteed to give a positive semidefinite
matrix \(\boldsymbol{\widehat{\Sigma}}\). Therefore, Newey and West
proposed a slightly modified estimator:

\[
\boldsymbol{\widehat{\Sigma}}_{i1,i2} = \max\left(0, 1-\frac{|i1-i2|}{L}\right)\widehat{\epsilon}_{i1}\widehat{\epsilon}_{i2}.
\]

This estimator shrinks the off-diagonal estimates
\(\widehat{\epsilon}_{i1}\widehat{\epsilon}_{i2}\) based on their
distance to the diagonal. It can be shown that this modification
restores positive semidefiniteness of \(\boldsymbol{\widehat{\Sigma}}\).

\hypertarget{sec-sandwich-inference}{%
\subsubsection{Inference based on sandwich standard
errors}\label{sec-sandwich-inference}}

We now have a matrix \(\widehat{\boldsymbol{\Omega}}\) such that

\[
\boldsymbol{\widehat{\beta}} \overset{\cdot}{\sim} N(\boldsymbol{\beta}, \widehat{\boldsymbol{\Omega}}).
\]

This allows us to construct confidence intervals and hypothesis tests
for each \(\beta_j\), by simply replacing \(\text{SE}(\beta_j)\) with
\(\sqrt{\widehat{\Omega}_{jj}}\). For contrasts and prediction
intervals, we can use the fact that
\(\boldsymbol{c}^T \boldsymbol{\beta} \overset{\cdot}{\sim} N(\boldsymbol{c}^T \boldsymbol{\beta}, \boldsymbol{c}^T \widehat{\boldsymbol{\Omega}} \boldsymbol{c})\),
so that
\(\text{CE}(\boldsymbol{c}^T \boldsymbol{\beta}) = \sqrt{\boldsymbol{c}^T \widehat{\boldsymbol{\Omega}} \boldsymbol{c}}\).
It is less obvious how to use the matrix
\(\widehat{\boldsymbol{\Omega}}\) to test the hypothesis
\(H_0: \boldsymbol{\beta}_S = \boldsymbol{0}\). To this end, we can use
a Wald test (we will discuss Wald tests in more detail in Chapter 4).
The Wald test statistic is

\[
W = \boldsymbol{\widehat{\beta}}_S^T (\widehat{\boldsymbol{\Omega}}_{S, S})^{-1} \boldsymbol{\widehat{\beta}}_S,
\]

which is asymptotically distributed as \(\chi^2_{|S|}\) under the null
hypothesis. It turns out that the usual regression \(F\)-test is
asymptotically equivalent to this Wald test.


\backmatter

\end{document}
