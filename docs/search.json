[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 9610 Lecture Notes",
    "section": "",
    "text": "1 Introduction"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "STAT 9610 Lecture Notes",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\nThis is a set of lecture notes developed for the PhD statistics course “STAT 9610: Statistical Methodology” at the University of Pennsylvania. Much of the content is adapted from Alan Agresti’s book Foundations of Linear and Generalized Linear Models (2015). These notes may contain typos and errors; if you find any such issues or have other suggestions for improvement, please notify the instructor via Ed Discussion."
  },
  {
    "objectID": "index.html#preview-linear-and-generalized-linear-models",
    "href": "index.html#preview-linear-and-generalized-linear-models",
    "title": "STAT 9610 Lecture Notes",
    "section": "1.2 Preview: Linear and generalized linear models",
    "text": "1.2 Preview: Linear and generalized linear models\nSee also Agresti 1.1, Dunn and Smyth 1.1-1.2, 1.5-1.6, 1.8-1.12\nThe overarching statistical goal addressed in this class is to learn about relationships between a response \\(y\\) and predictors \\(x_0, x_1, \\dots, x_{p-1}\\). This abstract formulation encompasses an extremely wide variety of applications. The most widely used set of statistical models to address such problems are generalized linear models, which are the focus of this class.\nLet’s start by recalling the linear model, the most fundamental of the generalized linear models. In this case, the response is continuous (\\(y \\in \\mathbb{R}\\)) and modeled as:\n\\[\ny = \\beta_0 x_0 + \\cdots + \\beta_{p-1} x_{p-1} + \\epsilon,\n\\tag{1.1}\\]\nwhere\n\\[\n\\epsilon \\sim (0, \\sigma^2), \\quad \\text{i.e.} \\ \\mathbb{E}[\\epsilon] = 0 \\ \\text{and} \\ \\text{Var}[\\epsilon] = \\sigma^2.\n\\tag{1.2}\\]\nWe view the predictors \\(x_0, \\dots, x_{p-1}\\) as fixed, so the only source of randomness in \\(y\\) is \\(\\epsilon\\). Another way of writing the linear model is:\n\\[\n\\mu \\equiv \\mathbb{E}[y] = \\beta_0 x_0 + \\cdots + \\beta_{p-1} x_{p-1} \\equiv \\eta.\n\\]\nNot all responses are continuous, however. In some cases, we have binary responses (\\(y \\in \\{0,1\\}\\)) or count responses (\\(y \\in \\mathbb{Z}\\)). In these cases, there is a mismatch between the:\n\\[\n\\textit{linear predictor } \\eta \\equiv \\beta_0 x_0 + \\cdots + \\beta_{p-1} x_{p-1}\n\\]\nand the\n\\[\n\\textit{mean response } \\mu \\equiv \\mathbb{E}[y].\n\\]\nThe linear predictor can take arbitrary real values (\\(\\eta \\in \\mathbb{R}\\)), but the mean response can lie in a restricted range, depending on the response type. For example, \\(\\mu \\in [0,1]\\) for binary \\(y\\) and \\(\\mu \\in [0, \\infty)\\) for count \\(y\\).\nFor these kinds of responses, it makes sense to model a transformation of the mean as linear, rather than the mean itself:\n\\[\ng(\\mu) = g(\\mathbb{E}[y]) = \\beta_0 x_0 + \\cdots + \\beta_{p-1} x_{p-1} = \\eta.\n\\]\nThis transformation \\(g\\) is called the link function. For binary \\(y\\), a common choice of link function is the logit link, which transforms a probability into a log-odds:\n\\[\n\\text{logit}(\\pi) \\equiv \\log \\frac{\\pi}{1-\\pi}.\n\\]\nSo the predictors contribute linearly on the log-odds scale rather than on the probability scale. For count \\(y\\), a common choice of link function is the log link.\nModels of the form\n\\[\ng(\\mu) = \\eta\n\\]\nare called generalized linear models (GLMs). They specialize to linear models for the identity link function, i.e., \\(g(\\mu) = \\mu\\). The focus of this course is methodologies to learn about the coefficients \\(\\boldsymbol{\\beta} \\equiv (\\beta_0, \\dots, \\beta_{p-1})^T\\) of a GLM based on a sample \\((\\boldsymbol{X}, \\boldsymbol{y}) \\equiv \\{(x_{i,0}, \\dots, x_{i,p-1}, y_i)\\}_{i = 1}^n\\) drawn from this distribution. Learning about the coefficient vector helps us learn about the relationship between the response and the predictors."
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "STAT 9610 Lecture Notes",
    "section": "1.3 Course outline",
    "text": "1.3 Course outline\nThis course is broken up into six units:\n\nUnit 1: Linear models: Estimation. The least squares point estimate \\(\\boldsymbol{\\widehat \\beta}\\) of \\(\\boldsymbol{\\beta}\\) based on a dataset \\((\\boldsymbol{X}, \\boldsymbol{y})\\) under the linear model assumptions.\nUnit 2: Linear models: Inference. Under the additional assumption that \\(\\epsilon \\sim N(0,\\sigma^2)\\), how to carry out statistical inference (hypothesis testing and confidence intervals) for the coefficients.\nUnit 3: Linear models: Misspecification. What to do when the linear model assumptions are not correct: What issues can arise, how to diagnose them, and how to fix them.\nUnit 4: GLMs: General theory. Estimation and inference for GLMs (generalizing Chapters 1 and 2). GLMs fit neatly into a unified theory based on exponential families.\nUnit 5: GLMs: Special cases. Looking more closely at the most important special cases of GLMs, including logistic regression and Poisson regression.\nUnit 6: Multiple testing. How to adjust for multiple hypothesis testing, both in the context of GLMs and more generally."
  },
  {
    "objectID": "index.html#notation",
    "href": "index.html#notation",
    "title": "STAT 9610 Lecture Notes",
    "section": "1.4 Notation",
    "text": "1.4 Notation\nWe will use the following notations in this course. Vector and matrix quantities will be bolded, whereas scalar quantities will not be. Capital letters will be used for matrices, and lowercase for vectors and scalars. No notational distinction will be made between random quantities and their realizations. The letters \\(i = 1, \\dots, n\\) and \\(j = 0, \\dots, p-1\\) will index samples and predictors, respectively. The predictors \\(\\{x_{ij}\\}_{i,j}\\) will be gathered into an \\(n \\times p\\) matrix \\(\\boldsymbol{X}\\). The rows of \\(\\boldsymbol{X}\\) correspond to samples, with the \\(i\\)th row denoted \\(\\boldsymbol{x}_{i*}\\). The columns of \\(\\boldsymbol{X}\\) correspond to predictors, with the \\(j\\)th column denoted \\(\\boldsymbol{x}_{*j}\\). The responses \\(\\{y_i\\}_i\\) will be gathered into an \\(n \\times 1\\) response vector \\(\\boldsymbol{y}\\). The notation \\(\\equiv\\) will be used for definitions."
  },
  {
    "objectID": "linear-models-estimation.html",
    "href": "linear-models-estimation.html",
    "title": "Linear models: Estimation",
    "section": "",
    "text": "In this unit, we will focus on estimation of coefficients in the linear regression model (eqs. 1.1 and 1.2). We start by discussing the interpretation of linear models (Chapter 2). Then, we discuss least squares estimates from the algebraic, geometric, and probabilistic perspectives (Chapter 3). We then discuss important properties of least squares estimates, including orthogonality relationships least squares estimation implies (Chapter 4) and the effects of collinearity (Chapter 5). We conclude with an R demo (Chapter 6)."
  },
  {
    "objectID": "interpreting-linear-models.html#sec-predictors-and-coefficients",
    "href": "interpreting-linear-models.html#sec-predictors-and-coefficients",
    "title": "2  Interpreting linear models",
    "section": "2.1 Predictors and coefficients",
    "text": "2.1 Predictors and coefficients\nSee also Agresti 1.2, Dunn and Smyth 1.4, 1.7, 2.7\nThe types of predictors \\(x_j\\) (e.g. binary or continuous) has less of an effect on the regression than the type of response, but it is still important to pay attention to the former.\nIntercepts. It is common to include an intercept in a linear regression model, a predictor \\(x_0\\) such that \\(x_{i0} = 1\\) for all \\(i\\). When an intercept is present, we index it as the 0th predictor. The simplest kind of linear model is the intercept-only model or the one-sample model: \\[\ny = \\beta_0 + \\epsilon.\n\\tag{2.1}\\] The parameter \\(\\beta_0\\) is the mean of the response.\nBinary predictors. In addition to an intercept, suppose we have a binary predictor \\(x_1 \\in \\{0,1\\}\\) (e.g. \\(x_1 = 1\\) for patients who took blood pressure medication and \\(x_1 = 0\\) for those who didn’t). This leads to the following linear model: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon.\n\\tag{2.2}\\] Here, \\(\\beta_0\\) is the mean response (say blood pressure) for observations with \\(x_1 = 0\\) and \\(\\beta_0 + \\beta_1\\) is the mean response for observations with \\(x_1 = 1\\). Therefore, the parameter \\(\\beta_1\\) is the difference in mean response between observations with \\(x_1 = 1\\) and \\(x_1 = 0\\). This parameter is sometimes called the effect or effect size of \\(x_1\\), though a causal relationship might or might not be present. The model (2.2) is sometimes called the two-sample model, because the response data can be split into two “samples”: those corresponding to \\(x_1 = 0\\) and those corresponding to \\(x_1 = 1\\).\nCategorical predictors. A binary predictor is a special case of a categorical predictor: A predictor taking two or more discrete values. Suppose we have a predictor \\(w \\in \\{w_0, w_1, \\dots, w_{C-1}\\}\\), where \\(C \\geq 2\\) is the number of categories and \\(w_0, \\dots, w_{C-1}\\) are the levels of \\(w\\). E.g. suppose \\(\\{w_0, \\dots, w_{C-1}\\}\\) is the collection of U.S. states, so that \\(C = 50\\). If we want to regress a response on the categorical predictor \\(w\\), we cannot simply set \\(x_1 = w\\) in the context of the linear regression (2.2). Indeed, \\(w\\) does not necessarily take numerical values. Instead, we need to add a predictor \\(x_j\\) for each of the levels of \\(w\\). In particular, define \\(x_j \\equiv 1(w = w_j)\\) for \\(j = 1, \\dots, C-1\\) and consider the regression \\[\ny = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_{C-1}x_{C-1} + \\epsilon.\n\\tag{2.3}\\] Here, category 0 is the base category, and \\(\\beta_0\\) represents the mean response in the base category. The coefficient \\(\\beta_j\\) represents the difference in mean response between the \\(j\\)th category and the base category.\nQuantitative predictors. A quantitative predictor is one that can take on any real value. For example, suppose that \\(x_1 \\in \\mathbb{R}\\), and consider the linear model \\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon.\n\\tag{2.4}\\] Now, the interpretation of \\(\\beta_1\\) is that an increase in \\(x_1\\) by 1 is associated with an increase in \\(y\\) by \\(\\beta_1\\). We must be careful to avoid saying “an increase in \\(x_1\\) by 1 causes \\(y\\) to increase by \\(\\beta_1\\)” unless we make additional causal assumptions. Note that the units of \\(x_1\\) matter. If \\(x_1\\) is the height of a person, then the value and the interpretation of \\(\\beta_1\\) changes depending on whether that height is measured in feet or in meters.\nOrdinal predictors. There is an awkward category of predictor in between categorical and continuous called ordinal. An ordinal predictor is one that takes a discrete number of values, but these values have an intrinsic ordering, e.g. \\(x_1 \\in \\{\\texttt{small}, \\texttt{medium}, \\texttt{large}\\}\\). It can be treated as categorical at the cost of losing the ordering information, or as continuous if one is willing to assign quantitative values to each category.\nMultiple predictors. A linear regression need not contain just one predictor (aside from an intercept). For example, let’s say \\(x_1\\) and \\(x_2\\) are two predictors. Then, a linear model with both predictors is \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon.\n\\tag{2.5}\\] When there are multiple predictors, the interpretation of coefficients must be revised somewhat. For example, \\(\\beta_1\\) in the above regression is the effect of an increase in \\(x_1\\) by 1 while holding \\(x_2\\) constant or while adjusting for \\(x_2\\) or while controlling for \\(x_2\\). If \\(y\\) is blood pressure, \\(x_1\\) is a binary predictor indicating blood pressure medication taken and \\(x_2\\) is sex, then \\(\\beta_1\\) is the effect of the medication on blood pressure while controlling for sex. In general, the coefficient of a predictor depends on what other predictors are in the model. As an extreme case, suppose the medication has no actual effect, but that men generally have higher blood pressure and higher rates of taking the medication. Then, the coefficient \\(\\beta_1\\) in the single regression model (2.2) would be nonzero but the coefficient in the multiple regression model (2.5) would be equal to zero. In this case, sex acts as a confounder.\nInteractions. Note that the multiple regression model (2.5) has the built-in assumption that the effect of \\(x_1\\) on \\(y\\) is the same for any fixed value of \\(x_2\\) (and vice versa). In some cases, the effect of one variable on the response may depend on the value of another variable. In this case, it’s appropriate to add another predictor called an interaction. Suppose \\(x_2\\) is quantitative (e.g. years of job experience) and \\(x_2\\) is binary (e.g. sex, with \\(x_2 = 1\\) meaning male). Then, we can define a third predictor \\(x_3\\) as the product of the first two, i.e. \\(x_3 = x_1x_2\\). This gives the regression model \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon.\n\\tag{2.6}\\] Now, the effect of adding another year of job experience is \\(\\beta_1\\) for females and \\(\\beta_1 + \\beta_3\\) for males. The coefficient \\(\\beta_3\\) is the difference in the effect of job experience between males and females."
  },
  {
    "objectID": "interpreting-linear-models.html#sec-linear-model-spaces",
    "href": "interpreting-linear-models.html#sec-linear-model-spaces",
    "title": "2  Interpreting linear models",
    "section": "2.2 Linear model spaces",
    "text": "2.2 Linear model spaces\nSee also Agresti 1.3-1.4, Dunn and Smyth 2.1, 2.2, 2.5.1\nThe matrix \\(\\boldsymbol{X}\\) is called the model matrix or the design matrix. Concatenating the linear model equations across observations gives us an equivalent formulation: \\[\n\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}; \\quad \\mathbb{E}[\\boldsymbol{\\epsilon}] = \\boldsymbol{0}, \\ \\text{Var}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\boldsymbol{I_n}\n\\] or \\[\n\\mathbb{E}[\\boldsymbol{y}] = \\boldsymbol{X} \\boldsymbol{\\beta} = \\boldsymbol{\\mu}.\n\\] As \\(\\boldsymbol{\\beta}\\) varies in \\(\\mathbb{R}^p\\), the set of possible vectors \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\) is defined \\[\nC(\\boldsymbol{X}) \\equiv \\{\\boldsymbol{\\mu} = \\boldsymbol{X} \\boldsymbol{\\beta}: \\boldsymbol{\\beta} \\in \\mathbb{R}^p\\}.\n\\] \\(C(\\boldsymbol{X})\\), called the model vector space, is a subspace of \\(\\mathbb{R}^n\\): \\(C(\\boldsymbol{X}) \\subseteq \\mathbb{R}^n\\). Since \\[\n\\boldsymbol{X} \\boldsymbol{\\beta} = \\beta_0 \\boldsymbol{x_{*0}} + \\cdots + \\beta_{p-1} \\boldsymbol{x_{*p-1}},\n\\] the model vector space is the column space of the matrix \\(\\boldsymbol{X}\\) (Figure 2.1).\n\n\n\nFigure 2.1: The model vector space.\n\n\nThe dimension of \\(C(\\boldsymbol{X})\\) is the rank of \\(\\boldsymbol{X}\\), i.e. the number of linearly independent columns of \\(\\boldsymbol{X}\\). If \\(\\text{rank}(\\boldsymbol{X}) &lt; p\\), this means that there are two different vectors \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\beta'}\\) such that \\(\\boldsymbol{X} \\boldsymbol{\\beta} = \\boldsymbol{X} \\boldsymbol{\\beta'}\\). Therefore, we have two values of the parameter vector that give the same model for \\(\\boldsymbol{y}\\). This makes \\(\\boldsymbol{\\beta}\\) not identifiable, and makes it impossible to reliably determine \\(\\boldsymbol{\\beta}\\) based on the data. For this reason, we will generally assume that \\(\\boldsymbol{\\beta}\\) is identifiable, i.e. \\(\\boldsymbol{X} \\boldsymbol{\\beta} \\neq \\boldsymbol{X} \\boldsymbol{\\beta'}\\) if \\(\\boldsymbol{\\beta} \\neq \\boldsymbol{\\beta'}\\). This is equivalent to the assumption that \\(\\text{rank}(\\boldsymbol{X}) = p\\). Note that this cannot hold when \\(p &gt; n\\), so for the majority of the course we will assume that \\(p \\leq n\\). In this case, \\(\\text{rank}(\\boldsymbol{X}) = p\\) if and only if \\(\\boldsymbol{X}\\) has full-rank.\nAs an example when \\(p \\leq n\\) but when \\(\\boldsymbol{\\beta}\\) is still not identifiable, consider the case of a categorical predictor. Suppose the categories of \\(w\\) were \\(\\{w_1, \\dots, w_{C-1}\\}\\), i.e. the baseline category \\(w_0\\) did not exist. In this case, the model (2.3) would not be identifiable because \\(x_0 = 1 = x_1 + \\cdots + x_{C-1}\\) and thus \\(x_{*0} = 1 = x_{*1} + \\cdots + x_{*,C-1}\\). Indeed, this means that one of the predictors can be expressed as a linear combination of the others, so \\(\\boldsymbol{X}\\) cannot have full rank. A simpler way of phrasing the problem is that we are describing \\(C-1\\) intrinsic parameters (the means in each of the \\(C-1\\) groups) with \\(C\\) model parameters. There must therefore be some redundancy. For this reason, if we include an intercept term in the model then we must designate one of our categories as the baseline and exclude its indicator from the model."
  },
  {
    "objectID": "least-squares-estimation.html#algebraic-perspective",
    "href": "least-squares-estimation.html#algebraic-perspective",
    "title": "3  Least squares estimation",
    "section": "3.1 Algebraic perspective",
    "text": "3.1 Algebraic perspective\nSee also Agresti 2.1.1, Dunn and Smyth 2.4.1, 2.5.2\nNow, suppose that we are given a dataset \\((\\boldsymbol{X}, \\boldsymbol{y})\\). How do we go about estimating \\(\\boldsymbol{\\beta}\\) based on this data? The canonical approach is the method of least squares: \\[\n\\boldsymbol{\\widehat{\\beta}} \\equiv \\underset{\\boldsymbol{\\beta}}{\\arg \\min}\\ \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2.\n\\] The quantity \\[\n\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2 = \\|\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}\\|^2 = \\sum_{i = 1}^n (y_i - \\widehat{\\mu}_i)^2\n\\] is called the residual sum of squares (RSS), and it measures the lack of fit of the linear regression model. We therefore want to choose \\(\\boldsymbol{\\widehat{\\beta}}\\) to minimize this lack of fit. Letting \\(L(\\boldsymbol{\\beta}) = \\frac{1}{2}\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2\\), we can do some calculus to derive that \\[\n\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}L(\\boldsymbol{\\beta}) = -\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}).\n\\] Setting this vector of partial derivatives equal to zero, we arrive at the normal equations: \\[\n-\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}) = 0 \\quad \\Longleftrightarrow \\quad \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} = \\boldsymbol{X}^T \\boldsymbol{y}.\n\\tag{3.1}\\] If \\(\\boldsymbol{X}\\) is full rank, the matrix \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) is invertible and we can therefore conclude that \\[\n\\boldsymbol{\\widehat{\\beta}} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}.\n\\tag{3.2}\\]"
  },
  {
    "objectID": "least-squares-estimation.html#probabilistic-perspective",
    "href": "least-squares-estimation.html#probabilistic-perspective",
    "title": "3  Least squares estimation",
    "section": "3.2 Probabilistic perspective",
    "text": "3.2 Probabilistic perspective\nSee also Agresti 2.7.1\n\n3.2.1 Least squares as maximum likelihood estimation\nNote that if \\(\\boldsymbol{\\epsilon}\\) is assumed to be \\(N(0,\\sigma^2 \\boldsymbol{I_n})\\), then the least squares solution would also be the maximum likelihood solution. Indeed, for \\(y_i \\sim N(\\mu_i, \\sigma^2)\\), the log-likelihood is:\n\\[\n\\log \\left[\\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - \\mu_i)^2}{2\\sigma^2}\\right)\\right] = \\text{constant} - \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n (y_i - \\mu_i)^2.\n\\]\n\n\n3.2.2 Gauss-Markov theorem\nNow that we have derived the least squares estimator, we can compute its bias and variance. To obtain the bias, we first calculate that:\n\\[\n\\mathbb{E}[\\widehat{\\boldsymbol{\\beta}}] = \\mathbb{E}[(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}] = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\mathbb{E}[\\boldsymbol{y}] = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\beta} = \\boldsymbol{\\beta}.\n\\]\nTherefore, the least squares estimator is unbiased. To obtain the variance, we compute:\n\\[\n\\begin{split}\n\\text{Var}[\\boldsymbol{\\widehat{\\beta}}] &= \\text{Var}[(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}] \\\\\n&= (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\text{Var}[\\boldsymbol{y}]\\boldsymbol{X} (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\\\\n&= (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T(\\sigma^2 \\boldsymbol{I_n})\\boldsymbol{X} (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\\\\n&= \\sigma^2 (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}.\n\\end{split}\n\\tag{3.3}\\]\n\nTheorem 3.1 (Gauss-Markov theorem) For homoskedastic linear models (eqs. (1.1) and (1.2)), the least squares coefficient estimates have the smallest covariance matrix (in the sense of positive semidefinite matrices) among all linear unbiased estimates of \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "least-squares-estimation.html#geometric-perspective",
    "href": "least-squares-estimation.html#geometric-perspective",
    "title": "3  Least squares estimation",
    "section": "3.3 Geometric perspective",
    "text": "3.3 Geometric perspective\nSee also Agresti 2.2.1-2.2.3\nThe following is the key geometric property of least squares (Figure 3.1).\n\nProposition 3.1 The mapping \\(\\boldsymbol{y} \\mapsto \\boldsymbol{\\widehat{\\mu}} = \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}} \\in C(\\boldsymbol{X})\\) is an orthogonal projection onto \\(C(\\boldsymbol{X})\\), with projection matrix\n\\[\n\\boldsymbol{H} \\equiv  \\boldsymbol{X}(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\quad (\\textit{the hat matrix}).\n\\tag{3.4}\\]\n\nGeometrically, this makes sense since we define \\(\\boldsymbol{\\widehat{\\beta}}\\) so that \\(\\boldsymbol{\\widehat{\\mu}} \\in C(\\boldsymbol{X})\\) is as close to \\(\\boldsymbol{y}\\) as possible. The shortest path between a point and a plane is the perpendicular. A simple example of \\(\\boldsymbol{H}\\) can be obtained by considering the intercept-only regression.\n\nProof. To prove that \\(\\boldsymbol{y} \\mapsto \\boldsymbol{\\widehat{\\mu}}\\) is an orthogonal projection onto \\(C(\\boldsymbol{X})\\), it suffices to show that:\n\\[\n\\boldsymbol{v}^T (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}) = 0 \\text{ for each } \\boldsymbol{v} \\in C(\\boldsymbol{X}).\n\\]\nSince the columns \\(\\{\\boldsymbol{x_{*0}}, \\dots, \\boldsymbol{x_{*p-1}}\\}\\) of \\(\\boldsymbol{X}\\) form a basis for \\(C(\\boldsymbol{X})\\), it suffices to show that \\(\\boldsymbol{x_{*j}}^T (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}) = 0\\) for each \\(j = 0, \\dots, p-1\\). This is a consequence of the normal equations \\(\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}) = 0\\) derived in (3.1).\nTo show that the projection matrix is \\(\\boldsymbol{H}\\) (3.4), it suffices to check that:\n\\[\n\\boldsymbol{\\widehat{\\mu}} = \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}} = \\boldsymbol{X}(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{y} \\equiv \\boldsymbol{H} \\boldsymbol{y}.\n\\]\n\n\n\n\nFigure 3.1: Least squares as orthogonal projection.\n\n\n\nProposition 3.2 If \\(\\boldsymbol{P}\\) is an orthogonal projection onto a subspace \\(\\boldsymbol{W}\\), then:\n\n\\(\\boldsymbol{P}\\) is idempotent, i.e., \\(\\boldsymbol{P}^2 = \\boldsymbol{P}\\).\nFor all \\(\\boldsymbol{v} \\in \\boldsymbol{W}\\), we have \\(\\boldsymbol{P}\\boldsymbol{v} = \\boldsymbol{v}\\), and for all \\(\\boldsymbol{v} \\in \\boldsymbol{W}^{\\perp}\\), we have \\(\\boldsymbol{P} \\boldsymbol{v} = 0\\).\n\\(\\text{trace}(\\boldsymbol{P}) = \\text{dim}(\\boldsymbol{W})\\).\n\n\nOne consequence of the geometric interpretation of least squares is that the fitted values \\(\\boldsymbol{\\widehat{\\mu}}\\) depend on \\(\\boldsymbol{X}\\) only through \\(C(\\boldsymbol{X})\\). As we will see in Homework 1, there are many different model matrices \\(\\boldsymbol{X}\\) leading to the same model space. Essentially, this reflects the fact that there are many different bases for the same vector space. Consider, for example, changing the units on the columns of \\(\\boldsymbol{X}\\). It can be verified that not just the fitted values \\(\\boldsymbol{\\widehat{\\mu}}\\) but also the predictions on a new set of features remain invariant to reparametrization (this follows from parts (a) and (b) of Homework 1 Problem 1). Therefore, while reparametrization can have a huge impact on the fitted coefficients, it has no impact on the predictions of linear regression."
  },
  {
    "objectID": "anova.html#analysis-of-variance",
    "href": "anova.html#analysis-of-variance",
    "title": "4  Analysis of variance",
    "section": "4.1 Analysis of variance",
    "text": "4.1 Analysis of variance\nThe orthogonality property of least squares, together with the Pythagorean theorem, leads to a fundamental relationship called the analysis of variance.\nLet’s say that \\(S \\subset \\{0, 1, \\dots, p-1\\}\\) is a subset of the predictors we wish to exclude from the model. First regress \\(\\boldsymbol{y}\\) on \\(\\boldsymbol{X}\\) to get \\(\\boldsymbol{\\widehat{\\beta}}\\) as usual. Then, we consider the partial model matrix \\(\\boldsymbol{X_{*,\\text{-}S}}\\) obtained by selecting all predictors except those in \\(S\\). Regressing \\(\\boldsymbol{y}\\) on \\(\\boldsymbol{X_{*, \\text{-}S}}\\) results in \\(\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\) (note: \\(\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\) is not necessarily obtained from \\(\\boldsymbol{\\widehat{\\beta}}\\) by extracting the coefficients corresponding to \\(\\text{-}S\\)).\n\nTheorem 4.1 \\[\n\\|\\boldsymbol{y} -  \\boldsymbol{X_{*, \\text{-}S}}\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\|^2 = \\|\\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X_{*, \\text{-}S}}\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\|^2 + \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}\\|^2.\n\\tag{4.1}\\]\n\n\nProof. Consider the three points \\(\\boldsymbol{y}\\), \\(\\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}\\), \\(\\boldsymbol{X_{*, \\text{-}S}}\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}} \\in \\mathbb{R}^n\\). Since \\(\\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}\\) and \\(\\boldsymbol{X_{*, \\text{-}S}}\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\) are both in \\(C(\\boldsymbol{X})\\), it follows by the orthogonal projection property that \\(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}\\) is orthogonal to \\(\\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X_{*, \\text{-}S}}\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\). In other words, these three points form a right triangle (Figure 4.1). The relationship (4.1) is then a consequence of the Pythagorean theorem.\n\n\n\n\nFigure 4.1: Pythagorean theorem for regression on a subset of predictors.\n\n\nWe will rely on this fundamental relationship throughout this course. One important special case is when \\(S = \\{1, \\dots, p-1\\}\\), i.e., the model without \\(S\\) is the intercept-only model. In this case, \\(\\boldsymbol{X_{*, \\text{-}S}} = \\boldsymbol{1_n}\\) and \\(\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}} = \\bar{y}\\). Therefore, equation (4.1) implies the following.\n\nProposition 4.1 \\[\n\\|\\boldsymbol{y} -  \\bar{y} \\boldsymbol{1_n}\\|^2 = \\|\\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}} - \\bar{y} \\boldsymbol{1_n}\\|^2 + \\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}\\|^2.\n\\]\nEquivalently, we can rewrite this equation as follows:\n\\[\n\\textnormal{SST} \\equiv \\sum_{i = 1}^n (y_i - \\bar{y})^2 = \\sum_{i = 1}^n (\\widehat{\\mu}_i - \\bar{y})^2 + \\sum_{i = 1}^n (y_i - \\widehat{\\mu}_i)^2 \\equiv \\textnormal{SSR} + \\textnormal{SSE}.\n\\tag{4.2}\\]"
  },
  {
    "objectID": "anova.html#r2-and-multiple-correlation",
    "href": "anova.html#r2-and-multiple-correlation",
    "title": "4  Analysis of variance",
    "section": "4.2 \\(R^2\\) and multiple correlation",
    "text": "4.2 \\(R^2\\) and multiple correlation\nThe ANOVA decomposition (4.2) of the variation in \\(\\boldsymbol{y}\\) into that explained by the linear regression model (SSR) and that left over (SSE) leads naturally to the definition of \\(R^2\\) as the fraction of variation in \\(\\boldsymbol{y}\\) explained by the linear regression model:\n\\[\nR^2 \\equiv \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\sum_{i = 1}^n (\\widehat{\\mu}_i - \\bar{y})^2}{\\sum_{i = 1}^n (y_i - \\bar{y})^2} = \\frac{\\|\\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}} - \\bar{y} \\boldsymbol{1_n}\\|^2}{\\|\\boldsymbol{y} -  \\bar{y} \\boldsymbol{1_n}\\|^2}.\n\\]\nBy the decomposition (4.2), we have \\(R^2 \\in [0,1]\\). The closer \\(R^2\\) is to 1, the more closely the data follow the fitted linear regression model. This intuition is formalized in the following result.\n\nProposition 4.2 \\(R^2\\) is the squared sample correlation between \\(\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\) and \\(\\boldsymbol{y}\\).\n\nFor this reason, the positive square root of \\(R^2\\) is called the multiple correlation coefficient.\n\nProof. The first step is to observe that the mean of \\(\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\) is \\(\\bar{y}\\) (this follows from the normal equations). Therefore, the sample correlation between \\(\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\) and \\(\\boldsymbol{y}\\) is the inner product of the unit-normalized vectors \\(\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\bar{y} \\boldsymbol{1}\\) and \\(\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}\\), which is the cosine of the angle between them. From the geometry of Figure 4.1, we find that the cosine of the angle between \\(\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\bar{y} \\boldsymbol{1}\\) and \\(\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}\\) is \\(\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\bar{y} \\boldsymbol{1}\\|/\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}\\|\\). Squaring this relation gives the desired conclusion."
  },
  {
    "objectID": "anova.html#r2-increases-as-predictors-are-added",
    "href": "anova.html#r2-increases-as-predictors-are-added",
    "title": "4  Analysis of variance",
    "section": "4.3 \\(R^2\\) increases as predictors are added",
    "text": "4.3 \\(R^2\\) increases as predictors are added\nThe \\(R^2\\) is an in-sample measure, i.e., it uses the same data to fit the model and to assess the quality of the fit. Therefore, it is generally an optimistic measure of the (out-of-sample) prediction error. One manifestation of this is that the \\(R^2\\) increases if any predictors are added to the model (even if these predictors are “junk”). To see this, it suffices to show that SSE decreases as we add predictors. Without loss of generality, suppose that we start with a model with all predictors except those in \\(S \\subset \\{0, 1, \\dots, p-1\\}\\) and compare it to the model including all the predictors \\(\\{0,1,\\dots,p-1\\}\\). We can read off from the Pythagorean theorem (4.1) that:\n\\[\n\\text{SSE}(\\boldsymbol{X_{*, \\text{-}S}}, \\boldsymbol{y}) \\equiv \\|\\boldsymbol{y} -  \\boldsymbol{X_{*, \\text{-}S}}\\boldsymbol{\\widehat{\\beta}_{\\text{-}S}}\\|^2 \\geq  \\|\\boldsymbol{y} -  \\boldsymbol{X}\\boldsymbol{\\widehat{\\beta}}\\|^2 \\equiv \\text{SSE}(\\boldsymbol{X}, \\boldsymbol{y}).\n\\]\nAdding many junk predictors will have the effect of degrading predictive performance but will nevertheless increase \\(R^2\\)."
  },
  {
    "objectID": "anova.html#special-cases",
    "href": "anova.html#special-cases",
    "title": "4  Analysis of variance",
    "section": "4.4 Special cases",
    "text": "4.4 Special cases\n\n4.4.1 The \\(C\\)-groups model\nSee also Agresti 2.3.2-2.3.3\n\n4.4.1.1 ANOVA decomposition for \\(C\\) groups model\nLet’s consider the special case of the ANOVA decomposition (4.2) when the model matrix \\(\\boldsymbol{X}\\) represents a single categorical predictor \\(w\\). In this case, each observation \\(i\\) is associated with one of the \\(C\\) classes of \\(w\\), which we denote \\(c(i) \\in \\{1, \\dots, C\\}\\). Let’s consider the \\(C\\) groups of observations \\(\\{i: c(i) = c\\}\\) for \\(c \\in \\{1, \\dots, C\\}\\). For example, \\(w\\) may be the type of a car (compact, midsize, minivan, etc.) and \\(y\\) might be its fuel efficiency in miles per gallon.\n\n\n\n\n\nIt is easy to check that the least squares fitted values \\(\\widehat{\\mu}_i\\) are simply the means of the corresponding groups:\n\\[\n\\widehat{\\mu}_i = \\bar{y}_{c(i)}, \\quad \\text{where}\\ \\bar{y}_{c(i)} \\equiv \\frac{\\sum_{i: c(i) = c} y_i}{|\\{i: c(i) = c\\}|}.\n\\]\nTherefore, we have:\n\\[\n\\text{SSR} = \\sum_{i = 1}^n (\\widehat{\\mu}_i - \\bar{y})^2 = \\sum_{i = 1}^n (\\bar{y}_{c(i)} - \\bar{y})^2 \\equiv \\text{between-groups sum of squares (SSB)}.\n\\]\nand\n\\[\n\\text{SSE} = \\sum_{i = 1}^n (y_i - \\widehat{\\mu}_i)^2 = \\sum_{i = 1}^n (y_i - \\bar{y}_{c(i)})^2 \\equiv \\text{within-groups sum of squares (SSW)}.\n\\]\nWe therefore obtain the following corollary of the ANOVA decomposition (4.2):\n\\[\n\\text{SST} = \\text{SSB} + \\text{SSW}.\n\\tag{4.3}\\]\n\n\n\n4.4.2 Simple linear regression\nSee also Agresti 2.1.3\nConsider a linear regression model with an intercept and one quantitative predictor, \\(x\\):\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon.\n\\tag{4.4}\\]\nThis is the simple linear regression model. In this case, we can compute that \\[\n\\widehat \\beta_1 = \\frac{\\sigma_y}{\\sigma_x} \\rho_{xy},\n\\tag{4.5}\\] where \\(\\rho_{xy}\\) is the sample correlation between \\(x\\) and \\(y\\), \\(\\sigma^2_x\\) is the sample variance of \\(x\\), and \\(\\sigma^2_y\\) is the sample variance of \\(y\\). Furthermore, we have \\[\n\\widehat \\beta_0 = \\bar y - \\widehat \\beta_1 \\bar x.\n\\tag{4.6}\\]\n\n4.4.2.1 ANOVA decomposition for simple linear regression\nFigure 4.2 gives an interpretation of the ANOVA decomposition (4.2) in the case of the simple linear regression model (4.4).\n\n\n\nFigure 4.2: ANOVA decomposition for simple linear regression.\n\n\n\n\n4.4.2.2 Connection between \\(R^2\\) and correlation\nThere is a connection between \\(R^2\\) and correlation in simple linear regression.\n\nProposition 4.3 Let \\(\\rho_{xy}\\) denote the sample correlation between \\(x\\) and \\(y\\), and let \\(R^2_{xy}\\) be the \\(R^2\\) from the simple linear regression (4.4). Then, we have: \\[\nR^2 = \\rho_{xy}^2.\n\\]\n\n\nProof. This fact is a consequence of Proposition 4.2.\n\n\n\n4.4.2.3 Regression to the mean\nSimple linear regression can be used to study the relationship between the same quantity across time (or generations). For example, let \\(x\\) and \\(y\\) be the height of a parent and child. This example motivated Sir Francis Galton to study linear regression in the first place. Alternatively, \\(x\\) and \\(y\\) can be a student’s score on a standardized test in two consecutive years, or the number of games won by a given sports team in two consecutive seasons. In this situation, it is reasonable to assume that the sample standard deviations of \\(x\\) and \\(y\\) are the same (or to normalize these variables to achieve this). In this case, equations (4.5) and (4.6) simplify to:\n\\[\n\\widehat{\\beta}_0 = \\bar{y} - \\rho_{xy} \\bar{x} \\quad \\text{and} \\quad \\widehat{\\beta}_1 = \\rho_{xy}.\n\\tag{4.7}\\]\nIt follows that:\n\\[\n|\\widehat{\\mu}_i - \\bar{y}| = |\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i - \\bar{y}| = |\\rho_{xy}(x_i - \\bar{x})| = |\\rho_{xy}| \\cdot |x_i - \\bar{x}|.\n\\]\nSince \\(|\\rho_{xy}| &lt; 1\\) unless \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) are perfectly correlated (by the Cauchy-Schwarz inequality), this means that:\n\\[\n|\\widehat{\\mu}_i - \\bar{y}| &lt; |x_i - \\bar{x}| \\quad \\text{for each } i.\n\\tag{4.8}\\]\nTherefore, we expect \\(y_i\\) to be closer to its mean than \\(x_i\\) is to its mean. This phenomenon is called regression to the mean (and is in fact the origin of the term “regression”). Many mistakenly attribute a causal mechanism to this phenomenon, when in reality it is simply a statistical artifact. For example, suppose \\(x_i\\) is the number of games a sports team won last season and \\(y_i\\) is the number of games it won this season. It is widely observed that teams with exceptional performance in a given season suffer a “winner’s curse,” performing worse in the next season. The reason for the winner’s curse is simple: teams perform exceptionally well due to a combination of skill and luck. While skill stays roughly constant from year to year, the team which performed exceptionally well in a given season is unlikely to get as lucky as it did the next season."
  },
  {
    "objectID": "collinearity.html#least-squares-estimates-in-the-orthogonal-case",
    "href": "collinearity.html#least-squares-estimates-in-the-orthogonal-case",
    "title": "5  Collinearity and adjustment",
    "section": "5.1 Least squares estimates in the orthogonal case",
    "text": "5.1 Least squares estimates in the orthogonal case\nThe simplest case to analyze is when \\(\\boldsymbol x_{*j}\\) is orthogonal to \\(\\boldsymbol X_{*,\\text{-}j}\\) in the sense that\n\\[\n\\boldsymbol x_{*j}^T \\boldsymbol X_{*,\\text{-}j} = \\boldsymbol{0}.\n\\tag{5.1}\\]\nIn this case, we can derive the least squares coefficient vector \\(\\boldsymbol{\\widehat{\\beta}} = (\\widehat{\\beta}_{j|\\text{-}j}, \\boldsymbol{\\widehat{\\beta}}_{\\text{-}j|j})\\) in the regression of \\(\\boldsymbol y\\) on \\(\\boldsymbol X\\):\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\widehat{\\beta}_{j|\\text{-}j} \\\\\n\\boldsymbol{\\widehat{\\beta}}_{\\text{-}j|j}\n\\end{pmatrix} &= (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y} \\\\\n&=\n\\begin{pmatrix}\n\\boldsymbol x_{*j}^T \\boldsymbol x_{*j} & \\boldsymbol{0} \\\\\n\\boldsymbol{0} & \\boldsymbol{X}_{*,\\text{-}j}^T \\boldsymbol{X}_{*,\\text{-}j}\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n\\boldsymbol x_{*j}^T \\\\\n\\boldsymbol{X}_{*,\\text{-}j}^T\n\\end{pmatrix} \\boldsymbol{y} \\\\\n&=\n\\begin{pmatrix}\n(\\boldsymbol x_{*j}^T \\boldsymbol x_{*j})^{-1} \\boldsymbol x_{*j}^T \\boldsymbol{y} \\\\\n(\\boldsymbol{X}_{*,\\text{-}j}^T \\boldsymbol{X}_{*,\\text{-}j})^{-1} \\boldsymbol{X}_{*,\\text{-}j}^T \\boldsymbol{y}\n\\end{pmatrix} \\\\\n&=\n\\begin{pmatrix}\n\\widehat{\\beta}_{j} \\\\\n\\boldsymbol{\\widehat{\\beta}}_{\\text{-}j}\n\\end{pmatrix}.\n\\end{split}\n\\tag{5.2}\\]\nTherefore, the least squares coefficient of \\(x_j\\) is the same regardless of whether the other predictors are included in the regression, i.e.\n\\[\n\\widehat{\\beta}_{j|\\text{-}j} = \\widehat{\\beta}_{j}.\n\\tag{5.3}\\]"
  },
  {
    "objectID": "collinearity.html#least-squares-estimates-via-orthogonalization",
    "href": "collinearity.html#least-squares-estimates-via-orthogonalization",
    "title": "5  Collinearity and adjustment",
    "section": "5.2 Least squares estimates via orthogonalization",
    "text": "5.2 Least squares estimates via orthogonalization\nThe orthogonality assumption (5.1) is almost never satisfied in practice. Usually, \\(\\boldsymbol{x}_{*j}\\) has a nonzero projection \\(\\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\widehat{\\gamma}}\\) onto \\(C(\\boldsymbol{X}_{*,\\text{-}j})\\):\n\\[\n\\boldsymbol{x}_{*j} = \\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\widehat{\\gamma}} + \\boldsymbol{x}^{\\perp}_{*j},\n\\]\nwhere \\(\\boldsymbol{x}^{\\perp}_{*j}\\) is the residual from regressing \\(\\boldsymbol{x}_{*j}\\) onto \\(\\boldsymbol{X}_{*,\\text{-}j}\\) and is therefore orthogonal to \\(C(\\boldsymbol{X}_{*,\\text{-}j})\\). In other words, \\(\\boldsymbol{x}^{\\perp}_{*j}\\) is the projection of \\(\\boldsymbol{x}_{*j}\\) onto the orthogonal complement of \\(C(\\boldsymbol{X}_{*,\\text{-}j})\\). Another way of framing this relationship is that \\(\\boldsymbol{x}^{\\perp}_{*j}\\) is the result of adjusting \\(\\boldsymbol{x}_{*j}\\) for \\(\\boldsymbol{X}_{*,\\text{-}j}\\).\nWith this decomposition, let us change basis from \\((\\boldsymbol{x}_{*j}, \\boldsymbol{X}_{*,\\text{-}j})\\) to \\((\\boldsymbol{x}^{\\perp}_{*j}, \\boldsymbol{X}_{*,\\text{-}j})\\) by the process explored in Homework 1 Question 1. Let us write:\n\\[\n\\begin{aligned}\n\\boldsymbol{y} &= \\boldsymbol{x}_{*j} \\beta_{j|\\text{-}j} + \\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\beta}_{\\text{-}j|j} + \\boldsymbol{\\epsilon} \\\\\n&\\Longleftrightarrow \\ \\boldsymbol{y} = (\\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\widehat{\\gamma}} + \\boldsymbol{x}^{\\perp}_{*j})\\beta_{j|\\text{-}j} + \\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\beta}_{\\text{-}j|j} + \\boldsymbol{\\epsilon} \\\\\n&\\Longleftrightarrow \\ \\boldsymbol{y} = \\boldsymbol{x}^{\\perp}_{*j}\\beta_{j|\\text{-}j} + \\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\beta}'_{\\text{-}j|j} + \\boldsymbol{\\epsilon}.\n\\end{aligned}\n\\]\nWhat this means is that \\(\\widehat{\\beta}_{j|\\text{-}j}\\), the least squares coefficient of \\(\\boldsymbol{x}_{*j}\\) in the regression of \\(\\boldsymbol{y}\\) on \\((\\boldsymbol{x}_{*j}, \\boldsymbol{X}_{*,\\text{-}j})\\), is also the least squares coefficient of \\(\\boldsymbol{x}^{\\perp}_{*j}\\) in the regression of \\(\\boldsymbol{y}\\) on \\((\\boldsymbol{x}^{\\perp}_{*j}, \\boldsymbol{X}_{*,\\text{-}j})\\). However, since \\(\\boldsymbol{x}^{\\perp}_{*j}\\) is orthogonal to \\(\\boldsymbol{X}_{*,\\text{-}j}\\) by construction, we can use the result (5.2) to conclude that:\n\\[\n\\widehat{\\beta}_{j|\\text{-}j} \\text{ is the least squares coefficient of } \\boldsymbol{x}^{\\perp}_{*j} \\text{ in the } \\textit{univariate} \\text{ regression of } \\boldsymbol{y} \\text{ on } \\boldsymbol{x}^{\\perp}_{*j}.\n\\]\nWe can solve this univariate regression explicitly to obtain:\n\\[\n\\widehat{\\beta}_{j|\\text{-}j} = \\frac{(\\boldsymbol{x}^{\\perp}_{*j})^T \\boldsymbol{y}}{\\|\\boldsymbol{x}^{\\perp}_{*j}\\|^2}.\n\\tag{5.4}\\]"
  },
  {
    "objectID": "collinearity.html#adjustment-and-partial-correlation",
    "href": "collinearity.html#adjustment-and-partial-correlation",
    "title": "5  Collinearity and adjustment",
    "section": "5.3 Adjustment and partial correlation",
    "text": "5.3 Adjustment and partial correlation\nEquivalently, letting \\(\\boldsymbol{\\widehat{\\beta}}_{\\text{-}j}\\) be the least squares estimate in the regression of \\(\\boldsymbol{y}\\) on \\(\\boldsymbol{X}_{*,\\text{-}j}\\) (note that this is not the same as \\(\\boldsymbol{\\widehat{\\beta}}_{\\text{-}j|j}\\)), we can write:\n\\[\n\\widehat{\\beta}_{j|\\text{-}j} = \\frac{(\\boldsymbol{x}^{\\perp}_{*j})^T(\\boldsymbol{y} - \\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\widehat{\\beta}}_{\\text{-}j})}{\\|\\boldsymbol{x}^{\\perp}_{*j}\\|^2} \\equiv \\frac{(\\boldsymbol{x}^{\\perp}_{*j})^T \\boldsymbol y^\\perp}{\\|\\boldsymbol{x}^{\\perp}_{*j}\\|^2}.\n\\]\nWe can interpret this result as follows:\n\nTheorem 5.1 The linear regression coefficient \\(\\widehat{\\beta}_{j|\\text{-}j}\\) results from first adjusting \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{x}_{*j}\\) for the effects of all other variables, and then regressing the residuals from \\(\\boldsymbol{y}\\) onto the residuals from \\(\\boldsymbol{x}_{*j}\\).\n\nIn this sense, the least squares coefficient for a predictor in a multiple linear regression reflects the effect of the predictor on the response after controlling for the effects of all other predictors.\nEconometricians calls this the Frisch-Waugh-Lovell (FWL) theorem, to acknowledge economists Ragnar Frisch and Frederick V. Waugh, who first derived the result in 1933, and Michael C. Lovell, who later rediscovered and extended it in 1963. In the statistical literature, this fact was known at least as early as 1907, when Yule documented it in his paper “On the Theory of Correlation for any Number of Variables, treated by a New System of Notation.”\nA related quantity is the partial correlation between \\(\\boldsymbol{x}_{*j}\\) and \\(\\boldsymbol{y}\\) after controlling for \\(\\boldsymbol{X}_{*,\\text{-}j}\\), defined as the empirical correlation between \\(\\boldsymbol{x}_{*j}^\\perp\\) and \\(\\boldsymbol{y}^\\perp\\): \\[\n\\rho(\\boldsymbol{x}_{*j}, \\boldsymbol{y}|\\boldsymbol{X}_{*,\\text{-}j}) \\equiv \\frac{(\\boldsymbol{x}_{*j}^\\perp)^T(\\boldsymbol{y}^\\perp)}{\\|\\boldsymbol{x}_{*j}^\\perp\\| \\|\\boldsymbol{y}^\\perp\\|}.\n\\] We can then connect the least squares coefficient \\(\\widehat{\\beta}_{j|\\text{-}j}\\) to this partial correlation: \\[\n\\widehat{\\beta}_{j|\\text{-}j} = \\frac{(\\boldsymbol{x}^{\\perp}_{*j})^T \\boldsymbol y^\\perp}{\\|\\boldsymbol{x}^{\\perp}_{*j}\\|^2} = \\frac{\\|\\boldsymbol{y}^\\perp\\|}{\\|\\boldsymbol{x}_{*j}^\\perp\\|}\\rho(\\boldsymbol{x}_{*j}, \\boldsymbol{y}|\\boldsymbol{X}_{*,\\text{-}j}),\n\\]\nin a similar spirit to equation (4.5)."
  },
  {
    "objectID": "collinearity.html#effects-of-collinearity",
    "href": "collinearity.html#effects-of-collinearity",
    "title": "5  Collinearity and adjustment",
    "section": "5.4 Effects of collinearity",
    "text": "5.4 Effects of collinearity\nCollinearity between a predictor \\(x_j\\) and the other predictors tends to make the estimate \\(\\widehat{\\beta}_{j|\\text{-}j}\\) unstable. Intuitively, this makes sense because it becomes harder to distinguish between the effects of predictor \\(x_j\\) and those of the other predictors on the response. To find the variance of \\(\\widehat{\\beta}_{j|\\text{-}j}\\) for a model matrix \\(\\boldsymbol{X}\\), we could in principle use the formula (3.3). However, this formula involves the inverse of the matrix \\(\\boldsymbol{X}^T \\boldsymbol{X}\\), which is hard to reason about. Instead, we can employ the formula (5.4) to calculate directly that:\n\\[\n\\text{Var}[\\widehat{\\beta}_{j|\\text{-}j}] = \\frac{\\sigma^2}{\\|\\boldsymbol{x}_{*j}^\\perp\\|^2}.\n\\tag{5.5}\\]\nWe see that the variance of \\(\\widehat{\\beta}_{j|\\text{-}j}\\) is inversely proportional to \\(\\|\\boldsymbol{x}_{*j}^\\perp\\|^2\\). This means that the greater the collinearity, the less of \\(\\boldsymbol{x}_{*j}\\) is left over after adjusting for \\(\\boldsymbol{X}_{*,\\text{-}j}\\), and the greater the variance of \\(\\widehat{\\beta}_{j|\\text{-}j}\\). To quantify the effect of this adjustment, suppose there were no other predictors other than the intercept term. Then, we would have:\n\\[\n\\text{Var}[\\widehat{\\beta}_{j|1}] = \\frac{\\sigma^2}{\\|\\boldsymbol{x}_{*j}-\\bar{x}_j \\boldsymbol{1}_n\\|^2}.\n\\]\nTherefore, we can rewrite the variance (5.5) as:\n\\[\n\\begin{split}\n\\text{Var}[\\widehat{\\beta}_{j|\\text{-}j}] &= \\frac{\\|\\boldsymbol{x}_{*j}-\\bar{x}_j \\boldsymbol{1}_n\\|^2}{\\|\\boldsymbol{x}_{*j}-\\boldsymbol{X}_{*,\\text{-}j}\\boldsymbol{\\widehat{\\gamma}}\\|^2} \\cdot \\text{Var}[\\widehat{\\beta}_{j|1}] \\\\\n&= \\frac{1}{1-R_j^2} \\cdot \\text{Var}[\\widehat{\\beta}_{j|1}] \\equiv \\text{VIF}_j \\cdot \\text{Var}[\\widehat{\\beta}_{j|1}],\n\\end{split}\n\\tag{5.6}\\]\nwhere \\(R_j^2\\) is the \\(R^2\\) value when regressing \\(\\boldsymbol{x}_{*j}\\) on \\(\\boldsymbol{X}_{*,\\text{-}j}\\) and VIF stands for variance inflation factor. The higher \\(R_j^2\\), the more of the variance in \\(\\boldsymbol{x}_{*j}\\) is explained by other predictors, the higher the variance in \\(\\widehat{\\beta}_{j|\\text{-}j}\\)."
  },
  {
    "objectID": "collinearity.html#application-average-treatment-effect-estimation-in-causal-inference",
    "href": "collinearity.html#application-average-treatment-effect-estimation-in-causal-inference",
    "title": "5  Collinearity and adjustment",
    "section": "5.5 Application: Average treatment effect estimation in causal inference",
    "text": "5.5 Application: Average treatment effect estimation in causal inference\nSuppose we’d like to study the effect of an exposure or treatment (e.g. taking a blood pressure medication) on a response \\(y\\) (e.g. blood pressure). In the Neyman-Rubin causal model, for a given individual \\(i\\) we denote by \\(y_i(1)\\) and \\(y_i(0)\\) the outcomes that would have occurred had the individual received the treatment and the control, respectively. These are called potential outcomes. Let \\(t_i \\in \\{0,1\\}\\) indicate whether the \\(i\\)th individual actually received treatment or control. Therefore, the observed outcome is1 \\[\ny_i^{\\text{obs}} = y_i(t_i).\n\\tag{5.7}\\] Based on the data \\(\\{(t_i, y_i^{\\text{obs}})\\}_{i = 1, \\dots, n}\\), the most basic goal is to estimate the\n\\[\n\\textit{average treatment effect} \\  \\tau \\equiv \\mathbb{E}[y(1) - y(0)],\n\\]\nwhere averaging is done over the population of individuals (often called units in causal inference). Of course, we do not observe both \\(y_i(1)\\) and \\(y_i(0)\\) for any unit \\(i\\). Additionally, usually in observational studies we have confounding variables \\(w_2, \\dots, w_{p-1}\\): variables that influence both the treatment assignment and the response (e.g. degree of health-seeking activity). It is important to control for these confounders in order to get an unbiased estimate of the treatment effect. Suppose the following linear model holds:\n\\[\ny(t) = \\beta_0 + \\beta_1 t + \\beta_2 w_2 + \\cdots + \\beta_{p-1} w_{p-1} + \\epsilon \\quad \\text{for } t \\in \\{0, 1\\}, \\quad \\text{where} \\ \\epsilon \\perp\\!\\!\\!\\perp t.\n\\]\nThis assumption can be broken down into the following statements:\n\nthe treatment effect is constant across units;\nthe response is a linear function of the treatment and observed confounders;\nthere is no unmeasured confounding.\n\nUnder these assumptions, we find that\n\\[\n\\tau \\equiv \\mathbb{E}[y(1) - y(0)] = \\beta_1.\n\\]\nUsing the relationship (5.7), we find that\n\\[\ny^{\\text{obs}} = \\beta_0 + \\beta_1 t + \\beta_2 w_2 + \\cdots + \\beta_{p-1} w_{p-1} + \\epsilon \\quad \\text{for } t \\in \\{0, 1\\}.\n\\]\nIn this case, the average treatment effect \\(\\tau\\) is identified as the coefficient \\(\\beta_1\\) in the above regression, i.e. \\(\\tau = \\beta_1\\). In other words, the causal parameter coincides with a parameter of the statistical model for the observed data. Therefore, the least squares estimate \\(\\widehat{\\beta}_1\\) is an unbiased estimate of the average treatment effect.\nIn this context, we can interpret Theorem 5.1 as follows: To get an estimate of the causal effect of an exposure on an outcome in the presence of confounders, first adjust both exposure and outcome for the confounders, and then estimate the effect of the adjusted exposure on the adjusted outcome via a univariate linear model. This is the essence of covariate adjustment in causal inference.\n\n\n\n\n\n\nNote\n\n\n\nCausal inference is a vast field, which lies mostly beyond the scope of STAT 9610; see STAT 9210 instead."
  },
  {
    "objectID": "collinearity.html#footnotes",
    "href": "collinearity.html#footnotes",
    "title": "5  Collinearity and adjustment",
    "section": "",
    "text": "This requires the stable unit treatment value assumption (SUTVA), which prevents issues like interference, where the treatment of individual \\(i\\) can be affected by the assigned treatments of other individuals.↩︎"
  },
  {
    "objectID": "r-demo-part-1.html#exploration",
    "href": "r-demo-part-1.html#exploration",
    "title": "6  R demo",
    "section": "6.1 Exploration",
    "text": "6.1 Exploration\nBefore modeling our data, let’s first explore it.\n\n# pairs plot\n\n# Q: What are the typical ranges of the variables?\n# Q: What are the relationships among the variables?\n\nscots_races |&gt;\n  select(-race) |&gt; # select() from dplyr for selecting columns\n  ggpairs() # ggpairs() from GGally to create pairs plot\n\n\n\n\n\n\n\n# mile time versus distance\n\n# Q: How does mile time vary with distance?\n# Q: What races deviate from this trend?\n# Q: How does climb play into it?\n\n# add mile time variable to scots_races\nscots_races &lt;- scots_races |&gt;\n  mutate(mile_time = time / distance) # mutate() from dplyr to add column\n\n\n# plot mile time versus distance\nscots_races |&gt;\n  ggplot(aes(x = distance, y = mile_time)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# add climb information as point color\nscots_races |&gt;\n  ggplot(aes(x = distance, y = mile_time, colour = climb)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# highlight extreme points\nscots_races_extreme &lt;- scots_races |&gt;\n  filter(distance &gt; 15 | mile_time &gt; 9) # filter() from dplyr to subset rows\n\n# plot mile time versus distance\nscots_races |&gt;\n  ggplot(aes(x = distance, y = mile_time, label = race, colour = climb)) +\n  geom_point() +\n  geom_text_repel(aes(label = race), data = scots_races_extreme)\n\n\n\n\n\n\n\n\n\n# clean up plot\nscots_races |&gt;\n  ggplot(aes(x = distance, y = mile_time, label = race, color = climb)) +\n  geom_point() +\n  geom_text_repel(aes(label = race), data = scots_races_extreme) +\n  labs(\n    x = \"Distance (miles)\",\n    y = \"Mile Time (minutes per mile)\",\n    color = \"Climb\\n(thousands of ft)\"\n  )"
  },
  {
    "objectID": "r-demo-part-1.html#linear-model-coefficient-interpretation",
    "href": "r-demo-part-1.html#linear-model-coefficient-interpretation",
    "title": "6  R demo",
    "section": "6.2 Linear model coefficient interpretation",
    "text": "6.2 Linear model coefficient interpretation\nLet’s fit some linear models and interpret the coefficients.\n\n# Q: What is the effect of an extra mile of distance on time?\n\nlm_fit &lt;- lm(time ~ distance + climb, data = scots_races)\ncoef(lm_fit)\n\n(Intercept)    distance       climb \n -13.108551    6.350955   11.780133 \n\n\n\n# Linear model with interaction\n\n# Q: What is the effect of an extra mile of distance on time\n#  for a run with low climb?\n\n# Q: What is the effect of an extra mile of distance on time\n#  for a run with high climb?\n\nlm_fit_int &lt;- lm(time ~ distance * climb, data = scots_races)\ncoef(lm_fit_int)\n\n   (Intercept)       distance          climb distance:climb \n    -0.7671925      4.9622542      3.7132519      0.6598256 \n\nscots_races |&gt;\n  summarise(min_climb = min(climb), max_climb = max(climb))\n\n# A tibble: 1 × 2\n  min_climb max_climb\n      &lt;dbl&gt;     &lt;dbl&gt;\n1       0.3       7.5\n\n\nLet’s take a look at the regression summary for lm_fit:\n\nlm_fit &lt;- lm(time ~ distance + climb, data = scots_races)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = time ~ distance + climb, data = scots_races)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.654  -4.842   1.110   4.667  27.762 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -13.1086     2.5608  -5.119 1.41e-05 ***\ndistance      6.3510     0.3578  17.751  &lt; 2e-16 ***\nclimb        11.7801     1.2206   9.651 5.37e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.734 on 32 degrees of freedom\nMultiple R-squared:  0.9717,    Adjusted R-squared:   0.97 \nF-statistic: 549.9 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\n\nWe get a coefficient of 6.35 with standard error 0.36 for distance, where the standard error is an estimate of the quantity (5.5)."
  },
  {
    "objectID": "r-demo-part-1.html#r2-and-sum-of-squared-decompositions.",
    "href": "r-demo-part-1.html#r2-and-sum-of-squared-decompositions.",
    "title": "6  R demo",
    "section": "6.3 \\(R^2\\) and sum-of-squared decompositions.",
    "text": "6.3 \\(R^2\\) and sum-of-squared decompositions.\nWe can extract the \\(R^2\\) from this fit by reading it off from the bottom of the summary, or by typing\n\nsummary(lm_fit)$r.squared\n\n[1] 0.971725\n\n\nWe can construct sum-of-squares decompositions (4.1) using the anova function. This function takes as arguments the partial model and the full model. For example, consider the partial model time ~ distance.\n\nlm_fit_partial &lt;- lm(time ~ distance, data = scots_races)\nanova(lm_fit_partial, lm_fit)\n\nAnalysis of Variance Table\n\nModel 1: time ~ distance\nModel 2: time ~ distance + climb\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     33 9546.9                                 \n2     32 2441.3  1    7105.6 93.14 5.369e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe find that adding the predictor climb reduces the RSS by 7106, from 9547 to 2441. As another example, we can compute the \\(R^2\\) by comparing the full model with the null model:\n\nlm_fit_null &lt;- lm(time ~ 1, data = scots_races)\nanova(lm_fit_null, lm_fit)\n\nAnalysis of Variance Table\n\nModel 1: time ~ 1\nModel 2: time ~ distance + climb\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     34 86340                                  \n2     32  2441  2     83899 549.87 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTherefore, the \\(R^2\\) is 83899/86340 = 0.972, consistent with the above regression summary."
  },
  {
    "objectID": "r-demo-part-1.html#adjustment-and-collinearity.",
    "href": "r-demo-part-1.html#adjustment-and-collinearity.",
    "title": "6  R demo",
    "section": "6.4 Adjustment and collinearity.",
    "text": "6.4 Adjustment and collinearity.\nWe can also test the adjustment formula (5.4) numerically. Let’s consider the coefficient of distance in the regression time ~ distance + climb. We can obtain this coefficient by first regressing climb out of distance and time:\n\nlm_dist_on_climb &lt;- lm(distance ~ climb, data = scots_races)\nlm_time_on_climb &lt;- lm(time ~ climb, data = scots_races)\n\nscots_races_resid &lt;- tibble(\n  dist_residuals = residuals(lm_dist_on_climb),\n  time_residuals = residuals(lm_time_on_climb)\n)\n\nlm_adjusted &lt;- lm(time_residuals ~ dist_residuals - 1,\n  data = scots_races_resid\n)\nsummary(lm_adjusted)\n\n\nCall:\nlm(formula = time_residuals ~ dist_residuals - 1, data = scots_races_resid)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.654  -4.842   1.110   4.667  27.762 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \ndist_residuals   6.3510     0.3471    18.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.474 on 34 degrees of freedom\nMultiple R-squared:  0.9078,    Adjusted R-squared:  0.9051 \nF-statistic: 334.8 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nWe find a coefficient of 6.35 with standard error 0.35, which matches that obtained in the original regression.\nWe can get the partial correlation between distance and time by taking the empirical correlation between the residuals. We can compare this quantity to the usual correlation.\n\nscots_races_resid |&gt;\n  summarise(cor(dist_residuals, time_residuals)) |&gt;\n  pull()\n\n[1] 0.9527881\n\nscots_races |&gt;\n  summarise(cor(distance, time)) |&gt;\n  pull()\n\n[1] 0.9430944\n\n\nIn this case, the two correlation quantities are similar.\nTo obtain the variance inflation factors defined in equation (5.6), we can use the vif function from the car package:\n\nvif(lm_fit)\n\ndistance    climb \n1.740812 1.740812 \n\n\nWhy are these two VIF values the same?"
  },
  {
    "objectID": "linear-models-inference.html",
    "href": "linear-models-inference.html",
    "title": "Linear models: Inference",
    "section": "",
    "text": "We now understand the least squares estimator \\(\\boldsymbol{\\widehat{\\beta}}\\) from geometric and algebraic points of view. In Unit 2, we switch to a probabilistic perspective to derive inferential statements for linear models, in the form of hypothesis tests and confidence intervals. In order to facilitate this, we will assume that the error terms are normally distributed:\n\\[\n\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \\text{where} \\ \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I}_n).\n\\] We first establish some building blocks necessary for linear models inference, primarily related to manipulating the normal distribution (Chapter 7). Then, we discuss univariate and multivariate hypothesis testing in linear models (Chapter 8), as well as the power of these hypothesis tests (Chapter 9). We then move on to the construction of confidence intervals and confidence regions (Chapter 10). We conclude with a discussion of practical considerations (Chapter 11) and an R demo (Chapter 12)."
  },
  {
    "objectID": "building-blocks.html#sec-mvrnorm",
    "href": "building-blocks.html#sec-mvrnorm",
    "title": "7  Building blocks",
    "section": "7.1 The multivariate normal distribution",
    "text": "7.1 The multivariate normal distribution\nRecall that a random vector \\(\\boldsymbol{w} \\in \\mathbb{R}^d\\) has a multivariate normal distribution with mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) if it has probability density\n\\[\np(\\boldsymbol{w}) = \\frac{1}{\\sqrt{(2\\pi)^{d}\\text{det}(\\boldsymbol{\\Sigma})}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{w} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{w} - \\boldsymbol{\\mu})\\right).\n\\]\nIt is also possible to define normal distributions in the case \\(\\text{det}(\\boldsymbol\\Sigma) = 0\\). These distributions are supported on subspaces of \\(\\mathbb{R}^d\\), and do not have densities with respect to the Lebesgue measure.\nThese random vectors have lots of special properties, including:\n\nLinear transformation: If \\(\\boldsymbol{w} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then \\(\\boldsymbol{A} \\boldsymbol{w} + \\boldsymbol{b} \\sim N(\\boldsymbol{A} \\boldsymbol{\\mu} + \\boldsymbol{b}, \\boldsymbol{A} \\boldsymbol{\\Sigma} \\boldsymbol{A}^\\top)\\).\nIndependence: If \\[\n\\begin{pmatrix}\\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\end{pmatrix} \\sim N\\left(\\begin{pmatrix}\\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix} , \\begin{pmatrix}\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{12}^\\top & \\boldsymbol{\\Sigma}_{22}\\end{pmatrix}\\right),\n\\] then \\(\\boldsymbol{w}_1 \\perp\\!\\!\\!\\perp \\boldsymbol{w}_2\\) if and only if \\(\\boldsymbol{\\Sigma}_{12} = \\boldsymbol{0}\\).\n\nAn important distribution related to the multivariate normal is the \\(\\chi^2_d\\) (chi-squared with \\(d\\) degrees of freedom) distribution, defined as\n\\[\n\\chi^2_d \\equiv \\sum_{j = 1}^d w_j^2 \\quad \\text{for} \\quad w_1, \\dots, w_d \\overset{\\text{i.i.d.}}{\\sim} N(0, 1).\n\\]"
  },
  {
    "objectID": "building-blocks.html#sec-lin-reg-dist",
    "href": "building-blocks.html#sec-lin-reg-dist",
    "title": "7  Building blocks",
    "section": "7.2 The distributions of linear regression estimates and residuals",
    "text": "7.2 The distributions of linear regression estimates and residuals\nSee also Dunn and Smyth 2.8.2\nThe most important distributional result in linear regression is that\n\\[\n\\boldsymbol{\\widehat{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}).\n\\tag{7.1}\\]\nIndeed, by the linear transformation property of the multivariate normal distribution,\\(\\boldsymbol{y} \\sim N(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2 \\boldsymbol{I}_n)\\) implies that\n\\[\n\\begin{split}\n\\boldsymbol{\\widehat{\\beta}} &= (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{y} \\\\\n&\\sim N((\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{X} \\boldsymbol{\\beta}, (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\sigma^2 \\boldsymbol{I}_n \\boldsymbol{X}(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}) \\\\\n&= N(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}).\n\\end{split}\n\\]\nNext, let’s consider the joint distribution of \\(\\boldsymbol{\\widehat{\\mu}} = \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\) and \\(\\boldsymbol{\\widehat{\\epsilon}} = \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\). We have\n\\[\n\\begin{split}\n\\begin{pmatrix} \\boldsymbol{\\widehat{\\mu}} \\\\ \\boldsymbol{\\widehat{\\epsilon}} \\end{pmatrix} &= \\begin{pmatrix} \\boldsymbol{H} \\boldsymbol{y} \\\\ (\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{y} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\boldsymbol{H} \\\\ \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\boldsymbol{y} \\\\\n&\\sim N\\left(\\begin{pmatrix} \\boldsymbol{H} \\\\ \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\boldsymbol{X} \\boldsymbol{\\beta}, \\begin{pmatrix} \\boldsymbol{H} \\\\ \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\cdot \\sigma^2 \\boldsymbol{I} \\begin{pmatrix} \\boldsymbol{H} & \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\right)\n\\\\\n&= N\\left(\\begin{pmatrix} \\boldsymbol{X} \\boldsymbol{\\beta} \\\\ \\boldsymbol{0} \\end{pmatrix}, \\begin{pmatrix} \\sigma^2 \\boldsymbol{H} & \\boldsymbol{0} \\\\ \\boldsymbol{0} & \\sigma^2(\\boldsymbol{I} - \\boldsymbol{H}) \\end{pmatrix} \\right).\n\\end{split}\n\\]\nIn other words,\n\\[\n\\boldsymbol{\\widehat{\\mu}} \\sim N(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2 \\boldsymbol{H}) \\quad \\text{and} \\quad \\boldsymbol{\\widehat{\\epsilon}} \\sim N(\\boldsymbol{0}, \\sigma^2(\\boldsymbol{I} - \\boldsymbol{H})), \\quad \\text{with} \\quad \\boldsymbol{\\widehat{\\mu}} \\perp\\!\\!\\!\\perp \\boldsymbol{\\widehat{\\epsilon}}.\n\\tag{7.2}\\]\nThe statistical independence between \\(\\boldsymbol{\\widehat{\\mu}}\\) and \\(\\boldsymbol{\\widehat{\\epsilon}}\\) is a result of the fact that these two quantities are projections of \\(\\boldsymbol{y}\\) onto two orthogonal subspaces: \\(C(\\boldsymbol{X})\\) and \\(C(\\boldsymbol{X})^\\perp\\) (Figure 7.1).\n\n\n\nFigure 7.1: The fitted vector \\(\\boldsymbol{\\widehat{\\mu}}\\) and the residual vector \\(\\boldsymbol{\\widehat{\\epsilon}}\\) are projections of \\(\\boldsymbol{y}\\) onto orthogonal subspaces.\n\n\nSince \\(\\boldsymbol{\\widehat{\\beta}}\\) is a deterministic function of \\(\\boldsymbol{\\widehat{\\mu}}\\) (in particular, \\(\\boldsymbol{\\widehat{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{\\widehat{\\mu}}\\)), it also follows that\n\\[\n\\boldsymbol{\\widehat{\\beta}} \\perp\\!\\!\\!\\perp \\boldsymbol{\\widehat{\\epsilon}}.\n\\tag{7.3}\\]"
  },
  {
    "objectID": "building-blocks.html#sec-noise-estimation",
    "href": "building-blocks.html#sec-noise-estimation",
    "title": "7  Building blocks",
    "section": "7.3 Estimation of the noise variance \\(\\sigma^2\\)",
    "text": "7.3 Estimation of the noise variance \\(\\sigma^2\\)\nSee also Dunn and Smyth 2.4.2, 2.5.3\nWe can’t quite do inference for \\(\\boldsymbol{\\beta}\\) based on the distributional result (7.1) because the noise variance \\(\\sigma^2\\) is unknown to us. Intuitively, since \\(\\sigma^2 = \\mathbb{E}[\\epsilon_i^2]\\), we can get an estimate of \\(\\sigma^2\\) by looking at the quantity \\(\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2\\). To get the distribution of this quantity, we need the following lemma:\n\nLemma 7.1 Let \\(\\boldsymbol{w} \\sim N(\\boldsymbol{0}, \\boldsymbol{P})\\) for some projection matrix \\(\\boldsymbol{P}\\). Then, \\(\\|\\boldsymbol{w}\\|^2 \\sim \\chi^2_d\\), where \\(d = \\text{trace}(\\boldsymbol{P})\\) is the dimension of the subspace onto which \\(\\boldsymbol{P}\\) projects.\n\n\nProof. Let \\(\\boldsymbol{P} = \\boldsymbol{U} \\boldsymbol{D} \\boldsymbol{U}^\\top\\) be an eigenvalue decomposition of \\(\\boldsymbol{P}\\), where \\(\\boldsymbol{U}\\) is orthogonal and \\(\\boldsymbol{D}\\) is a diagonal matrix with \\(D_{ii} \\in \\{0,1\\}\\). We have \\(\\boldsymbol{w} \\overset{d}{=} \\boldsymbol{U} \\boldsymbol{D} \\boldsymbol{z}\\) for \\(\\boldsymbol{z} \\sim N(0, \\boldsymbol{I}_n)\\). Therefore,\n\\[\n\\|\\boldsymbol{w}\\|^2 = \\|\\boldsymbol{D} \\boldsymbol{z}\\|^2 = \\sum_{i: D_{ii} = 1} z_i^2 \\sim \\chi^2_d, \\quad \\text{where } d = |\\{i: D_{ii} = 1\\}| = \\text{trace}(D) = \\text{trace}(\\boldsymbol{P}).\n\\]\n\nRecall that \\(\\boldsymbol{I} - \\boldsymbol{H}\\) is a projection onto the \\((n-p)\\)-dimensional space \\(C(\\boldsymbol{X})^\\perp\\), so by Lemma 7.1 and equation (7.2), we have\n\\[\n\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2 \\sim \\sigma^2 \\chi^2_{n-p}.\n\\tag{7.4}\\]\nFrom this result, it follows that \\(\\mathbb{E}[\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2] = \\sigma^2(n-p)\\), so\n\\[\n\\widehat{\\sigma}^2 \\equiv \\frac{1}{n-p}\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2\n\\tag{7.5}\\]\nis an unbiased estimate for \\(\\sigma^2\\). Why does the denominator need to be \\(n-p\\) rather than \\(n\\) for the estimator above to be unbiased? The reason for this is that the residuals \\(\\boldsymbol{\\widehat{\\epsilon}}\\) are the projection of the true noise vector \\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n\\) onto the \\((n-p)\\)-dimensional subspace \\(C(\\boldsymbol{X})^\\perp\\) (Figure 7.2). To see this, note that\n\\[\n\\boldsymbol{\\widehat{\\epsilon}} = (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{y} = (\\boldsymbol{I} - \\boldsymbol{H})(\\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) = (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{\\epsilon}.\n\\]\nTherefore, the norm of the residual vector will be smaller than that of the noise vector, especially to the extent that \\(p\\) is close to \\(n\\).\n\n\n\nFigure 7.2: The residual vector \\(\\boldsymbol{\\widehat{\\epsilon}}\\) is the projection of the noise vector \\(\\boldsymbol{\\epsilon}\\) onto \\(C(\\boldsymbol{X})^\\perp\\)."
  },
  {
    "objectID": "hypothesis-testing.html#sec-one-dim-testing",
    "href": "hypothesis-testing.html#sec-one-dim-testing",
    "title": "8  Hypothesis testing",
    "section": "8.1 Testing a one-dimensional parameter",
    "text": "8.1 Testing a one-dimensional parameter\nSee also Dunn and Smyth 2.8.3\n\n8.1.1 \\(t\\)-test for a single coefficient\nThe most common question to ask in a linear regression context is: Is the \\(j\\)th predictor associated with the response when controlling for the other predictors? In the language of hypothesis testing, this corresponds to the null hypothesis:\n\\[\nH_0: \\beta_j = 0\n\\tag{8.1}\\]\nAccording to equation (7.1), we have \\(\\widehat{\\beta}_j \\sim N(0, \\sigma^2/s_j^2)\\), where, as we learned in Chapter 1:\n\\[\ns_j^{2} \\equiv [(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}_{jj}]^{-1} = \\|\\boldsymbol{x}_{*j}^\\perp\\|^2.\n\\]\nTherefore,\n\\[\n\\frac{\\widehat{\\beta}_j}{\\sigma/s_j} \\sim N(0,1),\n\\tag{8.2}\\]\nand we are tempted to define a level \\(\\alpha\\) test of the null hypothesis (8.1) based on this normal distribution. While this is infeasible since we don’t know \\(\\sigma^2\\), we can substitute in the unbiased estimate (7.5) derived in Section 7.3. Then,\n\\[\n\\text{SE}(\\widehat{\\beta}_j) \\equiv \\frac{\\widehat{\\sigma}}{s_j}\n\\]\nis the standard error of \\(\\widehat{\\beta}_j\\), which is an approximation to the standard deviation of \\(\\widehat{\\beta}_j\\). Dividing \\(\\widehat{\\beta}_j\\) by its standard error gives us the \\(t\\)-statistic:\n\\[\nt_j \\equiv \\frac{\\widehat{\\beta}_j}{\\text{SE}(\\widehat{\\beta}_j)} = \\frac{\\widehat{\\beta}_j}{\\sqrt{\\frac{1}{n-p}\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2}/s_j}.\n\\]\nThis statistic is pivotal, in the sense that it has the same distribution for any \\(\\boldsymbol{\\beta}\\) such that \\(\\beta_j = 0\\). Indeed, we can rewrite it as:\n\\[\nt_j = \\frac{\\frac{\\widehat{\\beta}_j}{\\sigma/s_j}}{\\sqrt{\\frac{\\sigma^{-2}\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2}{n-p}}}.\n\\]\nRecalling the independence of \\(\\boldsymbol{\\widehat{\\beta}}\\) and \\(\\boldsymbol{\\widehat{\\epsilon}}\\) (7.3), the scaled chi-square distribution of \\(\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2\\) (7.4), and the standard normal distribution of \\(\\frac{\\widehat{\\beta}_j}{\\sigma/s_j}\\) (8.2), we find that, under \\(H_0:\\beta_j = 0\\), \\[\nt_j \\sim \\frac{N(0,1)}{\\sqrt{\\frac{1}{n-p}\\chi^2_{n-p}}}, \\quad \\text{with numerator and denominator independent.}\n\\]\nThis distribution is called the \\(t\\) distribution with \\(n-p\\) degrees of freedom and is denoted \\(t_{n-p}\\). This paves the way for the two-sided \\(t\\)-test:\n\\[\n\\phi_t(\\boldsymbol{X}, \\boldsymbol{y}) = 1(|t_j| &gt; t_{n-p}(1-\\alpha/2)),\n\\]\nwhere \\(t_{n-p}(1-\\alpha/2)\\) denotes the \\(1-\\alpha/2\\) quantile of \\(t_{n-p}\\). Note that, by the law of large numbers,\n\\[\n\\frac{1}{n-p}\\chi^2_{n-p} \\overset{P}{\\rightarrow} 1 \\quad \\text{as} \\quad n - p \\rightarrow \\infty,\n\\]\nso for large \\(n-p\\) we have \\(t_{j} \\sim t_{n-p} \\approx N(0,1)\\). Hence, the \\(t\\)-test is approximately equal to the following \\(z\\)-test:\n\\[\n\\phi_t(\\boldsymbol{X}, \\boldsymbol{y}) \\approx \\phi_z(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv 1(|t_j| &gt; z(1-\\alpha/2)),\n\\]\nwhere \\(z(1-\\alpha/2)\\) is the \\(1-\\alpha/2\\) quantile of \\(N(0,1)\\). The \\(t\\)-test can also be defined in a one-sided fashion if power against one-sided alternatives is desired.\n\n\n8.1.2 Example: One-sample model\nConsider the intercept-only linear regression model \\(y = \\beta_0 + \\epsilon\\), and let usapply the \\(t\\)-test derived above to test the null hypothesis \\(H_0: \\beta_0 = 0\\). We have \\(\\widehat{\\beta}_0 = \\bar{y}\\). Furthermore, we have\n\\[\n\\text{SE}^2(\\widehat{\\beta}_0) = \\frac{\\widehat{\\sigma}^2}{n}, \\quad \\text{where} \\quad \\widehat{\\sigma}^2 = \\frac{1}{n-1}\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}_n\\|^2.\n\\]\nHence, we obtain the \\(t\\) statistic:\n\\[\nt = \\frac{\\widehat{\\beta}_0}{\\text{SE}(\\widehat{\\beta}_0)} = \\frac{\\sqrt{n} \\bar{y}}{\\sqrt{\\frac{1}{n-1}\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}_n\\|^2}}.\n\\]\nAccording to the theory above, this test statistic has a null distribution of \\(t_{n-1}\\).\n\n\n8.1.3 Example: Two-sample model\nSuppose we have \\(x_1 \\in \\{0,1\\}\\), in which case the linear regression \\(y = \\beta_0 + \\beta_1 x_1 + \\epsilon\\) becomes a two-sample model. We can rewrite this model as:\n\\[\ny_i \\sim \\begin{cases}\nN(\\beta_0, \\sigma^2) \\quad &\\text{for } x_i = 0; \\\\\nN(\\beta_0 + \\beta_1, \\sigma^2) \\quad &\\text{for } x_i = 1.\n\\end{cases}\n\\]\nIt is often of interest to test the null hypothesis \\(H_0: \\beta_1 = 0\\), i.e., that the two groups have equal means. Let usdefine:\n\\[\n\\bar{y}_0 \\equiv \\frac{1}{n_0}\\sum_{i: x_i = 0} y_i, \\quad \\bar{y}_1 \\equiv \\frac{1}{n_1}\\sum_{i: x_i = 1} y_i, \\quad \\text{where} \\quad n_0 = |\\{i: x_i = 0\\}| \\text{ and } n_1 = |\\{i: x_i = 1\\}|.\n\\]\nThen, we have seen before that \\(\\widehat{\\beta}_0 = \\bar{y}_0\\) and \\(\\widehat{\\beta}_1 = \\bar{y}_1 - \\bar{y}_0\\). We can compute that:\n\\[\ns_1^2 \\equiv \\|\\boldsymbol{x}_{*1}^{\\perp}\\|^2 = \\|\\boldsymbol{x}_{*1} - \\frac{n_1}{n}\\boldsymbol{1}\\|^2 = n_1\\frac{n_0^2}{n^2} + n_0\\frac{n_1^2}{n^2} = \\frac{n_0 n_1}{n} = \\frac{1}{\\frac{1}{n_0} + \\frac{1}{n_1}}\n\\]\nand\n\\[\n\\widehat{\\sigma}^2 = \\frac{1}{n-2}\\left(\\sum_{i: x_i = 0}(y_i - \\bar{y}_0)^2 + \\sum_{i: x_i = 1}(y_i - \\bar{y}_1)^2\\right).\n\\]\nTherefore, we arrive at a \\(t\\)-statistic of:\n\\[\nt = \\frac{\\sqrt{\\frac{1}{\\frac{1}{n_0} + \\frac{1}{n_1}}}(\\bar{y}_1 - \\bar{y}_0)}{\\sqrt{\\frac{1}{n-2}\\left(\\sum_{i: x_i = 0}(y_i - \\bar{y}_0)^2 + \\sum_{i: x_i = 1}(y_i - \\bar{y}_1)^2\\right)}}.\n\\]\nUnder the null hypothesis, this statistic has a distribution of \\(t_{n-2}\\).\n\n\n8.1.4 \\(t\\)-test for a contrast among coefficients\nGiven a vector \\(\\boldsymbol{c} \\in \\mathbb{R}^p\\), the quantity \\(\\boldsymbol{c}^T \\boldsymbol{\\beta}\\) is sometimes called a contrast. For example, suppose \\(\\boldsymbol{c} = (1,-1, 0, \\dots, 0)\\). Then, \\(\\boldsymbol{c}^T \\boldsymbol{\\beta} = \\beta_1 - \\beta_2\\) is the difference in effects of the first and second predictors. We are sometimes interested in testing whether such a contrast is equal to zero, i.e., \\(H_0: \\boldsymbol{c}^T \\boldsymbol{\\beta} = 0\\). While this hypothesis can involve two or more of the predictors, the parameter \\(\\boldsymbol{c}^T \\boldsymbol{\\beta}\\) is still one-dimensional, and therefore we can still apply a \\(t\\)-test. Going back to the distribution \\(\\boldsymbol{\\widehat{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1})\\), we find that:\n\\[\n\\boldsymbol{c}^T\\boldsymbol{\\widehat{\\beta}} \\sim N(\\boldsymbol{c}^T\\boldsymbol{\\beta}, \\sigma^2\\boldsymbol{c}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{c}).\n\\tag{8.3}\\]\nTherefore, under the null hypothesis that \\(\\boldsymbol{c}^T \\boldsymbol{\\beta} = 0\\), we can derive that:\n\\[\n\\frac{\\boldsymbol{c}^T \\boldsymbol{\\widehat{\\beta}}}{\\widehat{\\sigma} \\sqrt{\\boldsymbol{c}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{c}}} \\sim t_{n-p},\n\\tag{8.4}\\]\ngiving us another \\(t\\)-test. Note that the \\(t\\)-tests described above can be recovered from this more general formulation by setting \\(\\boldsymbol{c} = \\boldsymbol{e}_j\\), the indicator vector with the \\(j\\)th coordinate equal to 1 and all others equal to zero."
  },
  {
    "objectID": "hypothesis-testing.html#sec-multi-dim-testing",
    "href": "hypothesis-testing.html#sec-multi-dim-testing",
    "title": "8  Hypothesis testing",
    "section": "8.2 Testing a multi-dimensional parameter",
    "text": "8.2 Testing a multi-dimensional parameter\nSee also Dunn and Smyth 2.10.1\n\n8.2.1 \\(F\\)-test for a group of coefficients\nNow we move on to the case of testing a multi-dimensional parameter: \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{0}\\) for some \\(S \\subseteq \\{0, 1, \\dots, p-1\\}\\). In other words, we would like to test\n\\[\nH_0: \\boldsymbol{y} = \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\beta}_{\\text{-}S} + \\boldsymbol{\\epsilon} \\quad \\text{versus} \\quad H_1: \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n\\]\nTo test this hypothesis, let us fit least squares coefficients \\(\\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\) and \\(\\boldsymbol{\\widehat{\\beta}}\\) for the partial model as well as the full model. If the partial model fits well, then the residuals \\(\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\) from this model will not be much larger than the residuals \\(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\) from the full model. To quantify this intuition, let us recall our analysis of variance decomposition from Chapter 1:\n\\[\n\\|\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\|^2 = \\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\|^2 + \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2.\n\\]\nLet us consider the ratio\n\\[\n\\frac{\\|\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\|^2 - \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2} = \\frac{\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\|^2}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2},\n\\tag{8.5}\\]\nwhich is the relative increase in the residual sum of squares when going from the full model to the partial model. To interpret this ratio geometrically, let us first examine the quantity \\(\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\) from the numerator. Letting \\(\\boldsymbol{H}\\) and \\(\\boldsymbol{H}_{\\text{-}S}\\) be the projection matrices for the full and partial models, we have \\[\n\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S} = (\\boldsymbol H - \\boldsymbol H_{\\text{-}S}) \\boldsymbol{y}.\n\\] It turns out the the matrix \\(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}\\) is a projection matrix:\n\nProposition 8.1 The matrix \\(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}\\) is a projection matrix onto the space \\(C(\\boldsymbol{X}_{*S}^\\perp)\\) spanned by the columns of \\(\\boldsymbol X_{*S}\\) adjusted for \\(\\boldsymbol X_{*,\\text{-}S}\\).\n\nFigure 8.1 illustrates this relationship.\n\n\n\nFigure 8.1: Geometry of the \\(F\\)-test. Orthogonality relationships stem from \\(C(\\boldsymbol{X}_{*,\\text{-}S}) \\perp C(\\boldsymbol{X}_{*S}^\\perp) \\perp C(\\boldsymbol{X})^\\perp\\).\n\n\n\nProof. Let \\(\\boldsymbol v \\in C(\\boldsymbol{X}_{*S}^\\perp)\\). Because \\(\\boldsymbol v\\) is orthogonal to \\(C(\\boldsymbol X_{*,\\text{-}S})\\) by construction, we have \\((\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S})\\boldsymbol v = \\boldsymbol{H}\\boldsymbol v - \\boldsymbol{H}_{\\text{-}S}\\boldsymbol v = \\boldsymbol v - \\boldsymbol 0 = \\boldsymbol v\\). On the other hand, let \\(v \\in C(\\boldsymbol X_{*, \\text{-}S})\\). Then, we have \\((\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S})\\boldsymbol v = \\boldsymbol{H}\\boldsymbol v - \\boldsymbol{H}_{\\text{-}S}\\boldsymbol v = \\boldsymbol v - \\boldsymbol v = \\boldsymbol 0\\). Finally, let \\(v \\in C(\\boldsymbol X)^\\perp\\). Then, we have \\((\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S})\\boldsymbol v = \\boldsymbol{H}\\boldsymbol v - \\boldsymbol{H}_{\\text{-}S}\\boldsymbol v = \\boldsymbol 0 - \\boldsymbol 0 = \\boldsymbol 0\\). From these three observations, it follows that \\(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}\\) is a projection matrix onto \\(C(\\boldsymbol{X}_{*S}^\\perp)\\).\n\nWith this additional intuition, let us rewrite the ratio (8.5) as\n\\[\n\\frac{\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\|^2}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2} = \\frac{\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2}{\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{y}\\|^2},\n\\]\nrevealing that the numerator and denominator are the squared norms of the projections of \\(\\boldsymbol{y}\\) onto \\(C(\\boldsymbol{X}_{*S}^\\perp)\\) and \\(C(\\boldsymbol{X})^\\perp\\), respectively (Figure 8.1). The numerator is expected to be large if \\(\\boldsymbol \\beta_{S} \\neq 0\\), so \\(\\boldsymbol y\\) will have a large projection onto \\(C(\\boldsymbol{X}_{*S}^\\perp)\\). We can view the denominator as a normalization term.\nNow, let us derive the distribution of this test statistic under the null hypothesis. If \\(\\boldsymbol \\beta_{S} = 0\\), then we have \\(\\boldsymbol{y} = \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\beta}_{\\text{-}S} + \\boldsymbol{\\epsilon}\\), and\n\\[\n(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{X}_{*,\\text{-}S} \\boldsymbol{\\beta}_{\\text{-}S} = (\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{X}_{*,\\text{-}S} \\boldsymbol{\\beta}_{\\text{-}S} = 0\n\\]\nbecause \\(\\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\beta}_{\\text{-}S} \\in C(\\boldsymbol{X}_{*, \\text{-}S})\\), and the latter space is orthogonal to both \\(C(\\boldsymbol{X}_{*,S}^\\perp)\\) and \\(C(\\boldsymbol{X})^\\perp\\). It follows that\n\\[\n\\frac{\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2}{\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{y}\\|^2} = \\frac{\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{\\epsilon}\\|^2}{\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{\\epsilon}\\|^2}.\n\\]\nSince the projection matrices in the numerator and denominator project onto orthogonal subspaces, we have \\((\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{\\epsilon} \\perp\\!\\!\\!\\perp (\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{\\epsilon}\\), with \\(\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{\\epsilon}\\|^2 \\sim \\sigma^2 \\chi^2_{|S|}\\) and \\(\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{\\epsilon}\\|^2 \\sim \\sigma^2 \\chi^2_{n-p}\\). Renormalizing numerator and denominator to have expectation 1 under the null, we arrive at the \\(F\\)-statistic\n\\[\nF \\equiv \\frac{(\\|\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{\\text{-}S}\\|^2 - \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2)/|S|}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2/(n-p)}.\n\\]\nWe have derived that under the null hypothesis,\n\\[\nF \\sim \\frac{\\chi^2_{|S|}/|S|}{\\chi^2_{n-p}/(n-p)}, \\quad \\text{with numerator and denominator independent.}\n\\]\nThis distribution is called the \\(F\\)-distribution with \\(|S|\\) and \\(n-p\\) degrees of freedom, and is denoted \\(F_{|S|, n-p}\\). Denoting by \\(F_{|S|, n-p}(1-\\alpha)\\) the \\(1-\\alpha\\) quantile of this distribution, we arrive at the \\(F\\)-test\n\\[\n\\phi_F(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv 1(F &gt; F_{|S|, n-p}(1-\\alpha)).\n\\]\nNote that the \\(F\\)-test searches for deviations of \\(\\boldsymbol{\\beta}_{S}\\) from zero in all directions, and does not have one-sided variants like the \\(t\\)-test.\n\n\n8.2.2 Example: Testing for any significant coefficients except the intercept\nSuppose \\(\\boldsymbol{x}_{*,0} = \\boldsymbol{1}_n\\) is an intercept term. Then, consider the null hypothesis \\(H_0: \\beta_1 = \\cdots = \\beta_{p-1} = 0\\). In other words, the null hypothesis is the intercept-only model, and the alternative hypothesis is the regression model with an intercept and \\(p-1\\) additional predictors. In this case, \\(S = \\{1, \\dots, p-1\\}\\) and \\(\\text{-}S = \\{0\\}\\). The corresponding \\(F\\) statistic is\n\\[\nF \\equiv \\frac{(\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}\\|^2 - \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2)/(p-1)}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2/(n-p)} = \\frac{\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\bar{y} \\boldsymbol{1}\\|^2/(p-1)}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2/(n-p)}\n\\]\nwith null distribution \\(F_{p-1, n-p}\\).\n\n\n8.2.3 Example: Testing for equality of group means in \\(C\\)-groups model\nAs a further special case, consider the \\(C\\)-groups model from Chapter 1. Recall the ANOVA decomposition\n\\[\n\\sum_{i = 1}^n (y_i - \\bar{y})^2 = \\sum_{i = 1}^n (\\bar{y}_{c(i)} - \\bar{y})^2 + \\sum_{i = 1}^n (y_i - \\bar{y}_{c(i)})^2 = \\text{SSB} + \\text{SSW}.\n\\]\nThe \\(F\\)-statistic in this case becomes\n\\[\nF = \\frac{\\sum_{i = 1}^n (\\bar{y}_{c(i)} - \\bar{y})^2/(C-1)}{\\sum_{i = 1}^n (y_i - \\bar{y}_{c(i)})^2/(n-C)} = \\frac{\\text{SSB}/(C-1)}{\\text{SSW}/(n-C)},\n\\]\nwith null distribution \\(F_{C-1, n-C}\\)."
  },
  {
    "objectID": "power.html#the-power-of-a-t-test",
    "href": "power.html#the-power-of-a-t-test",
    "title": "9  Power",
    "section": "9.1 The power of a \\(t\\)-test",
    "text": "9.1 The power of a \\(t\\)-test\n\n9.1.1 Power formula\nConsider the \\(t\\)-test of the null hypothesis \\(H_0: \\beta_j = 0\\). Suppose that, in reality, \\(\\beta_j \\neq 0\\). What is the probability the \\(t\\)-test will reject the null hypothesis? To answer this question, recall that \\(\\widehat \\beta_j \\sim N(\\beta_j, \\sigma^2/s_j^2)\\). Therefore,\n\\[\nt = \\frac{\\widehat \\beta_j}{\\text{SE}(\\widehat \\beta_j)} = \\frac{\\beta_j}{\\text{SE}(\\widehat \\beta_j)} + \\frac{\\widehat \\beta_j - \\beta_j}{\\text{SE}(\\widehat \\beta_j)} \\overset{\\cdot}{\\sim} N\\left(\\frac{\\beta_j s_j}{\\sigma}, 1\\right)\n\\tag{9.1}\\]\nHere we have made the approximation \\(\\text{SE}(\\widehat \\beta_j) \\approx \\frac{\\sigma}{s_j}\\), which is pretty good when \\(n-p\\) is large. Therefore, the power of the two-sided \\(t\\)-test is\n\\[\n\\mathbb{E}[\\phi_t] = \\mathbb{P}[\\phi_t = 1] \\approx \\mathbb{P}[|t| &gt; z_{1-\\alpha/2}] \\approx \\mathbb{P}\\left[\\left|N\\left(\\frac{\\beta_j s_j}{\\sigma}, 1\\right)\\right| &gt; z_{1-\\alpha/2}\\right]\n\\]\nTherefore, the quantity \\(\\frac{\\beta_j s_j}{\\sigma}\\) determines the power of the \\(t\\)-test. To understand \\(s_j\\) a little better, let’s assume that the rows \\(\\boldsymbol{x}_{i*}\\) of the model matrix are drawn i.i.d. from some distribution \\((x_0, \\dots, x_{p-1})\\). Then we have roughly\n\\[\n\\boldsymbol{x}_{*j}^\\perp \\approx \\boldsymbol{x}_{*j} - \\mathbb{E}[\\boldsymbol{x}_{*j}|\\boldsymbol{X}_{*, \\text{-}j}],\n\\]\nso \\(x_{ij}^\\perp \\approx x_{ij} - \\mathbb{E}[x_{ij}|\\boldsymbol{x}_{i,\\text{-}j}]\\). Hence,\n\\[\ns_j^2 \\equiv \\|\\boldsymbol{x}_{*j}^\\perp\\|^2 \\approx n\\mathbb{E}[(x_j-\\mathbb{E}[x_j|\\boldsymbol{x}_{\\text{-}j}])^2] = n\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]].\n\\]\nHence, we can rewrite the alternative distribution (9.1) as\n\\[\nt \\overset{\\cdot}{\\sim} N\\left(\\frac{\\beta_j \\cdot \\sqrt{n} \\cdot \\sqrt{\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]}}{\\sigma}, 1\\right)\n\\tag{9.2}\\]\nWe can see clearly now how the power of the \\(t\\)-test varies with the effect size \\(\\beta_j\\), the sample size \\(n\\), the degree of collinearity \\(\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]\\), and the noise standard deviation \\(\\sigma\\).\n\n\n9.1.2 Power of the \\(t\\)-test when predictors are added to the model\nAs we know, the outcome of a regression is a function of the predictors that are used. What happens to the \\(t\\)-test \\(p\\)-value for \\(H_0: \\beta_j = 0\\) when a predictor is added to the model? To keep things simple, let’s consider the\n\\[\n\\text{true underlying model:}\\ y = \\beta_0 x_0 + \\beta_1 x_1 + \\epsilon.\n\\]\nLet’s consider the power of testing \\(H_0: \\beta_0 = 0\\) in the regression models\n\\[\n\\text{model 0:}\\ y = \\beta_0 x_0 + \\epsilon \\quad \\text{versus} \\quad \\text{model 1:}\\ y = \\beta_0 x_0 + \\beta_1 x_1 + \\epsilon.\n\\]\nThere are four cases based on \\(\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}]\\) and the value of \\(\\beta_1\\) in the true model:\n\n\\(\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] \\neq 0\\) and \\(\\beta_1 \\neq 0\\). In this case, in model 0 we have omitted an important variable that is correlated with \\(\\boldsymbol{x}_{*0}\\). Therefore, the meaning of \\(\\beta_0\\) differs between model 0 and model 1, so it may not be meaningful to compare the \\(p\\)-values arising from these two models.\n\\(\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] \\neq 0\\) and \\(\\beta_1 = 0\\). In this case, we are adding a null predictor that is correlated with \\(x_{*0}\\). Recall that the power of the \\(t\\)-test hinges on the quantity \\(\\frac{\\beta_j \\cdot \\sqrt{n} \\cdot \\sqrt{\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]}}{\\sigma}\\). Adding the predictor \\(x_1\\) has the effect of reducing the conditional predictor variance \\(\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]\\), therefore reducing the power. This is a case of predictor competition.\n\\(\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] = 0\\) and \\(\\beta_1 \\neq 0\\). In this case, we are adding a non-null predictor that is orthogonal to \\(\\boldsymbol{x}_{*0}\\). While the conditional predictor variance \\(\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]\\) remains the same due to orthogonality, the residual variance \\(\\sigma^2\\) is reduced when going from model 0 to model 1. Therefore, in this case adding \\(x_1\\) to the model increases the power for testing \\(H_0: \\beta_0 = 0\\). This is a case of predictor collaboration.\n\\(\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] = 0\\) and \\(\\beta_1 = 0\\). In this case, we are adding an orthogonal null variable, which does not change the conditional predictor variance or the residual variance, and therefore keeps the power of the test the same.\n\nIn conclusion, adding a predictor can either increase or decrease the power of a \\(t\\)-test.\n\n\n9.1.3 Application: Adjusting for covariates in randomized experiments.\nCase 3 above, i.e., \\(\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] = 0\\) and \\(\\beta_1 \\neq 0\\), arises in the context of randomized experiments in causal inference. In this case, \\(y\\) represents the outcome, \\(x_0\\) represents the treatment, and \\(x_1\\) represents a covariate. Because the treatment is randomized, there is no correlation between \\(x_0\\) and \\(x_1\\). Therefore, it is not necessary to adjust for \\(x_1\\) in order to get an unbiased estimate of the average treatment effect. However, it is known that adjusting for covariates can lead to more precise estimates of the treatment effect due to the phenomenon discussed in case 3 above. This point is also related to the discussion in Chapter 1 about the fact that if \\(x_0\\) and \\(x_1\\) are orthogonal, then the least squares coefficient \\(\\widehat \\beta_0\\) is the same regardless of whether \\(x_1\\) is included in the model. As we see here, either including \\(x_1\\) in the model or adjusting \\(y\\) for \\(x_1\\) is necessary to get better power."
  },
  {
    "objectID": "power.html#the-power-of-an-f-test",
    "href": "power.html#the-power-of-an-f-test",
    "title": "9  Power",
    "section": "9.2 The power of an \\(F\\)-test",
    "text": "9.2 The power of an \\(F\\)-test\nNow let’s turn our attention to computing the power of the \\(F\\)-test. We have\n\\[\n\\begin{split}\nF &= \\frac{\\|\\boldsymbol{X}\\boldsymbol{\\widehat \\beta} - \\boldsymbol{X}_{*, \\text{-}S}\\boldsymbol{\\widehat \\beta}_{-S}\\|^2/|S|}{\\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\widehat \\beta}\\|^2/|n-p|} \\\\\n&= \\frac{\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2/|S|}{\\|(\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{y}\\|^2/|n-p|} \\\\\n&\\approx \\frac{\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2/|S|}{\\sigma^2}.\n\\end{split}\n\\]\nTo calculate the distribution of the numerator, we need to introduce the notion of a non-central chi-squared random variable.\n\nDefinition 9.1 For some vector \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^d\\), suppose \\(\\boldsymbol{z} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{I}_d)\\). Then, we define the distribution of \\(\\|\\boldsymbol{z}\\|^2\\) as the noncentral chi-square random variable with \\(d\\) degrees of freedom and noncentrality parameter \\(\\|\\boldsymbol{\\mu}\\|^2\\) and denote this distribution by \\(\\chi^2_d(\\|\\boldsymbol{\\mu}\\|^2)\\).\n\nThe following proposition states two useful facts about noncentral chi-square distributions.\n\nProposition 9.1 The following two relations hold:\n\nThe mean of a \\(\\chi^2_d(\\|\\boldsymbol{\\mu}\\|^2)\\) random variable is \\(d + \\|\\boldsymbol{\\mu}\\|^2\\).\nIf \\(\\boldsymbol{P}\\) is a projection matrix and \\(\\boldsymbol{y} = \\boldsymbol{\\mu} + \\boldsymbol{\\epsilon}\\), then \\(\\frac{1}{\\sigma^2}\\|\\boldsymbol{P} \\boldsymbol{y}\\|^2 \\sim \\chi^2_{\\text{tr}(\\boldsymbol{P})}\\left(\\frac{1}{\\sigma^2}\\|\\boldsymbol{P} \\boldsymbol{\\mu}\\|^2\\right).\\)\n\n\nIt therefore follows that\n\\[\n\\begin{split}\nF &\\approx \\frac{\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2/|S|}{\\sigma^2} \\\\\n&\\sim \\frac{1}{|S|}\\chi^2_{|S|}\\left(\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S})\\boldsymbol{X} \\boldsymbol{\\beta}\\|^2\\right) \\\\\n&= \\frac{1}{|S|}\\chi^2_{|S|}\\left(\\frac{1}{\\sigma^2}\\|\\boldsymbol{X}^\\perp_{*, S}\\boldsymbol{\\beta}_S\\|^2\\right).\n\\end{split}\n\\]\nAssuming as before that the rows of \\(\\boldsymbol{X}\\) are samples from a joint distribution, we can write\n\\[\n\\|\\boldsymbol{X}^\\perp_{*, S}\\boldsymbol{\\beta}_S\\|^2 \\approx n\\boldsymbol{\\beta}_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S.\n\\]\nTherefore,\n\\[\nF \\overset{\\cdot}{\\sim} \\frac{1}{|S|}\\chi^2_{|S|}\\left(\\frac{n\\beta_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S}{\\sigma^2}\\right)\n\\]\nwhich is similar in spirit to equation (9.2). To get a better sense of what this relationship implies for the power of the \\(F\\)-test, we find from the first part of Proposition 9.1 that, under the alternative,\n\\[\n\\begin{split}\n\\mathbb{E}[F] &\\approx \\mathbb{E}\\left[\\frac{1}{|S|}\\chi^2_{|S|}\\left(\\frac{n\\beta_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S}{\\sigma^2}\\right)\\right] \\\\\n&= 1 + \\frac{n\\beta_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S}{|S| \\cdot \\sigma^2}.\n\\end{split}\n\\]\nBy contrast, under the null, the mean of the \\(F\\)-statistic is 1. The \\(|S|\\) term in the denominator above suggests that testing larger sets of variables explaining the same amount of variation in \\(\\boldsymbol{y}\\) will hurt power. The test must accommodate for the fact that larger sets of variables will explain more of the variability in \\(y\\) even under the null hypothesis."
  },
  {
    "objectID": "confidence-intervals.html#sec-confidence-intervals-univariate",
    "href": "confidence-intervals.html#sec-confidence-intervals-univariate",
    "title": "10  Confidence intervals",
    "section": "10.1 Confidence intervals for univariate quantities",
    "text": "10.1 Confidence intervals for univariate quantities\n\n10.1.1 Confidence interval for a coefficient\nUnder \\(H_0: \\beta_j = 0\\), we showed that \\(\\frac{\\widehat \\beta_j}{\\widehat{\\sigma}/s_j} \\sim t_{n-p}\\). The same argument shows that for arbitrary \\(\\beta_j\\), we have\n\\[\n\\frac{\\widehat \\beta_j - \\beta_j}{\\widehat{\\sigma}/s_j} \\sim t_{n-p}.\n\\]\nWe can use this relationship to construct a confidence interval for \\(\\beta_j\\) as follows:\n\\[\n\\small\n\\begin{split}\n&1-\\alpha \\\\\n&\\quad = \\mathbb{P}[|t_{n-p}| \\leq t_{n-p}(1-\\alpha/2)] \\\\\n&\\quad = \\mathbb{P}\\left[\\left|\\frac{\\widehat \\beta_j - \\beta_j}{\\widehat{\\sigma}/s_j}\\right| \\leq t_{n-p}(1-\\alpha/2) \\right] \\\\\n&\\quad = \\mathbb{P}\\left[\\beta_j \\in \\left[\\widehat \\beta_j - \\frac{\\widehat{\\sigma}}{s_j}t_{n-p}(1-\\alpha/2), \\widehat \\beta_j + \\frac{\\widehat{\\sigma}}{s_j}t_{n-p}(1-\\alpha/2) \\right]\\right] \\\\\n&\\quad \\equiv \\mathbb{P}\\left[\\beta_j \\in \\left[\\widehat \\beta_j - \\text{SE}(\\widehat \\beta_j)t_{n-p}(1-\\alpha/2), \\widehat \\beta_j + \\text{SE}(\\widehat \\beta_j)t_{n-p}(1-\\alpha/2) \\right]\\right] \\\\\n&\\quad \\equiv \\mathbb{P}[\\beta_j \\in \\text{CI}(\\beta_j)].\n\\end{split}\n\\tag{10.1}\\]\nThe confidence interval \\(\\text{CI}(\\beta_j)\\) defined above therefore has \\(1-\\alpha\\) coverage. Because of the duality between confidence intervals and hypothesis tests, the factors contributing to powerful tests (Chapter 9) also lead to shorter confidence intervals.\n\n\n10.1.2 Confidence interval for \\(\\mathbb{E}[y|\\boldsymbol{\\tilde x}]\\)\nSuppose now that we have a new predictor vector \\(\\boldsymbol{\\tilde x} \\in \\mathbb{R}^p\\). The mean of the response for this predictor vector is \\(\\mathbb{E}[y|\\boldsymbol{\\tilde x}] = \\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}\\). Plugging in \\(\\boldsymbol{\\tilde x}\\) for \\(\\boldsymbol{c}\\) in the relation (8.3), we obtain\n\\[\n\\frac{\\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}}{\\widehat{\\sigma} \\sqrt{\\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x}}} \\sim t_{n-p}.\n\\]\nFrom this, we can derive that\n\\[\n\\begin{split}\n\\text{CI}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}) &\\equiv \\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\text{SE}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}}) \\cdot t_{n-p}(1-\\alpha/2) \\\\\n&\\equiv \\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\widehat{\\sigma} \\sqrt{\\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x}} \\cdot t_{n-p}(1-\\alpha/2)\n\\end{split}\n\\tag{10.2}\\]\nis a \\(1-\\alpha\\) confidence interval for \\(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}\\). Consider the special case of the simple linear regression \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\). Then, confidence intervals for \\(\\beta_0 + \\beta_1 \\tilde x\\) for each \\(\\tilde x \\in \\mathbb R\\) sweep out confidence bands for the regression line.\n\n\n\n\n\nWe see that the width of the confidence band appears to be the smallest around the center of the data. To verify this, let \\(\\bar x\\) be the mean of the observed \\(x\\) values. Centering \\(x\\) leads to the following reparameterized regression: \\[\ny = \\beta_0' + \\beta_1 (x - \\bar x) + \\epsilon.\n\\] The width of the confidence interval (10.2) is proportional to the square root of \\(\\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x}\\). Applying this to the centered vector \\(\\boldsymbol{\\tilde x} = (1, \\tilde x - \\bar x)^T\\) and the centered matrix \\(\\boldsymbol{X} = (\\boldsymbol 1, \\boldsymbol x - \\bar x \\boldsymbol 1)\\), we get \\[\n\\begin{split}\n\\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x} &= (1, \\tilde x - \\bar x) \\begin{pmatrix} n & 0 \\\\ 0 & \\sum_i (x_i - \\bar x)^2 \\end{pmatrix}^{-1} \\begin{pmatrix} 1 \\\\ \\tilde x - \\bar x \\end{pmatrix} \\\\\n&= \\frac{1}{n} + \\frac{(\\tilde x - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}.\n\\end{split}\n\\] We see that this quantity is minimized at \\(\\tilde x = \\bar x\\), as expected.\n\n\n10.1.3 Prediction interval for \\(y|\\boldsymbol{\\tilde x}\\)\nInstead of creating a confidence interval for a point on the regression line, we may want to create a confidence interval for a new draw \\(\\tilde y\\) of \\(y\\) for \\(\\boldsymbol{x} = \\boldsymbol{\\tilde x}\\), i.e., a prediction interval. Note that\n\\[\n\\begin{split}\n\\tilde y - \\boldsymbol{\\tilde x}^T \\widehat{\\beta} &= \\boldsymbol{\\tilde x}^T \\beta + \\tilde \\epsilon - \\boldsymbol{\\tilde x}^T \\widehat{\\beta} \\\\\n&= \\tilde \\epsilon + \\boldsymbol{\\tilde x}^T (\\beta-\\widehat{\\beta}) \\\\\n&\\sim N(0, \\sigma^2 + \\sigma^2 \\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x}).\n\\end{split}\n\\]\nTherefore, we have\n\\[\n\\frac{\\tilde y - \\boldsymbol{\\tilde x}^T \\widehat{\\beta}}{\\widehat{\\sigma}\\sqrt{1 + \\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x}}} \\sim t_{n-p},\n\\]\nwhich leads to the \\(1-\\alpha\\) prediction interval\n\\[\n\\begin{split}\n&\\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\widehat{\\sigma} \\sqrt{1+\\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{\\tilde x}} \\cdot t_{n-p}(1-\\alpha/2) \\\\\n&\\quad\\equiv \\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\text{SE}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}}) \\cdot t_{n-p}(1-\\alpha/2).\n\\end{split}\n\\tag{10.3}\\]\nRemark: Prediction with confidence in machine learning.\nThe entire field of supervised machine learning is focused on accurately predicting \\(\\tilde y\\) from \\(\\boldsymbol{\\tilde x}\\), usually using nonlinear functions \\(\\widehat{f}(\\boldsymbol{\\tilde x})\\). In addition to providing a guess for \\(\\tilde y\\), it is often useful to quantify the uncertainty in this guess. In other words, it is useful to come up with a prediction interval (or prediction region) \\(\\text{PI}(\\tilde y)\\) such that\n\\[\n\\mathbb{P}[\\tilde y \\in \\text{PI}(\\tilde y) \\mid \\boldsymbol{\\tilde x}] \\geq 1-\\alpha.\n\\tag{10.4}\\]\nFor example, in safety-critical applications of machine learning like self-driving cars, it is essential to have confidence in predictions. Unfortunately, beyond the realm of linear regression, it is hard to come up with intervals satisfying (10.4) for each point \\(\\boldsymbol{\\tilde x}\\). However, the emerging field of conformal inference provides guarantees on average over possible values of \\(\\boldsymbol{x}\\):\n\\[\n\\mathbb{P}[y \\in \\text{PI}(y)] = \\mathbb{E}[\\mathbb{P}[y \\in \\text{PI}(y) \\mid \\boldsymbol{x}]] \\geq 1-\\alpha.\n\\tag{10.5}\\]\nRemarkably, these guarantees place no assumption on the machine learning method used and require only that the data points on which \\(\\widehat{f}\\) is trained are exchangeable (an even weaker condition than i.i.d.). While the unconditional guarantee (10.5) is weaker than the conditional one (10.4), it can be obtained for modern machine learning and deep learning models."
  },
  {
    "objectID": "confidence-intervals.html#confidence-regions-and-simultaneous-intervals",
    "href": "confidence-intervals.html#confidence-regions-and-simultaneous-intervals",
    "title": "10  Confidence intervals",
    "section": "10.2 Confidence regions and simultaneous intervals",
    "text": "10.2 Confidence regions and simultaneous intervals\n\n10.2.1 Confidence regions\nA multivariate generalization of a confidence interval is a confidence region. We will discuss the construction of a confidence region for \\(\\boldsymbol{\\beta}\\) in the linear regression model. A \\(1-\\alpha\\) confidence region for \\(\\boldsymbol{\\beta}\\) is a set \\(\\text{CR}(\\boldsymbol{\\beta}) \\subseteq \\mathbb R^p\\) such that \\[\n\\mathbb{P}[\\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})] \\geq 1-\\alpha.\n\\]\nTo construct such a region, note first that\n\\[\n\\frac{\\frac{1}{p}\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2}{\\widehat{\\sigma}^2} \\sim F_{p, n-p}.\n\\]\nHence, we have\n\\[\n\\mathbb{P}[\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 \\leq p \\widehat{\\sigma}^2 F_{p, n-p}(1-\\alpha)] \\geq 1-\\alpha.\n\\]\nHence, the region\n\\[\n\\text{CR}(\\boldsymbol{\\beta}) \\equiv \\{\\boldsymbol{\\beta}: (\\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{\\beta})^T \\boldsymbol{X}^T \\boldsymbol{X} (\\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{\\beta})  \\leq p \\widehat{\\sigma}^2 F_{p, n-p}(1-\\alpha)\\} \\subseteq \\mathbb{R}^p\n\\]\nis a \\(1-\\alpha\\) confidence region for the vector \\(\\boldsymbol{\\beta}\\). It’s easy to see that \\(\\text{CR}(\\boldsymbol{\\beta})\\) is an ellipse centered at \\(\\boldsymbol{\\widehat{\\beta}}\\).\n\n\n\nFigure 10.1: Confidence region for \\(\\boldsymbol \\beta\\).\n\n\n\n\n10.2.2 Simultaneous intervals\nAs a byproduct of confidence regions for the multivariate \\(\\boldsymbol \\beta\\), we can construct simultaneous intervals for univariate quantities. To motivate the definition of simultaneous intervals, note that the intervals in Section 10.1 have pointwise coverage. For example, we have\n\\[\n\\mathbb{P}[\\beta_j \\in \\text{CI}(\\beta_j)] \\geq 1-\\alpha \\quad \\text{for each } j.\n\\]\nor\n\\[\n\\mathbb{P}[\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta} \\in \\text{CI}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta})] \\geq 1-\\alpha \\quad \\text{for each } \\boldsymbol{\\tilde x}.\n\\]\nSometimes a stronger simultaneous coverage guarantee is desired, e.g.,\n\\[\n\\mathbb{P}[\\beta_j \\in \\text{CI}^{\\text{sim}}(\\beta_j) \\ \\text{for each } j] \\geq 1-\\alpha\n\\tag{10.6}\\]\nor\n\\[\n\\mathbb{P}[\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta} \\in \\text{CI}^{\\text{sim}}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}) \\ \\text{for each } \\boldsymbol{\\tilde x}] \\geq 1-\\alpha.\n\\tag{10.7}\\]\nTo obtain such simultaneous confidence intervals, we can leverage the fact that the confidence region \\(\\text{CR}(\\boldsymbol \\beta)\\) is for the entire vector \\(\\boldsymbol{\\beta}\\). We can therefore define\n\\[\n\\text{CI}^{\\text{sim}}(\\beta_j) \\equiv \\{\\beta_j: \\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta}) \\}.\n\\]\nThen, these confidence intervals will satisfy the simultaneous coverage property (10.6). We will obtain a more explicit expression for \\(\\text{CI}^{\\text{sim}}(\\beta_j)\\) shortly.\n\n\n\nFigure 10.2: Confidence region and simultaneous and pointwise confidence intervals.\n\n\nSimilarly, we may define the simultaneous confidence regions\n\\[\n\\text{CI}^{\\text{sim}}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}) \\equiv \\{\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta}: \\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})\\}.\n\\]\nLet us find a more explicit expression for the latter interval. For notational ease, let us define \\(\\boldsymbol{\\Sigma} \\equiv \\boldsymbol{X}^T \\boldsymbol{X}\\). Then, note that if \\(\\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})\\), then by the Cauchy-Schwarz inequality we have\n\\[\n\\begin{split}\n(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta})^2 &= \\|\\boldsymbol{\\tilde x}^T (\\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\beta})\\|^2 \\\\\n&= \\|(\\boldsymbol{\\Sigma}^{-1/2}\\boldsymbol{\\tilde x})^T \\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\beta})\\|^2 \\\\\n&\\leq \\|(\\boldsymbol{\\Sigma}^{-1/2}\\boldsymbol{\\tilde x})\\|^2\\|\\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\beta})\\|^2 \\\\\n&\\leq \\boldsymbol{\\tilde x}^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\tilde x} p \\widehat{\\sigma}^2 F_{p, n-p}(1-\\alpha),\n\\end{split}\n\\]\ni.e.,\n\\[\n\\begin{split}\n\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta} &\\in \\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\widehat{\\sigma} \\sqrt{\\boldsymbol{\\tilde x}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{\\tilde x}} \\sqrt{pF_{p, n-p}(1-\\alpha)} \\\\\n&\\equiv \\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\text{SE}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\widehat{\\beta}})\\cdot\\sqrt{pF_{p, n-p}(1-\\alpha)}.\n\\end{split}\n\\tag{10.8}\\]\nDefining the above interval as \\(\\text{CI}^{\\text{sim}}(\\boldsymbol{\\tilde x}^T \\boldsymbol{\\beta})\\) gives us the simultaneous coverage property (10.7). These simultaneous intervals are called Working-Hotelling intervals. Comparing to equation (10.3), we see that the simultaneous interval is the pointwise interval expanded by a factor of \\(\\sqrt{pF_{p, n-p}(1-\\alpha)}/t_{n-p}(1-\\alpha/2)\\). In the case of simple linear regression, we can obtain simultaneous confidence bands (Working-Hotelling bands) for the regression line.\n\n\n\n\n\nSpecializing to the case \\(\\boldsymbol{\\tilde x} \\equiv \\boldsymbol{e_j}\\), we get an expression for the simultaneous intervals for each coordinate:\n\\[\n\\begin{split}\n\\text{CI}^{\\text{sim}}(\\beta_j) &\\equiv \\widehat \\beta_j \\pm \\widehat{\\sigma} \\sqrt{(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}_{jj}} \\sqrt{pF_{p, n-p}(1-\\alpha)} \\\\\n&\\equiv \\text{SE}(\\widehat \\beta_j)\\sqrt{pF_{p, n-p}(1-\\alpha)},\n\\end{split}\n\\tag{10.9}\\]\nwhich again is the pointwise interval (10.1) expanded by a factor of \\(\\sqrt{pF_{p, n-p}(1-\\alpha)}/t_{n-p}(1-\\alpha/2)\\)."
  },
  {
    "objectID": "practical-considerations.html#practical-versus-statistical-significance",
    "href": "practical-considerations.html#practical-versus-statistical-significance",
    "title": "11  Practical considerations",
    "section": "11.1 Practical versus statistical significance",
    "text": "11.1 Practical versus statistical significance\nYou can have a statistically significant effect that is not practically significant. The hypothesis testing framework is most useful in the case when the signal-to-noise ratio is relatively small. Otherwise, constructing a confidence interval for the effect size is a more meaningful approach."
  },
  {
    "objectID": "practical-considerations.html#correlation-versus-causation-and-simpsons-paradox",
    "href": "practical-considerations.html#correlation-versus-causation-and-simpsons-paradox",
    "title": "11  Practical considerations",
    "section": "11.2 Correlation versus causation, and Simpson’s paradox",
    "text": "11.2 Correlation versus causation, and Simpson’s paradox\nCausation can be elusive for several reasons. One is reverse causation, where it is not clear whether \\(X\\) causes \\(Y\\) or \\(Y\\) causes \\(X\\). Another is confounding, where there is a third variable \\(Z\\) that causes both \\(X\\) and \\(Y\\). For the latter reason, linear regression coefficients can be sensitive to the choice of other predictors to include and can be misleading if you omit important variables from the regression. A special and sometimes overlooked case of this is Simpson’s paradox, where an important discrete variable is omitted. Consider the example in Figure 11.1. Sometimes this discrete variable may seem benign, such as the year in which the data was collected. Such variables might or might not be measured.\n\n\n\nFigure 11.1: An example of Simpson’s paradox (source: Wikipedia)."
  },
  {
    "objectID": "practical-considerations.html#dealing-with-correlated-predictors",
    "href": "practical-considerations.html#dealing-with-correlated-predictors",
    "title": "11  Practical considerations",
    "section": "11.3 Dealing with correlated predictors",
    "text": "11.3 Dealing with correlated predictors\nIt depends on the goal. If we’re trying to tease apart effects of correlated predictors, then we have no choice but to proceed as usual despite lower power. Otherwise, we can test predictors in groups via the \\(F\\)-test to get higher power at the cost of lower “resolution.” Sometimes, it is recommended to simply remove predictors that are correlated with other predictors. This practice, however, is somewhat arbitrary and not recommended."
  },
  {
    "objectID": "practical-considerations.html#model-selection",
    "href": "practical-considerations.html#model-selection",
    "title": "11  Practical considerations",
    "section": "11.4 Model selection",
    "text": "11.4 Model selection\nWe need to ask ourselves: Why do we want to do model selection? It can either be for prediction purposes or for inferential purposes. If it is for prediction purposes, then we can apply cross-validation to select a model and we don’t need to think very hard about statistical significance. If it is for inference, then we need to be more careful. There are various classical model selection criteria (e.g., AIC, BIC), but it is not entirely clear what statistical guarantee we are getting for the resulting models. A simpler approach is to apply a \\(t\\)-test for each variable in the model, apply a multiple testing correction to the resulting \\(p\\)-values, and report the set of significant variables and the associated guarantee. Re-fitting the linear regression after model selection leads us into some dicey inferential territory due to selection bias. This is the subject of ongoing research, and the jury is still out on the best way of doing this."
  },
  {
    "objectID": "r-demo-part-2.html#exploration",
    "href": "r-demo-part-2.html#exploration",
    "title": "12  R demo",
    "section": "12.1 Exploration",
    "text": "12.1 Exploration\nLet’s first do a bit of exploration:\n\n# visualize distribution of housing prices, superimposing the mean\nhouses_data |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(color = \"black\", bins = 30) +\n  geom_vline(aes(xintercept = mean(price)),\n    colour = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n# compare median and mean price\nhouses_data |&gt;\n  summarise(\n    mean_price = mean(price),\n    median_price = median(price)\n  )\n\n# A tibble: 1 × 2\n  mean_price median_price\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       155.         133.\n\n\n\n# create a pairs plot of continuous variables\nhouses_data |&gt;\n  select(price, size, taxes) |&gt;\n  ggpairs()\n\n\n\n\n\n# see how price relates to beds\nhouses_data |&gt;\n  ggplot(aes(x = factor(beds), y = price)) +\n  geom_boxplot(fill = \"dodgerblue\")\n\n\n\n\n\n# see how price relates to baths\nhouses_data |&gt;\n  ggplot(aes(x = factor(baths), y = price)) +\n  geom_boxplot(fill = \"dodgerblue\")\n\n\n\n\n\n# see how price relates to new\nhouses_data |&gt;\n  ggplot(aes(x = factor(new), y = price)) +\n  geom_boxplot(fill = \"dodgerblue\")"
  },
  {
    "objectID": "r-demo-part-2.html#hypothesis-testing",
    "href": "r-demo-part-2.html#hypothesis-testing",
    "title": "12  R demo",
    "section": "12.2 Hypothesis testing",
    "text": "12.2 Hypothesis testing\nLet’s run a linear regression and interpret the summary. But first, we must decide whether to model beds/baths as categorical or continuous? We should probably model these as categorical, given the potentially nonlinear trend observed in the box plots.\n\nlm_fit &lt;- lm(price ~ factor(beds) + factor(baths) + new + size,\n  data = houses_data\n)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = price ~ factor(beds) + factor(baths) + new + size, \n    data = houses_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-179.306  -32.037   -2.899   19.115  152.718 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -19.26307   18.01344  -1.069 0.287730    \nfactor(beds)3   -16.46430   15.04669  -1.094 0.276749    \nfactor(beds)4   -12.48561   21.12357  -0.591 0.555936    \nfactor(beds)5  -101.14581   55.83607  -1.811 0.073366 .  \nfactor(baths)2    2.39872   15.44014   0.155 0.876885    \nfactor(baths)3   -0.70410   26.45512  -0.027 0.978825    \nfactor(baths)4  273.20079   83.65764   3.266 0.001540 ** \nnew              66.94940   18.50445   3.618 0.000487 ***\nsize              0.10882    0.01234   8.822 7.46e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.17 on 91 degrees of freedom\nMultiple R-squared:  0.7653,    Adjusted R-squared:  0.7446 \nF-statistic: 37.08 on 8 and 91 DF,  p-value: &lt; 2.2e-16\n\n\nWe can read off the test statistics and \\(p\\)-values for each variable from the regression summary, as well as for the \\(F\\)-test against the constant model from the bottom of the summary.\nLet’s use an \\(F\\)-test to assess whether the categorical baths variable is important.\n\nlm_fit_partial &lt;- lm(price ~ factor(beds) + new + size,\n  data = houses_data\n)\nanova(lm_fit_partial, lm_fit)\n\nAnalysis of Variance Table\n\nModel 1: price ~ factor(beds) + new + size\nModel 2: price ~ factor(beds) + factor(baths) + new + size\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     94 273722                                \n2     91 238289  3     35433 4.5104 0.005374 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat if we had not coded baths as a factor?\n\nlm_fit_not_factor &lt;- lm(price ~ factor(beds) + baths + new + size,\n  data = houses_data\n)\nanova(lm_fit_partial, lm_fit_not_factor)\n\nAnalysis of Variance Table\n\nModel 1: price ~ factor(beds) + new + size\nModel 2: price ~ factor(beds) + baths + new + size\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     94 273722                           \n2     93 273628  1     94.33 0.0321 0.8583\n\n\nIf we want to test for the equality of means across groups of a categorical predictor, without adjusting for other variables, we can use the ANOVA \\(F\\)-test. There are several equivalent ways of doing so:\n\n# just use the summary function\nlm_fit_baths &lt;- lm(price ~ factor(baths), data = houses_data)\nsummary(lm_fit_baths)\n\n\nCall:\nlm(formula = price ~ factor(baths), data = houses_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-146.44  -45.88   -7.89   22.22  352.01 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       90.21      19.51   4.624 1.17e-05 ***\nfactor(baths)2    57.68      21.72   2.656  0.00927 ** \nfactor(baths)3   174.52      31.13   5.607 1.97e-07 ***\nfactor(baths)4   496.79      82.77   6.002 3.45e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 80.44 on 96 degrees of freedom\nMultiple R-squared:  0.3881,    Adjusted R-squared:  0.369 \nF-statistic:  20.3 on 3 and 96 DF,  p-value: 2.865e-10\n\n# use the anova function as before\nlm_fit_const &lt;- lm(price ~ 1, data = houses_data)\nanova(lm_fit_const, lm_fit_baths)\n\nAnalysis of Variance Table\n\nModel 1: price ~ 1\nModel 2: price ~ factor(baths)\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     99 1015150                                  \n2     96  621130  3    394020 20.299 2.865e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# use the aov function\naov_fit &lt;- aov(price ~ factor(baths), data = houses_data)\nsummary(aov_fit)\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(baths)  3 394020  131340    20.3 2.86e-10 ***\nResiduals     96 621130    6470                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can also use an \\(F\\)-test to test for the presence of an interaction with a multi-class categorical predictor.\n\nlm_fit_interaction &lt;- lm(price ~ size * factor(beds), data = houses_data)\nsummary(lm_fit_interaction)\n\n\nCall:\nlm(formula = price ~ size * factor(beds), data = houses_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-232.643  -25.938   -0.942   19.172  155.517 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          50.12619   48.22282   1.039 0.301310    \nsize                  0.05037    0.04210   1.197 0.234565    \nfactor(beds)3      -103.85734   52.20373  -1.989 0.049620 *  \nfactor(beds)4      -143.90213   67.31359  -2.138 0.035185 *  \nfactor(beds)5      -507.88205  144.10191  -3.524 0.000663 ***\nsize:factor(beds)3    0.07589    0.04368   1.738 0.085633 .  \nsize:factor(beds)4    0.09234    0.04704   1.963 0.052638 .  \nsize:factor(beds)5    0.21147    0.05957   3.550 0.000609 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53.35 on 92 degrees of freedom\nMultiple R-squared:  0.7421,    Adjusted R-squared:  0.7225 \nF-statistic: 37.81 on 7 and 92 DF,  p-value: &lt; 2.2e-16\n\nlm_fit_size &lt;- lm(price ~ size + factor(beds), data = houses_data)\nanova(lm_fit_size, lm_fit_interaction)\n\nAnalysis of Variance Table\n\nModel 1: price ~ size + factor(beds)\nModel 2: price ~ size * factor(beds)\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     95 300953                                \n2     92 261832  3     39121 4.5819 0.004905 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nContrasts of regression coefficients can be tested using the glht() function from the multcomp package."
  },
  {
    "objectID": "r-demo-part-2.html#confidence-intervals",
    "href": "r-demo-part-2.html#confidence-intervals",
    "title": "12  R demo",
    "section": "12.3 Confidence intervals",
    "text": "12.3 Confidence intervals\nWe can construct pointwise confidence intervals for each coefficient using confint():\n\nconfint(lm_fit)\n\n                       2.5 %      97.5 %\n(Intercept)     -55.04455734  16.5184161\nfactor(beds)3   -46.35270691  13.4241025\nfactor(beds)4   -54.44498235  29.4737689\nfactor(beds)5  -212.05730801   9.7656895\nfactor(baths)2  -28.27123130  33.0686620\nfactor(baths)3  -53.25394742  51.8457394\nfactor(baths)4  107.02516067 439.3764122\nnew              30.19258305 103.7062177\nsize              0.08431972   0.1333284\n\n\nTo create simultaneous confidence intervals, we need a somewhat more manual approach. We start with the coefficients and standard errors:\n\ncoef(summary(lm_fit))\n\n                   Estimate  Std. Error     t value     Pr(&gt;|t|)\n(Intercept)     -19.2630706 18.01344052 -1.06937209 2.877304e-01\nfactor(beds)3   -16.4643022 15.04669172 -1.09421410 2.767490e-01\nfactor(beds)4   -12.4856067 21.12356937 -0.59107467 5.559357e-01\nfactor(beds)5  -101.1458092 55.83607248 -1.81147786 7.336590e-02\nfactor(baths)2    2.3987153 15.44014266  0.15535578 8.768849e-01\nfactor(baths)3   -0.7041040 26.45511871 -0.02661504 9.788251e-01\nfactor(baths)4  273.2007864 83.65764044  3.26570036 1.540093e-03\nnew              66.9494004 18.50445029  3.61801617 4.872475e-04\nsize              0.1088241  0.01233621  8.82151661 7.460814e-14\n\n\nThen we add lower and upper confidence interval endpoints based on the formula (10.9):\n\nalpha &lt;- 0.05\nn &lt;- nrow(houses_data)\np &lt;- length(coef(lm_fit))\nf_quantile &lt;- qf(1 - alpha, df1 = p, df2 = n - p)\ncoef(summary(lm_fit)) |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"Variable\") |&gt;\n  select(Variable, Estimate, `Std. Error`) |&gt;\n  mutate(\n    CI_lower = Estimate - `Std. Error` * sqrt(p * f_quantile),\n    CI_upper = Estimate + `Std. Error` * sqrt(p * f_quantile)\n  )\n\n        Variable     Estimate  Std. Error      CI_lower    CI_upper\n1    (Intercept)  -19.2630706 18.01344052  -95.38917389  56.8630327\n2  factor(beds)3  -16.4643022 15.04669172  -80.05271036  47.1241059\n3  factor(beds)4  -12.4856067 21.12356937 -101.75533960  76.7841262\n4  factor(beds)5 -101.1458092 55.83607248 -337.11309238 134.8214739\n5 factor(baths)2    2.3987153 15.44014266  -62.85244495  67.6498756\n6 factor(baths)3   -0.7041040 26.45511871 -112.50535022 111.0971422\n7 factor(baths)4  273.2007864 83.65764044  -80.34245635 626.7440292\n8            new   66.9494004 18.50445029  -11.25174573 145.1505465\n9           size    0.1088241  0.01233621    0.05669037   0.1609578\n\n\nNote that the simultaneous intervals are substantially larger.\nTo construct pointwise confidence intervals for the fit, we can use the predict() function:\n\npredict(lm_fit, newdata = houses_data, interval = \"confidence\") |&gt; head()\n\n        fit       lwr      upr\n1 193.52176 165.22213 221.8214\n2  79.98449  51.91430 108.0547\n3 150.64507 122.28397 179.0062\n4 191.71955 172.27396 211.1651\n5 124.30169  81.34488 167.2585\n6 376.74308 333.44559 420.0406\n\n\nTo get pointwise prediction intervals, we switch \"confidence\" to \"prediction\":\n\npredict(lm_fit, newdata = houses_data, interval = \"prediction\") |&gt; head()\n\n        fit       lwr      upr\n1 193.52176  88.00908 299.0344\n2  79.98449 -25.46688 185.4359\n3 150.64507  45.11589 256.1743\n4 191.71955  88.22951 295.2096\n5 124.30169  13.95069 234.6527\n6 376.74308 266.25901 487.2271\n\n\nTo construct simultaneous confidence intervals for the fit or predictions, we again need a slightly more manual approach. We call predict() again, but this time asking it for the standard errors rather than the confidence intervals:\n\npredictions &lt;- predict(lm_fit, newdata = houses_data, se.fit = TRUE)\nhead(predictions$fit)\n\n        1         2         3         4         5         6 \n193.52176  79.98449 150.64507 191.71955 124.30169 376.74308 \n\nhead(predictions$se.fit)\n\n        1         2         3         4         5         6 \n14.246855 14.131352 14.277804  9.789472 21.625709 21.797212 \n\n\nNow we can construct the simultaneous confidence intervals via the formula (10.8):\n\nf_quantile &lt;- qf(1 - alpha, df1 = p, df2 = n - p)\ntibble(\n  lower = predictions$fit - predictions$se.fit * sqrt(p * f_quantile),\n  upper = predictions$fit + predictions$se.fit * sqrt(p * f_quantile)\n)\n\n# A tibble: 100 × 2\n   lower upper\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 133.   254.\n 2  20.3  140.\n 3  90.3  211.\n 4 150.   233.\n 5  32.9  216.\n 6 285.   469.\n 7  82.8  145.\n 8 188.   331.\n 9 371.   803.\n10  57.3  128.\n# ℹ 90 more rows\n\n\nIn the case of simple linear regression, we can plot these pointwise and simultaneous confidence intervals as bands:\n\n# to produce confidence intervals for fits in general, use the predict() function\nn &lt;- nrow(houses_data)\np &lt;- 2\nalpha &lt;- 0.05\nlm_fit &lt;- lm(price ~ size, data = houses_data)\npredictions &lt;- predict(lm_fit, se.fit = TRUE)\nt_quantile &lt;- qt(1 - alpha / 2, df = n - p)\nf_quantile &lt;- qf(1 - alpha, df1 = p, df2 = n - p)\nhouses_data |&gt;\n  mutate(\n    fit = predictions$fit,\n    se = predictions$se.fit,\n    ptwise_width = t_quantile * se,\n    simultaneous_width = sqrt(p * f_quantile) * se\n  ) |&gt;\n  ggplot(aes(x = size)) +\n  geom_point(aes(y = price)) +\n  geom_line(aes(y = fit), color = \"blue\") +\n  geom_line(aes(y = fit + ptwise_width, color = \"Pointwise\")) +\n  geom_line(aes(y = fit - ptwise_width, color = \"Pointwise\")) +\n  geom_line(aes(y = fit + simultaneous_width, color = \"Simultaneous\")) +\n  geom_line(aes(y = fit - simultaneous_width, color = \"Simultaneous\")) +\n  theme(legend.title = element_blank(), legend.position = \"bottom\")"
  },
  {
    "objectID": "r-demo-part-2.html#predictor-competition-and-collaboration",
    "href": "r-demo-part-2.html#predictor-competition-and-collaboration",
    "title": "12  R demo",
    "section": "12.4 Predictor competition and collaboration",
    "text": "12.4 Predictor competition and collaboration\nLet’s look at the power of detecting the association between price and beds. We can imagine that beds and baths are correlated:\n\nhouses_data |&gt;\n  ggplot(aes(x = beds, y = baths)) +\n  geom_count()\n\n\n\n\nSo let’s see how significant beds is, with and without baths in the model:\n\nlm_fit_only_beds &lt;- lm(price ~ factor(beds), data = houses_data)\nsummary(lm_fit_only_beds)\n\n\nCall:\nlm(formula = price ~ factor(beds), data = houses_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-234.35  -50.63  -15.69   24.56  365.86 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     105.94      21.48   4.931 3.43e-06 ***\nfactor(beds)3    44.69      24.47   1.827 0.070849 .  \nfactor(beds)4   105.70      32.35   3.268 0.001504 ** \nfactor(beds)5   246.71      69.62   3.544 0.000611 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 93.65 on 96 degrees of freedom\nMultiple R-squared:  0.1706,    Adjusted R-squared:  0.1447 \nF-statistic: 6.583 on 3 and 96 DF,  p-value: 0.0004294\n\n\n\nlm_fit_only_baths &lt;- lm(price ~ factor(baths), data = houses_data)\nlm_fit_beds_baths &lt;- lm(price ~ factor(beds) + factor(baths), data = houses_data)\nanova(lm_fit_only_baths, lm_fit_beds_baths)\n\nAnalysis of Variance Table\n\nModel 1: price ~ factor(baths)\nModel 2: price ~ factor(beds) + factor(baths)\n  Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  \n1     96 621130                             \n2     93 572436  3     48693 2.637 0.05424 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see that the significance of beds dropped by two orders of magnitude. This is an example of predictor competition.\nOn the other hand, note that the variable new is not very correlated with beds:\n\nlm_fit &lt;- lm(new ~ beds, data = houses_data)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = new ~ beds, data = houses_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.15762 -0.11000 -0.11000 -0.08619  0.91381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.03857    0.14950   0.258    0.797\nbeds         0.02381    0.04871   0.489    0.626\n\nResidual standard error: 0.3157 on 98 degrees of freedom\nMultiple R-squared:  0.002432,  Adjusted R-squared:  -0.007747 \nF-statistic: 0.2389 on 1 and 98 DF,  p-value: 0.6261\n\n\nbut we know it has a substantial impact on price. Let’s look at the significance of the test that beds is not important when we add new to the model.\n\nlm_fit_only_new &lt;- lm(price ~ new, data = houses_data)\nlm_fit_beds_new &lt;- lm(price ~ new + factor(beds), data = houses_data)\nanova(lm_fit_only_new, lm_fit_beds_new)\n\nAnalysis of Variance Table\n\nModel 1: price ~ new\nModel 2: price ~ new + factor(beds)\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     98 787781                                  \n2     95 619845  3    167936 8.5795 4.251e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAdding new to the model made the \\(p\\)-value more significant by a factor of 10. This is an example of predictor collaboration."
  },
  {
    "objectID": "linear-models-misspecification.html",
    "href": "linear-models-misspecification.html",
    "title": "Linear models: Misspecification",
    "section": "",
    "text": "In our discussion of linear model inference in Unit 2, we assumed the normal linear model throughout:\n\\[\n\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \\text{where} \\ \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I}_n).\n\\]\nIn this unit, we will discuss what happens when this model is misspecified:\n\nNon-normality (Section 13.1): \\(\\boldsymbol{\\epsilon} \\sim (0, \\sigma^2 \\boldsymbol{I}_n)\\) but not \\(N(0, \\sigma^2 \\boldsymbol{I}_n)\\).\nHeteroskedastic and/or correlated errors (Section 13.2): \\(\\boldsymbol{\\epsilon} \\sim (0, \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\Sigma} \\neq \\sigma^2 \\boldsymbol{I}\\). This includes the case of heteroskedastic errors (\\(\\boldsymbol{\\Sigma}\\) is diagonal but not a constant multiple of the identity) and correlated errors (\\(\\boldsymbol{\\Sigma}\\) is not diagonal).\nModel bias (Section 13.3): It is not the case that \\(\\mathbb{E}[\\boldsymbol{y}] = \\boldsymbol{X} \\boldsymbol{\\beta}\\) for some \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\).\nOutliers (Section 13.4): For one or more \\(i\\), it is not the case that \\(y_i \\sim N(\\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}, \\sigma^2)\\).\n\nFor each type of misspecification, we will discuss its origins, consequences, detection, and fixes (Section 13.1-Section 13.4). We then discuss methodological approaches to address model misspecification, including asymptotic robust inference methods (Chapter 14), the bootstrap (Chapter 15), the permutation test (Chapter 16), and robust estimation (Chapter 17). We conclude with an R demo (Chapter 18)."
  },
  {
    "objectID": "misspecification-overview.html#sec-non-normality",
    "href": "misspecification-overview.html#sec-non-normality",
    "title": "13  Overview",
    "section": "13.1 Non-normality",
    "text": "13.1 Non-normality\n\n13.1.1 Origin\nNon-normality occurs when the distribution of \\(y|\\boldsymbol{x}\\) is either skewed or has heavier tails than the normal distribution. This may happen, for example, if there is some discreteness in \\(y\\).\n\n\n13.1.2 Consequences\nNon-normality is the most benign of linear model misspecifications. While we derived linear model inferences under the normality assumption, all the corresponding statements hold asymptotically without this assumption. Recall Homework 2 Question 1, or take for example the simpler problem of estimating the mean \\(\\mu\\) of a distribution based on \\(n\\) samples from it: We can test \\(H_0: \\mu = 0\\) and build a confidence interval for \\(\\mu\\) even if the underlying distribution is not normal. So if \\(n\\) is relatively large and \\(p\\) is relatively small, you need not worry too much. If \\(n\\) is small and the errors are highly skewed or heavy-tailed, we may have issues with incorrect standard errors.\n\n\n13.1.3 Detection\nNon-normality is a property of the error terms \\(\\epsilon_i\\). We do not observe these directly, but we can approximate them using the residuals:\n\\[\n\\widehat{\\epsilon}_i = y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat{\\beta}}.\n\\]\nRecall from equation (7.2) that \\(\\text{Var}[\\boldsymbol{\\widehat{\\epsilon}}] = \\sigma^2(\\boldsymbol{I} - \\boldsymbol{H})\\). Letting \\(h_i\\) be the \\(i\\)th diagonal entry of \\(\\boldsymbol{H}\\), it follows that \\(\\widehat{\\epsilon}_i \\sim (0, \\sigma^2(1-h_i))\\). The standardized residuals are defined as:\n\\[\nr_i = \\frac{\\widehat{\\epsilon}_i}{\\widehat{\\sigma} \\sqrt{1-h_i}}.\n\\tag{13.1}\\]\nUnder normality, we would expect \\(r_i \\overset{\\cdot}{\\sim} N(0,1)\\). We can therefore assess normality by producing a histogram or normal QQ-plot of these residuals.\n\n\n\n\n\n\n\n13.1.4 Fixes\nAs mentioned above, non-normality is not necessarily a problem that needs to be fixed, except in small samples. In small samples (but not too small!), we can apply the residual bootstrap for robust standard error computation and/or robust hypothesis testing."
  },
  {
    "objectID": "misspecification-overview.html#sec-heteroskedasticity",
    "href": "misspecification-overview.html#sec-heteroskedasticity",
    "title": "13  Overview",
    "section": "13.2 Heteroskedastic and correlated errors",
    "text": "13.2 Heteroskedastic and correlated errors\n\n13.2.1 Origin\nHeteroskedasticity can arise as follows. Suppose each observation \\(y_i\\) is actually the average of \\(n_i\\) underlying observations, each with variance \\(\\sigma^2\\). Then, the variance of \\(y_i\\) is \\(\\sigma^2/n_i\\), which will differ across \\(i\\) if \\(n_i\\) differ. It is also common to see the variance of a distribution increase as the mean increases (as in Figure 13.1), whereas for a linear model the variance of \\(y\\) stays constant as the mean of \\(y\\) varies.\nCorrelated errors can arise when observations have group, spatial, or temporal structure. Below are examples:\n\nGroup/clustered structure: We have 10 samples \\((\\boldsymbol{x}_{i*}, y_i)\\) each from 100 schools.\nSpatial structure: We have 100 soil samples from a \\(10\\times10\\) grid on a 1km \\(\\times\\) 1km field.\nTemporal structure: We have 366 COVID positivity rate measurements, one from each day of the year 2020.\n\nThe issue arises because there are common sources of variation among samples that are in the same group or spatially/temporally close to one another.\n\n\n13.2.2 Consequences\nAll normal linear model inference from Unit 2 hinges on the assumption that \\(\\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I})\\). If instead of \\(\\sigma^2 \\boldsymbol{I}\\) we have \\(\\text{Var}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\) for some matrix \\(\\boldsymbol{\\Sigma}\\), then we may suffer two consequences: wrong inference (in terms of confidence interval coverage and hypothesis test levels) and inefficient inference (in terms of confidence interval width and hypothesis test power). One way of seeing the consequence of heteroskedasticity for confidence interval coverage is the width of prediction intervals; see Figure 13.1 for intuition.\n\n\n\nFigure 13.1: Heteroskedasticity in a simple bivariate linear model (image source: source).\n\n\nLike with heteroskedastic errors, correlated errors can cause invalid standard errors. In particular, positively correlated errors typically cause standard errors to be smaller than they should be, leading to inflated Type-I error rates. For intuition, consider estimating the mean of a distribution based on \\(n\\) samples. Consider the cases when these samples are independent, compared to when they are perfectly correlated. The effective sample size in the former case is \\(n\\) and in the latter case is 1.\n\n\n13.2.3 Detection\nHeteroskedasticity is usually assessed via the residual plot (Figure 13.2). In this plot, the standardized residuals \\(r_i\\) (13.1) are plotted against the fitted values \\(\\widehat{\\mu}_i\\). In the absence of heteroskedasticity, the spread of the points around the origin should be roughly constant as a function of \\(\\widehat{\\mu}\\) (Figure 13.2(a)). A common sign of heteroskedasticity is the fan shape where variance increases as a function of \\(\\widehat{\\mu}\\) (Figure 13.2(c)).\n\n\n\nFigure 13.2: Residuals plotted against linear-model fitted values that reflect (a) model adequacy, (b) quadratic rather than linear relationship, and (c) nonconstant variance (image source: Agresti Figure 2.8).\n\n\nResidual plots once again come in handy to detect correlated errors. Instead of plotting the standardized residuals against the fitted values, we should plot the residuals against whatever variables we think might explain variation in the response that the regression does not account for. In the presence of group structures, we can plot residuals versus group (via a boxplot); in the presence of spatial or temporal structure, we can plot residuals as a function of space or time. If the residuals show a dependency on these variables, this suggests they are correlated. This dependency can be checked via formal means as well, e.g., via an ANOVA test in the case of groups or by estimating the autocorrelation function in the case of temporal structure."
  },
  {
    "objectID": "misspecification-overview.html#sec-model-bias",
    "href": "misspecification-overview.html#sec-model-bias",
    "title": "13  Overview",
    "section": "13.3 Model bias",
    "text": "13.3 Model bias\n\n13.3.1 Origin\nModel bias arises when predictors are left out of the regression model:\n\\[\n\\text{assumed model: } \\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}; \\quad \\text{actual model: } \\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{Z} \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}.\n\\tag{13.2}\\]\nWe may not always know about or measure all the variables that impact a response \\(\\boldsymbol{y}\\).\nModel bias can also arise when the predictors do not impact the response on the linear scale. For example:\n\\[\n\\text{assumed model: } \\mathbb{E}[\\boldsymbol{y}] = \\boldsymbol{X} \\boldsymbol{\\beta}; \\quad \\text{actual model: } g(\\mathbb{E}[\\boldsymbol{y}]) = \\boldsymbol{X} \\boldsymbol{\\beta}.\n\\tag{13.3}\\]\n\n\n13.3.2 Consequences\nIn cases of model bias, the parameters \\(\\boldsymbol{\\beta}\\) in the assumed linear model lose their meanings. The least squares estimate \\(\\boldsymbol{\\widehat{\\beta}}\\) will be a biased estimate for the parameter we probably actually want to estimate. In the case (13.2) when predictors are left out of the regression model, these additional predictors \\(\\boldsymbol{Z}\\) will act as confounders and create bias in \\(\\boldsymbol{\\widehat{\\beta}}\\) as an estimate of the \\(\\boldsymbol{\\beta}\\) parameters in the true model, unless \\(\\boldsymbol{X}^T \\boldsymbol{Z} = 0\\). As discussed in Unit 2, this can lead to misleading conclusions.\n\n\n13.3.3 Detection\nSimilarly to the detection of correlated errors, we can try to identify model bias by plotting the standardized residuals against predictors that may have been left out of the model. A good place to start is to plot standardized residuals against the predictors \\(\\boldsymbol{X}\\) (one at a time) that are in the model, since nonlinear transformations of these might have been left out. In this case, you would see something like Figure 13.2(b).\nIt is possible to formally test for model bias in cases when we have repeated observations of the response for each value of the predictor vector. In particular, suppose that \\(\\boldsymbol{x}_{i*} = \\boldsymbol{x}_c\\) for \\(c = c(i)\\) and predictor vectors \\(\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_C \\in \\mathbb{R}^p\\). Then, consider testing the following hypothesis:\n\\[\nH_0: y_i = \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta} + \\epsilon_i \\quad \\text{versus} \\quad H_1: y_i = \\beta_{c(i)} + \\epsilon_i.\n\\]\nThe model under \\(H_0\\) (the linear model) is nested in the model for \\(H_1\\) (the saturated model), and we can test this hypothesis using an \\(F\\)-test called the lack of fit \\(F\\)-test.\n\n\n13.3.4 Overview of fixes\nTo fix model bias in the case (13.2), ideally we would identify the missing predictors \\(\\boldsymbol{Z}\\) and add them to the regression model. This may not always be feasible or possible. To fix model bias in the case (13.3), it is sometimes advocated to find a transformation \\(g\\) (e.g., a square root or a logarithm) of \\(\\boldsymbol{y}\\) such that \\(\\mathbb{E}[g(\\boldsymbol{y})] = \\boldsymbol{X} \\boldsymbol{\\beta}\\). However, a better solution is to use a generalized linear model, which we will discuss starting in Unit 4."
  },
  {
    "objectID": "misspecification-overview.html#sec-outliers",
    "href": "misspecification-overview.html#sec-outliers",
    "title": "13  Overview",
    "section": "13.4 Outliers",
    "text": "13.4 Outliers\n\n13.4.1 Origin\nOutliers often arise due to measurement or data entry errors. An observation can be an outlier in \\(\\boldsymbol{x}\\), in \\(y\\), or both.\n\n\n13.4.2 Consequences\nAn outlier can have the effect of biasing the estimate \\(\\boldsymbol{\\widehat{\\beta}}\\). This occurs when an observation has outlying \\(\\boldsymbol{x}\\) as well as outlying \\(y\\).\n\n\n13.4.3 Detection\nThere are a few measures associated with an observation that can be used to detect outliers, though none are perfect. The first quantity is called the leverage, defined as:\n\\[\n\\text{leverage of observation } i \\equiv \\text{corr}^2(y_i, \\widehat{\\mu}_i).\n\\]\nThis quantity measures the extent to which the fitted value \\(\\widehat{\\mu}_i\\) is sensitive to the (noise in the) observation \\(y_i\\). It can be derived that:\n\\[\n\\text{leverage of observation } i = h_i,\n\\]\nwhich is the \\(i\\)th diagonal element of the hat matrix \\(\\boldsymbol{H}\\). This is related to the fact that \\(\\text{Var}[\\widehat{\\epsilon}_i] = \\sigma^2(1-h_i)\\). The larger the leverage, the smaller the variance of the residual, so the closer the line passes to the \\(i\\)th observation. The leverage of an observation is larger to the extent that \\(\\boldsymbol{x}_{i*}\\) is far from \\(\\boldsymbol{\\bar{x}}\\). For example, in the bivariate linear model \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\),\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i' = 1}^n (x_{i'} - \\bar{x})^2}.\n\\]\nNote that the average of the leverages is:\n\\[\n\\frac{1}{n}\\sum_{i = 1}^n h_i = \\frac{1}{n}\\text{trace}(\\boldsymbol{H}) = \\frac{p}{n}.\n\\]\nAn observation’s leverage is considered large if it is significantly larger than this, e.g., three times larger.\nNote that the leverage is not a function of \\(y_i\\), so a high-leverage point might or might not be an outlier in \\(y_i\\) and therefore might or might not have a strong impact on the regression. To assess more directly whether an observation is influential, we can compare the least squares fits with and without that observation. To this end, we define the Cook’s distance:\n\\[\nD_i = \\frac{\\sum_{i' = 1}^n (\\widehat{\\mu}_{i'} - \\widehat{\\mu}^{\\text{-}i}_{i'})^2}{p\\widehat{\\sigma}^2},\n\\]\nwhere \\(\\widehat{\\mu}^{\\text{-}i}_{i'} = \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat{\\beta}}^{\\text{-}i}\\) and \\(\\boldsymbol{\\widehat{\\beta}}^{\\text{-}i}\\) is the least squares estimate based on \\((\\boldsymbol{X}_{\\text{-}i,*}, \\boldsymbol{y}_{\\text{-}i})\\). An observation is considered influential if it has Cook’s distance greater than one.\nThere is a connection between Cook’s distance and leverage:\n\\[\nD_i = \\left(\\frac{y_i - \\widehat{\\mu}_i}{\\widehat{\\sigma} \\sqrt{1-h_{ii}}}\\right)^2 \\cdot \\frac{h_{ii}}{p(1-h_{ii})}.\n\\]\nWe recognize the first term as the standardized residual; therefore a point is influential if its residual and leverage are large.\nNote that Cook’s distance may not successfully identify outliers. For example, if there are groups of outliers, then they will mask each other in the calculation of Cook’s distance.\n\n\n13.4.4 Overview of fixes\nIf outliers can be detected, then the fix is to remove them from the regression. But, we need to be careful. Definitively determining whether observations are outliers can be tricky. Outlier detection can even be used as a way to commit fraud with data, as now-defunct blood testing start-up Theranos is alleged to have done. As an alternative to removing outliers, we can fit estimators \\(\\boldsymbol{\\widehat{\\beta}}\\) that are less sensitive to outliers; see Chapter 17."
  },
  {
    "objectID": "asymptotic-methods.html#sec-better-estimate",
    "href": "asymptotic-methods.html#sec-better-estimate",
    "title": "14  Asymptotic methods",
    "section": "14.1 Methods that build a better estimate of \\(\\boldsymbol{\\beta}\\)",
    "text": "14.1 Methods that build a better estimate of \\(\\boldsymbol{\\beta}\\)\n\n14.1.1 Generalized least squares\nLet us premultiply \\(\\boldsymbol y\\) by \\(\\boldsymbol \\Sigma^{-1/2}\\) to obtain \\[\n\\boldsymbol \\Sigma^{-1/2} \\boldsymbol y \\sim N(\\boldsymbol \\Sigma^{-1/2} \\boldsymbol X \\boldsymbol \\beta, \\boldsymbol I).\n\\] Viewing \\(\\boldsymbol \\Sigma^{-1/2} \\boldsymbol y\\) as the new response and \\(\\boldsymbol \\Sigma^{-1/2} \\boldsymbol X\\) as the new model matrix, we can apply the usual least squares estimator to obtain \\[\n\\boldsymbol{\\tilde \\beta}^{\\text{GLS}} = (\\boldsymbol X^T \\boldsymbol \\Sigma^{-1} \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol \\Sigma^{-1} \\boldsymbol y.\n\\tag{14.2}\\] This is the generalized least squares estimate for the model (14.1), and has the following distribution: \\[\n\\boldsymbol{\\tilde{\\beta}}^{\\text{GLS}} \\sim N(\\boldsymbol{\\beta}, (\\boldsymbol{X}^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}).\n\\] By the Gauss-Markov theorem, this is the best linear unbiased estimate of \\(\\boldsymbol{\\beta}\\), recovering efficiency. We would like to carry out inference based on the latter distributional result analogously to how we did so in Chapter 2, as long as we can estimate \\(\\boldsymbol \\Sigma\\) accurately enough.\n\n\n14.1.2 Models for \\(\\boldsymbol{\\Sigma}\\)\nThis class of methods typically postulates a parametric form for \\(\\boldsymbol{\\Sigma}\\), denoted by \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\nu})\\), where \\(\\boldsymbol{\\nu}\\) is a vector of parameters, and then proceed by estimating \\(\\boldsymbol \\nu\\). Below are a few examples of such parametric models:\n\nHeteroskedastic errors. In this case, we can assume that \\(\\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)\\), where \\[\n\\log \\sigma_i^2 = \\boldsymbol{x}_i^T \\boldsymbol{\\nu}.\n\\]\nClustered errors. Suppose that each observation \\(i\\) falls into a cluster \\(c(i)\\). Then, we can postulate a random effects model \\[\ny_i = \\boldsymbol{x}_i^T \\boldsymbol{\\beta} + \\delta_{c(i)} + \\tau_i, \\quad \\text{where} \\quad \\delta_c \\overset{\\text{i.i.d.}}\\sim N(0, \\sigma^2_{\\delta}), \\quad \\tau_i \\overset{\\text{i.i.d.}}\\sim N(0, \\sigma^2_{\\tau}).\n\\] This imposes a block-diagonal structure on \\(\\boldsymbol{\\Sigma}\\), where each block corresponds to a cluster.\nTemporal errors. If the observations have a temporal structure, we might impose an AR(1) model on the residuals: \\[\n\\epsilon_1 = \\tau_1; \\quad \\epsilon_i = \\rho \\epsilon_{i-1} + \\tau_i \\quad \\text{for } i &gt; 1, \\quad \\text{where} \\quad \\tau_i \\overset{\\text{i.i.d.}}\\sim N(0, \\sigma^2).\n\\] This imposes an approximately banded structure on \\(\\boldsymbol{\\Sigma}\\), with \\(\\Sigma_{i_1i_2} = \\sigma^2\\rho^{|i_1-i_2|}\\).\n\n\n\n\n\n\n\nRandom versus fixed effects models\n\n\n\nRandom effects models deal address correlated errors but not with model bias. The difference is that, in the case of correlated errors, the errors may be correlated with themselves but not with the regressors. In the case of model bias, the errors may be correlated with the regressors. To address model bias in the presence of clustering structure, fixed effects are necessary. Fixed effects models decrease model bias at the cost of increased variance, because more parameters must be estimated. Random effects models are more susceptible to model bias but have lower variance.\n\n\n\n\n14.1.3 Estimating \\(\\boldsymbol{\\Sigma}\\)\nGiven a parametric model for \\(\\boldsymbol{\\Sigma}\\), we can estimate \\(\\boldsymbol{\\nu}\\) by one of two approaches. The first approach, typical in statistics, is to maximize the likelihood as a function of \\((\\boldsymbol \\beta, \\boldsymbol \\nu)\\). The second approach, typical in econometrics, is to estimate \\(\\boldsymbol{\\beta}\\) using OLS, and then to fit \\(\\boldsymbol \\nu\\) based on the residuals. This gives us the estimate \\(\\boldsymbol{\\widehat \\Sigma} = \\boldsymbol{\\widehat \\Sigma}(\\boldsymbol{\\widehat \\nu})\\).\n\n\n14.1.4 Inferring about \\(\\boldsymbol \\beta\\) based on the estimate \\(\\boldsymbol{\\widehat \\Sigma}\\)\nWith an estimate \\(\\boldsymbol{\\widehat \\Sigma}\\) in hand, we can use it to build a (hopefully) better estimate of \\(\\boldsymbol{\\beta}\\), using the following plug-in version of the GLS estimate (14.2): \\[\n\\boldsymbol{\\widehat{\\beta}}^{\\text{FGLS}} \\equiv (\\boldsymbol{X}^T \\boldsymbol{\\widehat{\\Sigma}}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{\\widehat{\\Sigma}}^{-1}\\boldsymbol{y}.\n\\tag{14.3}\\]\nThis is called the feasible generalized least squares estimate (FGLS) in econometrics, to contrast it with the infeasible estimate that assumes \\(\\boldsymbol{\\Sigma}\\) is known exactly. Then, we can carry out inference based on the approximation distribution \\[\n\\boldsymbol{\\widehat{\\beta}}^{\\text{FGLS}} \\overset \\cdot \\sim N(\\boldsymbol{\\beta}, (\\boldsymbol{X}^T \\boldsymbol{\\widehat{\\Sigma}}^{-1}\\boldsymbol{X})^{-1}).\n\\tag{14.4}\\]"
  },
  {
    "objectID": "asymptotic-methods.html#sec-better-standard-errors",
    "href": "asymptotic-methods.html#sec-better-standard-errors",
    "title": "14  Asymptotic methods",
    "section": "14.2 Methods that build better standard errors for OLS estimate",
    "text": "14.2 Methods that build better standard errors for OLS estimate\nSometimes we don’t feel comfortable enough with our estimate of \\(\\boldsymbol{\\Sigma}\\) to actually modify the least squares estimator. So we want to keep using our least squares estimator, but still get standard errors robust to heteroskedastic or correlated errors. There are several strategies to computing valid standard errors in such situations.\n\n14.2.1 Sandwich standard errors\nLet’s say that \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\). Then, we can compute that the covariance matrix of the least squares estimate \\(\\boldsymbol{\\widehat{\\beta}}\\) is\n\\[\n\\text{Var}[\\boldsymbol{\\widehat{\\beta}}] = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}(\\boldsymbol{X}^T \\boldsymbol{\\Sigma} \\boldsymbol{X})(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}.\n\\tag{14.5}\\]\nNote that this expression reduces to the usual \\(\\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\) when \\(\\boldsymbol{\\Sigma} = \\sigma^2 \\boldsymbol{I}\\). It is called the sandwich variance because we have the \\((\\boldsymbol{X}^T \\boldsymbol{\\Sigma} \\boldsymbol{X})\\) term sandwiched between two \\((\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\) terms. If we have some estimate \\(\\boldsymbol{\\widehat{\\Sigma}}\\) of the covariance matrix, we can construct\n\\[\n\\widehat{\\text{Var}}[\\boldsymbol{\\widehat{\\beta}}] \\equiv (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}(\\boldsymbol{X}^T \\boldsymbol{\\widehat{\\Sigma}} \\boldsymbol{X})(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}.\n\\tag{14.6}\\]\nDifferent estimates \\(\\boldsymbol{\\widehat{\\Sigma}}\\) are appropriate in different situations. Below we consider three of the most common choices: one for heteroskedasticity (due to Huber-White), one for group-correlated errors (due to Liang-Zeger), and one for temporally-correlated errors (due to Newey-West).\n\n\n14.2.2 Specific instances of sandwich standard errors\nHuber-White standard errors\nSuppose \\(\\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)\\) for some variances \\(\\sigma_1^2, \\dots, \\sigma_n^2 &gt; 0\\). The Huber-White sandwich estimator is defined by (14.5), with\n\\[\n\\boldsymbol{\\widehat{\\Sigma}} \\equiv \\text{diag}(\\widehat{\\sigma}_1^2, \\dots, \\widehat{\\sigma}_n^2), \\quad \\text{where} \\quad \\widehat{\\sigma}_i^2 = (y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat{\\beta}})^2.\n\\]\nWhile each estimator \\(\\widehat{\\sigma}_i^2\\) is very poor, Huber and White’s insight was that the resulting estimate of the (averaged) quantity \\(\\boldsymbol{X}^T \\boldsymbol{\\widehat{\\Sigma}}\\boldsymbol{X}\\) is not bad. To see why, assume that \\((\\boldsymbol{x}_{i*}, y_i) \\overset{\\text{i.i.d.}}{\\sim} F\\) for some joint distribution \\(F\\). Then, we have that\n\\[\n\\begin{split}\n\\frac{1}{n}(\\boldsymbol{X}^T \\widehat{\\boldsymbol{\\Sigma}} \\boldsymbol{X} - \\boldsymbol{X}^T \\boldsymbol{\\Sigma} \\boldsymbol{X}) &= \\frac{1}{n} \\sum_{i=1}^n (\\widehat{\\sigma}_i^2 - \\sigma_i^2) \\boldsymbol{x}_{i*} \\boldsymbol{x}_{i*}^T \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n ((\\epsilon_i + \\boldsymbol{x}_{i*}^T(\\boldsymbol{\\beta} - \\boldsymbol{\\widehat{\\beta}}))^2 - \\sigma_i^2) \\boldsymbol{x}_{i*} \\boldsymbol{x}_{i*}^T \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i^2 \\boldsymbol{x}_{i*} \\boldsymbol{x}_{i*}^T + o_p(1) \\\\\n&\\overset p \\to 0.\n\\end{split}\n\\]\nThe last step holds by the law of large numbers, since \\(\\mathbb{E}[\\epsilon_i^2 \\boldsymbol{x}_{i*} \\boldsymbol{x}_{i*}^T] = 0\\) for each \\(i\\).\nLiang-Zeger standard errors\nNext, let’s consider the case of group-correlated errors. Suppose that the observations are clustered, with correlated errors among clusters but not between clusters. Suppose there are \\(C\\) clusters of observations, with the \\(i\\)th observation belonging to cluster \\(c(i) \\in \\{1, \\dots, C\\}\\). Suppose for the sake of simplicity that the observations are ordered so that clusters are contiguous. Let \\(\\boldsymbol{\\widehat{\\epsilon}}_c\\) be the vector of residuals in cluster \\(c\\), so that \\(\\boldsymbol{\\widehat{\\epsilon}} = (\\boldsymbol{\\widehat{\\epsilon}}_1, \\dots, \\boldsymbol{\\widehat{\\epsilon}}_C)\\). Then, the true covariance matrix is \\(\\boldsymbol{\\Sigma} = \\text{block-diag}(\\boldsymbol{\\Sigma}_1, \\dots, \\boldsymbol{\\Sigma}_C)\\) for some positive definite \\(\\boldsymbol{\\Sigma}_1, \\dots, \\boldsymbol{\\Sigma}_C\\). The Liang-Zeger estimator is then defined by (14.5), with\n\\[\n\\boldsymbol{\\widehat{\\Sigma}} \\equiv \\text{block-diag}(\\boldsymbol{\\widehat{\\Sigma}_1}, \\dots, \\boldsymbol{\\widehat{\\Sigma}_C}), \\quad \\text{where} \\quad  \\boldsymbol{\\widehat{\\Sigma}_c} \\equiv \\boldsymbol{\\widehat{\\epsilon}}_c \\boldsymbol{\\widehat{\\epsilon}}_c^T.\n\\]\nNote that the Liang-Zeger estimator is a generalization of the Huber-White estimator. Its justification is similar as well: while each \\(\\boldsymbol{\\widehat{\\Sigma}_c}\\) is a poor estimator, the resulting estimate of the (averaged) quantity \\(\\boldsymbol{X}^T \\boldsymbol{\\widehat{\\Sigma}}\\boldsymbol{X}\\) is not bad as long as the number of clusters is large. Liang-Zeger standard errors are referred to as “clustered standard errors” in the econometrics community. It is recommended to employ clustered standard errors even when using cluster-level fixed effects, in order to capture remaining within-cluster correlations.\nNewey-West standard errors\nFinally, consider the case when our observations \\(i\\) have a temporal structure, and we believe there to be nontrivial correlations between \\(\\epsilon_{i1}\\) and \\(\\epsilon_{i2}\\) for \\(|i1 - i2| \\leq L\\). Then, a natural extension of the Huber-White estimate of \\(\\boldsymbol{\\Sigma}\\) is \\(\\widehat{\\Sigma}_{i1,i2} = \\widehat{\\epsilon}_{i1}\\widehat{\\epsilon}_{i2}\\) for each pair \\((i1, i2)\\) such that \\(|i1 - i2| \\leq L\\). Unfortunately, this is not guaranteed to give a positive semidefinite matrix \\(\\boldsymbol{\\widehat{\\Sigma}}\\). Therefore, Newey and West proposed a slightly modified estimator:\n\\[\n\\boldsymbol{\\widehat{\\Sigma}}_{i1,i2} = \\max\\left(0, 1-\\frac{|i1-i2|}{L+1}\\right)\\widehat{\\epsilon}_{i1}\\widehat{\\epsilon}_{i2}.\n\\]\nThis estimator shrinks the off-diagonal estimates \\(\\widehat{\\epsilon}_{i1}\\widehat{\\epsilon}_{i2}\\) based on their distance to the diagonal. It can be shown that this modification restores positive semidefiniteness of \\(\\boldsymbol{\\widehat{\\Sigma}}\\)."
  },
  {
    "objectID": "asymptotic-methods.html#sec-sandwich-inference",
    "href": "asymptotic-methods.html#sec-sandwich-inference",
    "title": "14  Asymptotic methods",
    "section": "14.3 Inference based on an approximate covariance matrix",
    "text": "14.3 Inference based on an approximate covariance matrix\nWhether based on the relations (14.4) or (14.6), we end up with an estimator \\(\\boldsymbol{\\widehat \\beta}\\) and an approximate covariance matrix \\(\\widehat{\\boldsymbol{\\Omega}}\\), so that \\[\n\\boldsymbol{\\widehat{\\beta}} \\overset{\\cdot}{\\sim} N(\\boldsymbol{\\beta}, \\widehat{\\boldsymbol{\\Omega}}).\n\\]\nThis allows us to construct confidence intervals and hypothesis tests for each \\(\\beta_j\\), by simply replacing \\(\\text{SE}(\\beta_j)\\) with \\(\\sqrt{\\widehat{\\Omega}_{jj}}\\). For contrasts and prediction intervals, we can use the fact that \\(\\boldsymbol{c}^T \\boldsymbol{\\beta} \\overset{\\cdot}{\\sim} N(\\boldsymbol{c}^T \\boldsymbol{\\beta}, \\boldsymbol{c}^T \\widehat{\\boldsymbol{\\Omega}} \\boldsymbol{c})\\), so that \\(\\text{CE}(\\boldsymbol{c}^T \\boldsymbol{\\beta}) = \\sqrt{\\boldsymbol{c}^T \\widehat{\\boldsymbol{\\Omega}} \\boldsymbol{c}}\\). It is less obvious how to use the matrix \\(\\widehat{\\boldsymbol{\\Omega}}\\) to test the hypothesis \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{0}\\). To this end, we can use a Wald test (we will discuss Wald tests in more detail in Unit 4). The Wald test statistic is\n\\[\nW = \\boldsymbol{\\widehat{\\beta}}_S^T (\\widehat{\\boldsymbol{\\Omega}}_{S, S})^{-1} \\boldsymbol{\\widehat{\\beta}}_S,\n\\]\nwhich is asymptotically distributed as \\(\\chi^2_{|S|}\\) under the null hypothesis. This is based on the following result.\n\nLemma 14.1 Let \\(\\boldsymbol Z \\sim N(0, \\boldsymbol \\Sigma)\\) be a \\(d\\)-dimensional random vector, with \\(\\boldsymbol \\Sigma\\) invertible. Then, \\[\n\\boldsymbol Z^T \\boldsymbol \\Sigma^{-1} \\boldsymbol Z \\sim \\chi^2_d.\n\\]\n\nThis gives us the test \\[\n\\phi_{\\text{Wald}}(\\boldsymbol X, \\boldsymbol y) = \\mathbb{I}\\left\\{ \\boldsymbol{\\widehat{\\beta}}_S^T (\\widehat{\\boldsymbol{\\Omega}}_{S, S})^{-1} \\boldsymbol{\\widehat{\\beta}}_S &gt; \\chi^2_{|S|} (1-\\alpha) \\right\\}.\n\\tag{14.7}\\] As it turns out, under the usual linear model assumptions, this test is asymptotically equivalent to the usual \\(F\\)-test for the hypothesis \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{0}\\).\n\nProposition 14.1 The homoskedasticity-based \\(F\\)-statistic for the null hypothesis \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{0}\\) can be expressed as \\[\nF = \\boldsymbol{\\widehat{\\beta}}_S^T (\\widehat{\\boldsymbol{\\Omega}}_{S, S})^{-1} \\boldsymbol{\\widehat{\\beta}}_S / |S|,\n\\] allowing us to rewrite the Wald test as \\[\n\\phi_{\\text{Wald}}(\\boldsymbol X, \\boldsymbol y) = \\mathbb{I}\\left\\{ F &gt; \\tfrac{1}{|S|}\\chi^2_{|S|} (1-\\alpha) \\right\\}.\n\\] Since \\(F_{|S|, n-p} \\overset d \\to \\tfrac{1}{|S|}\\chi^2_{|S|}\\) as \\(n \\rightarrow \\infty\\), it follows that the \\(F\\)-test and the Wald test are asymptotically equivalent.\n\n\nProof. Recall from Chapter 8 that the \\(F\\)-test statistic can be expressed as \\[\nF = \\frac{\\|(\\boldsymbol H - \\boldsymbol H_{\\text{-}S})\\boldsymbol y \\|^2 / |S|}{\\|(\\boldsymbol I - \\boldsymbol H) \\boldsymbol y \\|^2 / (n-p)} = \\frac{\\|(\\boldsymbol H - \\boldsymbol H_{\\text{-}S})\\boldsymbol y \\|^2 / |S|}{\\widehat \\sigma^2},\n\\] where \\(\\boldsymbol H - \\boldsymbol H_{\\text{-}S}\\) is the projection matrix onto \\(C(\\boldsymbol X_{*, S}^\\perp)\\). Now, let \\(\\boldsymbol{\\widehat \\beta}\\) be the least squares estimate in the regression of \\(\\boldsymbol y\\) on \\(\\boldsymbol X\\). Then, we have \\[\n\\begin{split}\n(\\boldsymbol H - \\boldsymbol H_{\\text{-}S})\\boldsymbol y &= (\\boldsymbol H - \\boldsymbol H_{\\text{-}S})(\\boldsymbol X_{*,S} \\boldsymbol{\\widehat \\beta}_S + \\boldsymbol X_{*,\\text{-}S} \\boldsymbol{\\widehat \\beta}_{\\text{-}S} + \\boldsymbol{\\widehat \\epsilon}) \\\\\n&= (\\boldsymbol H - \\boldsymbol H_{\\text{-}S})\\boldsymbol X_{*,S} \\boldsymbol{\\widehat \\beta}_S \\\\\n&= \\boldsymbol X_{*,S}^\\perp \\boldsymbol{\\widehat \\beta}_S.\n\\end{split}\n\\] Therefore, we have \\[\n\\|(\\boldsymbol H - \\boldsymbol H_{\\text{-}S})\\boldsymbol y\\|^2 = \\boldsymbol{\\widehat \\beta}_S^T (\\boldsymbol X_{*,S}^\\perp)^T \\boldsymbol X_{*,S}^\\perp \\boldsymbol{\\widehat \\beta}_S.\n\\] Next, we claim that \\[\n(\\boldsymbol X_{*,S}^\\perp)^T \\boldsymbol X_{*,S}^\\perp = \\{[(\\boldsymbol X^T \\boldsymbol X)^{-1}]_{S,S}]\\}^{-1}.\n\\tag{14.8}\\] To see this, note that \\[\n\\boldsymbol{\\widehat \\beta}_S \\sim N(\\boldsymbol \\beta_S, \\sigma^2 [(\\boldsymbol X^T \\boldsymbol X)^{-1}]_{S,S}).\n\\tag{14.9}\\] On the other hand, since \\(\\boldsymbol{\\widehat \\beta}_S\\) can be obtained by regressing \\(\\boldsymbol y\\) onto \\(\\boldsymbol X_{*,S}^{\\perp}\\), we also have that \\[\n\\boldsymbol{\\widehat \\beta}_S \\sim N(\\boldsymbol \\beta_S, \\sigma^2 [(\\boldsymbol X_{*,S}^{\\perp})^T \\boldsymbol X_{*,S}^{\\perp}]^{-1}).\n\\tag{14.10}\\] Combining 14.9 and 14.10, we verify the claimed relationship 14.8. Therefore, we have \\[\nF = \\frac{\\boldsymbol{\\widehat \\beta}_S^T [(\\boldsymbol X^T \\boldsymbol X)^{-1}]_{S,S} \\boldsymbol{\\widehat \\beta}_S / |S|}{\\widehat \\sigma^2}.\n\\] Recalling that \\(\\boldsymbol{\\widehat \\Omega} = \\widehat \\sigma^2 (\\boldsymbol X^T \\boldsymbol X)^{-1}\\), we find that \\[\nF = \\boldsymbol{\\widehat \\beta}_S^T \\left(\\boldsymbol{\\widehat \\Omega}_{S,S}\\right)^{-1} \\boldsymbol{\\widehat \\beta}_S / |S|,\n\\] as desired."
  },
  {
    "objectID": "bootstrap.html#introduction-to-the-bootstrap",
    "href": "bootstrap.html#introduction-to-the-bootstrap",
    "title": "15  The bootstrap",
    "section": "15.1 Introduction to the bootstrap",
    "text": "15.1 Introduction to the bootstrap\nThe bootstrap can be used for either confidence interval construction or hypothesis testing, with confidence interval construction being a much more common application. For this reason, we will focus on confidence interval construction in remainder of this section as well as in Section 15.2 and Section 15.3. We will discuss bootstrap hypothesis testing in Section 15.4.\n\n15.1.1 Usual inference paradigm\nWe typically carry out linear model inference for \\(\\beta_j\\) by approximating the sampling distribution of \\(\\widehat\\beta_j\\), or a derivative thereof, such as the \\(t\\)-statistic. Under the standard linear model assumptions, we have \\[\ng(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta) \\equiv \\widehat{\\beta}_j - \\beta_j \\sim N(0, \\sigma^2 [(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}]_{jj})\n\\tag{15.1}\\] and \\[\ng(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta) \\equiv \\frac{\\widehat{\\beta}_j - \\beta_j}{\\text{s.e.}(\\widehat \\beta_j)} \\sim t_{n-p}.\n\\tag{15.2}\\] Here, \\(g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta)\\) denotes a derivative quantity, whose distribution is the basis for inference. If the lower and upper quantiles \\(\\mathbb Q_{\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta)]\\) and \\(\\mathbb Q_{1-\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta)]\\) are known, then we can construct a \\((1-\\alpha)\\) confidence interval for \\(\\beta_j\\) as \\[\n\\text{CI}(\\beta_j) \\equiv \\{\\beta_j: g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta) \\in [\\mathbb Q_{\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta)], \\mathbb Q_{1-\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta)]]\\}.\n\\tag{15.3}\\]\nUnder model misspecification, the distributions on the right-hand sides of equations (15.1) and (15.2) may no longer be valid.\n\n\n15.1.2 Bootstrap inference paradigm\nThe bootstrap is an approach to more robust inference that obtains such sampling distributions by a technique known as resampling. The core idea of the bootstrap is to use the data to construct an approximation to the data-generating distribution and then to approximate the sampling distribution of any statistic by simulating from this approximate data-generating distribution. In more detail, the bootstrap paradigm is as follows:\n\n\n\n\n\n\nBootstrap paradigm to build confidence intervals for \\(\\beta_j\\)\n\n\n\n\nUse the data \\((\\boldsymbol X, \\boldsymbol y)\\) to get an approximation \\(\\widehat F\\) for the data distribution \\(F\\).\nFor each \\(b = 1, \\dots, B\\),\n\nSample a bootstrap dataset \\((\\boldsymbol X^{(b)}, \\boldsymbol y^{(b)}) \\sim \\widehat F\\);\nFit the least squares estimate \\(\\boldsymbol{\\widehat \\beta}^{(b)}\\) based on \\((\\boldsymbol X^{(b)}, \\boldsymbol y^{(b)})\\);\nConstruct a derivative quantity \\(g(\\boldsymbol{\\widehat \\beta}^{(b)}, \\boldsymbol{\\widehat \\beta})\\), such as \\(\\widehat \\beta^{(b)}_j - \\widehat \\beta_j\\).\n\nExtract the empirical \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the derivative quantity \\(g(\\boldsymbol{\\widehat \\beta}^{(b)}, \\boldsymbol{\\widehat \\beta})\\), denoted \\(\\mathbb Q_{\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}^{(b)}, \\boldsymbol{\\widehat \\beta})]\\) and \\(\\mathbb Q_{1-\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}^{(b)}, \\boldsymbol{\\widehat \\beta})]\\).\nConstruct a \\((1-\\alpha)\\) confidence interval for \\(\\beta_j\\) by analogy with (15.3): \\[\n\\begin{split}\n&\\text{CI}^{\\text{boot}}(\\beta_j) \\\\\n&\\quad \\equiv \\{\\beta_j: g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol \\beta) \\in [\\mathbb Q_{\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}^{(b)}, \\boldsymbol{\\widehat \\beta})], \\mathbb Q_{1-\\alpha/2}[g(\\boldsymbol{\\widehat \\beta}^{(b)}, \\boldsymbol{\\widehat \\beta})]]\\}.\n\\end{split}\n\\tag{15.4}\\]\n\n\n\nThis approach, pioneered by Brad Efron in 1979, obviates the need for stringent assumptions and mathematical derivations to obtain limiting distributions, replacing these with added computation. The bootstrap is extremely flexible and can be adapted to apply in a variety of settings. Furthermore, bootstrap methods are typically more accurate than their asymptotic counterparts in finite samples. While the justification of the bootstrap is still asymptotic (requiring \\(\\widehat F\\) to approach \\(F\\)), the rate of convergence is often “second-order” \\(O(1/n)\\) rather than the usual “first-order” \\(O(1/\\sqrt{n})\\) of standard asymptotic inference. This faster second-order convergence gives the bootstrap an advantage in finite samples.\n\n\n15.1.3 Overview of bootstrap flavors\nThe bootstrap comes in a variety of flavors, dictated by the mechanism by which the data distribution \\(F\\) is learned (e.g. the parametric, residual, or pairs bootstraps), and the derivative quantity \\(g(\\cdot, \\cdot)\\) on which inference is based (e.g. the empirical bootstrap and the bootstrap-\\(t\\)). These two sets of flavors can be mixed and matched."
  },
  {
    "objectID": "bootstrap.html#sec-derivative-quantities",
    "href": "bootstrap.html#sec-derivative-quantities",
    "title": "15  The bootstrap",
    "section": "15.2 Derivative quantities on which to base inference",
    "text": "15.2 Derivative quantities on which to base inference\n\n15.2.1 The empirical bootstrap\nThe empirical bootstrap, the most common choice, is based on the quantity \\[\ng(\\boldsymbol{\\widehat \\beta}, \\boldsymbol{\\beta}) = \\widehat \\beta_j- \\beta_j,\n\\] We can derive that if \\[\n\\mathbb P\\left[\\widehat \\beta_j - \\beta_j \\in \\left[\\mathbb Q_{\\alpha/2}[\\widehat \\beta^{(b)}_j - \\widehat \\beta_j], \\mathbb Q_{1-\\alpha/2}[\\widehat \\beta^{(b)}_j - \\widehat \\beta_j]\\right]\\right] \\overset \\cdot \\geq 1-\\alpha,\n\\] then \\[\n\\mathbb P\\left[\\beta_j \\in \\left[\\widehat \\beta_j - \\mathbb Q_{1-\\alpha/2}[\\widehat \\beta^{(b)}_j - \\widehat \\beta_j], \\widehat \\beta_j - \\mathbb Q_{\\alpha/2}[\\widehat \\beta^{(b)}_j - \\widehat \\beta_j]\\right]\\right] \\overset \\cdot \\geq 1-\\alpha.\n\\] For this reason, we define \\[\n\\text{CI}^{\\text{boot}}(\\beta_j) \\equiv \\left[\\widehat \\beta_j - \\mathbb Q_{1-\\alpha/2}[\\widehat \\beta^{(b)}_j - \\widehat \\beta_j], \\widehat \\beta_j - \\mathbb Q_{\\alpha/2}[\\widehat \\beta^{(b)}_j - \\widehat \\beta_j]\\right].\n\\]\n\n\n\n\n\n\nThe percentile bootstrap\n\n\n\nA commonly used alternative to the empirical bootstrap is the percentile bootstrap, defined by \\[\n\\text{CI}^{\\text{boot}}(\\beta_j) \\equiv \\left[\\mathbb Q_{\\alpha/2}[\\widehat \\beta^{(b)}_j], \\mathbb Q_{1-\\alpha/2}[\\widehat \\beta^{(b)}_j]\\right].\n\\tag{15.5}\\] Here, the resampling distribution of \\(\\widehat \\beta_j\\) is used directly to construct the confidence interval. However, this approach does not fall within the bootstrap paradigm described above, and in particular, the formula (15.5) is not a special case of (15.4). The formula (15.5) can be viewed as seeking an interval within which \\(\\widehat \\beta_j\\) (rather than \\(\\beta_j\\) itself) falls with 95% probability. The percentile bootstrap is only justified when the distribution of \\(\\widehat \\beta^{(b)}_j\\) is symmetric about \\(\\widehat \\beta_j\\), in which case it coincides with the empirical bootstrap.\n\n\n\n\n15.2.2 The bootstrap-\\(t\\) method\nA weakness of the empirical bootstrap is that the quantity \\(g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol{\\beta}) = \\widehat \\beta_j - \\beta_j\\) has distribution \\(N(0, \\sigma^2 [(X^T X)^{-1}]_{jj})\\) (recalling equation (15.1)), which depends on the nuisance parameter \\(\\sigma^2\\). When we approximate this distribution by bootstrapping, we implicitly are substituting in an estimate of \\(\\sigma^2\\), which is itself subject to sampling variability. The empirical bootstrap does not account for this variability, because the distribution \\(\\widehat F\\) on which the estimate of \\(\\sigma^2\\) is based is held fixed throughout. To see this more clearly, consider the following example.\n\nExample 15.1 (Non-pivotality in the normal mean problem) Suppose that \\(\\boldsymbol y \\sim N(\\boldsymbol 1 \\beta_0, \\sigma^2 \\boldsymbol I)\\), and the goal is to construct a confidence interval for \\(\\beta_0\\). Defining \\(\\widehat \\beta_0 \\equiv \\bar y\\) and \\(\\widehat \\sigma^2 \\equiv \\tfrac 1 {n-1} \\sum_{i=1}^n (y_i - \\bar y)^2\\), consider the empirical bootstrap based on resampling \\(\\boldsymbol y^{(b)} \\sim N(\\boldsymbol 1 \\widehat \\beta_0, \\widehat \\sigma^2 \\boldsymbol I)\\). In this case, we will find that \\[\n\\widehat \\beta^{(b)}_0 - \\widehat \\beta_0 = \\bar y^{(b)} - \\bar y \\sim N(0, \\widehat \\sigma^2/n),\n\\] which will give rise to the bootstrap confidence interval \\[\n\\text{CI}^{\\text{boot}}(\\beta_0) = \\widehat \\beta_0 \\pm z_{1-\\alpha/2} \\tfrac{1}{\\sqrt{n}} \\widehat \\sigma.\n\\] The uncertainty in the estimate \\(\\widehat \\sigma^2\\) is not accounted for. We know from Unit 2 that, if the usual linear model assumptions are satisfied, we could account for this uncertainty by using a \\(t\\)-distribution instead of a normal distribution.\n\nThis issue can be addressed by bootstrapping a pivotal quantity \\(g(\\boldsymbol{\\widehat \\beta}, \\boldsymbol{\\beta})\\), i.e., a quantity whose distribution does not depend on unknown parameters (at least under standard assumptions). In the context of the linear model, the \\(t\\)-statistic (15.2) is pivotal. Bootstrapping the \\(t\\)-statistic, called the bootstrap-\\(t\\) method, is therefore a way to account for the uncertainty in the estimate of \\(\\sigma^2\\). To derive the bootstrap-\\(t\\) method, we approximate \\[\n\\mathbb{P} \\left[ \\frac{\\widehat{\\beta}_j - \\beta_j}{\\text{s.e.}(\\widehat{\\beta}_j)} \\in \\left[ \\mathbb{Q}_{\\alpha/2} \\left( \\frac{\\widehat{\\beta}_j^{(b)} - \\widehat{\\beta}_j}{\\text{s.e.}(\\widehat{\\beta}_j^{(b)})} \\right), \\mathbb{Q}_{1-\\alpha/2} \\left( \\frac{\\widehat{\\beta}_j^{(b)} - \\widehat{\\beta}_j}{\\text{s.e.}(\\widehat{\\beta}_j^{(b)})} \\right) \\right] \\right] \\overset{\\cdot}{\\geq} 1-\\alpha,\n\\] which justifies the bootstrap-\\(t\\) confidence interval \\[\n\\begin{split}\n&\\text{CI}^{\\text{boot}}(\\beta_j) \\\\\n&\\quad \\equiv \\left[\\widehat \\beta_j - \\text{s.e.}(\\widehat \\beta_j) \\mathbb Q_{1-\\alpha/2} \\left( \\frac{\\widehat \\beta_j^{(b)} - \\widehat \\beta_j}{\\text{s.e.}(\\widehat \\beta_j^{(b)})} \\right), \\widehat \\beta_j - \\text{s.e.}(\\widehat \\beta_j) \\mathbb Q_{\\alpha/2} \\left( \\frac{\\widehat \\beta_j^{(b)} - \\widehat \\beta_j}{\\text{s.e.}(\\widehat \\beta_j^{(b)})} \\right)\\right].\n\\end{split}\n\\]"
  },
  {
    "objectID": "bootstrap.html#sec-bootstrap-techniques",
    "href": "bootstrap.html#sec-bootstrap-techniques",
    "title": "15  The bootstrap",
    "section": "15.3 Techniques for learning the data distribution",
    "text": "15.3 Techniques for learning the data distribution\nThe methods for learning the data distribution lie on a spectrum based on how flexibly they model this distribution. Methods with less flexibility are more stable (i.e. less variable) but less robust. On the other hand, methods with more flexibility are more robust but less stable. The following table shows the methods in increasing order of flexibility, including which types of model misspecification they are robust to.\n\n\n\n\n\n\n\n\n\n\n\nMethod\nNon-normality\nHetero-skedasticity\nGroup correlation\nTemporal correlation\n\n\n\n\nParametric\nNo\nNo\nNo\nNo\n\n\nResidual\nYes\nNo\nNo\nNo\n\n\nPairs\nYes\nYes\nNo\nNo\n\n\nClustered\nYes\nYes\nYes\nNo\n\n\nMoving Blocks\nYes\nYes\nNo\nYes\n\n\n\n\nWe will present these methods in the same order, starting from the parametric bootstrap.\n\n15.3.1 The parametric bootstrap\nThe parametric bootstrap proceeds by specifying a parametric model for the data \\((\\boldsymbol X, \\boldsymbol y)\\), such as the one from Unit 2: \\[\n\\boldsymbol y = \\boldsymbol X \\boldsymbol \\beta + \\boldsymbol \\epsilon, \\quad \\boldsymbol \\epsilon \\sim N(0, \\sigma^2 \\boldsymbol I).\n\\tag{15.6}\\] The model matrix \\(\\boldsymbol X\\) is kept fixed and only the distribution of \\(\\boldsymbol y\\) (conditionally on \\(\\boldsymbol X\\)) is modeled. These parameters can be fit by maximum likelihood, as usual. Then, the bootstrapped datasets can be generated as follows: \\[\n\\boldsymbol X^{(b)} = \\boldsymbol X; \\quad \\boldsymbol y^{(b)} \\sim N(\\boldsymbol X \\boldsymbol{\\widehat \\beta}, \\widehat \\sigma^2 \\boldsymbol I).\n\\] In the context of the linear model, the parametric bootstrap is rarely used, particularly in the context of the standard parametric form (15.6) and the least squares estimator \\(\\boldsymbol{\\widehat \\beta}\\). Indeed, it offers no robustness to model misspecification and the distribution of the least squares estimator is known exactly, so there is no need to approximate it using resampling. In contexts beyond linear models or when estimators beyond the least squares estimator are of interest, the parametric bootstrap is useful because it can be used the approximate the distributions of analytically intractable estimators, substituting computation for math.\n\n\n15.3.2 The residual bootstrap\nThe residual bootstrap is based on a parametric model for \\(\\mathbb E[\\boldsymbol y \\mid \\boldsymbol X]\\) and a nonparametric model for the noise terms. In particular, suppose that \\[\ny_i = \\boldsymbol x_{i*}^T \\boldsymbol \\beta + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{i.i.d.}}{\\sim} G \\quad \\text{for } i = 1, \\dots, n,\n\\tag{15.7}\\] where \\(G\\) is an unknown distribution without a parametric form. Then, the data-generating distribution \\(F\\) is specified by the pair \\((\\boldsymbol \\beta, G)\\). As with the parametric bootstrap, the model matrix \\(\\boldsymbol X\\) is kept fixed and only the distribution of \\(\\boldsymbol y\\) (conditionally on \\(\\boldsymbol X\\)) is modeled. The parameter vector \\(\\boldsymbol \\beta\\) can be fit by least squares, as usual. Then, the distribution of the noise terms \\(\\epsilon_i\\) can be estimated by the empirical distribution of the residuals \\(\\widehat \\epsilon_i\\): \\[\n\\widehat G = \\frac{1}{n} \\sum_{i=1}^n \\delta_{\\widehat \\epsilon_i},\n\\] where \\(\\delta_{\\widehat \\epsilon_i}\\) is the Dirac delta function at \\(\\widehat \\epsilon_i\\). The bootstrapped datasets can then be generated as follows: \\[\nx_{i*}^{(b)} = x_{i*}; \\quad y_i^{(b)} = \\boldsymbol x_{i*}^T \\boldsymbol{\\widehat \\beta} + \\epsilon_i^{(b)}, \\quad \\epsilon_i^{(b)} \\overset{\\text{i.i.d.}}{\\sim} \\widehat G \\quad \\text{for } i = 1, \\dots, n.\n\\] Note that the sampling of \\(\\epsilon_i^{(b)} \\overset{\\text{i.i.d.}}{\\sim} \\widehat G\\) is equivalent to sampling with replacement from the residuals \\(\\widehat \\epsilon_i\\). By avoiding modeling \\(\\epsilon_i\\) as normal, the residual bootstrap is robust to non-normality. However, it is not robust to heteroskedasticity or correlated errors, because it models the \\(\\epsilon_i\\) as i.i.d. from some distribution.\n\n\n15.3.3 Pairs bootstrap\nWeakening the assumptions further, let us assume simply that \\[\n(\\boldsymbol{x}_{i*}, y_i) \\overset{\\text{i.i.d.}}{\\sim} F\n\\] for some joint distribution \\(F\\). Unlike the parametric and residual bootstraps, the pairs bootstrap treats the predictors \\(\\boldsymbol X\\) as random rather than fixed. We can then fit \\(\\widehat F\\) as the empirical distribution of the data: \\[\n\\widehat F = \\frac{1}{n} \\sum_{i=1}^n \\delta_{(\\boldsymbol{x}_{i*}, y_i)}.\n\\] The bootstrapped datasets can then be generated as follows: \\[\n(\\boldsymbol{x}_{i*}^{(b)}, y_i^{(b)}) \\overset{\\text{i.i.d.}}{\\sim} \\widehat F \\quad \\text{for } i = 1, \\dots, n.\n\\] Note that sampling the pairs \\((\\boldsymbol{x}_{i*}^{(b)}, y_i^{(b)})\\) from \\(\\widehat F\\) is equivalent to sampling with replacement from the rows of the original data. The benefit of the pairs bootstrap is that it does not assume homoskedasticity since the error variance is allowed to depend on \\(\\boldsymbol{x}_{i*}\\). Therefore, the pairs bootstrap addresses both non-normality and heteroskedasticity, though it does not address correlated errors (though variants of the pairs bootstrap do; see below). Note that the pairs bootstrap does not even assume that \\(\\mathbb{E}[y_i] = \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}\\) for some \\(\\boldsymbol{\\beta}\\). However, in the presence of model bias, it is unclear for what parameters we are even doing inference. While the pairs bootstrap assumes less than the residual bootstrap, it may be somewhat less efficient in the case when the assumptions of the latter are met. However, the pairs bootstrap is the most commonly applied flavor of the bootstrap.\n\n\n15.3.4 Clustered bootstrap\nIn the presence of clustered errors, the pairs bootstrap can be modified to the clustered bootstrap. The distributional assumption underlying the clustered bootstrap is the following: \\[\n\\{(\\boldsymbol{x}_{i*}, y_i): c(i) = c\\} \\overset{\\text{i.i.d.}}{\\sim} F \\quad \\text{for } c = 1, \\dots, C,\n\\tag{15.8}\\] where \\(c(i)\\) is the cluster to which observation \\(i\\) belongs. Therefore, entire clusters are modeled as coming i.i.d. from some distribution across clusters. As with the pairs bootstrap, this distribution is estimated by the empirical distribution of the data, and resampling from this distribution amounts to sampling entire clusters (rather than individual observations) from the original data, with replacement. This kind of resampling preserves the joint correlation structure within clusters. Note that the clustered bootstrap is a special case of the pairs bootstrap, where each pair forms its own cluster.\n\n\n15.3.5 Moving blocks bootstrap\nIn the case of temporally (or spatially) correlated errors, the pairs bootstrap can be modified to the moving blocks bootstrap. The distributional assumption underlying the moving blocks bootstrap is the same as that of the clustered bootstrap (15.8), except the clusters are defined as contiguous blocks of observations. The distribution across blocks is fit as the empirical distribution of all blocks of a given size, and resampling from this distribution amounts to sampling entire blocks (rather than individual observations) from the original data, with replacement. This kind of resampling preserves the joint correlation structure within temporal or spatial blocks, though it ignores correlations across boundaries of these blocks. Like the clustered bootstrap, the moving blocks bootstrap is a special case of the pairs bootstrap."
  },
  {
    "objectID": "bootstrap.html#sec-bootstrap-hypothesis-testing",
    "href": "bootstrap.html#sec-bootstrap-hypothesis-testing",
    "title": "15  The bootstrap",
    "section": "15.4 Bootstrap hypothesis testing",
    "text": "15.4 Bootstrap hypothesis testing\nThe bootstrap inference paradigm described in Section 15.1.2 is primarily for constructing confidence intervals. For one-dimensional quantities like \\(\\beta_j\\), confidence intervals can be used to perform hypothesis tests via duality. However, it is more challenging to use the bootstrap to create confidence regions for multi-dimensional quantities like \\(\\boldsymbol{\\beta}_S\\). Nevertheless, in some cases the bootstrap paradigm can be adapted to perform hypothesis tests directly.\n\n15.4.1 Bootstrap testing paradigm\n\n\n\n\n\n\nBootstrap paradigm to test \\(H_0: \\boldsymbol \\beta_S = 0\\)\n\n\n\n\nCompute a test statistic \\(T(\\boldsymbol X, \\boldsymbol y)\\) measuring the evidence against \\(H_0\\).\nUse the data \\((\\boldsymbol X, \\boldsymbol y)\\) to get an approximation \\(\\widehat F\\) for the data distribution \\(F\\).\nFind a null data distribution \\(\\widehat F_0\\) by “projecting” \\(\\widehat F\\) onto \\(H_0\\).\nFor each \\(b = 1, \\dots, B\\),\n\nSample a null bootstrap dataset \\((\\boldsymbol X^{(b)}, \\boldsymbol y^{(b)}) \\sim \\widehat F_0\\);\nEvaluate the test statistic on the resampled data to get \\(T(\\boldsymbol X^{(b)}, \\boldsymbol y^{(b)})\\).\n\nEvaluate the empirical quantile \\(\\mathbb Q_{1-\\alpha}(\\{T(\\boldsymbol X^{(b)}, \\boldsymbol y^{(b)})\\}_{b = 1}^B)\\).\nReject if \\(T(\\boldsymbol X, \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}(\\{T(\\boldsymbol X^{(b)}, \\boldsymbol y^{(b)})\\}_{b = 1}^B)\\).\n\n\n\nHere, the key new step is the third, in which a null data distribution \\(\\widehat F_0\\) is derived from the approximate data distribution \\(\\widehat F\\). The challenge is that, depending on the form of \\(\\widehat F\\), this may or may not be possible. In fact, obtaining \\(\\widehat F_0\\) from \\(\\widehat F\\) is easily done whenever the model for the data involves a parameter vector \\(\\boldsymbol \\beta\\), as is the case for the parametric and residual bootstraps (see the next section for more detail on the latter). In this case, \\(\\widehat F_0\\) can be obtained from \\(\\widehat F\\) by setting the coefficients \\(\\boldsymbol \\beta_S\\) to zero. On the other hand, for the pairs bootstrap and its variants, it is not clear how to obtain \\(\\widehat F_0\\) from \\(\\widehat F\\). Finally, note that the bootstrap testing paradigm is in principle compatible with any test statistic \\(T\\). A popular choice for \\(T\\) is the \\(F\\)-statistic for \\(H_0: \\boldsymbol \\beta_S = 0\\) from Unit 2.\n\n\n15.4.2 Testing with the residual bootstrap\nA commonly used bootstrap flavor for hypothesis testing is the residual bootstrap. Recalling the data-generating model (15.7), suppose \\(\\widehat F = (\\boldsymbol{\\widehat \\beta}, \\widehat G)\\) is the fitted model. Then, we can define \\(\\widehat F_0 = ((\\boldsymbol 0, \\boldsymbol{\\widehat \\beta}_{\\text{-}S}), \\widehat G)\\). Therefore, the bootstrapped null data are drawn from the following distribution: \\[\nx_{i*}^{(b)} = x_{i*}; \\quad y_i^{(b)} = \\boldsymbol x_{i, \\text{-}S}^T \\boldsymbol{\\widehat \\beta}_{\\text{-}S} + \\epsilon_i^{(b)}, \\quad \\epsilon_i^{(b)} \\overset{\\text{i.i.d.}}\\sim \\widehat G.\n\\] As before, the bootstrapped residuals \\(\\epsilon_i^{(b)}\\) are sampled with replacement from the set of original residuals."
  },
  {
    "objectID": "permutation-test.html#general-formulation-of-the-permutation-test",
    "href": "permutation-test.html#general-formulation-of-the-permutation-test",
    "title": "16  The permutation test",
    "section": "16.1 General formulation of the permutation test",
    "text": "16.1 General formulation of the permutation test\nNote that the null hypothesis can be formulated as follows: \\[\nH_0: y_i \\overset{\\text{i.i.d.}}\\sim G \\quad \\text{for some distribution } G.\n\\] If we view the model matrix \\(\\boldsymbol X\\) as random, then we can also formulate \\(H_0\\) as an independence null hypothesis: \\[\nH_0: \\boldsymbol{x}_{\\text{-}0} \\perp\\!\\!\\!\\perp y.\n\\] Both of these reformulations suggest that, under the null hypothesis, the null distribution of the data \\((\\boldsymbol X, \\boldsymbol y)\\) is invariant to permutations of \\(\\boldsymbol y\\), while keeping \\(\\boldsymbol X\\) fixed. In other words, \\[\n(\\boldsymbol X, \\pi \\boldsymbol y) \\overset{d}{=} (\\boldsymbol X, \\boldsymbol y), \\quad \\text{for all } \\pi \\in \\mathcal{S}_n,\n\\tag{16.3}\\] where \\(\\mathcal{S}_n\\) is the group of all permutations of \\(\\{1, \\ldots, n\\}\\) and \\(\\pi \\boldsymbol y\\) is the permuted response vector. Therefore, we can use permuted instances of the data to approximate the null distribution of any test statistic under \\(H_0\\). There are two instances of the permutation test: one based on the entire group \\(\\mathcal S_n\\) and the other based on a random sample of \\(\\mathcal S_n\\).\n\n16.1.1 Permutation test based on the entire permutation group\nConsider any test statistic \\(T: (\\boldsymbol X, \\boldsymbol y) \\mapsto \\mathbb R\\). For example, this may be the usual \\(F\\)-statistic for testing the hypothesis (16.2) in the model (16.1). Then, the permutation test based on the entire permutation group is as follows:\n\n\n\n\n\n\nPermutation test based on the entire permutation group\n\n\n\n\nCompute the observed value of the test statistic \\(T(\\boldsymbol X, \\boldsymbol y)\\).\nFor each \\(\\pi \\in \\mathcal{S}_n\\), compute the test statistic on the permuted data, \\(T(\\boldsymbol X, \\pi \\boldsymbol y)\\).\nCompute the quantile \\(\\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]\\).\nReject if \\(T(\\boldsymbol X, \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]\\).\n\n\n\nAs claimed at the outset of this chapter, this test has non-asymptotic Type-I error control.\n\nTheorem 16.1 For any \\(n\\), the permutation test based on the entire permutation group has Type-I error at most \\(\\alpha\\) for testing the null hypothesis \\(H_0: \\boldsymbol{\\beta}_{\\text{-}0} = \\boldsymbol{0}\\).\n\n\nProof. Suppose \\(H_0\\) holds. Let \\(\\tau \\in \\mathcal S_n\\). Then, by the permutation invariance property (16.3), we have \\[\n\\begin{split}\n&\\mathbb P[T(\\boldsymbol X, \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]] \\\\\n&\\quad= \\mathbb P[T(\\boldsymbol X, \\tau (\\tau^{-1} \\boldsymbol y)) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\tau (\\tau^{-1}  \\boldsymbol y)): \\pi \\in \\mathcal S_n\\}]] \\\\\n&\\quad = \\mathbb P[T(\\boldsymbol X, \\tau \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\tau \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]] \\\\\n&\\quad = \\mathbb P[T(\\boldsymbol X, \\tau \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]].\n\\end{split}\n\\] Therefore, the probability that \\(T(\\boldsymbol X, \\boldsymbol y)\\) exceeds the \\((1-\\alpha)\\)-quantile of the permutation distribution is the same as the probability that any other permuted test statistic exceeds the \\((1-\\alpha)\\)-quantile. Therefore, we have \\[\n\\begin{split}\n&\\mathbb P[T(\\boldsymbol X, \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]] \\\\\n&\\quad= \\frac{1}{|\\mathcal S_n|} \\sum_{\\tau \\in \\mathcal S_n} \\mathbb P[T(\\boldsymbol X, \\tau \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}]] \\\\\n&\\quad = \\mathbb E\\left[\\frac{|\\{\\tau \\in \\mathcal S_n: T(\\boldsymbol X, \\tau \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\pi \\boldsymbol y): \\pi \\in \\mathcal S_n\\}\\}|}{|\\mathcal S_n|}\\right] \\\\\n&\\quad \\leq \\alpha.\n\\end{split}\n\\tag{16.4}\\]\n\n\n\n16.1.2 Permutation test based on a sample of the permutation group\nThe permutation test based on the entire permutation group is computationally infeasible for large \\(n\\). Instead, we can use a random sample of the permutation group to approximate the null distribution of the test statistic.\n\n\n\n\n\n\nPermutation test based on a sample of the permutation group\n\n\n\n\nCompute the observed value of the test statistic \\(T(\\boldsymbol X, \\boldsymbol y)\\).\nDraw a random sample \\((\\pi_1, \\dots, \\pi_B)\\) from \\(\\mathcal S_n\\).\nFor each \\(b = 1, \\dots, B\\), compute the test statistic on the permuted data, \\(T(\\boldsymbol X, \\pi_b \\boldsymbol y)\\).\nCompute the quantile \\(\\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\boldsymbol y): b = 1, \\dots, B\\}]\\).\nReject if \\(T(\\boldsymbol X, \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\boldsymbol y): b = 1, \\dots, B\\}]\\).\n\n\n\nThis gives us not just an approximation to the permutation test based on the entire permutation group, but a finite-sample valid test in its own right. The inclusion of the original test statistic \\(T(\\boldsymbol X, \\boldsymbol y)\\) in the quantile computation ensures that the test has finite-sample Type-I error control; see the exchangeability-based argument in the proof.\n\nTheorem 16.2 For any \\(n\\), the permutation test based on a sample of the permutation group has Type-I error at most \\(\\alpha\\) for testing the null hypothesis \\(H_0: \\boldsymbol{\\beta}_{\\text{-}0} = \\boldsymbol{0}\\).\n\n\nProof. Suppose \\(H_0\\) holds. We claim that the \\(B+1\\) test statistics \\[\n\\{T(\\boldsymbol X, \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\boldsymbol y): b = 1, \\dots, B\\}\n\\] are exchangeable, i.e., their joint distribution is independent of their ordering. To see that, let \\(\\tau\\) be a randomly sampled permutation from \\(\\mathcal S_n\\). Then, by the permutation invariance property (16.3), we have \\[\n\\begin{split}\n&\\{T(\\boldsymbol X, \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\boldsymbol y): b = 1, \\dots, B\\} \\\\\n&\\quad\\stackrel{d}{=} \\{T(\\boldsymbol X, \\tau \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\tau \\boldsymbol y): b = 1, \\dots, B\\}.\n\\end{split}\n\\] It is not hard to see that \\(\\{\\tau, \\pi_1 \\tau, \\dots, \\pi_B\\tau\\}\\) is an i.i.d. sample from \\(\\mathcal S_n\\), from which the claimed exchangeability follows. From this exchangeability, we get that \\[\n\\begin{split}\n&\\mathbb P[T(\\boldsymbol X, \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\boldsymbol y): b = 1, \\dots, B\\}]] \\\\\n&\\quad= \\mathbb P[T(\\boldsymbol X, \\pi_b \\boldsymbol y) &gt; \\mathbb Q_{1-\\alpha}[\\{T(\\boldsymbol X, \\boldsymbol y)\\} \\cup \\{T(\\boldsymbol X, \\pi_b \\boldsymbol y): b = 1, \\dots, B\\}]]\n\\end{split}\n\\] for each \\(b = 1, \\dots, B\\), from which Type-I error control follows by the same argument as in the derivation (16.4).\n\n\n\n16.1.3 Obtaining \\(p\\)-values from permutation tests\nIn some cases, it is desirable to extract \\(p\\)-values from permutation tests, rather than just the decision to accept or reject at a fixed level \\(\\alpha\\). The \\(p\\)-value is the smallest level at which the null hypothesis can be rejected, i.e., the probability under the permutation distribution of observing a test statistic at least as extreme as the observed test statistic. For permutation tests based on the full permutation group, the \\(p\\)-value can be computed as follows: \\[\np = \\frac{1}{|\\mathcal S_n|}\\sum_{\\tau \\in \\mathcal S_n} \\mathbb I\\{T(\\boldsymbol X, \\tau \\boldsymbol y) \\geq T(\\boldsymbol X, \\boldsymbol y)\\}.\n\\] For permutation tests based on a sample of the permutation group, the \\(p\\)-value can be computed as \\[\np = \\frac{1}{B+1}\\left(1 + \\sum_{b=1}^B \\mathbb I\\{T(\\boldsymbol X, \\pi_b \\boldsymbol y) \\geq T(\\boldsymbol X, \\boldsymbol y)\\}\\right).\n\\tag{16.5}\\]\n\n\n\n\n\n\nWarning\n\n\n\nA common mistake is to omit the “1+” term from the numerator and denominator of equation (16.5). These terms are essential for constructing a valid \\(p\\)-value. In particular, these terms prevent the \\(p\\)-value from being exactly zero."
  },
  {
    "objectID": "permutation-test.html#special-case-two-groups-model",
    "href": "permutation-test.html#special-case-two-groups-model",
    "title": "16  The permutation test",
    "section": "16.2 Special case: Two-groups model",
    "text": "16.2 Special case: Two-groups model\nThe most common application of the permutation test is to the two-groups model: \\[\ny_i = \\beta_0 + \\beta_1 x_{i,1} + \\epsilon_i, \\quad \\text{where } x_{i,1} \\in \\{0, 1\\}.\n\\] The goal here is to test whether the binary “treatment” variable has any effect on the response variable: \\[\nH_0: \\beta_1 = 0.\n\\] To make the two groups more explicit, we can write \\[\n\\{y_i: x_{i,1} = 0\\} \\overset{\\text{i.i.d.}}\\sim G_0, \\quad \\{y_i: x_{i,1} = 1\\} \\overset{\\text{i.i.d.}}\\sim G_1,\n\\] and the null hypothesis can be reformulated as \\[\nH_0: G_0 = G_1.\n\\] In this case, the permutation mechanism randomly reassigns observations to the two groups. A commonly used test statistic \\(T\\) used in conjunction with this test is the difference in means between the two groups. While the permutation test controls Type-I error exactly under the hypothesis that the two groups come from exactly the same distribution, we might want to test a weaker hypothesis that the two groups have the same mean. It turns out that, at least asymptotically, the permutation test controls Type-I error under this weaker null hypothesis if it is based on a studentized statistic, such as \\[\nT(\\boldsymbol X, \\boldsymbol y) = \\frac{\\bar y_1 - \\bar y_0}{\\sqrt{\\frac{\\hat \\sigma_0^2}{n_0} + \\frac{\\hat \\sigma_1^2}{n_1}}},\n\\] where \\(\\bar y_0\\) and \\(\\bar y_1\\) are the sample means of the two groups, \\(\\hat \\sigma_0^2\\) and \\(\\hat \\sigma_1^2\\) are the sample variances of the two groups, and \\(n_0\\) and \\(n_1\\) are the sample sizes of the two groups."
  },
  {
    "objectID": "permutation-test.html#permutation-test-versus-bootstrap",
    "href": "permutation-test.html#permutation-test-versus-bootstrap",
    "title": "16  The permutation test",
    "section": "16.3 Permutation test versus bootstrap",
    "text": "16.3 Permutation test versus bootstrap\nThe bootstrap and permutation are both resampling-based tests that use computation as a substitute for mathematical derivations of sampling distributions. Both methods have better finite-sample performance than their asymptotic counterparts. The bootstrap and the permutation test are typically considered primarily in the context of confidence interval construction and hypothesis testing, respectively, although the bootstrap can also be used for hypothesis testing in certain cases. The key difference is that the permutation test has valid Type-I error control in finite samples, while the bootstrap requires an asymptotic justification (even if the asymptotic convergence is faster than typical CLT-based asymptotics). Furthermore, the bootstrap is somewhat more versatile than the permutation test, as the latter is restricted to testing null hypotheses about all non-intercept coefficients."
  },
  {
    "objectID": "robust-estimation.html#drawback-of-squared-error-loss",
    "href": "robust-estimation.html#drawback-of-squared-error-loss",
    "title": "17  Robust estimation and inference",
    "section": "17.1 Drawback of squared error loss",
    "text": "17.1 Drawback of squared error loss\nSuppose that \\[\ny_i = \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{i.i.d.}}\\sim G, \\quad i = 1, \\ldots, n, \\quad\n\\tag{17.1}\\] for some distribution \\(G\\). If the distribution \\(G\\) has heavy tails, then the residuals will contain outliers. Recall that the least squares estimate is defined as \\[\n\\boldsymbol{\\widehat \\beta} \\equiv \\underset{\\boldsymbol{\\beta}}{\\arg \\min}\\ \\sum_{i = 1}^n L(y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}), \\quad \\text{where} \\quad L(d) \\equiv \\frac{1}{2}d^2.\n\\] The squared error loss \\(L(d)\\) is sensitive to outliers in the sense that a large value of \\(d_i \\equiv y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}\\) can have a significant impact on the loss function \\(L(d_i)\\). The least squares estimate, as the minimizer of this loss function, is therefore sensitive to outliers."
  },
  {
    "objectID": "robust-estimation.html#the-huber-loss",
    "href": "robust-estimation.html#the-huber-loss",
    "title": "17  Robust estimation and inference",
    "section": "17.2 The Huber loss",
    "text": "17.2 The Huber loss\nOne way of addressing this challenge is to replace the squared error loss with a different loss that does not grow so quickly in \\(y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}\\). A popular choice for such a loss function is the Huber loss:\n\\[\nL_\\delta(d) =\n\\begin{cases}\n\\frac{1}{2}d^2, \\quad &\\text{if } |d| \\leq \\delta; \\\\\n\\delta(|d|-\\tfrac12\\delta), \\quad &\\text{if } |d| &gt; \\delta.\n\\end{cases}\n\\]\nThis function is differentiable at the origin, like the squared error loss, but grows linearly as opposed to quadratically."
  },
  {
    "objectID": "robust-estimation.html#scale-estimation",
    "href": "robust-estimation.html#scale-estimation",
    "title": "17  Robust estimation and inference",
    "section": "17.3 Scale estimation",
    "text": "17.3 Scale estimation\nThe choice of \\(\\delta &gt; 0\\) depends on the scale of the noise terms \\(\\epsilon_i\\). Supposing that \\(\\text{Var}[\\epsilon_i] = \\sigma^2\\), a large residual is one where \\(|\\epsilon_i/\\sigma|\\) is large. In this sense, \\(\\delta\\) should be on the same scale as \\(\\sigma\\). Of course, \\(\\sigma\\) is unknown, so a first step towards obtaining a robust estimate is to estimate \\(\\sigma\\). While we would usually estimate \\(\\sigma\\) based on the residuals from the least squares estimate, this approach is not robust to outliers. Instead, we can obtain a pilot estimate of the coefficients using the least absolute deviation (LAD) estimator, a scale-free and outlier-robust estimate: \\[\n\\boldsymbol{\\widehat{\\beta}}^{\\text{LAD}} \\equiv \\underset{\\boldsymbol{\\beta}}{\\arg \\min}\\ \\sum_{i = 1}^n |y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}|.\n\\] Then, we can estimate \\(\\sigma\\) from the residuals based on the LAD estimate. Since some of these residuals are outliers, it is better to avoid simply taking a sample variance. Instead, we can use the median absolute deviation (MAD) of the residuals, which is a robust estimate of the scale of the noise terms. \\[\n\\widehat{\\sigma} \\equiv \\frac{1}{0.675}\\text{median}\\left\\{ |y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat{\\beta}}^{\\text{LAD}}| \\right\\}.\n\\] The purpose of the scaling factor of \\(0.675\\) is to connect the MAD to the standard deviation of the distribution of \\(\\epsilon_i\\); it is derived based on the normal distribution.\n\n\n\n\n\n\nNote\n\n\n\nIn principle, \\(\\widehat{\\boldsymbol \\beta}^{\\text{LAD}}\\) could be used not just for estimation of \\(\\sigma\\) but also for inference for \\(\\boldsymbol \\beta\\) itself. However, the LAD estimator may be less efficient than the Huber estimator, so the latter estimator is usually preferred."
  },
  {
    "objectID": "robust-estimation.html#huber-estimation",
    "href": "robust-estimation.html#huber-estimation",
    "title": "17  Robust estimation and inference",
    "section": "17.4 Huber estimation",
    "text": "17.4 Huber estimation\nWith an estimate of \\(\\sigma\\) in hand, we can use the Huber loss function to estimate \\(\\boldsymbol{\\beta}\\): \\[\n\\boldsymbol{\\widehat{\\beta}}^{\\text{Huber}} \\equiv \\underset{\\boldsymbol{\\beta}}{\\arg \\min}\\ \\sum_{i = 1}^n L_\\delta\\left(\\frac{y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}}{\\widehat \\sigma}\\right).\n\\]\nA common choice for \\(\\delta\\) is \\(\\delta = 1.345\\), which makes the Huber estimator 95% efficient relative to the least squares estimator under normality. The resulting \\(\\boldsymbol{\\widehat{\\beta}}^{\\text{Huber}}\\) is an M-estimator. We can compute this estimator by taking a derivative of the objective and setting it to zero: \\[\n\\sum_{i = 1}^n L'_\\delta\\left(\\frac{y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}}{\\widehat \\sigma}\\right) \\boldsymbol{x}_{i*} = 0.\n\\] Unlike least squares, this equation does not have a closed-form solution. However, it can be solved using an iterative algorithm. Under certain assumptions, the resulting estimator can be shown to be consistent."
  },
  {
    "objectID": "robust-estimation.html#inference-based-on-huber-estimates",
    "href": "robust-estimation.html#inference-based-on-huber-estimates",
    "title": "17  Robust estimation and inference",
    "section": "17.5 Inference based on Huber estimates",
    "text": "17.5 Inference based on Huber estimates\nWe can construct hypothesis tests and confidence intervals using \\(\\boldsymbol{\\widehat{\\beta}}^{\\text{Huber}}\\) based on the following result.\n\nTheorem 17.1 (Asymptotic normality of Huber estimator (informal)) Suppose the data \\((\\boldsymbol X, \\boldsymbol y)\\) follow the model (17.1), with fixed design matrix \\(\\boldsymbol X\\). Then, if \\(\\hat \\sigma\\) is a consistent estimator of \\(\\sigma\\) and if the noise distribution \\(G\\) is symmetric, then\n\\[\n\\boldsymbol{\\widehat{\\beta}}^{\\text{Huber}} \\overset \\cdot \\sim N(\\boldsymbol{\\beta}, v (\\boldsymbol X^T \\boldsymbol X)^{-1}), \\quad \\text{where} \\quad v \\equiv \\sigma^2 \\frac{\\mathbb E[L'_\\delta(\\epsilon_i/\\sigma)^2]}{\\mathbb E[L''_\\delta(\\epsilon_i/\\sigma)]^2}.\n\\] Letting \\(\\hat \\epsilon_i \\equiv y_i - \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat{\\beta}}^{\\text{Huber}}\\), we can estimate \\(v\\) via \\[\n\\widehat v \\equiv \\hat \\sigma^2 \\frac{\\frac{1}{n} \\sum_{i = 1}^n L'_\\delta(\\hat \\epsilon_i/\\hat \\sigma)^2}{\\left(\\frac{1}{n} \\sum_{i = 1}^n L''_\\delta(\\hat \\epsilon_i/\\hat \\sigma)\\right)^2}.\n\\] Under appropriate regularity conditions, \\(\\widehat v\\) is a consistent estimator of \\(v\\), so that \\[\n\\boldsymbol{\\widehat{\\beta}}^{\\text{Huber}} \\overset \\cdot \\sim N(\\boldsymbol{\\beta}, \\hat v (\\boldsymbol X^T \\boldsymbol X)^{-1}).\n\\]"
  },
  {
    "objectID": "r-demo-part-3.html#heteroskedasticity",
    "href": "r-demo-part-3.html#heteroskedasticity",
    "title": "18  R demo",
    "section": "18.1 Heteroskedasticity",
    "text": "18.1 Heteroskedasticity\nNext, let’s look at another dataset, from the Current Population Survey (CPS).\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\n\ncps_data &lt;- read_tsv(\"data/cps2.tsv\")\ncps_data\n\n# A tibble: 1,000 × 10\n    wage  educ exper female black married union south fulltime metro\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1  2.03    13     2      1     0       0     0     1        0     0\n 2  2.07    12     7      0     0       0     0     0        0     1\n 3  2.12    12    35      0     0       0     0     1        1     1\n 4  2.54    16    20      1     0       0     0     1        1     1\n 5  2.68    12    24      1     0       1     0     1        0     1\n 6  3.09    13     4      0     0       0     0     1        0     1\n 7  3.16    13     1      0     0       0     0     0        0     0\n 8  3.17    12    22      1     0       1     0     1        0     1\n 9  3.2     12    23      0     0       1     0     1        1     1\n10  3.27    12     4      1     0       0     0     0        1     1\n# ℹ 990 more rows\n\n\nSuppose we want to regress wage on educ, exper, and metro.\n\nlm_fit &lt;- lm(wage ~ educ + exper + metro, data = cps_data)\n\n\n18.1.1 Diagnostics\nLet’s take a look at the standard linear model diagnostic plots built into R.\n\n# residuals versus fitted\nplot(lm_fit, which = 1)\n\n\n\n# residual QQ plot\nplot(lm_fit, which = 2)\n\n\n\n# residuals versus leverage (with Cook's distance)\nplot(lm_fit, which = 5)\n\n\n\n\nThe residuals versus fitted plot suggests significant heteroskedasticity, with variance growing as a function of the fitted value.\n\n\n18.1.2 Sandwich standard errors\nTo get standard errors robust to this heteroskedasticity, we can use one of the robust estimators discussed in Section 13.2. Most of the robust standard error constructions discussed in that section are implemented in the R package sandwich.\n\nlibrary(sandwich)\n\nFor example, Huber-White’s heteroskedasticity-consistent estimate \\(\\widehat{\\text{Var}}[\\boldsymbol{\\widehat \\beta}]\\) can be obtained via vcovHC:\n\nHW_cov &lt;- vcovHC(lm_fit)\nHW_cov\n\n             (Intercept)          educ         exper         metro\n(Intercept)  1.484328645 -0.0967891868 -0.0096871141 -0.1218518012\neduc        -0.096789187  0.0070467982  0.0004037764  0.0018334348\nexper       -0.009687114  0.0004037764  0.0002517826  0.0008369831\nmetro       -0.121851801  0.0018334348  0.0008369831  0.1197713348\n\n\nCompare this to the traditional estimate:\n\nusual_cov &lt;- vcovHC(lm_fit, type = \"const\")\nusual_cov\n\n             (Intercept)          educ         exper         metro\n(Intercept)  1.157049852 -0.0671656102 -0.0070323974 -0.1287058354\neduc        -0.067165610  0.0048945781  0.0001924359 -0.0018227782\nexper       -0.007032397  0.0001924359  0.0002320022  0.0001471354\nmetro       -0.128705835 -0.0018227782  0.0001471354  0.1858394060\n\n# extract the variance estimates from the diagonal\ntibble(\n  variable = rownames(usual_cov),\n  usual_variance = sqrt(diag(usual_cov)),\n  HW_variance = sqrt(diag(HW_cov))\n)\n\n# A tibble: 4 × 3\n  variable    usual_variance HW_variance\n  &lt;chr&gt;                &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)         1.08        1.22  \n2 educ                0.0700      0.0839\n3 exper               0.0152      0.0159\n4 metro               0.431       0.346 \n\n\nBootstrap standard errors are also implemented in sandwich:\n\n# pairs bootstrap\nbootstrap_cov &lt;- vcovBS(lm_fit, type = \"xy\")\ntibble(\n  variable = rownames(usual_cov),\n  usual_variance = diag(usual_cov),\n  HW_variance = diag(HW_cov),\n  bootstrap_variance = diag(bootstrap_cov)\n)\n\n# A tibble: 4 × 4\n  variable    usual_variance HW_variance bootstrap_variance\n  &lt;chr&gt;                &lt;dbl&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 (Intercept)       1.16        1.48               1.54    \n2 educ              0.00489     0.00705            0.00769 \n3 exper             0.000232    0.000252           0.000245\n4 metro             0.186       0.120              0.116   \n\n\nThe covariance estimate produced by sandwich can be easily integrated into linear model inference using the package lmtest.\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# fit linear model as usual\nlm_fit &lt;- lm(wage ~ educ + exper + metro, data = cps_data)\n\n# robust t-tests for coefficients\ncoeftest(lm_fit, vcov. = vcovHC)\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -9.913984   1.218330 -8.1374 1.197e-15 ***\neduc         1.233964   0.083945 14.6996 &lt; 2.2e-16 ***\nexper        0.133244   0.015868  8.3972 &lt; 2.2e-16 ***\nmetro        1.524104   0.346080  4.4039 1.178e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# robust confidence intervals for coefficients\ncoefci(lm_fit, vcov. = vcovHC)\n\n                  2.5 %     97.5 %\n(Intercept) -12.3047729 -7.5231954\neduc          1.0692342  1.3986938\nexper         0.1021058  0.1643816\nmetro         0.8449747  2.2032337\n\n# robust F-test\nlm_fit_partial &lt;- lm(wage ~ educ, data = cps_data) # a partial model\nwaldtest(lm_fit_partial, lm_fit, vcov = vcovHC)\n\nWald test\n\nModel 1: wage ~ educ\nModel 2: wage ~ educ + exper + metro\n  Res.Df Df      F    Pr(&gt;F)    \n1    998                        \n2    996  2 40.252 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n18.1.3 Bootstrap confidence intervals\nOne R package for performing bootstrap inference is simpleboot. Let’s see how to get pairs bootstrap distributions for the coefficient estimates.\n\nlibrary(simpleboot)\n\nSimple Bootstrap Routines (1.1-8)\n\nboot_out &lt;- lm.boot(\n  lm.object = lm_fit, # input the fit object from lm()\n  R = 1000\n) # R is the number of bootstrap replicates\nperc(boot_out) # get the percentile 95% confidence intervals\n\n      (Intercept)     educ     exper     metro\n2.5%   -12.593797 1.071418 0.1031716 0.8449157\n97.5%   -7.645662 1.417886 0.1681944 2.1429463\n\n\nWe can extract the resampling distributions for the coefficient estimates using the samples function:\n\nsamples(boot_out, name = \"coef\")[, 1:5]\n\n                    1          2          3           4           5\n(Intercept) -9.506667 -9.9339448 -7.8354993 -11.5963414 -11.2588844\neduc         1.174144  1.2300298  1.1084766   1.3158311   1.3653399\nexper        0.149829  0.1301164  0.1103397   0.1497207   0.1170897\nmetro        1.866064  1.5479406  1.6266666   1.8888379   1.7062736\n\n\nWe can plot these as follows:\n\nboot_pctiles &lt;- boot_out |&gt;\n  perc() |&gt;\n  t() |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"var\") |&gt;\n  filter(var != \"(Intercept)\")\n\nsamples(boot_out, name = \"coef\") |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"var\") |&gt;\n  filter(var != \"(Intercept)\") |&gt;\n  pivot_longer(-var, names_to = \"resample\", values_to = \"coefficient\") |&gt;\n  group_by(var) |&gt;\n  ggplot(aes(x = coefficient)) +\n  geom_histogram(bins = 30, colour = \"black\") +\n  geom_vline(aes(xintercept = `2.5%`), data = boot_pctiles, linetype = \"dashed\") +\n  geom_vline(aes(xintercept = `97.5%`), data = boot_pctiles, linetype = \"dashed\") +\n  facet_wrap(~var, scales = \"free\")\n\n\n\n\nIn this case, the bootstrap sampling distributions look roughly normal."
  },
  {
    "objectID": "r-demo-part-3.html#group-correlated-errors",
    "href": "r-demo-part-3.html#group-correlated-errors",
    "title": "18  R demo",
    "section": "18.2 Group-correlated errors",
    "text": "18.2 Group-correlated errors\nCredit for this data example: https://www.r-bloggers.com/2021/05/clustered-standard-errors-with-r/.\nLet’s consider the nslwork data from the webuse package:\n\nlibrary(webuse)\nnlswork_orig &lt;- webuse(\"nlswork\")\nnlswork &lt;- nlswork_orig |&gt;\n  filter(idcode &lt;= 100) |&gt;\n  select(idcode, year, ln_wage, age, tenure, union) |&gt;\n  na.omit() |&gt;\n  mutate(\n    union = as.integer(union),\n    idcode = as.factor(idcode)\n  )\nnlswork\n\n# A tibble: 386 × 6\n   idcode  year ln_wage   age tenure union\n   &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n 1 1         72    1.59    20  0.917     1\n 2 1         77    1.78    25  1.5       0\n 3 1         80    2.55    28  1.83      1\n 4 1         83    2.42    31  0.667     1\n 5 1         85    2.61    33  1.92      1\n 6 1         87    2.54    35  3.92      1\n 7 1         88    2.46    37  5.33      1\n 8 2         71    1.36    19  0.25      0\n 9 2         77    1.73    25  2.67      1\n10 2         78    1.69    26  3.67      1\n# ℹ 376 more rows\n\n\nThe data comes from the US National Longitudinal Survey (NLS) and contains information about more than 4,000 young working women. We’re interested in the relationship between wage (here as log-scaled GNP-adjusted wage) ln_wage and survey participant’s current age, job tenure in years, and union membership as independent variables. It’s a longitudinal survey, so subjects were asked repeatedly between 1968 and 1988, and each subject is identified by a unique idcode idcode. Here we restrict attention to the first 100 subjects and remove any rows with missing data.\nLet’s start by fitting a linear regression of the log wage on age, tenure, union, and the interaction between tenure and union:\n\nlm_fit &lt;- lm(ln_wage ~ age + tenure + union + tenure:union, data = nlswork)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = ln_wage ~ age + tenure + union + tenure:union, data = nlswork)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.42570 -0.28330  0.01694  0.27303  1.65052 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.379103   0.099658  13.838  &lt; 2e-16 ***\nage           0.013553   0.003388   4.000 7.60e-05 ***\ntenure        0.022175   0.008051   2.754  0.00617 ** \nunion         0.309936   0.070344   4.406 1.37e-05 ***\ntenure:union -0.009629   0.012049  -0.799  0.42473    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4099 on 381 degrees of freedom\nMultiple R-squared:  0.1811,    Adjusted R-squared:  0.1725 \nF-statistic: 21.07 on 4 and 381 DF,  p-value: 1.047e-15\n\n\nLet’s plot the residuals against the individuals:\n\nnlswork |&gt;\n  mutate(resid = lm_fit$residuals) |&gt;\n  ggplot(aes(x = idcode, y = resid)) +\n  geom_boxplot() +\n  labs(\n    x = \"Subject\",\n    y = \"Residual\"\n  ) +\n  theme(axis.text.x = element_blank())\n\n\n\n\nClearly, there is dependency among the residuals within subjects. Therefore, we have either model bias, or correlated errors, or both. To help assess whether we have model bias or not, we must check whether the variables of interest are correlated with the grouping variable idcode. We can check this with a plot, e.g., for the tenure variable:\n\nnlswork |&gt;\n  ggplot(aes(x = idcode, y = tenure)) +\n  geom_boxplot() +\n  labs(\n    x = \"Subject\",\n    y = \"Tenure\"\n  ) +\n  theme(axis.text.x = element_blank())\n\n\n\n\nAgain, there seems to be nontrivial association between tenure and idcode. We can check this more formally with an ANOVA test:\n\nsummary(aov(tenure ~ idcode, data = nlswork))\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nidcode       81   2529  31.220   3.558 8.83e-16 ***\nResiduals   304   2668   8.775                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, in this case, we do have model bias on our hands. We can address this using fixed effects for each subject.\n\nlm_fit_FE &lt;- lm(ln_wage ~ age + tenure + union + tenure:union + idcode, data = nlswork)\nlm_fit_FE |&gt;\n  summary() |&gt;\n  coef() |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"var\") |&gt;\n  filter(!grepl(\"idcode\", var)) |&gt; # remove coefficients for fixed effects\n  column_to_rownames(var = \"var\")\n\n                Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  1.882478232 0.131411504 14.325064 8.022367e-36\nage          0.005630809 0.003109803  1.810664 7.119315e-02\ntenure       0.020756426 0.006964417  2.980353 3.114742e-03\nunion        0.174619394 0.060646038  2.879321 4.272027e-03\ntenure:union 0.014974113 0.009548509  1.568215 1.178851e-01\n\n\nNote the changes in the standard errors and p-values. Sometimes, we may have remaining correlation among residuals even after adding cluster fixed effects. Therefore, it is common practice to compute clustered (i.e., Liang-Zeger) standard errors in conjunction with cluster fixed effects. We can get clustered standard errors via the vcovCL function from sandwich:\n\nLZ_cov &lt;- vcovCL(lm_fit_FE, cluster = nlswork$idcode)\ncoeftest(lm_fit_FE, vcov. = LZ_cov)[, ] |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"var\") |&gt;\n  filter(!grepl(\"idcode\", var)) |&gt; # remove coefficients for fixed effects\n  column_to_rownames(var = \"var\")\n\n                Estimate  Std. Error    t value     Pr(&gt;|t|)\n(Intercept)  1.882478232 0.157611390 11.9437956 3.667970e-27\nage          0.005630809 0.006339777  0.8881715 3.751601e-01\ntenure       0.020756426 0.011149190  1.8616981 6.362342e-02\nunion        0.174619394 0.101970509  1.7124500 8.784708e-02\ntenure:union 0.014974113 0.009646023  1.5523613 1.216301e-01\n\n\nAgain, note the changes in the standard errors and p-values."
  },
  {
    "objectID": "r-demo-part-3.html#autocorrelated-errors",
    "href": "r-demo-part-3.html#autocorrelated-errors",
    "title": "18  R demo",
    "section": "18.3 Autocorrelated errors",
    "text": "18.3 Autocorrelated errors\nLet’s take a look at the EuStockMarkets data built into R, containing the daily closing prices of major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. Let’s regress DAX on FTSE and take a look at the residuals:\n\nlm_fit &lt;- lm(DAX ~ FTSE, data = EuStockMarkets)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = DAX ~ FTSE, data = EuStockMarkets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-408.43 -172.53  -45.71  137.68  989.96 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.331e+03  2.109e+01  -63.12   &lt;2e-16 ***\nFTSE         1.083e+00  5.705e-03  189.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 240.3 on 1858 degrees of freedom\nMultiple R-squared:  0.951, Adjusted R-squared:  0.9509 \nF-statistic: 3.604e+04 on 1 and 1858 DF,  p-value: &lt; 2.2e-16\n\n\nWe find an extremely significant association between the two stock indices. But let’s examine the residuals for autocorrelation:\n\nEuStockMarkets |&gt;\n  as.data.frame() |&gt;\n  mutate(\n    date = row_number(),\n    resid = lm_fit$residuals\n  ) |&gt;\n  ggplot(aes(x = date, y = resid)) +\n  geom_line() +\n  labs(\n    x = \"Day\",\n    y = \"Residual\"\n  )\n\n\n\n\nThere is clearly some autocorrelation in the residuals. Let’s quantify it using the autocorrelation function (acf() in R):\n\nacf(lm_fit$residuals, lag.max = 1000)\n\n\n\n\nWe see that the autocorrelation gets into a reasonably low range around lag 200. We can then construct Newey-West standard errors based on this lag:\n\nNW_cov &lt;- NeweyWest(lm_fit)\ncoeftest(lm_fit, vcov. = NW_cov)\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -1331.2374  4398.3722 -0.3027   0.7622\nFTSE            1.0831     1.4645  0.7396   0.4597\n\n\nWe see that the p-value for the association goes from 2e-16 to 0.46, after accounting for autocorrelation."
  },
  {
    "objectID": "r-demo-part-3.html#outliers",
    "href": "r-demo-part-3.html#outliers",
    "title": "18  R demo",
    "section": "18.4 Outliers",
    "text": "18.4 Outliers\nLet’s take a look at the crime data from HW2:\n\n# read crime data\ncrime_data &lt;- read_tsv(\"data/Statewide_crime.dat\")\n\nRows: 51 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): STATE\ndbl (5): Violent, Murder, Metro, HighSchool, Poverty\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read and transform population data\npopulation_data &lt;- read_csv(\"data/state-populations.csv\")\n\nRows: 52 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (8): rank, Pop, Growth, Pop2018, Pop2010, growthSince2010, Percent, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopulation_data &lt;- population_data |&gt;\n  filter(State != \"Puerto Rico\") |&gt;\n  select(State, Pop) |&gt;\n  rename(state_name = State, state_pop = Pop)\n\n# collate state abbreviations\nstate_abbreviations &lt;- tibble(\n  state_name = state.name,\n  state_abbrev = state.abb\n) |&gt;\n  add_row(state_name = \"District of Columbia\", state_abbrev = \"DC\")\n\n# add CrimeRate to crime_data\ncrime_data &lt;- crime_data |&gt;\n  mutate(STATE = ifelse(STATE == \"IO\", \"IA\", STATE)) |&gt;\n  rename(state_abbrev = STATE) |&gt;\n  left_join(state_abbreviations, by = \"state_abbrev\") |&gt;\n  left_join(population_data, by = \"state_name\") |&gt;\n  mutate(CrimeRate = Violent / state_pop) |&gt;\n  select(state_abbrev, CrimeRate, Metro, HighSchool, Poverty)\n\ncrime_data\n\n# A tibble: 51 × 5\n   state_abbrev CrimeRate Metro HighSchool Poverty\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 AK           0.000819   65.6       90.2     8  \n 2 AL           0.0000871  55.4       82.4    13.7\n 3 AR           0.000150   52.5       79.2    12.1\n 4 AZ           0.0000682  88.2       84.4    11.9\n 5 CA           0.0000146  94.4       81.3    10.5\n 6 CO           0.0000585  84.5       88.3     7.3\n 7 CT           0.0000867  87.7       88.8     6.4\n 8 DE           0.000664   80.1       86.5     5.8\n 9 FL           0.0000333  89.3       85.9     9.7\n10 GA           0.0000419  71.6       85.2    10.8\n# ℹ 41 more rows\n\n\nLet’s fit the linear regression:\n\n# note: we make the state abbreviations row names for better diagnostic plots\nlm_fit &lt;- lm(CrimeRate ~ Metro + HighSchool + Poverty, data = crime_data |&gt; column_to_rownames(var = \"state_abbrev\"))\n\nWe can get the standard linear regression diagnostic plots as follows:\n\n# residuals versus fitted\nplot(lm_fit, which = 1)\n\n\n\n# residual QQ plot\nplot(lm_fit, which = 2)\n\n\n\n# residuals versus leverage (with Cook's distance)\nplot(lm_fit, which = 5)\n\n\n\n\nThe information underlying these diagnostic plots can be extracted as follows:\n\ntibble(\n  state = crime_data$state_abbrev,\n  std_residual = rstandard(lm_fit),\n  fitted_value = fitted.values(lm_fit),\n  leverage = hatvalues(lm_fit),\n  cooks_dist = cooks.distance(lm_fit)\n)\n\n# A tibble: 51 × 5\n   state std_residual fitted_value leverage cooks_dist\n   &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 AK           2.17     0.000227    0.0463   0.0574  \n 2 AL          -0.422    0.000200    0.0769   0.00371 \n 3 AR           1.10    -0.000132    0.153    0.0547  \n 4 AZ          -1.02     0.000344    0.0568   0.0156  \n 5 CA          -0.264    0.0000839   0.114    0.00224 \n 6 CO          -0.383    0.000163    0.0405   0.00155 \n 7 CT          -0.175    0.000134    0.0561   0.000456\n 8 DE           2.81    -0.0000888   0.0754   0.161   \n 9 FL          -0.804    0.000252    0.0452   0.00764 \n10 GA          -0.599    0.000207    0.0232   0.00213 \n# ℹ 41 more rows\n\n\nClearly, DC is an outlier. We can either run a robust estimation procedure or redo the analysis without DC. Let’s try both. First, we try robust regression using rlm() from the MASS package:\n\nrlm_fit &lt;- MASS::rlm(CrimeRate ~ Metro + HighSchool + Poverty, data = crime_data)\nsummary(rlm_fit)\n\n\nCall: rlm(formula = CrimeRate ~ Metro + HighSchool + Poverty, data = crime_data)\nResiduals:\n       Min         1Q     Median         3Q        Max \n-8.297e-05 -3.787e-05 -2.249e-05  4.407e-05  2.063e-03 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept) -0.0009  0.0004    -2.2562\nMetro        0.0000  0.0000    -1.2963\nHighSchool   0.0000  0.0000     2.6506\nPoverty      0.0000  0.0000     2.7546\n\nResidual standard error: 6.048e-05 on 47 degrees of freedom\n\n\nFor some reason, the p-values are not computed automatically. We can compute them ourselves instead:\n\nsummary(rlm_fit)$coef |&gt;\n  as.data.frame() |&gt;\n  rename(Estimate = Value) |&gt;\n  mutate(`p value` = 2 * dnorm(-abs(`t value`)))\n\n                 Estimate   Std. Error   t value    p value\n(Intercept) -8.538466e-04 3.784466e-04 -2.256188 0.06260042\nMetro       -8.639252e-07 6.664623e-07 -1.296285 0.34439400\nHighSchool   1.037849e-05 3.915573e-06  2.650568 0.02378865\nPoverty      1.252839e-05 4.548172e-06  2.754600 0.01795833\n\n\nTo see the robust estimation action visually, let’s consider a univariate example:\n\nlm_fit &lt;- lm(CrimeRate ~ Metro, data = crime_data)\nrlm_fit &lt;- MASS::rlm(CrimeRate ~ Metro, data = crime_data)\n\n# collate the fits into a tibble\nline_fits &lt;- tibble(\n  method = c(\"Usual\", \"Robust\"),\n  intercept = c(\n    coef(lm_fit)[\"(Intercept)\"],\n    coef(rlm_fit)[\"(Intercept)\"]\n  ),\n  slope = c(\n    coef(lm_fit)[\"Metro\"],\n    coef(rlm_fit)[\"Metro\"]\n  )\n)\n\n\n# usual and robust univariate fits\n# plot the fits\ncrime_data |&gt;\n  ggplot() +\n  geom_point(aes(x = Metro, y = CrimeRate)) +\n  geom_abline(aes(intercept = intercept, slope = slope, colour = method), data = line_fits)\n\n\n\n\nNext, let’s try removing DC and running a usual linear regression.\n\nlm_fit_no_dc &lt;- lm(CrimeRate ~ Metro + HighSchool + Poverty,\n  data = crime_data |&gt;\n    filter(state_abbrev != \"DC\") |&gt;\n    column_to_rownames(var = \"state_abbrev\")\n)\n\n# residuals versus fitted\nplot(lm_fit_no_dc, which = 1)\n\n\n\n# residual QQ plot\nplot(lm_fit_no_dc, which = 2)\n\n\n\n# residuals versus leverage (with Cook's distance)\nplot(lm_fit_no_dc, which = 5)"
  },
  {
    "objectID": "glm-general-theory.html",
    "href": "glm-general-theory.html",
    "title": "Generalized linear models: General theory",
    "section": "",
    "text": "Chapters 1-3 focused on the most common class of models used in applications: linear models. Despite their versatility, linear models do not apply in all situations. In particular, they are not designed to deal with binary or count responses. In Chapter 4, we introduce generalized linear models (GLMs), a generalization of linear models that encompasses a wide variety of incredibly useful models, including logistic regression and Poisson regression.\nWe’ll start Chapter 4 by introducing exponential dispersion models (Section Chapter 19), a generalization of the Gaussian distribution that serves as the backbone of GLMs. Then we formally define a GLM, demonstrating logistic regression and Poisson regression as special cases (Section Chapter 20). Next, we discuss maximum likelihood inference in GLMs (Section Chapter 21). Finally, we discuss how to carry out statistical inference in GLMs (Section Chapter 22)."
  },
  {
    "objectID": "exponential-dispersion-models.html#definition",
    "href": "exponential-dispersion-models.html#definition",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.1 Definition",
    "text": "19.1 Definition\nLet’s start with the Gaussian distribution. If \\(y \\sim N(\\mu, \\sigma^2)\\), then it has the following density with respect to the Lebesgue measure \\(\\nu\\) on \\(\\mathbb{R}\\):\n\\[\n\\begin{split}\nf_{\\mu, \\sigma^2}(y) &= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right) \\\\\n&= \\exp\\left(\\frac{\\mu y - \\frac{1}{2}\\mu^2}{\\sigma^2}\\right) \\cdot \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac1{2\\sigma^2} y^2\\right).\n\\end{split}\n\\]\nWe can consider a more general class of densities with respect to any measure \\(\\nu\\):\n\\[\nf_{\\theta, \\phi}(y) \\equiv \\exp\\left(\\frac{\\theta y - \\psi(\\theta)}{\\phi}\\right)h(y, \\phi), \\quad \\theta \\in \\Theta \\subseteq \\mathbb{R}, \\ \\phi &gt; 0.\n\\tag{19.1}\\]\nHere \\(\\theta\\) is called the natural parameter, \\(\\psi\\) is called the log-partition function, \\(\\Theta \\equiv \\{\\theta: \\psi(\\theta) &lt; \\infty\\}\\) is called the natural parameter space,1 \\(\\phi &gt; 0\\) is called the dispersion parameter, and \\(h\\) is called the base density. The distribution with density \\(f_{\\theta, \\phi}\\) with respect to a measure \\(\\nu\\) on \\(\\mathbb{R}\\) is called an exponential dispersion model (EDM).2 Sometimes, we parameterize this distribution using its mean and dispersion, writing\n\\[\ny \\sim \\text{EDM}(\\mu, \\phi).\n\\]\nWhen \\(\\phi = 1\\), the distribution becomes a one-parameter natural exponential family (see Figure 19.1).\n\n\n\nFigure 19.1: Relationship between exponential dispersion models and one-parameter exponential families.\n\n\nThe following proposition presents a useful property of EDMs, which facilitates inference by ruling out pathological cases.\n\nProposition 19.1 The support of \\(y \\sim \\text{EDM}(\\mu, \\phi)\\) remains fixed as \\((\\mu, \\phi)\\) vary."
  },
  {
    "objectID": "exponential-dispersion-models.html#examples",
    "href": "exponential-dispersion-models.html#examples",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.2 Examples",
    "text": "19.2 Examples\n\n19.2.1 Normal distribution\nAs derived above, \\(y \\sim N(\\mu, \\sigma^2)\\) is an EDM with\n\\[\n\\theta = \\mu, \\quad \\psi(\\theta) = -\\frac 12 \\theta^2, \\quad \\phi = \\sigma^2, \\quad h(y, \\phi) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac1{2\\sigma^2} y^2\\right).\n\\]\n\n\n19.2.2 Bernoulli distribution\nSuppose \\(y \\sim \\text{Ber}(\\mu)\\). Then, we have\n\\[\nf(y) = \\mu^{y}(1-\\mu)^{1-y} = \\exp\\left(y \\log \\frac{\\mu}{1-\\mu} + \\log(1-\\mu) \\right).\n\\]\nTherefore, we have \\(\\theta = \\log \\frac{\\mu}{1-\\mu}\\), so that \\(\\log(1-\\mu) = -\\log(1+e^\\theta)\\). It follows that\n\\[\n\\theta = \\log \\frac{\\mu}{1-\\mu}, \\quad \\psi(\\theta) = \\log(1+e^\\theta), \\quad \\phi = 1, \\quad h(y) = 1.\n\\]\nHence, the Bernoulli distribution is an EDM, as well as a one-parameter exponential family. Note that \\(\\text{Ber}(0)\\) and \\(\\text{Ber}(1)\\) are not included in this class of EDMs, because there is no \\(\\theta \\in \\Theta = \\mathbb{R}\\) that gives rise to \\(\\mu = 0\\) or \\(\\mu = 1\\). Hence, \\(\\mu \\in (0,1)\\), and the support of any Bernoulli EDM is \\(\\{0,1\\}\\).\n\n\n19.2.3 Binomial distribution\nConsider the binomial proportion \\(y\\): \\(my \\sim \\text{Bin}(m, \\mu)\\). We have\n\\[\n\\begin{split}\nf(y) &= {m \\choose my}\\mu^{my}(1-\\mu)^{m(1-y)} \\\\\n&= \\exp\\left(m\\left(y \\log \\frac{\\mu}{1-\\mu} + \\log(1-\\mu)\\right)\\right){m \\choose my},\n\\end{split}\n\\]\nso\n\\[\n\\theta = \\log\\frac{\\mu}{1-\\mu}, \\quad \\psi(\\theta) = \\frac{e^{\\theta}}{1 + e^{\\theta}}, \\quad \\phi = 1/m, \\quad h(y, \\phi) = {m \\choose my}.\n\\]\nNote that \\(\\text{Bin}(m, 0)\\) and \\(\\text{Bin}(m, 1)\\) are not included in this class of EDMs, for the same reason as above. Hence, \\(\\mu \\in (0,1)\\), and the support of any binomial EDM is \\(\\{0,\\frac{1}{m}, \\frac{2}{m}, \\dots, 1\\}\\).\n\n\n19.2.4 Poisson distribution\nSuppose \\(y \\sim \\text{Poi}(\\mu)\\). We have\n\\[\nf(y) = e^{-\\mu}\\frac{\\mu^y}{y!} = \\exp(y \\log \\mu - \\mu)\\frac{1}{y!}.\n\\]\nTherefore, we have \\(\\theta = \\log \\mu\\), so that \\(\\mu = e^\\theta\\). It follows that\n\\[\n\\theta = \\log \\mu, \\quad \\psi(\\theta) = e^\\theta,\\quad \\phi = 1, \\quad h(y) = \\frac{1}{y!}.\n\\]\nHence, the Poisson distribution is an EDM, as well as a one-parameter exponential family. Note that \\(\\text{Poi}(0)\\) is not included in this class of EDMs, because there is no \\(\\theta \\in \\Theta = \\mathbb{R}\\) that gives rise to \\(\\mu = 0\\). Hence, \\(\\mu \\in (0,\\infty)\\), and the support of any Poisson EDM is \\(\\mathbb{N}\\).\nMany other examples fall into this class, including the negative binomial, gamma, and inverse-Gaussian distributions. We will see at least some of these in the next chapter."
  },
  {
    "objectID": "exponential-dispersion-models.html#moments-of-exponential-dispersion-model-distributions",
    "href": "exponential-dispersion-models.html#moments-of-exponential-dispersion-model-distributions",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.3 Moments of exponential dispersion model distributions",
    "text": "19.3 Moments of exponential dispersion model distributions\nIt turns out that the derivatives of the log-partition function \\(\\psi\\) give the moments of \\(y\\). Indeed, let’s start with the relationship\n\\[\n\\int f_{\\theta, \\phi}(y)d\\nu(y) = \\int \\exp\\left(\\frac{\\theta y - \\psi(\\theta)}{\\phi}\\right)h(y, \\phi) d\\nu(y) = 1.\n\\]\nDifferentiating in \\(\\theta\\) and interchanging the derivative and the integral, we obtain\n\\[\n0 = \\frac{d}{d\\theta} \\int f_{\\theta, \\phi}(y)dy = \\int \\frac{y - \\dot \\psi(\\theta)}{\\phi}f_{\\theta, \\phi}(y) dy,\n\\]\nfrom which it follows that\n\\[\n\\dot \\psi(\\theta) = \\int \\dot \\psi(\\theta)f_{\\theta, \\phi}(y)dy = \\int y f_{\\theta, \\phi}(y)dy = \\mathbb{E}[y] \\equiv \\mu.\n\\tag{19.2}\\]\nThus, the first derivative of the log partition function is the mean of \\(y\\). Differentiating again, we get\n\\[\n\\begin{split}\n\\phi \\cdot \\ddot \\psi(\\theta) &= \\phi \\int \\ddot \\psi(\\theta) f_{\\theta, \\phi}(y) d\\nu(y) \\\\\n&= \\int (y - \\dot \\psi(\\theta))^2 f_{\\theta, \\phi}(y) dy = \\int (y - \\mu)^2f_{\\theta, \\phi}(y) d\\nu(y) \\\\\n&= \\text{Var}[y].\n\\end{split}\n\\]\nThus, the second derivative of the log-partition function multiplied by the dispersion parameter is the variance of \\(y\\). The following proposition summarizes these results.\n\nProposition 19.2 (EDM moments) If \\(y \\sim \\text{EDM}(\\mu, \\phi)\\), then \\[\n\\mathbb E[y] = \\dot \\psi(\\theta), \\quad \\text{Var}[y] = \\phi \\cdot \\ddot \\psi(\\theta).\n\\]"
  },
  {
    "objectID": "exponential-dispersion-models.html#relationships-among-the-mean-variance-and-natural-parameter",
    "href": "exponential-dispersion-models.html#relationships-among-the-mean-variance-and-natural-parameter",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.4 Relationships among the mean, variance, and natural parameter",
    "text": "19.4 Relationships among the mean, variance, and natural parameter\n\n19.4.1 Relationship between the mean and the natural parameter\nThe log-partition function \\(\\psi\\) induces a connection (19.2) between the natural parameter \\(\\theta\\) and the mean \\(\\mu\\). Because\n\\[\n\\frac{d\\mu}{d\\theta} = \\frac{d}{d\\theta}\\dot \\psi(\\theta) = \\ddot \\psi(\\theta) = \\frac{1}{\\phi}\\text{Var}[y] &gt; 0,\n\\tag{19.3}\\]\nit follows that \\(\\mu\\) is a strictly increasing function of \\(\\theta\\), so in particular the mapping between \\(\\mu\\) and \\(\\theta\\) is bijective. Therefore, we can think of equivalently parameterizing the distribution via \\(\\mu\\) or \\(\\theta\\).\n\n\n19.4.2 Relationship between the mean and variance\nNote that the mean of an EDM, together with the dispersion parameter, determines its variance (since it determines the natural parameter \\(\\theta\\)). Define\n\\[\nV(\\mu) \\equiv \\frac{d\\mu}{d\\theta},\n\\]\nso that \\(\\text{Var}[y] = \\phi V(\\mu)\\). For example, a Poisson random variable with mean \\(\\mu\\) has variance \\(\\mu\\) and a Bernoulli random variable with mean \\(\\mu\\) has \\(V(\\mu) = \\mu(1-\\mu)\\). The mean-variance relationship turns out to characterize the EDM, i.e. an EDM with mean equal to its variance is the Poisson distribution. For all EDMs except the normal distribution, the variance depends nontrivially on the mean. Therefore, heteroskedasticity is a natural feature of EDMs (rather than a pathology that needs to be corrected for)."
  },
  {
    "objectID": "exponential-dispersion-models.html#the-unit-deviance",
    "href": "exponential-dispersion-models.html#the-unit-deviance",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.5 The unit deviance",
    "text": "19.5 The unit deviance\nA key quantity in the analysis of normal linear regression models is \\((y, \\mu) \\mapsto (y - \\mu)^2\\), which is a notion of distance between the parameter \\(\\mu\\) and the observation \\(y\\). The unit deviance is a generalization of this quantity to EDMs. As a starting point, consider the log-likelihood of an EDM: \\[\n\\ell(y, \\mu) = \\frac{\\theta y - \\psi(\\theta)}{\\phi} + \\log h(y, \\phi) = \\frac{\\theta(\\mu) y - \\psi(\\theta(\\mu))}{\\phi} + \\log h(y, \\phi),\n\\] where \\(\\theta(\\mu) = \\dot \\psi^{-1}(\\mu)\\), recalling the relationship (19.2). The quantity \\(\\ell(y, \\mu)\\) is larger to the extent that \\(\\mu\\) is a better fit for \\(y\\). Furthermore, it is easy to verify that \\(\\mu \\mapsto \\ell(y, \\mu)\\) is maximized by \\(\\mu = y\\). Motivated by this observation, we calculate that twice the log-likelihood ratio between \\(\\mu = \\mu\\) and \\(\\mu = y\\) is \\[\n\\begin{split}\n&2(\\ell(y, y) - \\ell(y, \\mu)) \\\\\n&\\quad= \\frac{2\\{[\\theta(y) y - \\psi(\\theta(y))] - [\\theta(\\mu) y - \\psi(\\theta(\\mu))]\\}}{\\phi} \\\\\n&\\quad\\equiv \\frac{d(y, \\mu)}{\\phi}.\n\\end{split}\n\\] The quantity in the numerator is the unit deviance \\(d(y, \\mu)\\), defined as the dispersion \\(\\phi\\) times twice the log-likelihood ratio between \\(y\\) and \\(\\mu\\). As we will see in Section 19.5.1, \\(d(y, \\mu)\\) generalizes the quantity \\((y - \\mu)^2\\) for the normal distribution. The following proposition summarizes a few key properties of the unit deviance.\n\nProposition 19.3 (Unit deviance properties) When viewed as a function of \\(\\mu\\), the unit deviance \\(d(y, \\mu)\\) is nonnegative function that achieves a unique global minimum of zero for \\(\\mu = y\\) and increases as \\(\\mu\\) moves away from \\(y\\).\n\n\nProof. Differentiating \\(d(y, \\mu)\\) in \\(\\mu\\), we have \\[\n\\frac{\\partial d(y, \\mu)}{\\partial \\mu} = \\frac{\\partial d(y, \\mu)}{\\partial \\theta} \\frac{\\partial \\theta}{\\partial \\mu} = \\frac{\\mu - y}{V(\\mu)}.\n\\] Since \\(V(\\mu) &gt; 0\\), this establishes that \\(d(y, \\mu)\\) decreases on \\(\\mu \\in (\\infty, y)\\) and then increases on \\((y, \\infty)\\), Therefore, \\(d(y, \\mu) \\geq d(y, y) = 0\\) for all \\(\\mu\\).\n\n\n19.5.1 Example: Normal distribution\nFor the normal distribution, we have \\(\\theta = \\mu\\) and \\(\\psi(\\theta) = \\frac12 \\theta^2\\). Therefore, \\[\n\\begin{split}\nd(y, \\mu) &= 2\\{[\\theta(y) y - \\psi(\\theta(y))] - [\\theta(\\mu) y - \\psi(\\theta(\\mu))]\\} \\\\\n&= 2\\{[y^2 - \\tfrac12 y^2] - [\\mu y - \\tfrac12 \\mu^2]\\} \\\\\n&= (y - \\mu)^2.\n\\end{split}\n\\] Figure 19.2 displays an example of the normal unit deviance.\n\n\n\n\n\nFigure 19.2: The normal unit deviance for \\(y = 4\\).\n\n\n\n\n\n\n19.5.2 Example: Poisson distribution\nFor the Poisson distribution, we have \\(\\theta = \\log \\mu\\) and \\(\\psi(\\theta) = e^\\theta\\), so\n\\[\n\\begin{split}\nd(y, \\mu) &= 2\\{[\\theta(y) y - \\psi(\\theta(y))] - [\\theta(\\mu) y - \\psi(\\theta(\\mu))]\\} \\\\\n&= 2\\{[y\\log y - y] - [y \\log \\mu - \\mu]\\} \\\\\n&= 2\\left(y\\log \\frac{y}{\\mu} - (y - \\mu)\\right).\n\\end{split}\n\\]\nSee Figure 19.3 for an example of the shape of this function.\n\n\n\n\n\nFigure 19.3: The Poisson unit deviance for \\(y = 4\\).\n\n\n\n\nNote that the Poisson deviance is asymmetric about \\(\\mu = y\\). This is a consequence of the nontrivial mean-variance relationship for the Poisson distribution. In particular, the Poisson distribution’s variance grows with its mean. Therefore, an observation of \\(y = 4\\) is less likely to have come from a Poisson distribution with mean \\(\\mu = 2\\) than from a Poisson distribution with mean \\(\\mu = 6\\)."
  },
  {
    "objectID": "exponential-dispersion-models.html#small-dispersion-approximations-to-an-edm",
    "href": "exponential-dispersion-models.html#small-dispersion-approximations-to-an-edm",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.6 Small-dispersion approximations to an EDM",
    "text": "19.6 Small-dispersion approximations to an EDM\nIf the dispersion \\(\\phi\\) is small, then that means that \\(y\\) is a fairly precise estimate of \\(\\mu\\), similar to an average of multiple independent samples from a mean-\\(\\mu\\) distribution. Consider, for example, that \\(\\frac{1}{m}\\text{Bin}(m, \\mu)\\) is the mean of \\(m\\) i.i.d. draws from \\(\\text{Ber}(\\mu)\\). In this case, we can use either the normal approximation or the saddlepoint approximation to approximate the EDM density. For the sake of this section, we will abuse notation by denoting by \\(f_{\\mu, \\phi}\\) the EDM with mean \\(\\mu\\) and dispersion \\(\\phi\\).\n\n19.6.1 The normal approximation\n\n19.6.1.1 The approximation\nFor small values of \\(\\phi\\), we can hope to approximate \\(f_{\\mu, \\phi}\\) with a normal distribution. Recall that the mean and variance of this distribution are \\(\\mu\\) and \\(\\phi \\cdot V(\\mu)\\), respectively. The central limit theorem gives\n\\[\n\\frac{y - \\mu}{\\sqrt{\\phi \\cdot V(\\mu)}} \\rightarrow_d N(0,1) \\quad \\text{as} \\quad \\phi \\rightarrow 0,\n\\]\nso\n\\[\ny \\overset \\cdot \\sim N(\\mu, \\phi \\cdot V(\\mu)) \\equiv \\tilde f^{\\text{normal}}_{\\mu, \\phi}.\n\\]\nFor example, we have\n\\[\n\\text{Poi}(\\mu) \\approx N(\\mu, \\mu).\n\\]\nFor the normal EDM, note that the normal approximation is exact. One consequence of the normal approximation is\n\\[\n\\frac{(y - \\mu)^2}{\\phi \\cdot V(\\mu)} \\overset \\cdot \\sim \\chi^2_1.\n\\tag{19.4}\\]\nThis fact will be useful to us as we carry out inference for GLMs.\n\n\n19.6.1.2 Normal approximation accuracy\nWe have\n\\[\n\\tilde f^{\\text{normal}}_{\\mu, \\phi}(y) = f_{\\mu, \\phi}(y) + O(\\sqrt{\\phi}).\n\\]\nIn practice, the rule of thumb for the applicability of this approximation to get statements like (19.4) is that\n\\[\n\\tau \\equiv \\frac{\\phi \\cdot V(\\mu)}{(\\mu - \\text{boundary})^2} \\leq \\frac{1}{5}.\n\\]\nHere, “boundary” represents the nearest boundary of the parameter space to \\(\\mu\\). For example, if \\(y \\sim \\frac{1}{m}\\text{Bin}(m, \\mu)\\), then we have\n\\[\n\\begin{split}\n\\tau &= \\frac{\\frac{1}{m} \\cdot \\mu \\cdot (1-\\mu)}{\\min(\\mu, 1-\\mu)^2} \\\\\n&= \\frac{1}{m} \\cdot \\max\\left(\\frac{\\mu}{1-\\mu}, \\frac{1-\\mu}{\\mu}\\right) \\\\\n&\\approx \\frac{1}{m} \\cdot \\max\\left(\\frac 1 \\mu, \\frac 1 {1-\\mu}\\right),\n\\end{split}\n\\]\nso \\(\\tau \\leq 1/5\\) roughly if \\(m \\mu \\leq 5\\) and \\(m (1-\\mu) \\leq 5\\). For Poisson distributions, we always have \\(\\tau = 1\\), but for some reason small-dispersion asymptotics still applies as \\(\\mu \\rightarrow \\infty\\) as opposed to \\(\\tau \\rightarrow 0\\). The criterion \\(\\tau \\leq 1/5\\) is satisfied when \\(\\mu \\leq 5\\).\n\n\n\n19.6.2 The saddlepoint approximation\n\n19.6.2.1 The approximation\nAnother approximation to the EDM density is the saddlepoint approximation, which tends to be more accurate than the normal approximation. The reason the normal approximation may be inaccurate is that the quality of the central limit approximation degrades as one enters the tails of the distribution. In particular, the normal approximation to \\(f_{\\mu, \\phi}(y)\\) may be poor if \\(\\mu\\) is far from \\(y\\). The saddlepoint approximation is built on the observation that the EDM density for \\(f_{\\mu, \\phi}(y)\\) can be written in terms of the density \\(f_{y, \\phi}(y)\\); the latter density is by definition evaluated at its mean. Indeed,\n\\[\n\\begin{split}\nf_{\\mu, \\phi}(y) &\\equiv \\exp\\left(\\frac{\\theta y - \\psi(\\theta)}{\\phi}\\right)h(y, \\phi) \\\\\n&= \\exp\\left(-\\frac{d(y, \\mu)}{2\\phi}\\right)\\exp\\left(\\frac{\\theta(y) y - \\psi(\\theta(y))}{\\phi}\\right) h(y, \\phi) \\\\\n&= \\exp\\left(-\\frac{d(y, \\mu)}{2\\phi}\\right)f_{y, \\phi}(y).\n\\end{split}\n\\tag{19.5}\\]\nNow, we apply the central limit theorem to approximate \\(f_{y, \\phi}(y)\\):\n\\[\nf_{y, \\phi}(y) \\approx \\frac{1}{\\sqrt{2\\pi \\phi V(y)}}.\n\\]\nSubstituting this approximation into (19.5), we obtain the saddlepoint approximation:\n\\[\nf_{\\mu, \\phi}(y) \\approx \\frac{1}{\\sqrt{2\\pi \\phi V(y)}}\\exp\\left(-\\frac{d(y, \\mu)}{2\\phi}\\right) \\equiv \\widetilde f^{\\text{saddle}}_{\\mu, \\phi}(y).\n\\]\nFor the normal EDM, note that the normal approximation is exact. For the Poisson distribution, we get\n\\[\n\\widetilde f^{\\text{saddle}}_{\\mu, \\phi}(y) = \\frac{1}{\\sqrt{2\\pi y}}\\exp\\left(-y \\log \\frac y \\mu + (y - \\mu)\\right).\n\\]\nThe approximation can be shown to lead to the following consequence:\n\\[\n\\frac{d(y, \\mu)}{\\phi} \\overset \\cdot \\sim \\chi^2_1.\n\\tag{19.6}\\]\nHere, we are using the unit deviance rather than the squared distance to measure the deviation of \\(\\mu\\) from \\(y\\). This fact will be useful to us as we carry out inference for GLMs.\n\n\n19.6.2.2 Saddlepoint approximation accuracy\nWe have still used a normal approximation, but this time we have used it to approximate \\(f_{y, \\phi}(y)\\) instead of \\(f_{\\mu, \\phi}(y)\\). Since the normal approximation is applied to a distribution (\\(f_{y, \\phi}\\)) at its mean, we expect it to be more accurate than a normal approximation applied to a distribution (\\(f_{\\mu, \\phi}\\)) at a point potentially far from its mean. The saddlepoint approximation yields an approximation to the density that is multiplicative rather than additive, and of order \\(O(\\phi)\\) rather than \\(O(\\sqrt{\\phi})\\):\n\\[\n\\tilde f^{\\text{saddle}}_{\\mu, \\phi}(y) = f_{\\mu, \\phi}(y) \\cdot (1 + O(\\phi)).\n\\]\nIn practice, the rule of thumb for the applicability of this approximation to get statements like (19.6) is that \\(\\tau \\leq 1/3\\); the looser requirement on \\(\\tau\\) reflects the greater accuracy of the saddlepoint approximation. This translates to \\(m\\mu \\geq 3\\) and \\(m(1-\\mu) \\geq 3\\) for the binomial and \\(\\mu \\geq 3\\) for the Poisson.\n\n\n\n19.6.3 Comparing the two approximations\nThe saddlepoint approximation is more accurate than the normal approximation, as discussed above. However, the accuracy of the saddlepoint approximation relies on the assumption that the entire parametric form of the EDM is correctly specified. On the other hand, the accuracy of the normal distribution requires only that the first two moments of the EDM are correctly specified."
  },
  {
    "objectID": "generalized-linear-models.html#definition",
    "href": "generalized-linear-models.html#definition",
    "title": "20  GLM definition",
    "section": "20.1 Definition",
    "text": "20.1 Definition\nWe define \\(\\{(y_i, \\boldsymbol x_{i*})\\}_{i = 1}^n\\) as following a generalized linear model based on the exponential dispersion model \\(f_{\\theta, \\phi}\\), monotonic and differentiable link function \\(g\\), offset terms \\(o_i \\in \\mathbb R\\), and observation weights \\(w_i &gt; 0\\) if\n\\[\ny_i \\overset{\\text{ind}} \\sim \\text{EDM}(\\mu_i, \\phi_0/w_i), \\quad \\eta_i \\equiv g(\\mu_i) = o_i + \\boldsymbol x^T_{i*}\\boldsymbol{\\beta}.\n\\tag{20.1}\\]\nThe offset terms \\(o_i\\) and observation weights \\(w_i\\) are both known in advance. The free parameters in a GLM are the coefficients \\(\\boldsymbol{\\beta}\\) and, possibly, the parameter \\(\\phi_0\\) controlling the dispersion. We will see examples where \\(\\phi_0\\) is known (e.g. Poisson regression) and those where \\(\\phi_0\\) is unknown (e.g. linear regression).\nThe “default” choice for the link function \\(g\\) is the canonical link function\n\\[\ng(\\mu) = \\dot \\psi^{-1}(\\mu),\n\\]\nwhich, given the relationship (19.2), gives \\(\\eta = \\dot \\psi^{-1}(\\mu) = \\theta\\), i.e. the linear predictor coincides with the natural parameter. As discussed in the context of equation (19.3), \\(\\dot \\psi^{-1}\\) is a valid link function because it is monotonic and differentiable. Canonical link functions are very commonly used with GLMs because they lead to various nice properties that general GLMs do not enjoy (e.g. concave log-likelihood)."
  },
  {
    "objectID": "generalized-linear-models.html#examples",
    "href": "generalized-linear-models.html#examples",
    "title": "20  GLM definition",
    "section": "20.2 Examples",
    "text": "20.2 Examples\n\n20.2.1 Example: Linear regression model\nThe linear regression model is a special case of a GLM, with \\(\\phi_0 = \\sigma^2\\) (unknown), \\(w_i = 1\\), \\(o_i = 0\\), and identity (canonical) link function:\n\\[\ny_i \\overset{\\text{ind}}\\sim N(\\mu_i, \\sigma^2); \\quad \\eta_i = \\mu_i = \\boldsymbol x_{i*}^T \\boldsymbol{\\beta}.\n\\]\n\n\n20.2.2 Example: Weighted linear regression model\nIf each observation \\(y_i\\) is the mean of \\(m_i\\) independent repeated observations, then we get a weighted linear regression model, with \\(\\phi_0 = \\sigma^2\\) (unknown), \\(w_i = m_i\\), \\(o_i = 0\\), and identity (canonical) link function:\n\\[\ny_i \\overset{\\text{ind}}\\sim N(\\mu_i, {\\textstyle \\frac{\\sigma^2}{m_i}}); \\quad \\eta_i = \\mu_i = \\boldsymbol x_{i*}^T \\boldsymbol{\\beta}.\n\\]\n\n\n20.2.3 Example: Ungrouped logistic regression model\nThe ungrouped logistic regression model is the GLM based on the Bernoulli EDM with \\(\\phi_0 = 1\\) (known), \\(w_i = 1\\), \\(o_i = 0\\), and the canonical link function:\n\\[\ny_i \\overset{\\text{ind}}\\sim \\text{Ber}(\\mu_i); \\quad \\eta_i = \\theta_i = \\log\\frac{\\mu_i}{1-\\mu_i} = \\boldsymbol x_{i*}^T \\boldsymbol{\\beta}.\n\\]\nThus the canonical link function for logistic regression is the logistic link function \\(g(\\mu) = \\log \\frac{\\mu}{1-\\mu}\\).\n\n\n20.2.4 Example: Grouped logistic regression model\nSuppose \\(y_i\\) is a binomial proportion based on \\(m_i\\) trials. The grouped logistic regression model is the GLM based on the binomial EDM with \\(\\phi_0 = 1\\) (known), \\(w_i = 1/m_i\\), \\(o_i = 0\\), and the canonical link function:\n\\[\nm_i y_i \\sim \\text{Bin}(m_i, \\mu_i); \\quad \\eta_i = \\log \\frac{\\mu_i}{1-\\mu_i} = o_i + \\boldsymbol x^T_{i*}\\boldsymbol{\\beta}.\n\\]\nNote that a binomial proportion \\(y_i\\) based on \\(m_i\\) trials and a success probability of \\(\\mu_i\\) can be equivalently represented as \\(m_i\\) independent Bernoulli draws with the same success probability \\(\\mu_i\\). Therefore, any grouped logistic regression model can be equivalently represented as an ungrouped logistic regression model with \\(\\sum_{i = 1}^n m_i\\) observations. We will see that, despite this equivalence, grouped logistic regression models have some useful properties that ungrouped logistic regression models do not.\n\n\n20.2.5 Example: Poisson regression model\nPoisson regression is the Poisson EDM with \\(\\phi_0 = 1\\) (known), \\(w_i = 1\\), \\(o_i = 0\\), and the canonical link function:\n\\[\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\eta_i = \\theta_i = \\log \\mu_i = \\boldsymbol x_{i*}^T \\boldsymbol{\\beta}.\n\\]\nThus the canonical link function for Poisson regression is the log link function \\(g(\\mu) = \\log \\mu\\)."
  },
  {
    "objectID": "parameter-estimation.html#sec-glm-likelihood",
    "href": "parameter-estimation.html#sec-glm-likelihood",
    "title": "21  Parameter estimation",
    "section": "21.1 The GLM likelihood, score, and Fisher information",
    "text": "21.1 The GLM likelihood, score, and Fisher information\nThe log-likelihood of a GLM is:\n\\[\n\\log \\mathcal L(\\boldsymbol{\\beta}) = \\sum_{i = 1}^n \\frac{\\theta_i y_i - \\psi(\\theta_i)}{\\phi_0/w_i} + \\sum_{i = 1}^n \\log h(y_i, \\phi_0/w_i).\n\\tag{21.1}\\]\nLet’s differentiate this with respect to \\(\\boldsymbol{\\beta}\\), using the chain rule:\n\\[\n\\begin{split}\n  \\frac{\\partial \\log \\mathcal L(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} &= \\frac{\\partial \\log \\mathcal L(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\theta}}\\frac{\\partial \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\mu}} \\frac{\\partial \\boldsymbol{\\mu}}{\\partial \\boldsymbol{\\eta}}\\frac{\\partial \\boldsymbol{\\eta}}{\\partial \\boldsymbol{\\beta}} \\\\\n  &=  (\\boldsymbol{y} - \\boldsymbol{\\mu})^T \\text{diag}(\\phi_0/w_i)^{-1} \\cdot \\text{diag}(\\ddot{\\psi}(\\theta_i))^{-1} \\cdot \\text{diag}\\left(\\frac{\\partial\\mu_i}{\\partial \\eta_i}\\right) \\cdot \\boldsymbol{X}\\\\\n  &= \\frac{1}{\\phi_0}(\\boldsymbol{y} - \\boldsymbol{\\mu})^T \\text{diag}\\left(\\frac{w_i}{V(\\mu_i)(d\\eta_i/d\\mu_i)^2}\\right)\\cdot \\text{diag}\\left(\\frac{\\partial\\eta_i}{\\partial \\mu_i}\\right) \\cdot \\boldsymbol{X} \\\\\n  &\\equiv \\frac{1}{\\phi_0}(\\boldsymbol{y} - \\boldsymbol{\\mu})^T \\boldsymbol{W} \\boldsymbol{M} \\boldsymbol{X}.\n\\end{split}\n\\]\nHere, \\(\\boldsymbol{W} \\equiv \\text{diag}(W_i)\\) is a diagonal matrix of working weights and \\(\\boldsymbol{M} \\equiv \\text{diag}\\left(\\frac{\\partial\\eta_i}{\\partial \\mu_i}\\right) = \\text{diag}(g'(\\mu_i))\\) is a diagonal matrix of link derivatives. Transposing, we get the score vector:\n\\[\n\\boldsymbol{U}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} (\\boldsymbol{y} - \\boldsymbol{\\mu}).\n\\tag{21.2}\\]\nTo get the Fisher information matrix, note first that:\n\\[\n\\text{Var}[\\boldsymbol{y}] = \\text{diag}\\left(\\phi_0\\frac{V(\\mu_i)}{w_i}\\right) = \\phi_0 \\boldsymbol{W}^{-1} \\boldsymbol{M}^{-2}\n\\tag{21.3}\\]\nwe can compute the covariance matrix of the score vector:\n\\[\n\\begin{split}\n\\boldsymbol{I}(\\boldsymbol{\\beta}) = \\text{Var}[\\boldsymbol{U}(\\boldsymbol{\\beta})] &= \\frac{1}{\\phi^2_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} \\text{Var}[\\boldsymbol{y}] \\boldsymbol{M} \\boldsymbol{W} \\boldsymbol{X} \\\\\n&= \\frac{1}{\\phi^2_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} \\phi_0 \\boldsymbol{W}^{-1}\\boldsymbol{M}^{-2} \\boldsymbol{M} \\boldsymbol{W} \\boldsymbol{X} \\\\\n&= \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X}.\n\\end{split}\n\\tag{21.4}\\]"
  },
  {
    "objectID": "parameter-estimation.html#sec-mle-glm",
    "href": "parameter-estimation.html#sec-mle-glm",
    "title": "21  Parameter estimation",
    "section": "21.2 Maximum likelihood estimation of \\(\\boldsymbol{\\beta}\\)",
    "text": "21.2 Maximum likelihood estimation of \\(\\boldsymbol{\\beta}\\)\nTo estimate \\(\\boldsymbol{\\beta}\\), we can set the score vector to zero:\n\\[\n\\frac{1}{\\phi_0}\\boldsymbol{X}^T \\widehat{\\boldsymbol{M}} \\widehat{\\boldsymbol{W}} (\\boldsymbol{y} - \\widehat{\\boldsymbol{\\mu}}) = 0 \\quad \\Longleftrightarrow \\quad \\boldsymbol{X}^T \\text{diag}\\left(\\frac{w_i}{V(\\widehat \\mu_i)g'(\\widehat \\mu_i)}\\right)(\\boldsymbol{y} - \\widehat{\\boldsymbol{\\mu}}) = 0.\n\\]\nThese equations are called the normal equations. Unfortunately, unlike least squares, the normal equations cannot be solved analytically for \\(\\widehat{\\boldsymbol{\\beta}}\\). They are solved numerically instead; see Section 21.3. Note that \\(\\phi_0\\) cancels from the normal equations, and therefore the coefficients \\(\\boldsymbol{\\beta}\\) can be estimated without estimating the dispersion. Recall that we have seen this phenomenon for least squares. Also note that the normal equations simplify when the canonical link function is used, so that \\(\\eta_i = \\theta_i\\). Assuming additionally that \\(w_i = 1\\), we get:\n\\[\n\\boldsymbol{\\widehat M} \\boldsymbol{\\widehat W} = \\text{diag}\\left(\\frac{\\widehat{\\partial \\mu_i/\\partial \\theta_i}}{V(\\widehat \\mu_i)}\\right) = \\frac{\\ddot{\\psi}(\\widehat \\theta_i)}{\\ddot{\\psi}(\\widehat \\theta_i)} = 1,\n\\]\nso the normal equations reduce to:\n\\[\n\\boldsymbol{X}^T (\\boldsymbol{y} - \\widehat{\\boldsymbol{\\mu}}) = 0.\n\\tag{21.5}\\]\nWe recognize these as the normal equation for linear regression. Since both ungrouped logistic regression and Poisson regression also use canonical links and have unit weights, the simplified normal equations (21.5) apply to the latter regressions as well.\nIn the linear regression case, we interpreted the normal equations (21.5) as an orthogonality statement: \\(\\boldsymbol{y} - \\widehat{\\boldsymbol{\\mu}} \\perp C(\\boldsymbol{X})\\). In the case of GLMs, the set \\(C(\\boldsymbol{X}) \\equiv \\{\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{y}]: \\boldsymbol{\\beta} \\in \\mathbb{R}^p\\}\\) is no longer a linear space. In fact, it is a nonlinear transformation of the column space of \\(\\boldsymbol{X}\\) (a \\(p\\)-dimensional manifold in \\(\\mathbb{R}^n\\)):\n\\[\nC(\\boldsymbol{X}) \\equiv \\{\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{y}]: \\boldsymbol{\\beta} \\in \\mathbb{R}^p\\} = \\{g^{-1}(\\boldsymbol{X} \\boldsymbol{\\beta}): \\boldsymbol{\\beta} \\in \\mathbb{R}^p\\}.\n\\]\nTherefore, we cannot view the mapping \\(\\boldsymbol{y} \\mapsto \\boldsymbol{\\widehat \\mu}\\) as a linear projection. Nevertheless, it is possible to interpret \\(\\boldsymbol{\\widehat \\mu}\\) as the “closest” point (in some sense) to \\(\\boldsymbol{y}\\) in \\(C(\\boldsymbol{X})\\). To see this, recall the deviance form of the EDM density (19.4). Taking a logarithm and summing over \\(i = 1, \\dots, n\\), we find the following expression for the negative log likelihood:\n\\[\n\\begin{split}\n-\\log \\mathcal L(\\boldsymbol{\\beta}) &= \\sum_{i = 1}^n \\frac{d(y_i, \\mu_i)}{2\\phi_i} + C \\\\\n&= \\frac{\\sum_{i = 1}^n w_id(y_i, \\mu_i)}{2\\phi_0} + C  \\\\\n&\\equiv \\frac{D(\\boldsymbol{y}, \\boldsymbol{\\mu})}{2\\phi_0} + C \\\\\n&\\equiv \\frac12 D^*(\\boldsymbol{y}, \\boldsymbol{\\mu}) + C.\n\\end{split}\n\\tag{21.6}\\]\n\\(D(\\boldsymbol{y}, \\boldsymbol{\\mu})\\) is called the deviance or the total deviance, and it can be interpreted as a kind of distance between the mean vector \\(\\boldsymbol{\\mu}\\) and the observation vector \\(\\boldsymbol{y}\\). For example, in the linear model case, \\(D(\\boldsymbol{y}, \\boldsymbol{\\mu}) = \\|\\boldsymbol{y} - \\boldsymbol{\\mu}\\|^2\\). The quantity \\(D^*(\\boldsymbol{y}, \\boldsymbol{\\mu})\\) is called the scaled deviance. In the linear model case, \\(D^*(\\boldsymbol{y}, \\boldsymbol{\\mu}) = \\frac{\\|\\boldsymbol{y} - \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\\). Therefore, maximizing the GLM log likelihood is equivalent to minimizing the deviance:\n\\[\n\\boldsymbol{\\widehat \\beta} = \\underset{\\boldsymbol{\\beta}}{\\arg \\min}\\ D(\\boldsymbol{y}, \\boldsymbol{\\mu}(\\boldsymbol{\\beta})), \\quad \\text{so that} \\quad \\boldsymbol{\\widehat \\mu} = \\underset{\\boldsymbol{\\mu} \\in C(\\boldsymbol{X})}{\\arg \\min}\\ D(\\boldsymbol{y}, \\boldsymbol{\\mu}).\n\\]"
  },
  {
    "objectID": "parameter-estimation.html#sec-irls",
    "href": "parameter-estimation.html#sec-irls",
    "title": "21  Parameter estimation",
    "section": "21.3 Iteratively reweighted least squares",
    "text": "21.3 Iteratively reweighted least squares\n\n21.3.1 Log-concavity of GLM likelihood\nBefore talking about maximizing the GLM log-likelihood, we investigate the concavity of this function. We claim that, in the case when the canonical link is used, \\(\\log \\mathcal L(\\boldsymbol{\\beta})\\) is a concave function of \\(\\boldsymbol{\\beta}\\), which implies that this function is “easy to optimize”, i.e., has no local maxima.\n\nProposition 21.1 Proposition: If \\(g\\) is the canonical link function, then the function \\(\\log \\mathcal L(\\boldsymbol{\\beta})\\) defined in 21.1 is concave in \\(\\boldsymbol{\\beta}\\).\n\n\nProof. It suffices to show that \\(\\psi\\) is a convex function since then \\(\\log \\mathcal L(\\boldsymbol{\\beta})\\) would be the sum of a linear function of \\(\\boldsymbol{\\beta}\\) and the composition of a concave function with a linear function. To verify that \\(\\psi\\) is convex, it suffices to recall that \\(\\ddot{\\psi}(\\theta) = \\frac{1}{\\phi}\\text{Var}_\\theta[y] &gt; 0\\).\n\nProposition 21.1 gives us confidence that an iterative algorithm will converge to the global maximum of the likelihood. We present such an iterative algorithm next.\n\n\n21.3.2 Newton-Raphson\nWe can maximize the log-likelihood (21.1) via the Newton-Raphson algorithm, which involves the gradient and Hessian of the function we would like to maximize. We derive the Newton-Raphson algorithm for canonical GLMs. In this case, the gradient is the score vector (21.2), while the Hessian is the Fisher information (21.4).1 The Newton-Raphson iteration is therefore:\n\\[\n\\begin{split}\n\\boldsymbol{\\widehat \\beta}^{(t+1)} &= \\boldsymbol{\\widehat \\beta}^{(t)} - (\\nabla^2_{\\boldsymbol{\\beta}} \\log \\mathcal L(\\boldsymbol{\\widehat \\beta}^{(t)}))^{-1} \\nabla_{\\boldsymbol{\\beta}} \\log \\mathcal L(\\boldsymbol{\\widehat \\beta}^{(t)}) \\\\\n&= \\boldsymbol{\\widehat \\beta}^{(t)} + (\\boldsymbol{X}^T \\boldsymbol{\\widehat W}^{(t)}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{\\widehat \\mu}^{(t)}).\n\\end{split}\n\\tag{21.7}\\]\nSee Figure 21.1.\n\n\n\nFigure 21.1: Newton-Raphson iteratively approximates the log likelihood via a quadratic function and maximizing that function.\n\n\n\n\n21.3.3 Iteratively reweighted least squares (IRLS)\nA nice interpretation of the Newton-Raphson algorithm is as a sequence of weighted least squares fits, known as the iteratively reweighted least squares (IRLS) algorithm. Suppose that we have a current estimate \\(\\boldsymbol{\\widehat \\beta}^{(t)}\\), and suppose we are looking for a vector \\(\\boldsymbol{\\beta}\\) near \\(\\boldsymbol{\\widehat \\beta}^{(t)}\\) that fits the model even better. We have:\n\\[\n\\begin{split}\n\\mathbb{E}_{\\boldsymbol{\\beta}}[\\boldsymbol{y}] &= g^{-1}(\\boldsymbol{X} \\boldsymbol{\\beta}) \\\\\n&\\approx g^{-1}(\\boldsymbol{X} \\boldsymbol{\\widehat \\beta}^{(t)}) + \\text{diag}(\\partial \\mu_i/\\partial \\eta_i)(\\boldsymbol{X} \\boldsymbol{\\beta} - \\boldsymbol{X} \\boldsymbol{\\widehat \\beta}^{(t)}) \\\\\n&= \\boldsymbol{\\widehat \\mu}^{(t)} + (\\boldsymbol{\\widehat M}^{(t)})^{-1}(\\boldsymbol{X} \\boldsymbol{\\beta} - \\boldsymbol{X} \\boldsymbol{\\widehat \\beta}^{(t)})\n\\end{split}\n\\]\nand\n\\[\n\\text{Var}_{\\boldsymbol{\\beta}}[\\boldsymbol{y}] \\approx \\phi_0 (\\boldsymbol{\\widehat W}^{(t)})^{-1}(\\boldsymbol{\\widehat M}^{(t)})^{-2} = \\phi_0 \\boldsymbol{\\widehat W}^{(t)},\n\\]\nrecalling equation (21.3). Thus, up to the first two moments, near \\(\\boldsymbol{\\beta} = \\boldsymbol{\\widehat \\beta}^{(t)}\\) the distribution of \\(\\boldsymbol{y}\\) is approximately:\n\\[\n\\begin{align}\n\\boldsymbol{y} = \\boldsymbol{\\widehat \\mu}^{(t)} + (\\boldsymbol{\\widehat M}^{(t)})^{-1}(\\boldsymbol{X} \\boldsymbol{\\beta} - \\boldsymbol{X} \\boldsymbol{\\widehat \\beta}^{(t)}) + \\boldsymbol{\\epsilon}, \\\\\n\\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\phi_0 \\boldsymbol{\\widehat W}^{(t)}),\n\\end{align}\n\\]\nor, equivalently:\n\\[\n\\begin{align}\n\\boldsymbol{z}^{(t)} \\equiv \\boldsymbol{\\widehat M}^{(t)}(\\boldsymbol{y} - \\boldsymbol{\\widehat \\mu}^{(t)}) + \\boldsymbol{X} \\boldsymbol{\\widehat \\beta}^{(t)} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}', \\\\\n\\boldsymbol{\\epsilon}' \\sim N(\\boldsymbol{0}, \\phi_0 (\\boldsymbol{\\widehat W}^{(t)})^{-1}).\n\\end{align}\n\\tag{21.8}\\]\nThe regression of the adjusted response variable \\(\\boldsymbol{z}^{(t)}\\) on \\(\\boldsymbol{X}\\) leaves us with a weighted linear regression (hence the name working weights for \\(W_i\\)), whose maximum likelihood estimate is:\n\\[\n\\boldsymbol{\\widehat \\beta}^{(t+1)} = (\\boldsymbol{X}^T \\boldsymbol{\\widehat W}^{(t)} \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{\\widehat W}^{(t)} \\boldsymbol{z}^{(t)},\n\\tag{21.9}\\]\nwhich we define as our next iterate. It’s easy to verify that the IRLS iteration (21.9) is equivalent to the Newton-Raphson iteration (21.7). Note that we have derived these algorithms for canonical links; they each can be derived for non-canonical links but need not be equivalent in this more general case."
  },
  {
    "objectID": "glm-inference.html#sec-preliminaries",
    "href": "glm-inference.html#sec-preliminaries",
    "title": "22  Inference in GLMs",
    "section": "22.1 Preliminaries",
    "text": "22.1 Preliminaries\n\n22.1.1 Inferential goals\nThere are two types of inferential goals: hypothesis testing and confidence interval/region construction.\n\n22.1.1.1 Hypothesis testing\n\nSingle coefficient: \\(H_0: \\beta_j = \\beta_j^0\\) versus \\(H_1: \\beta_j \\neq \\beta_j^0\\) for some \\(\\beta_j^0 \\in \\mathbb{R}\\).\nGroup of coefficients: \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{\\beta}_S^0\\) versus \\(H_1: \\boldsymbol{\\beta}_S \\neq \\boldsymbol{\\beta}_S^0\\) for some \\(S \\subset \\{0,\\dots,p-1\\}\\) and some \\(\\boldsymbol{\\beta}_S^0 \\in \\mathbb{R}^{|S|}\\).\nGoodness of fit: The goodness of fit null hypothesis is that the GLM (20.1) is correctly specified. Consider the saturated model: \\[\ny_i \\overset{\\text{ind}} \\sim \\text{EDM}(\\mu_i, \\phi_0/w_i) \\quad \\text{for} \\quad i = 1,\\dots,n.\n\\tag{22.1}\\] Let \\[\n\\mathcal{M}^{\\text{GLM}} \\equiv \\{\\boldsymbol{\\mu}: \\mu_i = \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta} + o_i \\text{ for some } \\boldsymbol{\\beta} \\in \\mathbb{R}^p\\}\n\\] be the set of mean vectors consistent with the GLM. Then, the goodness of fit testing problem is \\(H_0: \\boldsymbol{\\mu} \\in \\mathcal{M}^{\\text{GLM}}\\) versus \\(H_1: \\boldsymbol{\\mu} \\notin \\mathcal{M}^{\\text{GLM}}\\).\n\n\n\n22.1.1.2 Confidence interval/region construction\n\nConfidence interval for a single coefficient: Here, the goal is to produce a confidence interval \\(\\text{CI}(\\beta_j)\\) for a coefficient \\(\\beta_j\\).\nConfidence region for a group of coefficients: Here, the goal is to produce a confidence region \\(\\text{CR}(\\boldsymbol{\\beta}_S)\\) for a group of coefficients \\(\\boldsymbol{\\beta}_S\\).\nConfidence interval for a fitted value: In GLMs, fitted values can either be considered for parameters on the linear scale (\\(\\eta_i = \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta} + o_i\\)) or the mean scale (\\(\\mu_i = g^{-1}(\\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta} + o_i)\\)). The goal, then, is to produce confidence intervals \\(\\text{CI}(\\eta_i)\\) or \\(\\text{CI}(\\mu_i)\\) for \\(\\eta_i\\) or \\(\\mu_i\\), respectively.\n\n\n\n\n22.1.2 Inferential tools\nInference in GLMs is based on asymptotic likelihood theory. These asymptotics can be based on large-sample asymptotics or small-dispersion asymptotics. Large-sample asymptotics are applicable for testing hypotheses and estimating parameters within models where the number of parameters is fixed while the sample size grows. Small-dispersion asymptotics are applicable for testing hypotheses and estimating parameters within models where the dispersion is small, regardless of the sample size. Large-sample asymptotics apply to testing and estimating coefficients in GLMs (20.1) with a fixed number of parameters as the sample size grows, but not to testing goodness of fit. Indeed, goodness-of-fit tests refer to the saturated model (22.1), whose number of parameters grows with \\(n\\). Small-dispersion asymptotics, on the other hand, apply to goodness-of-fit testing.\nHypothesis tests (and, by inversion, confidence intervals) can be constructed in three asymptotically equivalent ways: Wald tests, likelihood ratio tests (LRT), and score tests. These tests can be justified using either large-sample or small-dispersion asymptotics, depending on the context. Despite their asymptotic equivalence, in finite samples, some tests may be preferable to others (though for normal linear models, these tests are equivalent in finite samples as well). See Figure 22.1.\n\n\n\nFigure 22.1: A comparison of the three asymptotic methods for GLM inference."
  },
  {
    "objectID": "glm-inference.html#sec-wald-inference",
    "href": "glm-inference.html#sec-wald-inference",
    "title": "22  Inference in GLMs",
    "section": "22.2 Wald inference",
    "text": "22.2 Wald inference\nWald inference is based on the following asymptotic normality statement:\n\\[\n\\boldsymbol{\\widehat \\beta} \\overset{\\cdot}{\\sim} N(\\boldsymbol{\\beta}, \\boldsymbol{I}^{-1}(\\boldsymbol{\\beta})) = N(\\boldsymbol{\\beta}, \\phi_0(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\beta}) \\boldsymbol{X})^{-1}),\n\\tag{22.2}\\]\nrecalling our derivation of the Fisher information from equation (21.4). This normal approximation can be justified via the central limit theorem in the context of large-sample asymptotics or small-dispersion asymptotics. Wald inference is easy to carry out, and for this reason, it is considered the default type of inference. However, as we will see in Unit 5, it also tends to be the least accurate in small samples. Furthermore, Wald tests are usually not applied for testing goodness of fit.\n\n22.2.1 Wald test for \\(\\beta_j = \\beta_j^0\\) (known \\(\\phi_0\\))\nBased on the Wald approximation (22.2), under the null hypothesis, we have:\n\\[\n\\begin{split}\n\\widehat \\beta_j &\\overset{\\cdot}{\\sim} N(\\beta_j^0, \\phi_0[(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\beta}) \\boldsymbol{X})^{-1}]_{jj}) \\\\\n&\\approx N(\\beta_j^0, \\phi_0[(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{jj}) \\\\\n&\\equiv N(\\beta_j^0, \\text{SE}(\\widehat \\beta_j)^2),\n\\end{split}\n\\]\nwhere we have used a plug-in estimator of the variance. This leads us to the Wald \\(z\\)-test:\n\\[\n\\phi(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv 1\\left(\\left|\\frac{\\widehat \\beta_j - \\beta_j^0}{\\text{SE}(\\widehat \\beta_j)}\\right| &gt; z_{1-\\alpha/2}\\right).\n\\]\nSince a one-dimensional parameter is being tested, we can make the test one-sided if desired.\n\n\n22.2.2 Wald test for \\(\\boldsymbol{\\beta}_S = \\boldsymbol{\\beta}_S^0\\) (known \\(\\phi_0\\))\nExtending the reasoning above, we have under the null hypothesis that:\n\\[\n\\boldsymbol{\\widehat \\beta}_S \\overset{\\cdot}{\\sim} N(\\boldsymbol{\\beta}_S^0, \\phi_0[(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\beta}) \\boldsymbol{X})^{-1}]_{S,S}) \\approx N(\\boldsymbol{\\beta}_S^0, \\phi_0[(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{S,S}),\n\\]\nand therefore:\n\\[\n\\frac{1}{\\phi_0} (\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S^0)^T \\left([(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{S,S}\\right)^{-1}(\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S^0) \\overset{\\cdot}{\\sim} \\chi^2_{|S|}.\n\\]\nHence, we have the Wald \\(\\chi^2\\) test:\n\\[\n\\begin{split}\n&\\phi(\\boldsymbol{X}, \\boldsymbol{y}) \\\\\n&\\equiv 1\\left(\\frac{1}{\\phi_0} (\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S^0)^T \\left([(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{S,S}\\right)^{-1}(\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S^0) &gt; \\chi^2_{|S|}(1-\\alpha)\\right).\n\\end{split}\n\\]\n\n\n22.2.3 Wald confidence interval for \\(\\beta_j\\) (known \\(\\phi_0\\))\nInverting the Wald test for \\(\\beta_j\\), we get a Wald confidence interval:\n\\[\n\\text{CI}(\\beta_j) \\equiv \\widehat \\beta_j \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\widehat \\beta_j),\n\\tag{22.3}\\] where \\[\n\\text{SE}(\\widehat \\beta_j) \\equiv \\sqrt{\\phi_0[(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{jj}}.\n\\]\n\n\n22.2.4 Wald confidence region for \\(\\boldsymbol{\\beta}_S\\) (known \\(\\phi_0\\))\nBy inverting the test of \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{\\beta}_S^0\\), we get the Wald confidence region:\n\\[\n\\small\n\\begin{split}\n\\text{CR}(\\boldsymbol{\\beta}_S) \\equiv \\left\\{\\boldsymbol{\\beta}_S: \\frac{1}{\\phi_0} (\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S)^T \\left([(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{S,S}\\right)^{-1}(\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S) \\leq \\chi^2_{|S|}(1-\\alpha)\\right\\}.\n\\end{split}\n\\]\nIf \\(S = \\{0, 1, \\dots, p-1\\}\\), we are left with:\n\\[\n\\text{CR}(\\boldsymbol{\\beta}_S) \\equiv \\left\\{\\boldsymbol{\\beta}: \\frac{1}{\\phi_0} (\\boldsymbol{\\widehat \\beta} - \\boldsymbol{\\beta})^T \\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X} (\\boldsymbol{\\widehat \\beta} - \\boldsymbol{\\beta}) \\leq \\chi^2_{p}(1-\\alpha)\\right\\}.\n\\]\n\n\n22.2.5 Wald confidence intervals for \\(\\eta_i\\) and \\(\\mu_i\\) (known \\(\\phi_0\\))\nGiven the Wald approximation (22.2), we have:\n\\[\n\\widehat \\eta_i \\equiv o_i + \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat \\beta} \\overset{\\cdot}{\\sim} N(\\eta_i, \\phi_0 \\cdot \\boldsymbol{x}_{i*}^T (\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1} \\boldsymbol{x}_{i*}) \\equiv N(\\eta_i, \\text{SE}(\\hat \\eta_i)^2).\n\\]\nHence, the Wald interval for \\(\\eta_i\\) is:\n\\[\n\\text{CI}(\\eta_i) \\equiv o_i + \\boldsymbol{x}_{i*}^T \\boldsymbol{\\widehat\\beta} \\pm z_{1-\\alpha/2} \\cdot \\text{SE}(\\hat \\eta_i),\n\\] where \\[\n\\text{SE}(\\hat \\eta_i) \\equiv \\sqrt{\\phi_0 \\boldsymbol{x}_{i*}^T (\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1} \\boldsymbol{x}_{i*}}.\n\\]\nA confidence interval for \\(\\mu_i \\equiv \\mathbb{E}_{\\boldsymbol{\\beta}}[y_i] = g^{-1}(\\eta_i)\\) can be obtained by applying the monotonic function \\(g^{-1}\\) to the endpoints of the confidence interval for \\(\\eta_i\\). Note that the resulting confidence interval may be asymmetric. We can get a symmetric interval by applying the delta method, but this interval would be less accurate because it involves the delta method approximation in addition to the Wald approximation.\n\n\n22.2.6 Wald inference when \\(\\phi_0\\) is unknown\nWhen \\(\\phi_0\\) is unknown, we need to plug in an estimate \\(\\widetilde \\phi_0\\) (e.g. the deviance-based or Pearson-based estimate). Now our standard errors are \\[\n\\text{SE}(\\widehat \\beta_j) \\equiv \\sqrt{\\widetilde \\phi_0 \\cdot [(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{jj}},\n\\] and our test statistic for \\(H_0: \\beta_j = \\beta_j^0\\) is \\[\n\\frac{\\widehat \\beta_j - \\beta_j^0}{\\sqrt{\\widetilde \\phi_0}\\sqrt{[(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{jj}}}.\n\\]\nUnlike linear regression, it is not the case in general that \\(\\boldsymbol{\\widehat \\beta}\\) and \\(\\widetilde \\phi_0\\) are independent. Nevertheless, they are asymptotically independent. Therefore, the above statistic is approximately distributed as \\(t_{n-p}\\). Hence, the test for \\(H_0: \\beta_j = \\beta_j^0\\) is:\n\\[\n\\phi(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv 1\\left(\\left|\\frac{\\widehat \\beta_j - \\beta_j^0}{\\text{SE}(\\widehat \\beta_j)}\\right| &gt; t_{n-p}(1-\\alpha/2)\\right).\n\\]\nLikewise, we would replace \\(z_{1-\\alpha}\\) by \\(t_{n-p}(1-\\alpha/2)\\) for all tests and confidence intervals concerning univariate quantities. For multivariate quantities, we will get approximate \\(F\\) distributions instead of approximate \\(\\chi^2\\) distributions. For example:\n\\[\n\\frac{\\frac{1}{|S|}(\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S^0)^T \\left([(\\boldsymbol{X}^T \\boldsymbol{W}(\\boldsymbol{\\widehat \\beta}) \\boldsymbol{X})^{-1}]_{S,S}\\right)^{-1}(\\boldsymbol{\\widehat \\beta}_S - \\boldsymbol{\\beta}_S^0)}{\\widetilde \\phi_0} \\overset{\\cdot}{\\sim} F_{|S|, n-p}.\n\\]"
  },
  {
    "objectID": "glm-inference.html#sec-score-inference",
    "href": "glm-inference.html#sec-score-inference",
    "title": "22  Inference in GLMs",
    "section": "22.4 Score-based inference",
    "text": "22.4 Score-based inference\nScore-based inference can be used for the same set of inferential tasks as likelihood ratio inference.\n\n22.4.1 Testing multiple coefficients (\\(\\phi_0\\) known)\nLet \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{\\beta}_S^0\\) be a null hypothesis about a subset of variables \\(S\\), and let \\(\\boldsymbol{\\widehat{\\beta}}^0\\) be the maximum likelihood estimate under this null hypothesis. In particular, let \\(\\boldsymbol{\\widehat{\\beta}}^0_S \\equiv \\boldsymbol{\\beta}_S^0\\) and \\[\n\\boldsymbol{\\widehat{\\beta}}^0_{\\text{-}S} = \\underset{\\boldsymbol{\\beta}_{\\text{-}S}}{\\arg \\max}\\ \\ell(\\boldsymbol{\\beta}^0_{S}, \\boldsymbol{\\beta}_{\\text{-}S}).\n\\tag{22.5}\\] Let us partition the score vector \\(\\boldsymbol U(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} \\equiv (\\boldsymbol U_S(\\boldsymbol \\beta), \\boldsymbol U_{\\text{-}S}(\\boldsymbol \\beta))\\). We have \\[\n\\begin{split}\n\\boldsymbol U(\\boldsymbol \\beta) &= { \\boldsymbol U_S(\\boldsymbol \\beta) \\choose \\boldsymbol U_{\\text{-}S}(\\boldsymbol \\beta) } \\\\\n&\\sim N\\left(0, \\boldsymbol I(\\boldsymbol \\beta)\\right) = N\\left(0, \\begin{bmatrix} \\boldsymbol I_{S,S}(\\boldsymbol \\beta) & \\boldsymbol I_{S,\\text{-}S}(\\boldsymbol \\beta) \\\\ \\boldsymbol I_{\\text{-}S,S}(\\boldsymbol \\beta) & \\boldsymbol I_{\\text{-}S,\\text{-}S}(\\boldsymbol \\beta) \\end{bmatrix}\\right).\n\\end{split}\n\\tag{22.6}\\] This approximation can be justified either by small-dispersion asymptotics or large-sample asymptotics (both based on the central limit theorem). The score test statistic is based on plugging in the null estimate \\(\\boldsymbol{\\widehat{\\beta}}^0\\) into the score vector and extracting the components corresponding to \\(S\\): \\[\n\\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0).\n\\] This vector does not have the distribution obtained from the coordinates \\(S\\) of equation (22.6) because an estimate is plugged in. Instead, we can derive the distribution of this vector by conditioning on \\(\\boldsymbol U_{\\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0) = 0\\): \\[\n\\begin{split}\n\\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0) &\\overset d \\approx \\left.\\mathcal L(\\boldsymbol U_S(\\boldsymbol \\beta) \\mid \\boldsymbol U_{\\text{-}S}(\\boldsymbol \\beta) = 0)\\right|_{\\boldsymbol \\beta = \\boldsymbol{\\widehat{\\beta}}^0} \\\\\n& \\overset \\cdot \\sim N\\left(0, \\boldsymbol I_{S,S}(\\boldsymbol{\\widehat{\\beta}}^0) - \\boldsymbol I_{S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)\\boldsymbol I_{\\text{-}S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)^{-1}\\boldsymbol I_{\\text{-}S, S}(\\boldsymbol{\\widehat{\\beta}}^0)\\right).\n\\end{split}\n\\tag{22.7}\\] The second line is obtained from (22.6) using the formula for a conditional distribution of a multivariate normal distribution. Therefore, the score test is based on the following chi-square approximation: \\[\n\\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0)^T \\left[\\boldsymbol I_{S,S}(\\boldsymbol{\\widehat{\\beta}}^0) - \\boldsymbol I_{S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)\\boldsymbol I_{\\text{-}S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)^{-1}\\boldsymbol I_{\\text{-}S, S}(\\boldsymbol{\\widehat{\\beta}}^0)\\right]^{-1} \\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0) \\overset \\cdot \\sim \\chi^2_{|S|}.\n\\] Recalling the expressions for the score (21.2) and Fisher information matrix (21.4) in GLMs, we derive the score statistic \\[\n\\begin{split}\n&T^2_{\\text{score}}(\\boldsymbol X, \\boldsymbol y) \\equiv \\\\\n&\\frac{1}{\\phi_0}(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}^0)^T \\boldsymbol{\\widehat{W}}^0 \\boldsymbol{\\widehat{M}}^0 \\boldsymbol{X}_{*,S} \\times \\\\\n& \\quad \\left[\\boldsymbol X_{*,S}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,S} - \\boldsymbol X_{*,S}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,\\text{-}S}(\\boldsymbol X_{*,\\text{-}S}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,\\text{-}S})^{-1}\\boldsymbol X_{*,\\text{-}S}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,S} \\right]^{-1} \\times \\\\\n& \\quad \\boldsymbol{X}_{*,S}^T \\boldsymbol{\\widehat{M}}^0 \\boldsymbol{\\widehat{W}}^0 (\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}^0).\n\\end{split}\n\\] The score test is therefore \\[\n\\phi(\\boldsymbol X, \\boldsymbol y) \\equiv 1(T^2_{\\text{score}}(\\boldsymbol X, \\boldsymbol y) &gt; \\chi^2_{|S|}(1-\\alpha)).\n\\] An equivalent formulation of the score test can be derived by noting that \\[\n\\left[I_{S,S}(\\boldsymbol{\\widehat{\\beta}}^0) - \\boldsymbol I_{S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)\\boldsymbol I_{\\text{-}S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)^{-1}\\boldsymbol I_{\\text{-}S, S}(\\boldsymbol{\\widehat{\\beta}}^0)\\right]^{-1} = [\\boldsymbol I(\\boldsymbol{\\widehat{\\beta}}^0)^{-1}]_{S,S}.\n\\] Hence, we have \\[\n\\begin{split}\n&\\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0)^T \\left[\\boldsymbol I_{S,S}(\\boldsymbol{\\widehat{\\beta}}^0) - \\boldsymbol I_{S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)\\boldsymbol I_{\\text{-}S, \\text{-}S}(\\boldsymbol{\\widehat{\\beta}}^0)^{-1}\\boldsymbol I_{\\text{-}S, S}(\\boldsymbol{\\widehat{\\beta}}^0)\\right]^{-1} \\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0) \\\\\n&\\quad= \\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0)^T [\\boldsymbol I(\\boldsymbol{\\widehat{\\beta}}^0)^{-1}]_{S,S} \\boldsymbol U_S(\\boldsymbol{\\widehat{\\beta}}^0) \\\\\n&\\quad= \\boldsymbol U(\\boldsymbol{\\widehat{\\beta}}^0)^T \\boldsymbol I(\\boldsymbol{\\widehat{\\beta}}^0)^{-1} \\boldsymbol U(\\boldsymbol{\\widehat{\\beta}}^0),\n\\end{split}\n\\] where the last step used the fact that \\(\\boldsymbol U_{\\text{-}S}(\\boldsymbol{\\widehat \\beta}^{0}) = 0\\). Specializing to GLMs, we find that the score test statistic can be written as \\[\n\\begin{split}\n&T^2_{\\text{score}}(\\boldsymbol X, \\boldsymbol y) = \\\\\n&\\frac{1}{\\phi_0} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}^0) \\boldsymbol{\\widehat W}^0 \\boldsymbol{\\widehat M}^0 \\boldsymbol X (\\boldsymbol X^T \\boldsymbol{\\widehat W}^0 \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol{\\widehat M}^0 \\boldsymbol{\\widehat W}^0 (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}^0).\n\\end{split}\n\\tag{22.8}\\]\n\n\n22.4.2 Testing a single coefficient (\\(\\phi_0\\) known)\nIf \\(S = \\{j\\}\\), the normal approximation (22.7) specializes to \\[\nT_{\\text{score}} \\overset{\\cdot}{\\sim} N(0, 1),\n\\] where \\[\n\\begin{split}\n&T_{\\text{score}} = \\\\\n&\\frac{\\boldsymbol{x}_{*,j}^T \\boldsymbol{\\widehat{M}}^0 \\boldsymbol{\\widehat{W}}^0 (\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}^0)}{\\sqrt{\\phi_0 (\\boldsymbol x_{*,j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol x_{*,j} - \\boldsymbol x_{*,j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,\\text{-}j}(\\boldsymbol X_{*,\\text{-}j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,\\text{-}j})^{-1}\\boldsymbol X_{*,\\text{-}j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol x_{*,j})}}.\n\\end{split}\n\\]\nUnlike its multivariate counterparts, we can construct not just a two-sided test but also one-sided tests based on this normal approximation. For example, below is a right-sided score test for \\(H_0: \\beta_j = \\beta_j^0\\):\n\\[\n\\phi(\\boldsymbol{X}, \\boldsymbol{y}) = 1\\left(T_{\\text{score}} &gt; z_{1-\\alpha}\\right).\n\\]\nThe nice thing about the score test is that the model need only be fit under the null hypothesis. Therefore, computation can be recycled if \\(\\boldsymbol X_{*, \\text{-}j}\\) is a standard set of control variables and there are several options for \\(\\boldsymbol x_{*, j}\\) to test.\n\n\n22.4.3 Confidence interval for a single coefficient (\\(\\phi_0\\) known)\nJust as with the likelihood ratio test, it is possible to invert a score test for a single coefficient to obtain a confidence interval. It is uncommon to invert a multivariate test to obtain a confidence region for multiple coordinates of \\(\\boldsymbol{\\beta}\\), given the computationally expensive search across a grid of possible \\(\\boldsymbol{\\beta}\\) values.\n\n\n22.4.4 Goodness of fit testing (\\(\\phi_0\\) known)\nWe can view goodness of fit testing as testing a hypothesis about the coefficients in the following augmented GLM: \\[\ny_i \\sim \\text{EDM}(\\mu_i, \\phi_i); \\quad g(\\mu_i) = \\boldsymbol x_{i*}^T \\boldsymbol \\beta + \\boldsymbol z_{i*}^T \\boldsymbol \\gamma,\n\\] where \\(\\boldsymbol Z \\in \\mathbb R^{n \\times (n-p)}\\) is a matrix of extra variables so that the columns of the augmented model matrix \\(\\widetilde{\\boldsymbol X} \\equiv [\\boldsymbol X, \\boldsymbol Z]\\) form a basis for \\(\\mathbb R^n\\). Then, the goodness of fit null hypothesis is \\(H_0: \\boldsymbol \\gamma = \\boldsymbol 0\\), and the alternative hypothesis is the saturated model. To test this hypothesis, we use the score test statistic in the form of equation (22.8) with the augmented model matrix \\(\\widetilde{\\boldsymbol X}\\) in place of \\(\\boldsymbol X\\) and the GLM fits \\(\\boldsymbol {\\widehat \\mu}, \\boldsymbol{\\widehat W}, \\boldsymbol{\\widehat M}\\) in place of \\(\\boldsymbol{\\widehat \\mu}^0, \\boldsymbol{\\widehat W}^0, \\boldsymbol{\\widehat M}^0\\) to reflect that the full original GLM is being fit under the null hypothesis. Therefore, we get the score statistic \\[\nT^2_{\\text{score}} = \\frac{1}{\\phi_0} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}) \\boldsymbol{\\widehat W} \\boldsymbol{\\widehat M} \\widetilde{\\boldsymbol X} (\\widetilde{\\boldsymbol X}^T \\boldsymbol{\\widehat W} \\widetilde{\\boldsymbol X})^{-1} \\widetilde{\\boldsymbol X}^T \\boldsymbol{\\widehat M} \\boldsymbol{\\widehat W} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}).\n\\] Now, note that the matrix \\(\\widetilde{\\boldsymbol X}\\) is a full-rank square matrix, and therefore it is invertible. Hence, we can simplify the statistic as follows: \\[\n\\begin{split}\nT^2_{\\text{score}} &= \\frac{1}{\\phi_0} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}) \\boldsymbol{\\widehat W} \\boldsymbol{\\widehat M} \\widetilde{\\boldsymbol X} \\widetilde{\\boldsymbol X}^{-1} \\boldsymbol{\\widehat W}^{-1}((\\widetilde{\\boldsymbol X})^{T})^{-1} \\widetilde{\\boldsymbol X}^T \\boldsymbol{\\widehat M} \\boldsymbol{\\widehat W} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}) \\\\\n&= \\frac{1}{\\phi_0} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}) \\boldsymbol{\\widehat W} \\boldsymbol{\\widehat M} \\boldsymbol{\\widehat W}^{-1} \\boldsymbol{\\widehat M} \\boldsymbol{\\widehat W} (\\boldsymbol y - \\boldsymbol{\\widehat \\mu}) \\\\\n&= \\frac{1}{\\phi_0}(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}})^T \\boldsymbol{\\widehat{W}} \\boldsymbol{\\widehat{M}}^2 (\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}) \\\\\n&= \\frac{1}{\\phi_0}(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}})^T \\text{diag}\\left( \\frac{w_i}{V(\\widehat{\\mu}_i)} \\right) (\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}) \\\\\n&= \\frac{1}{\\phi_0}\\sum_{i=1}^n \\frac{w_i (y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)} \\\\\n&\\equiv \\frac{1}{\\phi_0} X^2,\n\\end{split}\n\\] where \\(X^2\\) is the Pearson chi-square statistic. Therefore, the score test for goodness of fit is:\n\\[\n\\phi(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv 1\\left(X^2 &gt; \\phi_0 \\chi^2_{n-p}(1-\\alpha)\\right).\n\\]\nIn the context of contingency table analysis (see the next chapter), this test reduces to the Pearson chi-square test of independence between two categorical variables. This test was proposed in 1900; it was only pointed out about a century later that this is a score test (Smyth 2003).\nAs with the likelihood ratio test for goodness of fit, the score test for goodness of fit must be justified by small-dispersion asymptotics. In particular, the score test for goodness of fit relies on the central limit theorem for small dispersions. To verify whether this approximation is accurate, we can apply the rules of thumb from Section 19.6.1.2 for each observation \\(y_i\\), when it is drawn from the distribution fitted under the GLM (rather than the saturated model). For instance, we can check that \\(m \\hat \\mu_i \\geq 5\\) and \\(m(1-\\hat \\mu_i) \\geq 5\\) in the case of grouped logistic regression of \\(\\hat \\mu_i \\geq 5\\) for Poisson regression. Here, \\(\\hat \\mu_i\\) are the fitted means under the GLM.\n\n\n22.4.5 Score test inference for \\(\\phi_0\\) unknown\nScore test inference for one or more coefficients \\(\\boldsymbol{\\beta}_S\\) can be achieved by replacing \\(\\phi_0\\) with one of its estimators and replacing the normal and chi-square distributions with \\(t\\) and \\(F\\) distributions, respectively. For example, the score test for a single coefficient \\(\\beta_j\\) is:\n\\[\n\\phi(\\boldsymbol{X}, \\boldsymbol{y}) = 1\\left(T_{\\text{score}} &gt; t_{n-p}(1-\\alpha)\\right),\n\\] where \\[\n\\begin{split}\n&T_{\\text{score}} = \\\\\n&\\frac{\\boldsymbol{x}_{*,j}^T \\boldsymbol{\\widehat{M}}^0 \\boldsymbol{\\widehat{W}}^0 (\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}^0)}{\\sqrt{\\tilde \\phi_0 (\\boldsymbol x_{*,j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol x_{*,j} - \\boldsymbol x_{*,j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,\\text{-}j}(\\boldsymbol X_{*,\\text{-}j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol X_{*,\\text{-}j})^{-1}\\boldsymbol X_{*,\\text{-}j}^T \\boldsymbol {\\widehat W}^{0}\\boldsymbol x_{*,j})}}.\n\\end{split}\n\\]\nThe \\(t\\) and \\(F\\) distributions are not exact in finite samples, but are better approximations than the normal and chi-square distributions. The score test for goodness of fit is not applicable in the case when \\(\\phi_0\\) is unknown, similarly to the likelihood ratio test. Indeed, note the relationship between the Pearson goodness of fit test, which rejects when \\(\\frac{1}{\\phi_0}X^2 &gt; \\chi^2_{n-p}(1-\\alpha)\\), and the Pearson estimator of the dispersion parameter: \\(\\widetilde{\\phi}_0 \\equiv \\frac{X^2}{n-p}\\). If we try to plug in the Pearson estimator for the dispersion into the Pearson goodness of fit test, we end up with a test statistic deterministically equal to \\(n-p\\). This reflects the fact that the residual degrees of freedom can either be used to estimate the dispersion or to test goodness of fit; they cannot be used for both."
  },
  {
    "objectID": "r-demo-part-4.html#sec-crime-data",
    "href": "r-demo-part-4.html#sec-crime-data",
    "title": "23  R demo",
    "section": "23.1 Crime data",
    "text": "23.1 Crime data\nLet’s revisit the crime data from Homework 2, this time fitting a logistic regression to it.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# read crime data\ncrime_data &lt;- read_tsv(\"data/Statewide_crime.dat\")\n\n# read and transform population data\npopulation_data &lt;- read_csv(\"data/state-populations.csv\")\npopulation_data &lt;- population_data |&gt;\n  filter(State != \"Puerto Rico\") |&gt;\n  select(State, Pop) |&gt;\n  rename(state_name = State, state_pop = Pop)\n\n# collate state abbreviations\nstate_abbreviations &lt;- tibble(\n  state_name = state.name,\n  state_abbrev = state.abb\n) |&gt;\n  add_row(state_name = \"District of Columbia\", state_abbrev = \"DC\")\n\n# add CrimeRate to crime_data\ncrime_data &lt;- crime_data |&gt;\n  mutate(STATE = ifelse(STATE == \"IO\", \"IA\", STATE)) |&gt;\n  rename(state_abbrev = STATE) |&gt;\n  filter(state_abbrev != \"DC\") |&gt; # remove outlier\n  left_join(state_abbreviations, by = \"state_abbrev\") |&gt;\n  left_join(population_data, by = \"state_name\") |&gt;\n  mutate(CrimeRate = Violent / state_pop) |&gt;\n  select(state_abbrev, CrimeRate, Metro, HighSchool, Poverty, state_pop)\n\ncrime_data\n\n# A tibble: 50 × 6\n   state_abbrev CrimeRate Metro HighSchool Poverty state_pop\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 AK           0.000819   65.6       90.2     8      724357\n 2 AL           0.0000871  55.4       82.4    13.7   4934193\n 3 AR           0.000150   52.5       79.2    12.1   3033946\n 4 AZ           0.0000682  88.2       84.4    11.9   7520103\n 5 CA           0.0000146  94.4       81.3    10.5  39613493\n 6 CO           0.0000585  84.5       88.3     7.3   5893634\n 7 CT           0.0000867  87.7       88.8     6.4   3552821\n 8 DE           0.000664   80.1       86.5     5.8    990334\n 9 FL           0.0000333  89.3       85.9     9.7  21944577\n10 GA           0.0000419  71.6       85.2    10.8  10830007\n# ℹ 40 more rows\n\n\nWe can fit a GLM using the glm command, specifying as additional arguments the observation weights as well as the exponential dispersion model. In this case, the weights are the state populations and the family is binomial:\n\nglm_fit &lt;- glm(CrimeRate ~ Metro + HighSchool + Poverty,\n  weights = state_pop,\n  family = \"binomial\",\n  data = crime_data\n)\n\nWe can print the summary table as usual:\n\nsummary(glm_fit)\n\n\nCall:\nglm(formula = CrimeRate ~ Metro + HighSchool + Poverty, family = \"binomial\", \n    data = crime_data, weights = state_pop)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.609e+01  3.520e-01  -45.72   &lt;2e-16 ***\nMetro       -2.586e-02  5.727e-04  -45.15   &lt;2e-16 ***\nHighSchool   9.106e-02  3.450e-03   26.39   &lt;2e-16 ***\nPoverty      6.077e-02  4.852e-03   12.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15590  on 49  degrees of freedom\nResidual deviance: 11742  on 46  degrees of freedom\nAIC: 12136\n\nNumber of Fisher Scoring iterations: 5\n\n\nAmazingly, everything is very significant! This is because the weights for each observation (the state populations) are very high, effectively making the sample size very high. But frankly, this is a bit suspicious. Glancing at the bottom of the regression summary, we see a residual deviance of 11742 on 46 degrees of freedom. This part of the summary refers to the deviance-based goodness of fit test. Under the null hypothesis that the model fits well, we expect that the residual deviance has a distribution of \\(\\chi^2_{46}\\), which has a mean of 46.\nLet’s formally check the goodness of fit. We can extract the residual deviance and residual degrees of freedom from the GLM fit:\n\nglm_fit$deviance\n\n[1] 11742.28\n\nglm_fit$df.residual\n\n[1] 46\n\n\nWe can then compute the chi-square \\(p\\)-value:\n\n# compute based on residual deviance from fit object\npchisq(glm_fit$deviance,\n  df = glm_fit$df.residual,\n  lower.tail = FALSE\n)\n\n[1] 0\n\n# compute residual deviance as sum of squares of residuals\npchisq(sum(resid(glm_fit, \"deviance\")^2),\n  df = glm_fit$df.residual,\n  lower.tail = FALSE\n)\n\n[1] 0\n\n\nWow, we get a \\(p\\)-value of zero! Let’s try doing a score-based (i.e., Pearson) goodness of fit test:\n\npchisq(sum(resid(glm_fit, \"pearson\")^2),\n  df = glm_fit$df.residual,\n  lower.tail = FALSE\n)\n\n[1] 0\n\n\nAlso zero. So we need to immediately stop using this model for inference about these data, since it fits the data very poorly. We will discuss how to build a better model for the crime data in the next unit. For now, we turn to analyzing a different dataset."
  },
  {
    "objectID": "r-demo-part-4.html#sec-noisy-miner-data",
    "href": "r-demo-part-4.html#sec-noisy-miner-data",
    "title": "23  R demo",
    "section": "23.2 Noisy miner data",
    "text": "23.2 Noisy miner data\nCredit: Generalized Linear Models With Examples in R textbook.\nLet’s consider the noisy miner dataset. Noisy miners are a small but aggressive native Australian bird. We want to know how the number of these birds observed in a patch of land depends on various factors of that patch of land.\n\nlibrary(GLMsData)\ndata(\"nminer\")\nnminer |&gt; as_tibble()\n\n# A tibble: 31 × 8\n   Miners  Eucs  Area Grazed Shrubs Bulokes Timber Minerab\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n 1      0     2    22      0      1     120     16       0\n 2      0    10    11      0      1      67     25       0\n 3      1    16    51      0      1      85     13       3\n 4      1    20    22      0      1      45     12       2\n 5      1    19     4      0      1     160     14       8\n 6      1    18    61      0      1      75      6       1\n 7      1    12    16      0      1     100     12       8\n 8      1    16    14      0      1     321     15       5\n 9      0     3     5      0      1     275      8       0\n10      1    12     6      1      0     227     10       4\n# ℹ 21 more rows\n\n\nSince the response is a count, we can model it as a Poisson random variable. Let’s fit that GLM:\n\nglm_fit &lt;- glm(Minerab ~ . - Miners, family = \"poisson\", data = nminer)\nsummary(glm_fit)\n\n\nCall:\nglm(formula = Minerab ~ . - Miners, family = \"poisson\", data = nminer)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.886345   0.875737  -1.012    0.311    \nEucs         0.129309   0.021757   5.943 2.79e-09 ***\nArea        -0.028736   0.013241  -2.170    0.030 *  \nGrazed       0.140831   0.364622   0.386    0.699    \nShrubs       0.335828   0.375059   0.895    0.371    \nBulokes      0.001469   0.001773   0.828    0.408    \nTimber      -0.006781   0.009074  -0.747    0.455    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 150.545  on 30  degrees of freedom\nResidual deviance:  54.254  on 24  degrees of freedom\nAIC: 122.41\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe exclude Miners because this is just a binarized version of the response variable. Things look a bit better on the GOF front:\n\npchisq(sum(resid(glm_fit, \"deviance\")^2),\n  df = glm_fit$df.residual,\n  lower.tail = FALSE\n)\n\n[1] 0.000394186\n\npchisq(sum(resid(glm_fit, \"pearson\")^2),\n  df = glm_fit$df.residual,\n  lower.tail = FALSE\n)\n\n[1] 0.0001185197\n\n\nStill, there is some model misspecification, but for now, we still proceed with the rest of the analysis.\nThe standard errors shown in the summary are based on the Wald test. We can get Wald confidence intervals based on these standard errors by using the formula:\n\nglm_fit |&gt;\n  summary() |&gt;\n  coef() |&gt;\n  as.data.frame() |&gt;\n  transmute(`2.5 %` = Estimate + qnorm(0.025)*`Std. Error`,\n            `97.5 %` = Estimate + qnorm(0.025)*`Std. Error`)\n\n                   2.5 %       97.5 %\n(Intercept) -2.602757559 -2.602757559\nEucs         0.086666177  0.086666177\nArea        -0.054686818 -0.054686818\nGrazed      -0.573814583 -0.573814583\nShrubs      -0.399274191 -0.399274191\nBulokes     -0.002007061 -0.002007061\nTimber      -0.024565751 -0.024565751\n\n\nOr, we can simply use confint.default():\n\nconfint.default(glm_fit)\n\n                   2.5 %       97.5 %\n(Intercept) -2.602757559  0.830066560\nEucs         0.086666177  0.171951888\nArea        -0.054686818 -0.002784651\nGrazed      -0.573814583  0.855476296\nShrubs      -0.399274191  1.070929206\nBulokes     -0.002007061  0.004944760\nTimber      -0.024565751  0.011002885\n\n\nOr, we might want LRT-based confidence intervals, which are given by confint():\n\nconfint(glm_fit)\n\nWaiting for profiling to be done...\n\n\n                  2.5 %       97.5 %\n(Intercept) -2.63176754  0.812111327\nEucs         0.08782624  0.173336323\nArea        -0.05658079 -0.004456166\nGrazed      -0.57858596  0.855903871\nShrubs      -0.38600748  1.090319407\nBulokes     -0.00214123  0.004838901\nTimber      -0.02483241  0.010820749\n\n\nIn this case, the two sets of confidence intervals seem fairly similar.\nNow, we can get prediction intervals, either on the linear predictor scale or on the mean scale:\n\npred_linear &lt;- predict(glm_fit, newdata = nminer[31,], se.fit = TRUE)\npred_mean &lt;- predict(glm_fit, newdata = nminer[31,], type = \"response\", se.fit = TRUE)\n\npred_linear\n\n$fit\n       31 \n0.6556799 \n\n$se.fit\n[1] 0.2635664\n\n$residual.scale\n[1] 1\n\npred_mean\n\n$fit\n      31 \n1.926452 \n\n$se.fit\n       31 \n0.5077481 \n\n$residual.scale\n[1] 1\n\nlog(pred_mean$fit)\n\n       31 \n0.6556799 \n\n\nWe see that the prediction on the linear predictor scale is exactly the logarithm of the prediction on the mean scale. However, the standard error given on the mean scale uses the delta method. We prefer to directly transform the confidence interval from the linear scale using the inverse link function (in this case, the exponential):\n\n# using delta method\nc(pred_mean$fit + qnorm(0.025)*pred_mean$se.fit,\n  pred_mean$fit + qnorm(0.975)*pred_mean$se.fit)\n\n       31        31 \n0.9312839 2.9216197 \n\n# using transformation\nexp(c(pred_linear$fit + qnorm(0.025)*pred_linear$se.fit,\n      pred_linear$fit + qnorm(0.975)*pred_linear$se.fit))\n\n      31       31 \n1.149238 3.229285 \n\n\nIn this case, the intervals obtained are somewhat different. We can plot confidence intervals for the fit in a univariate case (e.g., regressing Minerab on Eucs) using geom_smooth():\n\nnminer |&gt;\n  ggplot(aes(x = Eucs, y = Minerab)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"glm\",\n              method.args = list(family = \"poisson\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also test the coefficients in the model. The Wald tests for individual coefficients were already given by the regression summary above. We might want to carry out likelihood ratio tests for individual coefficients instead. For example, let’s do this for Eucs:\n\nglm_fit_partial &lt;- glm(Minerab ~ . - Miners - Eucs, family = \"poisson\", data = nminer)\nanova(glm_fit_partial, glm_fit, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Minerab ~ (Miners + Eucs + Area + Grazed + Shrubs + Bulokes + \n    Timber) - Miners - Eucs\nModel 2: Minerab ~ (Miners + Eucs + Area + Grazed + Shrubs + Bulokes + \n    Timber) - Miners\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        25     95.513                          \n2        24     54.254  1   41.259 1.333e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Eucs variable is quite significant! We can manually carry out the LRT as a sanity check:\n\ndeviance_partial &lt;- glm_fit_partial$deviance\ndeviance_full &lt;- glm_fit$deviance\nlrt_stat &lt;- deviance_partial - deviance_full\np_value &lt;- pchisq(lrt_stat, df = 1, lower.tail = FALSE)\ntibble(lrt_stat, p_value)\n\n# A tibble: 1 × 2\n  lrt_stat  p_value\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     41.3 1.33e-10\n\n\nWe can test groups of variables using the likelihood ratio test as well."
  },
  {
    "objectID": "glm-special-cases.html",
    "href": "glm-special-cases.html",
    "title": "Generalized linear models: Special cases",
    "section": "",
    "text": "Chapter 4 developed a general theory for GLMs. In Chapter 5, we specialize this theory to several important cases, including logistic regression and Poisson regression."
  },
  {
    "objectID": "logistic-regression.html#sec-logistic-model",
    "href": "logistic-regression.html#sec-logistic-model",
    "title": "24  Logistic regression",
    "section": "24.1 Model definition and interpretation",
    "text": "24.1 Model definition and interpretation\n\n24.1.1 Model definition.\nRecall from Chapter 4 that the logistic regression model is \\[\nm_i y_i \\overset{\\text{ind}} \\sim \\text{Bin}(m_i, \\pi_i); \\quad \\text{logit}(\\pi_i) = \\log\\frac{\\pi_i}{1-\\pi_i} = \\boldsymbol{x}^T_{i*}\\boldsymbol{\\beta}.\n\\] Here we use the canonical logit link function, although other link functions are possible. We also set the offsets to 0. The interpretation of the parameter \\(\\beta_j\\) is that a unit increase in \\(x_j\\)—other predictors held constant—is associated with an (additive) increase of \\(\\beta_j\\) on the log-odds scale or a multiplicative increase of \\(e^{\\beta_j}\\) on the odds scale. Note that logistic regression data come in two formats: ungrouped and grouped. For ungrouped data, we have \\(m_1 = \\dots = m_n = 1\\), so \\(y_i \\in \\{0,1\\}\\) are Bernoulli random variables. For grouped data, we can have several independent Bernoulli observations per predictor \\(\\boldsymbol{x}_{i*}\\), which give rise to binomial proportions \\(y_i \\in [0,1]\\). This happens most often when all the predictors are discrete. You can always convert grouped data into ungrouped data, but not necessarily vice versa. We’ll discuss below that the grouped and ungrouped formulations of logistic regression have the same MLE and standard errors but different deviances.\n\n\n24.1.2 Generative model equivalent.\nConsider the following generative model for \\((\\boldsymbol{x}, y) \\in \\mathbb{R}^{p-1} \\times \\{0,1\\}\\): \\[\ny \\sim \\text{Ber}(\\pi); \\quad \\boldsymbol{x}|y \\sim \\begin{cases} N(\\boldsymbol{\\mu}_0, \\boldsymbol{V}) \\quad \\text{if } y = 0 \\\\ N(\\boldsymbol{\\mu}_1, \\boldsymbol{V}) \\quad \\text{if } y = 1 \\end{cases}.\n\\] Then, we can derive that \\(y|\\boldsymbol{x}\\) follows a logistic regression model (called a discriminative model because it conditions on \\(\\boldsymbol{x}\\)). Indeed, \\[\n\\begin{aligned}\n\\text{logit}(p(y = 1|\\boldsymbol{x})) &= \\log\\frac{p(y = 1)p(\\boldsymbol{x}|y = 1)}{p(y = 0)p(\\boldsymbol{x}|y = 0)} \\\\\n&= \\log\\frac{\\pi \\exp\\left(-\\frac12(\\boldsymbol{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{V}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_1)\\right)}{(1-\\pi) \\exp\\left(-\\frac12(\\boldsymbol{x} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{V}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_0)\\right)} \\\\\n&= \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{V}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0) \\\\\n&\\equiv \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}_{-0}.\n\\end{aligned}\n\\] This is another natural route to motivating the logistic regression model.\n\n\n24.1.3 Special case: \\(2 \\times 2\\) contingency table.\nSuppose that \\(x \\in \\{0,1\\}\\), and consider the logistic regression model \\(\\text{logit}(\\pi_i) = \\beta_0 + \\beta_1 x_i\\). For example, suppose that \\(x \\in \\{0,1\\}\\) encodes treatment (1) and control (0) in a clinical trial, and \\(y_i \\in \\{0,1\\}\\) encodes success (1) and failure (0). We make \\(n\\) observations of \\((x_i, y_i)\\) in this ungrouped setup. The parameter \\(e^{\\beta_1}\\) can be interpreted as the odds ratio: \\[\ne^{\\beta_1} = \\frac{\\mathbb{P}[y = 1|x=1]/\\mathbb{P}[y = 0|x=1]}{\\mathbb{P}[y = 1|x=0]/\\mathbb{P}[y = 0|x=0]}.\n\\] This parameter is the multiple by which the odds of success increase when going from control to treatment. We can summarize such data via the \\(2 \\times 2\\) contingency table (Table 24.1). A grouped version of this data would be \\(\\{(x_1, y_1) = (0, 7/24), (x_2, y_2) = (1, 9/21)\\}\\). The null hypothesis \\(H_0: \\beta_1 = 0 \\Longleftrightarrow H_0: e^{\\beta_1} = 1\\) states that the success probability in both rows of the table is the same.\n\n\nTable 24.1: An example of a \\(2 \\times 2\\) contingency table.\n\n\n\nSuccess\nFailure\nTotal\n\n\n\n\nTreatment\n9\n12\n21\n\n\nControl\n7\n17\n24\n\n\nTotal\n16\n29\n45"
  },
  {
    "objectID": "logistic-regression.html#sec-logistic-case-control",
    "href": "logistic-regression.html#sec-logistic-case-control",
    "title": "24  Logistic regression",
    "section": "24.2 Logistic regression with case-control studies",
    "text": "24.2 Logistic regression with case-control studies\nIn a prospective study (e.g. a clinical trial), we assign treatment or control (i.e., \\(x\\)) to individuals, and then observe a binary outcome (i.e., \\(y\\)). Sometimes, the outcome \\(y\\) takes a long time to measure or has a highly imbalanced distribution in the population (e.g., the development of lung cancer). In this case, an appealing study design is the retrospective study, where individuals are sampled based on their response values (e.g., presence of lung cancer) rather than their treatment/exposure status (e.g., smoking). It turns out that a logistic regression model is appropriate for such retrospective study designs as well.\nIndeed, suppose that \\(y|\\boldsymbol{x}\\) follows a logistic regression model. Let’s try to figure out the distribution of \\(y|\\boldsymbol{x}\\) in the retrospectively gathered sample. Letting \\(z \\in \\{0,1\\}\\) denote the indicator that an observation is sampled, define \\(\\rho_1 \\equiv \\mathbb{P}[z = 1|y = 1]\\) and \\(\\rho_0 \\equiv \\mathbb{P}[z = 1|y = 0]\\), and assume that \\(\\mathbb{P}[z = 1, y, \\boldsymbol{x}] = \\mathbb{P}[z = 1 | y]\\). The latter assumption states that the predictors \\(\\boldsymbol{x}\\) were not used in the retrospective sampling process. Then,\n\\[\n\\begin{split}\n\\text{logit}(\\mathbb{P}[y = 1|z = 1, \\boldsymbol{x}]) &= \\log \\frac{\\rho_1 \\mathbb{P}[y = 1|\\boldsymbol{x}]}{\\rho_0 \\mathbb{P}[y = 0|\\boldsymbol{x}]} \\\\\n&= \\log \\frac{\\rho_1}{\\rho_0} + \\text{logit}(\\mathbb{P}[y = 1|\\boldsymbol{x}]) \\\\\n&= \\left(\\log \\frac{\\rho_1}{\\rho_0} + \\beta_0\\right) + \\boldsymbol{x}^T \\boldsymbol{\\beta}_{-0}.\n\\end{split}\n\\]\nThus, conditioning on retrospective sampling changes only the intercept term, but preserves the coefficients of \\(\\boldsymbol{x}\\). Therefore, we can carry out inference for \\(\\boldsymbol{\\beta}_{-0}\\) in the same way regardless of whether the study design is prospective or retrospective."
  },
  {
    "objectID": "logistic-regression.html#sec-estimation-inference",
    "href": "logistic-regression.html#sec-estimation-inference",
    "title": "24  Logistic regression",
    "section": "24.3 Estimation and inference",
    "text": "24.3 Estimation and inference\n\n24.3.1 Score and Fisher information\nRecall from Chapter 4 that\n\\[\n\\boldsymbol{U}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} (\\boldsymbol{y} - \\boldsymbol{\\mu}) \\quad \\text{and} \\quad \\boldsymbol{I}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X},\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{W} &\\equiv \\text{diag}\\left(\\frac{w_i}{V(\\mu_i)(d\\eta_i/d\\mu_i)^2}\\right), \\\\\n\\boldsymbol{M} &\\equiv \\text{diag}\\left(\\frac{\\partial\\eta_i}{\\partial \\mu_i}\\right).\n\\end{aligned}\n\\]\nSince logistic regression uses a canonical link function, we get the following simplifications:\n\\[\n\\begin{aligned}\n\\boldsymbol{W} &= \\text{diag}\\left(w_i V(\\mu_i)\\right) = \\text{diag}\\left(m_i \\pi_i(1-\\pi_i)\\right), \\\\\n\\boldsymbol{M} &= \\text{diag}\\left(\\frac{1}{\\pi_i(1-\\pi_i)}\\right).\n\\end{aligned}\n\\]\nHere we have substituted the notation \\(\\boldsymbol{\\pi}\\) for \\(\\boldsymbol{\\mu}\\), and recall that for logistic regression, \\(\\phi_0 = 1\\), \\(w_i = m_i\\), and \\(V(\\pi_i) = \\pi_i(1-\\pi_i)\\). Therefore, the score equations for logistic regression are\n\\[\n0 = \\boldsymbol{X}^T \\text{diag}\\left(m_i\\right)(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}) \\quad \\Longleftrightarrow \\quad \\sum_{i = 1}^n m_i x_{ij}(y_i-\\widehat{\\pi}_i) = 0,\n\\tag{24.1}\\] for \\(j = 0, \\dots, p-1\\). We can solve these equations using IRLS. The Fisher information is\n\\[\n\\boldsymbol{I}(\\boldsymbol{\\beta}) = \\boldsymbol{X}^T \\text{diag}\\left(m_i \\pi_i(1-\\pi_i)\\right) \\boldsymbol{X}.\n\\]\n\n\n24.3.2 Wald inference\nUsing the results in the previous paragraph, we can carry out Wald inference based on the normal approximation\n\\[\n\\boldsymbol{\\widehat \\beta} \\overset \\cdot \\sim N\\left(\\boldsymbol \\beta, \\left(\\boldsymbol X^T\\text{diag}(m_i \\widehat \\pi_i(1-\\widehat \\pi_i))\\boldsymbol X\\right)^{-1}\\right).\n\\]\nThis approximation holds for \\(\\sum_{i = 1}^n m_i \\rightarrow \\infty\\).\n\n\n24.3.3 Example: \\(2 \\times 2\\) contingency table.\nSuppose we have a \\(2 \\times 2\\) contingency table. The grouped logistic regression formulation of these data is\n\\[\ny_0 \\sim \\frac{1}{m_0}\\text{Bin}(m_0, \\pi_0); \\quad y_1 \\sim \\frac{1}{m_1}\\text{Bin}(m_1, \\pi_1); \\quad \\text{logit}(\\pi_i) = \\beta_0 + \\beta_1 x_i.\n\\]\nIn this case, we have \\(n = p = 2\\), so the grouped logistic regression model is saturated. Therefore, we have\n\\[\n\\hat \\pi_0 = y_0, \\quad \\text{and} \\quad \\hat \\pi_1 = y_1, \\quad \\text{so} \\quad \\hat \\beta_1 = \\log \\frac{\\hat \\pi_1 / (1 - \\hat \\pi_1)}{\\hat \\pi_0 / (1 - \\hat \\pi_0)} = \\log \\frac{y_1 / (1 - y_1)}{y_0 / (1 - y_0)}.\n\\]\nThe squared Wald standard error for \\(\\hat \\beta_1\\) is\n\\[\n\\begin{split}\n\\text{SE}^2(\\widehat \\beta_1) &\\equiv \\left[\\left(\\boldsymbol X^T\\text{diag}(m_i \\widehat \\pi_i(1-\\widehat \\pi_i))\\boldsymbol X\\right)^{-1}\\right]_{22} \\\\\n&= \\left[\\left(\\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}^T\\begin{pmatrix} m_0y_0(1-y_0) & 0 \\\\ 0 & m_1y_1(1-y_1) \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\\right)^{-1}\\right]_{22} \\\\\n&= \\left[\\left(\\begin{pmatrix} m_0 y_0 (1-y_0) + m_1 y_1 (1-y_1) & m_1 y_1(1-y_1) \\\\ m_1 y_1(1-y_1) & m_1 y_1(1-y_1) \\end{pmatrix}\\right)^{-1}\\right]_{22} \\\\\n&= \\frac{m_0 y_0 (1-y_0) + m_1 y_1 (1-y_1)}{m_0y_0(1-y_0) \\cdot m_1y_1(1-y_1)} \\\\\n&= \\frac{1}{m_0y_0(1-y_0)} + \\frac{1}{m_1y_1(1-y_1)}.\n\\end{split}\n\\]\nTherefore, the Wald test for \\(H_0: \\beta_1 = 0\\) rejects if\n\\[\n\\left|\\frac{\\hat \\beta_1}{\\text{SE}(\\hat \\beta_1)}\\right| = \\left|\\frac{\\log \\frac{y_1 / (1 - y_1)}{y_0 / (1 - y_0)}}{\\sqrt{\\frac{1}{m_0y_0(1-y_0)} + \\frac{1}{m_1y_1(1-y_1)}}}\\right| &gt; z_{1-\\alpha/2}.\n\\]\n\n\n24.3.4 Hauck-Donner effect.\nUnfortunately, Wald inference in finite samples does not always perform very well. The Wald test above is known to be conservative if one or more of the mean parameters (in this case, \\(\\pi_i\\)) tends to the edge of the parameter space (in this case, \\(\\pi_i \\rightarrow 0\\) or \\(\\pi_i \\rightarrow 1\\)). This is called the Hauck-Donner effect. As an example, consider testing \\(H_0: \\beta_0 = 0\\) in the intercept-only model\n\\[\nmy \\sim \\text{Bin}(m, \\pi); \\quad \\text{logit}(\\pi) = \\beta_0.\n\\]\nThe Wald test statistic is \\(z \\equiv \\widehat \\beta/\\text{SE} = \\text{logit}(y)\\sqrt{my(1-y)}\\). This test statistic actually tends to decrease as \\(y \\rightarrow 1\\) (see Figure 24.1), since the standard error grows faster than the estimate itself. So the test statistic becomes less significant as we go further away from the null! A similar situation arises in the \\(2 \\times 2\\) contingency table example above, where the Wald test for \\(H_0: \\beta_1 = 0\\) becomes less significant as \\(y_0 \\rightarrow 0\\) and \\(y_1 \\rightarrow 1\\). As a limiting case of this, the Wald test is undefined if \\(y_0 = 0\\) and \\(y_1 = 1\\). This situation is a special case of perfect separability in logistic regression: when a hyperplane in covariate space separates observations with \\(y_i = 0\\) from those with \\(y_i = 1\\). Some of the maximum likelihood coefficient estimates are infinite in this case, causing the Wald test to be undefined since it uses these coefficient estimates as test statistics.\n\n\n\n\n\nFigure 24.1: The Hauck-Donner effect: The Wald statistic for testing \\(H_0: \\pi = 0.5\\) within the model \\(my \\sim \\text{Bin}(m, \\pi)\\) decreases as the proportion \\(y\\) approaches 1. Here, \\(m = 25\\).\n\n\n\n\n\n\n24.3.5 Likelihood ratio inference\n\n24.3.5.1 The Bernoulli and binomial deviance.\nLet’s first compute the deviance of a Bernoulli or binomial model. These deviances are the same because these two models have the same natural parameter and log-partition function. The unit deviance is\n\\[\nt(y, \\pi) = y \\log \\pi + (1-y)\\log(1-\\pi).\n\\]\nHence, we have\n\\[\nt(y, y) = y \\log y + (1-y) \\log(1-y).\n\\]\nHence, the unit deviance is\n\\[\nd(y, \\mu) \\equiv 2(t(y,y)-t(y,\\pi)) = 2\\left(y \\log \\frac{y}{\\pi} + (1-y)\\log \\frac{1-y}{1-\\pi}\\right).\n\\]\nThe total deviance, therefore, is\n\\[\n\\begin{split}\nD(\\boldsymbol y, \\hat{\\boldsymbol \\pi}) &\\equiv \\sum_{i = 1}^n w_i d(y_i, \\widehat \\pi_i) \\\\\n&= 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{y_i}{\\widehat \\pi_i} + m_i(1-y_i) \\log\\frac{1-y_i}{1-\\widehat \\pi_i}\\right).\n\\end{split}\n\\tag{24.2}\\]\n\n\n\n24.3.6 Comparing the deviances of grouped and ungrouped logistic regression models.\nLet us pause to compare the total deviances of grouped and ungrouped logistic regression models. Consider the following grouped and ungrouped models:\n\\[\ny^{\\text{grp}}_i \\overset{\\text{ind}} \\sim \\frac{1}{m_i}\\text{Bin}(m_i, \\pi_i) \\quad \\text{and} \\quad y^{\\text{ungrp}}_{ik} \\overset{\\text{ind}} \\sim \\text{Ber}(\\pi_i), \\quad k = 1, \\dots, m_i,\n\\] where \\[\n\\text{logit}(\\pi_i) = \\boldsymbol x_{i*}^T \\boldsymbol \\beta.\n\\]\nThe relationship between the grouped and ungrouped observations is that\n\\[\ny^{\\text{grp}}_i = \\frac{1}{m_i}\\sum_{k = 1}^{m_i} y^{\\text{ungrp}}_{ik} \\equiv \\bar y^{\\text{ungrp}}_i.\n\\]\nSince the grouped and ungrouped logistic regression models have the same likelihoods, it follows that they have the same maximum likelihood estimates \\(\\widehat{\\boldsymbol \\beta}\\) and \\(\\widehat{\\boldsymbol \\pi}\\). However, the total deviances of the two models are different. The total deviance of the grouped model can be derived from equation (24.2):\n\\[\n\\begin{split}\n&D(\\boldsymbol y^{\\text{grp}}, \\hat{\\boldsymbol \\pi}) \\\\\n&\\quad= 2\\sum_{i = 1}^n \\left(m_i y^{\\text{grp}}_i \\log \\frac{y^{\\text{grp}}_i}{\\widehat \\pi_i} + m_i(1-y^{\\text{grp}}_i) \\log\\frac{1-y^{\\text{grp}}_i}{1-\\widehat \\pi_i}\\right).\n\\end{split}\n\\tag{24.3}\\]\nOn the other hand, the total deviance of the ungrouped model is\n\\[\n\\begin{split}\n&D(\\boldsymbol y^{\\text{ungrp}}, \\hat{\\boldsymbol \\pi}) \\\\\n&= 2\\sum_{i = 1}^n \\sum_{k = 1}^{m_i} \\left(y^{\\text{ungrp}}_{ik} \\log \\frac{y^{\\text{ungrp}}_{ik}}{\\widehat \\pi_i} + (1-y^{\\text{ungrp}}_{ik}) \\log\\frac{1-y^{\\text{ungrp}}_{ik}}{1-\\widehat \\pi_i}\\right) \\\\\n&= 2\\sum_{i = 1}^n \\sum_{k = 1}^{m_i} \\left(y^{\\text{ungrp}}_{ik} \\log \\frac{1}{\\widehat \\pi_i} + (1-y^{\\text{ungrp}}_{ik}) \\log\\frac{1}{1-\\widehat \\pi_i}\\right) \\\\\n&= 2\\sum_{i = 1}^n \\left(m_i y^{\\text{grp}}_i \\log \\frac{1}{\\widehat \\pi_i} + m_i(1-y^{\\text{grp}}_i) \\log\\frac{1}{1-\\widehat \\pi_i}\\right).\n\\end{split}\n\\tag{24.4}\\]\nIn the second line, we used the fact that \\(y \\log y \\rightarrow 0\\) and \\((1-y)\\log(1-y) \\rightarrow 0\\) as \\(y \\rightarrow 0\\) or \\(y \\rightarrow 1\\). Comparing the grouped (24.3) and ungrouped (24.4) total deviances, we see that these are given by related, but different expressions. Because small dispersion asymptotics applies to the grouped model but not the ungrouped model, we have that under small-dispersion asymptotics,\n\\[\nD(\\boldsymbol y^{\\text{grp}}, \\hat{\\boldsymbol \\pi}) \\overset \\cdot \\sim \\chi^2_{n-p} \\quad \\text{but} \\quad D(\\boldsymbol y^{\\text{ungrp}}, \\hat{\\boldsymbol \\pi}) \\not \\sim \\chi^2_{n-p}.\n\\]\n\n24.3.6.1 Likelihood ratio inference for one or more coefficients.\nLetting \\(\\boldsymbol{\\widehat \\pi}_0\\) and \\(\\boldsymbol{\\widehat \\pi}_1\\) be the MLEs from two nested models, we can then express the likelihood ratio statistic as\n\\[\nD(\\boldsymbol y, \\boldsymbol{\\widehat \\pi}_0) - D(\\boldsymbol y, \\boldsymbol{\\widehat \\pi}_1) = 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{\\widehat \\pi_{i1}}{\\widehat \\pi_{i0}} + m_i(1-y_i) \\log\\frac{1-\\widehat \\pi_{i1}}{1-\\widehat \\pi_{i0}}\\right).\n\\]\nNote that this expression holds for grouped or ungrouped logistic regression models. We can then construct a likelihood ratio test in the usual way. Likelihood ratio inference can be justified by either large-sample or small-dispersion asymptotics."
  },
  {
    "objectID": "logistic-regression.html#sec-goodness-of-fit",
    "href": "logistic-regression.html#sec-goodness-of-fit",
    "title": "24  Logistic regression",
    "section": "24.4 Goodness of fit testing",
    "text": "24.4 Goodness of fit testing\nIn grouped logistic regression, we can also use the likelihood ratio test to test goodness of fit. To do so, we compare the total deviance of the fitted model (24.2) to a chi-squared quantile. In particular, the deviance-based goodness of fit test rejects when:\n\\[\n\\begin{split}\n&D(\\boldsymbol{y}, \\hat{\\boldsymbol{\\pi}}) = \\\\\n&2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{y_i}{\\widehat \\pi_i} + m_i(1-y_i) \\log\\frac{1-y_i}{1-\\widehat \\pi_i}\\right) &gt; \\chi^2_{n-p}(1-\\alpha).\n\\end{split}\n\\tag{24.5}\\]\nThis test is justified by small-dispersion asymptotics based on the saddlepoint approximation, which is decent when \\(\\min(m_i \\pi_i, (1-m_i)\\pi_i) \\geq 3\\) for each \\(i\\)."
  },
  {
    "objectID": "logistic-regression.html#sec-example-2x2-table",
    "href": "logistic-regression.html#sec-example-2x2-table",
    "title": "24  Logistic regression",
    "section": "24.5 Example: \\(2 \\times 2\\) table",
    "text": "24.5 Example: \\(2 \\times 2\\) table\nLet us revisit the example of the \\(2 \\times 2\\) table model, within which we would like to test \\(H_0: \\beta_1 = 0\\). Note that we can view this as a goodness of fit test of the intercept-only model in a grouped logistic regression model since the alternative model is the saturated model (it has two observations and two parameters). To compute the likelihood ratio statistic, we first need to fit the intercept-only model. The score equations (24.1) reduce to:\n\\[\nm_0 (y_0 - \\hat \\pi) + m_1 (y_1 - \\hat \\pi) = 0 \\quad \\Longrightarrow \\quad \\hat \\pi_0 = \\hat \\pi_1 = \\hat \\pi = \\frac{m_0 y_0 + m_1 y_1}{m_0 + m_1}.\n\\]\nTherefore, the deviance-based test of \\(H_0: \\beta_1 = 0\\) rejects when:\n\\[\n\\begin{split}\nD(\\boldsymbol{y}, \\boldsymbol{\\widehat \\pi}) &= 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{y_i}{\\widehat \\pi_i} + m_i(1-y_i) \\log\\frac{1-y_i}{1-\\widehat \\pi_i}\\right) \\\\\n&= \\left(m_0 y_0 \\log\\frac{y_0}{\\hat \\pi} + m_0(1-y_0)\\log\\frac{1-y_0}{1-\\hat \\pi}\\right) + \\\\\n&\\quad \\quad \\left(m_1 y_1 \\log\\frac{y_1}{\\hat \\pi} + m_1(1-y_1)\\log\\frac{1-y_1}{1-\\hat \\pi}\\right) \\\\\n&&gt; \\chi^2_{1}(1-\\alpha).\n\\end{split}\n\\]\nLikelihood ratio inference can give nontrivial conclusions in cases when Wald inference cannot, e.g. in the case of perfect separability. In the above example, suppose \\(y_0 = 0\\) and \\(y_1 = 1\\), giving perfect separability. Then, we can use the fact that \\(y \\log y \\rightarrow 0\\) and \\((1-y)\\log(1-y) \\rightarrow 0\\) as \\(y \\rightarrow 0\\) or \\(y \\rightarrow 1\\) to see that:\n\\[\n\\begin{split}\nD(\\boldsymbol{y}, \\boldsymbol{\\widehat \\pi}) &= 2\\left(m_0 \\log\\frac{1}{1-\\hat \\pi} + m_1 \\log\\frac{1}{\\hat \\pi}\\right) \\\\\n&= 2\\left(m_0 \\log \\frac{m_0 + m_1}{m_0} + m_1 \\log \\frac{m_0 + m_1}{m_1}\\right).\n\\end{split}\n\\tag{24.6}\\]\nThis gives us a finite value, which we can compare to \\(\\chi^2_{1}(1-\\alpha)\\) to test \\(H_0: \\beta_1 = 0\\). Even though the likelihood ratio statistic is still defined, we do still have to be careful because the data may suggest that the parameters are too close to the boundary of the parameter space. However, the rate at which the test breaks down as the parameters approach this boundary is slower than the rate at which the Wald test breaks down."
  },
  {
    "objectID": "logistic-regression.html#sec-score-based-inference",
    "href": "logistic-regression.html#sec-score-based-inference",
    "title": "24  Logistic regression",
    "section": "24.6 Score-based inference",
    "text": "24.6 Score-based inference\nHere we present only the score-based goodness-of-fit test. Recalling Section 22.4.4, the score statistic for goodness of fit is Pearson’s \\(X^2\\) statistic:\n\\[\nX^2 = \\sum_{i = 1}^n \\frac{w_i (y_i - \\widehat \\mu_i)^2}{V(\\widehat \\mu_i)} = \\sum_{i = 1}^n \\frac{m_i(y_i - \\widehat \\pi_i)^2}{\\widehat \\pi_i(1-\\widehat \\pi_i)}.\n\\tag{24.7}\\]\nThis test is justified by small-dispersion asymptotics based on the central limit theorem, which is decent when \\(\\min(m_i \\pi_i, (1-m_i)\\pi_i) \\geq 5\\) for each \\(i\\)."
  },
  {
    "objectID": "logistic-regression.html#sec-fisher-exact-test",
    "href": "logistic-regression.html#sec-fisher-exact-test",
    "title": "24  Logistic regression",
    "section": "24.7 Fisher’s exact test",
    "text": "24.7 Fisher’s exact test\nAs an alternative to asymptotic tests for logistic regression, in the case of \\(2 \\times 2\\) tables, there is an exact test of \\(H_0: \\beta_1 = 0\\). Suppose we have:\n\\[\ns_1 = m_1y_1 \\sim \\text{Bin}(m_1, \\pi_1) \\quad \\text{and} \\quad s_2 = m_2y_2 \\sim \\text{Bin}(m_2, \\pi_2).\n\\tag{24.8}\\]\nThe trick is to conduct inference conditional on \\(s_1 + s_2\\). Note that under \\(H_0: \\pi_1 = \\pi_2\\), we have:\n\\[\n\\begin{split}\n&\\mathbb{P}[s_1 = t | s_1+s_2 = v] \\\\\n&\\quad= \\mathbb{P}[s_1 = t | s_1 + s_2 = v] \\\\\n&\\quad= \\frac{\\mathbb{P}[s_1 = t, s_2 = v-t]}{\\mathbb{P}[s_1 + s_2 = v]} \\\\\n&\\quad= \\frac{{m_1 \\choose t}\\pi^{t}(1-\\pi)^{m_1 - t}{m_2 \\choose v-t}\\pi^{v-t}(1-\\pi)^{m_2 - (v-t)}}{{m_1 + m_2 \\choose v}\\pi^v (1-\\pi)^{m_1 + m_2 - v}} \\\\\n&\\quad= \\frac{{m_1 \\choose t}{m_2 \\choose v-t}}{{m_1 + m_2 \\choose v}}.\n\\end{split}\n\\tag{24.9}\\]\nTherefore, a finite-sample \\(p\\)-value to test \\(H_0: \\pi_1 = \\pi_2\\) versus \\(H_1: \\pi_1 &gt; \\pi_2\\) is \\(\\mathbb{P}[s_1 \\geq t | s_1 + s_2]\\), which can be computed exactly based on the formula above."
  },
  {
    "objectID": "poisson-regression.html#sec-model-definition",
    "href": "poisson-regression.html#sec-model-definition",
    "title": "25  Poisson regression",
    "section": "25.1 Model definition and interpretation",
    "text": "25.1 Model definition and interpretation\nThe Poisson regression model (with offsets) is:\n\\[\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\log \\mu_i = o_i + \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}.\n\\tag{25.1}\\]\nBecause the log of the mean is linear in the predictors, Poisson regression models are often called loglinear models. To interpret the coefficients, note that a unit increase in \\(x_j\\) (while keeping the other variables fixed) is associated with an increase in the predicted mean by a factor of \\(e^{\\beta_j}\\)."
  },
  {
    "objectID": "poisson-regression.html#sec-example-modeling-rates",
    "href": "poisson-regression.html#sec-example-modeling-rates",
    "title": "25  Poisson regression",
    "section": "25.2 Example: Modeling rates",
    "text": "25.2 Example: Modeling rates\nOne cool feature of the Poisson model is that rates can be easily modeled with the help of offsets. Let’s say that the count \\(y_i\\) is collected over the course of a time interval of length \\(t_i\\), or a spatial region with area \\(t_i\\), or a population of size \\(t_i\\), etc. Then, it is meaningful to model:\n\\[\ny_i \\overset{\\text{ind}} \\sim \\text{Poi}(\\pi_i t_i), \\quad \\log \\pi_i = \\boldsymbol{x}^T_{i*}\\boldsymbol{\\beta},\n\\]\nwhere \\(\\pi_i\\) represents the rate of events per day / per square mile / per capita, etc. In other words:\n\\[\ny_i \\overset{\\text{ind}} \\sim \\text{Poi}(\\mu_i), \\quad \\log \\mu_i = \\log t_i + \\boldsymbol{x}^T_{i*}\\boldsymbol{\\beta},\n\\]\nwhich is exactly equation (25.1) with offsets \\(o_i = \\log t_i\\). For example, in single-cell RNA-sequencing, \\(y_i\\) is the number of reads aligning to a gene in cell \\(i\\) and \\(t_i\\) is the total number of reads measured in the cell, a quantity called the sequencing depth. We might use a Poisson regression model to carry out a differential expression analysis between two cell types."
  },
  {
    "objectID": "poisson-regression.html#sec-estimation-inference",
    "href": "poisson-regression.html#sec-estimation-inference",
    "title": "25  Poisson regression",
    "section": "25.3 Estimation and inference",
    "text": "25.3 Estimation and inference\n\n25.3.1 Score, Fisher information, and Wald inference\nWe found in Chapter @ch-glm-theory that the score and Fisher information for Poisson regression are:\n\\[\n\\boldsymbol{U}(\\boldsymbol{ \\beta}) = \\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{\\mu}),\n\\]\nand:\n\\[\n\\boldsymbol{I}(\\boldsymbol{\\beta}) = \\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X} = \\boldsymbol{X}^T \\text{diag}(V(\\mu_i))\\boldsymbol{X} = \\boldsymbol{X}^T \\text{diag}(\\mu_i)\\boldsymbol{X},\n\\]\nrespectively. Hence, the normal equations for the MLE are:\n\\[\n\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{\\hat \\mu}).\n\\]\nWald inference is based on the approximation:\n\\[\n\\boldsymbol{\\hat \\beta} \\overset \\cdot \\sim N(\\boldsymbol{\\beta}, (\\boldsymbol{X}^T \\text{diag}(\\hat \\mu_i)\\boldsymbol{X})^{-1}).\n\\]\n\n\n25.3.2 Likelihood ratio inference\nFor likelihood ratio inference, we first derive the total deviance. The unit deviance of a Poisson distribution is:\n\\[\nd(y, \\mu) = y \\log \\frac{y}{\\mu} - (y - \\mu).\n\\]\nHence, the total deviance is:\n\\[\nD(\\boldsymbol{y}, \\boldsymbol{\\mu}) = \\sum_{i = 1}^n d(y_i, \\mu_i) = \\sum_{i = 1}^n \\left(y_i \\log \\frac{y_i}{\\mu_i} - (y_i - \\mu_i)\\right).\n\\]\nThe residual deviance is then:\n\\[\nD(\\boldsymbol{y}, \\boldsymbol{\\hat\\mu}) = \\sum_{i = 1}^n \\left(y_i \\log \\frac{y_i}{\\hat \\mu_i} - (y_i - \\hat \\mu_i)\\right) = \\sum_{i = 1}^n y_i \\log \\frac{y_i}{\\hat \\mu_i}.\n\\]\nThe last equality holds for any model containing the intercept, since by the normal equations we have \\(\\sum_{i = 1}^n (y_i - \\hat \\mu_i) = \\boldsymbol{1}^T (\\boldsymbol{y} - \\boldsymbol{\\hat \\mu}) = 0\\). We can carry out a likelihood ratio test for \\(H_0: \\boldsymbol{\\beta_S} = \\boldsymbol{0}\\) via:\n\\[\nD(\\boldsymbol{y}, \\boldsymbol{\\hat \\mu^0}) - D(\\boldsymbol{y}, \\boldsymbol{\\hat \\mu}) = \\sum_{i = 1}^n y_i \\log \\frac{\\hat \\mu_i}{\\hat \\mu^0_{i}} \\overset{\\cdot}\\sim \\chi^2_{|S|}.\n\\]\nWe can carry out a goodness-of-fit test via:\n\\[\nD(\\boldsymbol{y}, \\boldsymbol{\\hat\\mu}) = \\sum_{i = 1}^n y_i \\log \\frac{y_i}{\\hat \\mu_i} \\overset{\\cdot}\\sim \\chi^2_{n - p}.\n\\]\n\n\n25.3.3 Score-based inference\nRecalling equation (22.5), the score test for \\(H_0: \\beta_j = 0\\) is based on:\n\\[\n\\frac{\\boldsymbol{x}_{j*}^T (\\boldsymbol{y} - \\boldsymbol{\\widehat \\mu}^0)}{\\sqrt{\\left([(\\boldsymbol{X}^T \\text{diag}(\\hat \\mu^0_{i})\\boldsymbol{X})^{-1}]_{jj}\\right)^{-1}}} \\overset{\\cdot}\\sim N(0, 1).\n\\]\nOn the other hand, the score test for goodness-of-fit is based on the Pearson \\(X^2\\) statistic:\n\\[\nX^2 \\equiv \\sum_{i = 1}^n \\frac{(y_i - \\hat \\mu_i)^2}{\\hat \\mu_i} \\overset{\\cdot}\\sim \\chi^2_{n - p}.\n\\]"
  },
  {
    "objectID": "poisson-regression.html#sec-poisson-multinomial",
    "href": "poisson-regression.html#sec-poisson-multinomial",
    "title": "25  Poisson regression",
    "section": "25.4 Relationship between Poisson and multinomial distributions",
    "text": "25.4 Relationship between Poisson and multinomial distributions\nSuppose that \\(y_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i)\\) for \\(i = 1, \\dots, n\\). Then,\n\\[\n\\begin{split}\n\\mathbb{P}\\left[y_1 = m_1, \\dots, y_n = m_n \\left| \\sum_{i}y_i = m\\right.\\right] &= \\frac{\\mathbb{P}[y_1 = m_1, \\dots, y_n = m_n]}{\\mathbb{P}[\\sum_{i}y_i = m]} \\\\\n&= \\frac{\\prod_{i = 1}^n e^{-\\mu_i}\\frac{\\mu_i^{m_i}}{m_i!}}{e^{-\\sum_i \\mu_i}\\frac{(\\sum_i \\mu_i)^m}{m!}} \\\\\n&= {m \\choose m_1, \\dots, m_n} \\prod_{i = 1}^n \\pi_i^{m_i}; \\quad \\pi_i \\equiv \\frac{\\mu_i}{\\sum_{i' = 1}^n \\mu_{i'}}.\n\\end{split}\n\\]\nWe recognize the last expression as the probability mass function of the multinomial distribution with parameters \\((\\pi_1, \\dots, \\pi_n)\\) summing to one. In words, the joint distribution of a set of independent Poisson distributions conditional on their sum is a multinomial distribution."
  },
  {
    "objectID": "poisson-regression.html#sec-example-2x2-tables",
    "href": "poisson-regression.html#sec-example-2x2-tables",
    "title": "25  Poisson regression",
    "section": "25.5 Example: \\(2 \\times 2\\) contingency tables",
    "text": "25.5 Example: \\(2 \\times 2\\) contingency tables\n\n25.5.1 Poisson model for \\(2 \\times 2\\) contingency tables\nLet’s say that we have two binary random variables \\(x_1, x_2 \\in \\{0,1\\}\\) with joint distribution \\(\\mathbb{P}(x_1 = j, x_2 = k) = \\pi_{jk}\\) for \\(j,k \\in \\{0,1\\}\\). We collect a total of \\(n\\) samples from this joint distribution and summarize the counts in a \\(2 \\times 2\\) table, where \\(y_{jk}\\) is the number of times we observed \\((x_1, x_2) = (j,k)\\), so that:\n\\[\n(y_{00}, y_{01}, y_{10}, y_{11})|n \\sim \\text{Mult}(n, (\\pi_{00}, \\pi_{01}, \\pi_{10}, \\pi_{11})).\n\\]\nOur primary question is whether these two random variables are independent, i.e.\n\\[\n\\pi_{jk} = \\pi_{j+}\\pi_{+k}, \\quad \\text{where} \\quad \\pi_{j+} \\equiv \\mathbb{P}[x_1 = j] = \\pi_{j1} + \\pi_{j2}; \\quad \\pi_{+k} \\equiv \\mathbb{P}[x_2 = k] = \\pi_{1k} + \\pi_{2k}.\n\\tag{25.2}\\]\nWe can express this equivalently as:\n\\[\n\\pi_{00}\\pi_{11} = \\pi_{01}\\pi_{10}.\n\\]\nIn other words, we can express the independence hypothesis concisely as:\n\\[\nH_0: \\frac{\\pi_{11}\\pi_{00}}{\\pi_{10}\\pi_{01}} = 1.\n\\tag{25.3}\\]\nLet’s arbitrarily assume that, additionally, \\(n \\sim \\text{Poi}(\\mu_{++})\\). Then, by the relationship between Poisson and multinomial distributions, we have:\n\\[\ny_{jk} \\overset{\\text{ind}} \\sim \\text{Poi}(\\mu_{++}\\pi_{jk}).\n\\]\nLet \\(i \\in \\{1,2,3,4\\}\\) index the four pairs \\((x_1, x_2) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}\\), so that:\n\\[\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_{12}x_{i1} x_{i2}, \\quad i = 1, \\dots, 4,\n\\tag{25.4}\\]\nwhere:\n\\[\n\\beta_0 = \\log \\mu_{++} + \\log \\pi_{00}; \\quad \\beta_1 = \\log \\frac{\\pi_{10}}{\\pi_{00}}; \\quad \\beta_2 = \\log \\frac{\\pi_{01}}{\\pi_{00}}; \\quad \\beta_{12} = \\log\\frac{\\pi_{11}\\pi_{00}}{\\pi_{10}\\pi_{01}}.\n\\tag{25.5}\\]\nNote that the independence hypothesis (25.3) reduces to the hypothesis \\(H_0: \\beta_{12} = 0\\):\n\\[\nH_0: \\frac{\\pi_{11}\\pi_{00}}{\\pi_{10}\\pi_{01}} = 1 \\quad \\Longleftrightarrow \\quad H_0: \\beta_{12} = 0.\n\\]\nSo the presence of an interaction in the Poisson regression is equivalent to a lack of independence between \\(x_1\\) and \\(x_2\\). We can test the latter hypothesis using our standard tools for Poisson regression.\nFor example, we can use the Pearson \\(X^2\\) goodness-of-fit test. To apply this test, we must find the fitted means under the null hypothesis model:\n\\[\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}, \\quad i = 1, \\dots, 4.\n\\tag{25.6}\\]\nThe normal equations give us the following:\n\\[\ny_{++} \\equiv \\sum_{j, k = 0}^1 y_{jk} = \\sum_{j, k = 0}^1 \\hat \\mu_{jk} \\equiv \\hat \\mu_{++}; \\quad y_{+1} \\equiv \\sum_{j = 0}^1 y_{j1} = \\sum_{j = 0}^1 \\hat \\mu_{j1} \\equiv \\hat \\mu_{+1}; \\quad y_{1+} \\equiv \\sum_{k = 0}^1 y_{1k} = \\sum_{k = 0}^1 \\hat \\mu_{1k} \\equiv \\hat \\mu_{1+}.\n\\]\nBy combining these equations, we arrive at:\n\\[\n\\hat \\mu_{++} = y_{++}; \\quad \\hat \\mu_{j+} = y_{j+} \\text{ for all } j \\in \\{0, 1\\}; \\quad \\hat \\mu_{+k} = y_{+k} \\text{ for all } k \\in \\{0, 1\\}.\n\\]\nTherefore, the fitted means under the null hypothesis model 25.6 are:\n\\[\n\\hat \\mu_{jk} = \\hat \\mu_{++}\\hat \\pi_{jk} = \\hat \\mu_{++}\\hat \\pi_{j+}\\hat \\pi_{+k} = y_{++}\\frac{y_{j+}}{y_{++}}\\frac{y_{+k}}{y_{++}} = \\frac{y_{j+}y_{+k}}{y_{++}}.\n\\]\nHence, we have:\n\\[\nX^2 = \\sum_{j,k = 0}^1 \\frac{(y_{jk} - \\widehat \\mu_{jk})^2}{\\widehat \\mu_{jk}} = \\sum_{j, k = 0}^1 \\frac{(y_{jk} - y_{j+}y_{+k}/y_{++})^2}{y_{j+}y_{+k}/y_{++}}.\n\\]\nAlternatively, we can use the likelihood ratio test, which gives:\n\\[\nG^2 = 2\\sum_{j,k = 0}^1 y_{jk}\\log\\frac{y_{jk}}{\\widehat \\mu_{jk}} = 2\\sum_{j, k = 0}^1 y_{jk}\\log\\frac{y_{jk}}{y_{j+}y_{+k}/y_{++}}.\n\\]\nWe would compare both \\(X^2\\) and \\(G^2\\) to a \\(\\chi^2_1\\) distribution.\n\n\n25.5.2 Inference is the same regardless of conditioning on margins\nNow, our data might actually have been collected such that \\(n \\sim \\text{Poi}(\\mu_{++})\\), or maybe \\(n\\) was fixed in advance. Is the Poisson inference proposed above actually valid in the latter case? In fact, it is! To see this, let us consider the log likelihoods of the two models:\n\\[\np_{\\boldsymbol{\\mu}}(\\boldsymbol{y}) = p_{\\mu_{++}}(y_{++} = n)p_{\\boldsymbol{\\pi}}(\\boldsymbol{y} | y_{++} = n),\n\\]\nso:\n\\[\n\\log p_{\\boldsymbol{\\mu}}(\\boldsymbol{y}) = \\log p_{\\mu_{++}}(y_{++} = n) + \\log p_{\\boldsymbol{\\pi}}(\\boldsymbol{y} | y_{++} = n) = C + \\log p_{\\boldsymbol{\\pi}}(\\boldsymbol{y} | y_{++} = n).\n\\]\nIn other words, the log-likelihoods of the Poisson and multinomial models, as a function of \\(\\boldsymbol{\\pi}\\), differ from each other by a constant. Therefore, any likelihood-based inference in these models is equivalent. The same argument shows that conditioning on the row or column totals (as opposed to the overall total) also yields the same exact inference. Therefore, regardless of the sampling mechanism, we can always conduct an independence test in a \\(2 \\times 2\\) table via a Poisson regression.\n\n\n25.5.3 Equivalence among Poisson and logistic regressions\nWe’ve talked about two ways to view a \\(2 \\times 2\\) contingency table. In the logistic regression view, we thought about one variable as a predictor and the other as a response, seeking to test whether the predictor has an impact on the response. In the Poisson regression view, we thought about the two variables symmetrically, seeking to test independence. It turns out that these two perspectives are equivalent. Recall that we have derived in equations 25.4 and 25.5 that \\(x_1 \\perp \\!\\!\\! \\perp x_2\\) if and only if \\(\\beta_{12} = 0\\) in the Poisson regression:\n\\[\n\\log y_i \\overset{\\text{ind}}{\\sim} \\text{Poi}(\\mu_i), \\quad \\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_{12} x_{i1}x_{i2}, \\quad i = 1, \\dots, 4.\n\\]\nHowever, we have:\n\\[\n\\beta_{12} = \\log \\frac{\\pi_{11}\\pi_{00}}{\\pi_{01}\\pi_{10}} = \\log \\frac{\\pi_{11}/\\pi_{01}}{\\pi_{01}/\\pi_{00}} = \\log \\frac{\\mathbb{P}[x_2 = 1 \\mid x_1 = 1] / \\mathbb{P}[x_2 = 0 \\mid x_1 = 1]}{\\mathbb{P}[x_2 = 1 \\mid x_1 = 0] / \\mathbb{P}[x_2 = 0 \\mid x_1 = 0]}.\n\\]\nRecalling the logistic regression of \\(x_2\\) on \\(x_1\\):\n\\[\n\\text{logit } \\mathbb{P}[x_2 = 1 \\mid x_1] = \\tilde{\\beta_0} + \\tilde{\\beta_1} x_1,\n\\tag{25.7}\\]\nand that \\(\\tilde{\\beta_1}\\) is the log odds ratio, we conclude that:\n\\[\n\\beta_{12} = \\tilde{\\beta_1},\n\\]\nso \\(x_1 \\perp \\!\\!\\! \\perp x_2\\) if and only if \\(\\tilde{\\beta_1} = 0\\). Due to the equivalence between Poisson and multinomial distributions, the hypothesis tests and confidence intervals for the log odds ratio \\(\\beta_{12}\\) (or \\(\\tilde{\\beta_1}\\)) obtained from Poisson and logistic regressions will be the same."
  },
  {
    "objectID": "poisson-regression.html#sec-poisson-jk-tables",
    "href": "poisson-regression.html#sec-poisson-jk-tables",
    "title": "25  Poisson regression",
    "section": "25.6 Example: Poisson models for \\(J \\times K\\) contingency tables",
    "text": "25.6 Example: Poisson models for \\(J \\times K\\) contingency tables\nSuppose now that \\(x_1 \\in \\{1, \\dots, J\\}\\) and \\(x_2 \\in \\{1, \\dots, K\\}\\). Then, we denote \\(\\mathbb{P}[x_1 = j, x_2 = k] = \\pi_{jk}\\). We still are interested in testing for independence between \\(j\\) and \\(k\\), which amounts to a goodness-of-fit test for the Poisson model:\n\\[\ny_{jk} \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_{jk}); \\quad \\log \\mu_{jk} = \\beta_0 + \\beta^1_j + \\beta^2_k.\n\\]\nThe score (Pearson) and deviance-based goodness-of-fit statistics for this test are:\n\\[\nX^2 = \\sum_{j = 1}^J \\sum_{k = 1}^K \\frac{(y_{jk} - \\widehat \\mu_{jk})^2}{\\widehat \\mu_{jk}} \\quad \\text{and} \\quad G^2 = 2\\sum_{j = 1}^J \\sum_{k = 1}^K y_{jk}\\log \\frac{y_{jk}}{\\widehat \\mu_{jk}},\n\\]\nwhere \\(\\widehat \\mu_{jk} = \\widehat y_{++}\\frac{y_{j+}}{y_{++}}\\frac{y_{+k}}{y_{++}}\\). Like with the \\(2 \\times 2\\) case, the test is the same regardless of whether we condition on the row sums, column sums, total count, or if we do not condition at all. The degrees of freedom in the full model is \\(JK\\), while the degrees of freedom in the partial model is \\(J + K - 1\\), so the degrees of freedom for the goodness-of-fit test is \\(JK - J - K + 1 = (J - 1)(K - 1)\\). Pearson erroneously concluded that the test had \\(JK - 1\\) degrees of freedom, which, when Fisher corrected it, created a lot of animosity between these two statisticians."
  },
  {
    "objectID": "poisson-regression.html#sec-poisson-jkl-tables",
    "href": "poisson-regression.html#sec-poisson-jkl-tables",
    "title": "25  Poisson regression",
    "section": "25.7 Example: Poisson models for \\(J \\times K \\times L\\) contingency tables",
    "text": "25.7 Example: Poisson models for \\(J \\times K \\times L\\) contingency tables\nThese ideas can be extended to multi-way tables, for example, three-way tables. If we have \\(x_1 \\in \\{1, \\dots, J\\}\\), \\(x_2 \\in \\{1, \\dots, K\\}\\), \\(x_3 \\in \\{1, \\dots, L\\}\\), then we might be interested in testing several kinds of null hypotheses:\n\nMutual independence: \\(H_0: x_1 \\perp \\!\\!\\! \\perp x_2 \\perp \\!\\!\\! \\perp x_3\\).\nJoint independence: \\(H_0: x_1 \\perp \\!\\!\\! \\perp (x_2, x_3)\\).\nConditional independence: \\(H_0: x_1 \\perp \\!\\!\\! \\perp x_2 \\mid x_3\\).\n\nThese three null hypotheses can be shown to be equivalent to the Poisson regression model:\n\\[\ny_{jkl} \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_{jkl}),\n\\]\nwhere:\n\\[\n\\log \\mu_{jkl} = \\beta_0 + \\beta^1_j + \\beta^2_k + \\beta^3_l \\quad \\text{(mutual independence)};\n\\]\n\\[\n\\log \\mu_{jkl} = \\beta_0 + \\beta^1_j + \\beta^2_k + \\beta^3_l + \\beta^{2,3}_{kl} \\quad \\text{(joint independence)};\n\\]\n\\[\n\\log \\mu_{jkl} = \\beta_0 + \\beta^1_j + \\beta^2_k + \\beta^3_l + \\beta^{1,3}_{jl} + \\beta^{2,3}_l \\quad \\text{(conditional independence)}.\n\\]"
  },
  {
    "objectID": "negative-binomial-regression.html#overdispersion",
    "href": "negative-binomial-regression.html#overdispersion",
    "title": "26  Negative binomial regression",
    "section": "26.1 Overdispersion",
    "text": "26.1 Overdispersion\nA pervasive issue with Poisson regression is overdispersion: that the variances of observations are greater than the corresponding means. A common cause of overdispersion is omitted variable bias. Suppose that \\(y \\sim \\text{Poi}(\\mu)\\), where \\(\\log \\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). However, we omitted variable \\(x_2\\) and are considering the GLM based on \\(\\log \\mu = \\beta_0 + \\beta_1 x_1\\). If \\(\\beta_2 \\neq 0\\) and \\(x_2\\) is correlated with \\(x_1\\), then we have a confounding issue. Let’s consider the more benign situation that \\(x_2\\) is independent of \\(x_1\\). Then, we have\n\\[\n\\mathbb{E}[y|x_1] = \\mathbb{E}[\\mathbb{E}[y|x_1, x_2]|x_1] = \\mathbb{E}[e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}|x_1] = e^{\\beta_0 + \\beta_1 x_1}\\mathbb{E}[e^{\\beta_2 x_2}] = e^{\\beta'_0 + \\beta_1 x_1}.\n\\tag{26.1}\\]\nSo in the model for the mean of \\(y\\), the impact of omitted variable \\(x_2\\) seems only to have impacted the intercept. Let’s consider the variance of \\(y\\):\n\\[\n\\text{Var}[y|x_1] = \\mathbb{E}[\\text{Var}[y|x_1, x_2]|x_1] + \\text{Var}[\\mathbb{E}[y|x_1, x_2]|x_1] = e^{\\beta'_0 + \\beta_1 x_1} + e^{2(\\beta'_0 + \\beta_1 x_1)}\\text{Var}[e^{\\beta_2 x_2}] &gt; e^{\\beta'_0 + \\beta_1 x_1} = \\mathbb{E}[y|x_1].\n\\tag{26.2}\\]\nSo indeed, the variance is larger than what we would have expected under the Poisson model."
  },
  {
    "objectID": "negative-binomial-regression.html#hierarchical-poisson-regression",
    "href": "negative-binomial-regression.html#hierarchical-poisson-regression",
    "title": "26  Negative binomial regression",
    "section": "26.2 Hierarchical Poisson regression",
    "text": "26.2 Hierarchical Poisson regression\nLet’s say that \\(y|\\boldsymbol{x} \\sim \\text{Poi}(\\lambda)\\), where \\(\\lambda|\\boldsymbol{x}\\) is random due to the fluctuations of the omitted variables. A common distribution used to model nonnegative random variables is the gamma distribution \\(\\Gamma(\\mu, k)\\), parameterized by a mean \\(\\mu &gt; 0\\) and a shape \\(k &gt; 0\\). This distribution has probability density function\n\\[\nf(\\lambda; k, \\mu) = \\frac{(k/\\mu)^k}{\\Gamma(k)}e^{-k\\lambda/\\mu}\\lambda^{k-1},\n\\tag{26.3}\\]\nwith mean and variance given by\n\\[\n\\mathbb{E}[\\lambda] = \\mu; \\quad \\text{Var}[\\lambda] = \\mu^2/k.\n\\tag{26.4}\\]\nTherefore, it makes sense to augment the Poisson regression model as follows:\n\\[\n\\lambda|\\boldsymbol{x} \\sim \\Gamma(\\mu, k), \\quad \\log \\mu = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\quad y | \\lambda \\sim \\text{Poi}(\\lambda).\n\\tag{26.5}\\]"
  },
  {
    "objectID": "negative-binomial-regression.html#negative-binomial-distribution",
    "href": "negative-binomial-regression.html#negative-binomial-distribution",
    "title": "26  Negative binomial regression",
    "section": "26.3 Negative binomial distribution",
    "text": "26.3 Negative binomial distribution\nA simpler way to write the hierarchical model (26.5) would be to marginalize out \\(\\lambda\\). Doing so leaves us with a count distribution called the negative binomial distribution:\n\\[\n\\lambda \\sim \\Gamma(\\mu, k),\\  y | \\lambda \\sim \\text{Poi}(\\lambda) \\quad \\Longrightarrow \\quad y \\sim \\text{NegBin}(\\mu, k).\n\\tag{26.6}\\]\nThe negative binomial probability mass function is\n\\[\np(y; \\mu, k) = \\int_0^\\infty \\frac{(k/\\mu)^k}{\\Gamma(k)}e^{-k\\lambda/\\mu}\\lambda^{k-1}e^{-\\lambda}\\frac{\\lambda^y}{y!}d\\lambda = \\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left(\\frac{\\mu}{\\mu + k}\\right)^{y}\\left(\\frac{k}{\\mu + k}\\right)^{k}.\n\\tag{26.7}\\]\nThis random variable has mean and variance given by\n\\[\n\\mathbb{E}[y] = \\mathbb{E}[\\lambda] = \\mu \\quad \\text{and} \\quad \\text{Var}[y] = \\mathbb{E}[\\lambda] + \\text{Var}[\\lambda] = \\mu + \\frac{\\mu^2}{k}.\n\\tag{26.8}\\]\nAs we send \\(k \\rightarrow \\infty\\), the distribution of \\(\\lambda\\) tends to a point mass and the negative binomial distribution tends to \\(\\text{Poi}(\\mu)\\)."
  },
  {
    "objectID": "negative-binomial-regression.html#negative-binomial-as-exponential-dispersion-model",
    "href": "negative-binomial-regression.html#negative-binomial-as-exponential-dispersion-model",
    "title": "26  Negative binomial regression",
    "section": "26.4 Negative binomial as exponential dispersion model",
    "text": "26.4 Negative binomial as exponential dispersion model\nLet us see whether we can express the negative binomial model as an exponential dispersion model. First, let us write out the probability mass function:\n\\[\np(y; \\mu, k) = \\exp\\left(y \\log \\frac{\\mu}{\\mu + k} - k \\log \\frac{\\mu + k}{k}\\right)\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}.\n\\tag{26.9}\\]\nUnfortunately, we run into difficulties expressing this probability mass function in EDM form, because there is not a neat decoupling between the natural parameter and the dispersion parameter. Indeed, for unknown \\(k\\), the negative binomial model is not an EDM. However, we can still express the negative binomial model as an EDM (in fact, a one-parameter exponential family) if we treat \\(k\\) as known. In particular, we can read off that\n\\[\n\\theta = \\log \\frac{\\mu}{\\mu + k}, \\quad \\psi(\\theta) = k\\log \\frac{\\mu + k}{k} = -k\\log(1-e^{\\theta}).\n\\tag{26.10}\\]\nAn alternate parameterization of the negative binomial model is via \\(\\gamma = 1/k\\). With this parameterization, we have\n\\[\n\\mathbb{E}[y] = \\mu \\quad \\text{and} \\quad \\text{Var}[y] = \\mu + \\gamma \\mu^2.\n\\tag{26.11}\\]\nHere, \\(\\gamma\\) acts as a kind of dispersion parameter, as the variance of \\(y\\) grows with \\(\\gamma\\). Note that the relationship between \\(\\text{Var}[y]\\) and \\(\\gamma\\) is not exactly proportional, as it is in EDMs. Nevertheless, the \\(\\gamma\\) parameter is often called the negative binomial dispersion. Note that setting \\(\\gamma = 0\\) recovers the Poisson distribution."
  },
  {
    "objectID": "negative-binomial-regression.html#negative-binomial-regression",
    "href": "negative-binomial-regression.html#negative-binomial-regression",
    "title": "26  Negative binomial regression",
    "section": "26.5 Negative binomial regression",
    "text": "26.5 Negative binomial regression\nLet’s revisit the hierarchical model 26.5, writing it more succinctly in terms of the negative binomial distribution:\n\\[\ny_i \\overset{\\text{ind}}{\\sim} \\text{NegBin}(\\mu_i, \\gamma), \\quad \\log \\mu_i = \\boldsymbol{x}^T \\boldsymbol{\\beta}.\n\\tag{26.12}\\]\nNotice that we typically assume that all observations share the same dispersion parameter \\(\\gamma\\). Reading off from equation (26.10), we see that the canonical link function for the negative binomial distribution is \\(\\mu \\mapsto \\log \\frac{\\mu}{\\mu + k}\\). However, typically for negative binomial regression we use the log link \\(g(\\mu) = \\log \\mu\\) instead. This is the link of Poisson regression, and leads to more interpretable coefficient estimates. This is our first example of a non-canonical link!"
  },
  {
    "objectID": "negative-binomial-regression.html#score-and-fisher-information",
    "href": "negative-binomial-regression.html#score-and-fisher-information",
    "title": "26  Negative binomial regression",
    "section": "26.6 Score and Fisher information",
    "text": "26.6 Score and Fisher information\nRecall from Chapter 4 that\n\\[\n\\boldsymbol{U}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} (\\boldsymbol{y} - \\boldsymbol{\\mu}) \\quad \\text{and} \\quad \\boldsymbol{I}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X},\n\\tag{26.13}\\]\nwhere\n\\[\n\\boldsymbol{W} \\equiv \\text{diag}\\left(\\frac{w_i}{V(\\mu_i)(d\\eta_i/d\\mu_i)^2}\\right) \\quad \\text{and} \\quad \\boldsymbol{M} \\equiv \\text{diag}\\left(\\frac{\\partial\\eta_i}{\\partial \\mu_i}\\right).\n\\tag{26.14}\\]\nIn our case, we have\n\\[\nw_i = 1; \\quad V(\\mu_i) = \\mu_i + \\gamma \\mu_i^2; \\quad \\frac{\\partial\\eta_i}{\\partial \\mu_i} = \\frac{1}{\\mu_i}.\n\\tag{26.15}\\]\nPutting this together, we find that\n\\[\n\\boldsymbol{W} = \\text{diag}\\left(\\frac{\\mu_i}{1 + \\gamma \\mu_i}\\right); \\quad \\boldsymbol{M} = \\text{diag}\\left(\\frac{1}{1 + \\gamma \\mu_i}\\right).\n\\tag{26.16}\\]"
  },
  {
    "objectID": "negative-binomial-regression.html#estimation-in-negative-binomial-regression",
    "href": "negative-binomial-regression.html#estimation-in-negative-binomial-regression",
    "title": "26  Negative binomial regression",
    "section": "26.7 Estimation in negative binomial regression",
    "text": "26.7 Estimation in negative binomial regression\nNegative binomial regression is an EDM when \\(\\gamma\\) is known, but typically the dispersion parameter is unknown. Note that there is a dependency in \\(\\psi\\) on \\(k\\) (i.e. on \\(\\gamma\\)), which complicates things. It means that the estimate \\(\\boldsymbol{\\widehat{\\beta}}\\) depends on the parameter \\(\\gamma\\) (this does not happen, for example, in the normal linear model case).1 Therefore, estimation in negative binomial regression is typically an iterative procedure, where at each step \\(\\boldsymbol{\\beta}\\) is estimated for the current value of \\(\\gamma\\) and then \\(\\gamma\\) is estimated based on the updated value of \\(\\boldsymbol{\\beta}\\). Let’s discuss each of these tasks in turn. Given a value of \\(\\widehat{\\gamma}\\), we have the normal equations:\n\\[\n\\boldsymbol{X}^T \\text{diag}\\left(\\frac{1}{1 + \\widehat{\\gamma} \\widehat{\\mu}_i}\\right)(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}) = 0.\n\\tag{26.17}\\]\nThis reduces to the Poisson normal equations when \\(\\widehat{\\gamma} = 0\\). Solving these equations for a fixed value of \\(\\widehat{\\gamma}\\) can be done via IRLS, as usual. Estimating \\(\\gamma\\) for a fixed value of \\(\\boldsymbol{\\widehat{\\beta}}\\) can be done in several ways, including setting to zero the derivative of the likelihood with respect to \\(\\gamma\\). This results in a nonlinear equation (not given here) that can be solved iteratively."
  },
  {
    "objectID": "negative-binomial-regression.html#wald-inference",
    "href": "negative-binomial-regression.html#wald-inference",
    "title": "26  Negative binomial regression",
    "section": "26.8 Wald inference",
    "text": "26.8 Wald inference\nWald inference is based on\n\\[\n\\widehat{\\text{Var}}[\\boldsymbol{\\widehat{\\beta}}] = (\\boldsymbol{X}^T \\boldsymbol{\\widehat{W}} \\boldsymbol{X})^{-1}, \\quad \\text{where} \\quad \\boldsymbol{\\widehat{W}} = \\text{diag}\\left(\\frac{\\widehat{\\mu}_i}{1 + \\widehat{\\gamma} \\widehat{\\mu}_i}\\right).\n\\tag{26.18}\\]"
  },
  {
    "objectID": "negative-binomial-regression.html#likelihood-ratio-test-inference",
    "href": "negative-binomial-regression.html#likelihood-ratio-test-inference",
    "title": "26  Negative binomial regression",
    "section": "26.9 Likelihood ratio test inference",
    "text": "26.9 Likelihood ratio test inference\nThe negative binomial deviance is\n\\[\nD(\\boldsymbol{y}; \\boldsymbol{\\widehat{\\mu}}) = 2\\sum_{i = 1}^n \\left(y_i \\log \\frac{y_i}{\\widehat{\\mu}_i} - \\left(y_i + \\frac{1}{\\widehat{\\gamma}}\\right)\\log \\frac{1 + \\widehat{\\gamma} y_i}{1 + \\widehat{\\gamma} \\widehat{\\mu}_i}\\right).\n\\tag{26.19}\\]\nWe can use this for comparing nested models, but not for goodness of fit testing! The issue is that we have estimated the parameter \\(\\gamma\\), whereas goodness of fit tests are applicable only when the dispersion parameter is known."
  },
  {
    "objectID": "negative-binomial-regression.html#testing-for-overdispersion",
    "href": "negative-binomial-regression.html#testing-for-overdispersion",
    "title": "26  Negative binomial regression",
    "section": "26.10 Testing for overdispersion",
    "text": "26.10 Testing for overdispersion\nIt is reasonable to want to test for overdispersion, i.e., to test the null hypothesis \\(H_0: \\gamma = 0\\). This is somewhat of a tricky task because \\(\\gamma = 0\\) is at the edge of the parameter space. We can do so using a likelihood ratio test. As it turns out, the likelihood ratio statistic \\(T^{\\text{LRT}}\\) has asymptotic null distribution\n\\[\nT^{\\text{LRT}} \\equiv 2(\\ell^{\\text{NB}} - \\ell^{\\text{Poi}}) \\overset \\cdot \\sim \\frac{1}{2}\\delta_0 + \\frac{1}{2}\\chi^2_1.\n\\tag{26.20}\\]\nHere, \\(\\delta_0\\) is the delta mass at zero. The reason for this is that, under the null, we can view the estimated dispersion parameter as being symmetrically distributed around 0. However, since the dispersion parameter is nonnegative, this means it gets rounded up to 0 with probability 1/2. Therefore, the likelihood ratio test for \\(H_0: \\gamma = 0\\) rejects when\n\\[\nT^{\\text{LRT}} &gt; \\chi^2_1(1-2\\alpha).\n\\tag{26.21}\\]\nNote that the above test for overdispersion can be viewed as a goodness of fit test for the Poisson GLM. It is different from the usual GLM goodness of fit tests, because the saturated model against which the latter tests stays in the Poisson family. Nevertheless, significant results in standard goodness of fit tests for Poisson GLMs are often an indication of overdispersion. Or, they may indicate omitted variable bias (e.g., you forgot to include an interaction), so it’s somewhat tricky."
  },
  {
    "objectID": "negative-binomial-regression.html#overdispersion-in-logistic-regression",
    "href": "negative-binomial-regression.html#overdispersion-in-logistic-regression",
    "title": "26  Negative binomial regression",
    "section": "26.11 Overdispersion in logistic regression",
    "text": "26.11 Overdispersion in logistic regression\nNote that overdispersion is potentially an issue not only in Poisson regression models but in logistic regression models as well. Dealing with overdispersion in the latter case is more tricky, because the analog of the negative binomial model (the beta-binomial model) is not an exponential family. An alternate route to dealing with overdispersion is quasi-likelihood modeling, but this topic is beyond the scope of the course."
  },
  {
    "objectID": "negative-binomial-regression.html#footnotes",
    "href": "negative-binomial-regression.html#footnotes",
    "title": "26  Negative binomial regression",
    "section": "",
    "text": "Having said that, the dependency between \\(\\boldsymbol{\\widehat{\\beta}}\\) and \\(\\widehat{\\gamma}\\) is weak, as the two are asymptotically independent parameters.↩︎"
  },
  {
    "objectID": "r-demo-part-5.html#sec-contingency-table",
    "href": "r-demo-part-5.html#sec-contingency-table",
    "title": "27  R demo",
    "section": "27.1 Contingency table analysis",
    "text": "27.1 Contingency table analysis\nLet’s take a look at the UC Berkeley admissions data:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(tidyr)\n\nucb_data &lt;- UCBAdmissions |&gt; as_tibble()\nucb_data\n\n# A tibble: 24 × 4\n   Admit    Gender Dept      n\n   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Admitted Male   A       512\n 2 Rejected Male   A       313\n 3 Admitted Female A        89\n 4 Rejected Female A        19\n 5 Admitted Male   B       353\n 6 Rejected Male   B       207\n 7 Admitted Female B        17\n 8 Rejected Female B         8\n 9 Admitted Male   C       120\n10 Rejected Male   C       205\n# ℹ 14 more rows\n\n\nIt contains data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex. Let’s see whether there is an association between Gender and Admit. Let’s first aggregate over department:\n\nucb_data_agg &lt;- ucb_data |&gt;\n  group_by(Admit, Gender) |&gt;\n  summarise(n = sum(n), .groups = \"drop\")\nucb_data_agg\n\n# A tibble: 4 × 3\n  Admit    Gender     n\n  &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;\n1 Admitted Female   557\n2 Admitted Male    1198\n3 Rejected Female  1278\n4 Rejected Male    1493\n\n\nLet’s see what the admissions rates are by gender:\n\nucb_data_agg |&gt;\n  group_by(Gender) |&gt;\n  summarise(`Admission rate` = sum(n*(Admit == \"Admitted\"))/sum(n))\n\n# A tibble: 2 × 2\n  Gender `Admission rate`\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Female            0.304\n2 Male              0.445\n\n\nThis suggests that men have substantially higher admission rates than women. Let’s see if we can confirm this using either a Fisher’s exact test or a Pearson chi-square test.\n\n# first convert to 2x2 table format\nadmit_vs_gender &lt;- ucb_data_agg |&gt;\n  pivot_wider(names_from = Gender, values_from = n) |&gt;\n  column_to_rownames(var = \"Admit\")\nadmit_vs_gender\n\n         Female Male\nAdmitted    557 1198\nRejected   1278 1493\n\n# Fisher exact test (note that the direction of the effect can be deduced)\nfisher.test(admit_vs_gender)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  admit_vs_gender\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.4781839 0.6167675\nsample estimates:\nodds ratio \n 0.5432254 \n\n# Chi-square test\nchisq.test(admit_vs_gender)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  admit_vs_gender\nX-squared = 91.61, df = 1, p-value &lt; 2.2e-16\n\n\nAs a sanity check, let’s run the Poisson regression underlying the chi-square test above.\n\npois_fit &lt;- glm(n ~ Admit + Gender + Admit*Gender,\n                family = \"poisson\",\n                data = ucb_data_agg)\nsummary(pois_fit)\n\n\nCall:\nglm(formula = n ~ Admit + Gender + Admit * Gender, family = \"poisson\", \n    data = ucb_data_agg)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               6.32257    0.04237 149.218   &lt;2e-16 ***\nAdmitRejected             0.83049    0.05077  16.357   &lt;2e-16 ***\nGenderMale                0.76584    0.05128  14.933   &lt;2e-16 ***\nAdmitRejected:GenderMale -0.61035    0.06389  -9.553   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  4.8635e+02  on 3  degrees of freedom\nResidual deviance: -3.4062e-13  on 0  degrees of freedom\nAIC: 43.225\n\nNumber of Fisher Scoring iterations: 2\n\n\nBased on all of these tests, there seems to be a very substantial difference in admissions rates based on gender. That is not good.\nBut perhaps, women tend to apply to more selective departments? Let’s look into this:\n\nucb_data |&gt;\n  group_by(Dept) |&gt;\n  summarise(admissions_rate = sum(n*(Admit == \"Admitted\"))/sum(n),\n            prop_female_applicants = sum(n*(Gender == \"Female\"))/sum(n)) |&gt;\n  ggplot(aes(x = admissions_rate, y = prop_female_applicants)) +\n  geom_point() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Admissions rate\",\n       y = \"Proportion female applicants\")\n\n\n\n\n\n\n\n\nIndeed, it does seem that female applicants typically applied to more selective departments! This suggests that it is very important to control for department when evaluating the association between admissions and gender. To do this, we can run a test for conditional independence in the \\(J \\times K \\times L\\) table:\n\npois_fit &lt;- glm(n ~ Admit + Dept + Gender + Admit:Dept + Gender:Dept,\n                family = \"poisson\",\n                data = ucb_data)\npchisq(sum(resid(pois_fit, \"pearson\")^2),\n  df = pois_fit$df.residual,\n  lower.tail = FALSE\n)\n\n[1] 0.002840164\n\n\nStill we find a significant effect! But what is the direction of the effect? The chi-square test does not tell us. We can simply compute the admissions rates by department and plot them:\n\nucb_data |&gt;\n  group_by(Dept, Gender) |&gt;\n  summarise(`Admission rate` = sum(n*(Admit == \"Admitted\"))/sum(n),\n            .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Gender, values_from = `Admission rate`) |&gt;\n  ggplot(aes(x = Female, y = Male, label = Dept)) +\n  geom_point() +\n  ggrepel::geom_text_repel() +\n  geom_abline(color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Female admission rate\",\n       y = \"Male admission rate\")\n\n\n\n\n\n\n\n\nNow the difference doesn’t seem so huge, with most departments close to even and with department A heavily skewed towards admitting women!"
  },
  {
    "objectID": "r-demo-part-5.html#sec-crime-data",
    "href": "r-demo-part-5.html#sec-crime-data",
    "title": "27  R demo",
    "section": "27.2 Revisiting the crime data, again",
    "text": "27.2 Revisiting the crime data, again\n\nlibrary(tidyverse)\n\nHere we are again, face to face with the crime data, with one last chance to get the analysis right. Let’s load and preprocess it, as before.\n\n# read crime data\ncrime_data &lt;- read_tsv(\"data/Statewide_crime.dat\")\n\n# read and transform population data\npopulation_data &lt;- read_csv(\"data/state-populations.csv\")\npopulation_data &lt;- population_data |&gt;\n  filter(State != \"Puerto Rico\") |&gt;\n  select(State, Pop) |&gt;\n  rename(state_name = State, state_pop = Pop)\n\n# collate state abbreviations\nstate_abbreviations &lt;- tibble(\n  state_name = state.name,\n  state_abbrev = state.abb\n) |&gt;\n  add_row(state_name = \"District of Columbia\", state_abbrev = \"DC\")\n\n# add CrimeRate to crime_data\ncrime_data &lt;- crime_data |&gt;\n  mutate(STATE = ifelse(STATE == \"IO\", \"IA\", STATE)) |&gt;\n  rename(state_abbrev = STATE) |&gt;\n  filter(state_abbrev != \"DC\") |&gt; # remove outlier\n  left_join(state_abbreviations, by = \"state_abbrev\") |&gt;\n  left_join(population_data, by = \"state_name\") |&gt;\n  select(state_abbrev, Violent, Metro, HighSchool, Poverty, state_pop)\n\ncrime_data\n\n# A tibble: 50 × 6\n   state_abbrev Violent Metro HighSchool Poverty state_pop\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 AK               593  65.6       90.2     8      724357\n 2 AL               430  55.4       82.4    13.7   4934193\n 3 AR               456  52.5       79.2    12.1   3033946\n 4 AZ               513  88.2       84.4    11.9   7520103\n 5 CA               579  94.4       81.3    10.5  39613493\n 6 CO               345  84.5       88.3     7.3   5893634\n 7 CT               308  87.7       88.8     6.4   3552821\n 8 DE               658  80.1       86.5     5.8    990334\n 9 FL               730  89.3       85.9     9.7  21944577\n10 GA               454  71.6       85.2    10.8  10830007\n# ℹ 40 more rows\n\n\nLet’s recall the logistic regression we ran on these data in Chapter 4:\n\nbin_fit &lt;- glm(Violent / state_pop ~ Metro + HighSchool + Poverty,\n  weights = state_pop,\n  family = \"binomial\",\n  data = crime_data\n)\n\nWe had found very poor results from the goodness of fit test for this model. We have therefore omitted some important variables and/or we have serious overdispersion on our hands.\nWe haven’t discussed in any detail how to deal with overdispersion in logistic regression models, so let’s try a Poisson model instead. The natural way to model rates using Poisson distributions is via offsets:\n\npois_fit &lt;- glm(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),\n  family = \"poisson\",\n  data = crime_data\n)\nsummary(pois_fit)\n\n\nCall:\nglm(formula = Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)), \n    family = \"poisson\", data = crime_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.609e+01  3.520e-01  -45.72   &lt;2e-16 ***\nMetro       -2.585e-02  5.727e-04  -45.15   &lt;2e-16 ***\nHighSchool   9.106e-02  3.450e-03   26.39   &lt;2e-16 ***\nPoverty      6.077e-02  4.852e-03   12.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 15589  on 49  degrees of freedom\nResidual deviance: 11741  on 46  degrees of freedom\nAIC: 12135\n\nNumber of Fisher Scoring iterations: 5\n\n\nAgain, everything is significant, and again, the regression summary shows that we have a huge residual deviance. This was to be expected, given that \\(\\text{Bin}(m, \\pi) \\approx \\text{Poi}(m\\pi)\\) for large \\(m\\) and small \\(\\pi\\). So, the natural thing to try is a negative binomial regression! Negative binomial regression is not implemented in the regular glm package, but glm.nb() from the MASS package is a dedicated function for this task. Let’s see what we get:\n\nnb_fit &lt;- MASS::glm.nb(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),\n  data = crime_data\n)\nsummary(nb_fit)\n\n\nCall:\nMASS::glm.nb(formula = Violent ~ Metro + HighSchool + Poverty + \n    offset(log(state_pop)), data = crime_data, init.theta = 1.467747388, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -10.254088   5.273418  -1.944   0.0518 .\nMetro        -0.012188   0.008518  -1.431   0.1525  \nHighSchool    0.028052   0.052482   0.535   0.5930  \nPoverty      -0.026852   0.068449  -0.392   0.6948  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.4677) family taken to be 1)\n\n    Null deviance: 59.516  on 49  degrees of freedom\nResidual deviance: 55.487  on 46  degrees of freedom\nAIC: 732.58\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.468 \n          Std. Err.:  0.268 \n\n 2 x log-likelihood:  -722.575 \n\n\nAha! Things are not looking so significant anymore! And the residual deviance is not as huge! Although, we must be careful! The residual deviance no longer has the usual \\(\\chi^2\\) distribution because of the estimated dispersion parameter. So we don’t really have an easy goodness of fit test. The estimated value of \\(\\gamma\\) (confusingly called \\(\\theta\\) in the summary) is significantly different from zero, indicating overdispersion. Let’s formally test for overdispersion using the nonstandard likelihood ratio test discussed above:\n\nT_LRT &lt;- 2 * (as.numeric(logLik(nb_fit)) - as.numeric(logLik(pois_fit)))\np_LRT &lt;- pchisq(T_LRT, df = 1, lower.tail = FALSE)/2\np_LRT\n\n[1] 0\n\n\nSo at the very least the NB model fits much better than the Poisson model. Let’s do some inference based on this model. For example, we can get Wald confidence intervals:\n\nconfint.default(nb_fit)\n\n                   2.5 %      97.5 %\n(Intercept) -20.58979658 0.081620714\nMetro        -0.02888413 0.004507747\nHighSchool   -0.07481066 0.130915138\nPoverty      -0.16100973 0.107305015\n\n\nOr we can get LRT-based (i.e. profile) confidence intervals:\n\nconfint(nb_fit)\n\nWaiting for profiling to be done...\n\n\n                   2.5 %       97.5 %\n(Intercept) -19.20209590 -0.860399348\nMetro        -0.03153902  0.006365841\nHighSchool   -0.06265118  0.115318303\nPoverty      -0.13930110  0.085200541\n\n\nOr we can get confidence intervals for the predicted means:\n\npredict(nb_fit,\n  newdata = crime_data |&gt; column_to_rownames(var = \"state_abbrev\"),\n  type = \"response\",\n  se.fit = TRUE\n)\n\n$fit\n       AK        AL        AR        AZ        CA        CO        CT        DE \n 116.1520  617.7064  375.4895  700.6931 3257.5300  725.1538  436.7863  127.2572 \n       FL        GA        HI        ID        IL        IN        IA        KS \n2232.2308 1301.2937  157.1416  263.8572 1379.1847  954.3366  546.5503  439.0649 \n       KY        LA        MA        MD        ME        MI        MN        MO \n 541.5706  391.6745  747.7454  737.0032  274.2879 1322.9956  970.4078  871.2829 \n       MS        MT        NC        ND        NE        NH        NJ        NM \n 380.6756  199.4947 1313.0904  134.8128  305.0634  261.1975  966.9940  204.3311 \n       NV        NY        OH        OK        OR        PA        RI        SC \n 327.7316 1926.3861 1477.1713  495.9711  517.8397 1600.0813   96.3565  684.9102 \n       SD        TN        TX        UT        VA        VT        WA        WI \n 160.9225  867.0224 2423.0647  416.6648 1244.5168  148.1635 1012.1932  892.0644 \n       WV        WY \n 226.4515  100.1906 \n\n$se.fit\n       AK        AL        AR        AZ        CA        CO        CT        DE \n 21.00552 143.65071 130.44272 165.08459 910.57769 121.34777  85.53768  32.15169 \n       FL        GA        HI        ID        IL        IN        IA        KS \n427.89514 173.04544  31.73873  40.28262 239.43324 147.21049 104.05752  68.82044 \n       KY        LA        MA        MD        ME        MI        MN        MO \n133.28938 129.40665 150.23524 158.93816  92.04222 171.28409 216.32477 110.88843 \n       MS        MT        NC        ND        NE        NH        NJ        NM \n138.28105  65.60335 379.90855  26.74061  69.62560  66.73731 220.88371  59.26953 \n       NV        NY        OH        OK        OR        PA        RI        SC \n 64.30971 387.25204 241.24541  95.44911  81.97419 220.42078  33.97964 119.45174 \n       SD        TN        TX        UT        VA        VT        WA        WI \n 41.50215 169.68896 738.95321 107.62725 209.14651  51.32810 191.75629 137.35158 \n       WV        WY \n 71.55328  22.79279 \n\n$residual.scale\n[1] 1\n\n\nWe can carry out some hypothesis tests as well, e.g. to test \\(H_0: \\beta_{\\text{Metro}} = 0\\):\n\nnb_fit_partial &lt;- MASS::glm.nb(Violent ~ HighSchool + Poverty + offset(log(state_pop)),\n  data = crime_data\n)\nanova_fit &lt;- anova(nb_fit_partial, nb_fit)\nanova_fit\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: Violent\n                                                  Model    theta Resid. df\n1         HighSchool + Poverty + offset(log(state_pop)) 1.428675        47\n2 Metro + HighSchool + Poverty + offset(log(state_pop)) 1.467747        46\n     2 x log-lik.   Test    df LR stat.   Pr(Chi)\n1       -724.1882                                \n2       -722.5753 1 vs 2     1 1.612878 0.2040877"
  },
  {
    "objectID": "multiple-testing-intro.html#sec-multiplicity-problem",
    "href": "multiple-testing-intro.html#sec-multiplicity-problem",
    "title": "28  Introduction",
    "section": "28.1 The multiplicity problem",
    "text": "28.1 The multiplicity problem\nWhen R prints a regression summary, it adds stars to variables based on their \\(p\\)-values. Variables with \\(p\\)-values below 0.05 get one star, those with \\(p\\)-values below 0.01 get two stars, and those with \\(p\\)-values below 0.001 get three stars. A natural strategy for selecting significant variables is to choose those with one or more stars. However, the issue with this strategy is that even null variables (those with coefficients of zero) will sometimes have small \\(p\\)-values by chance (Figure 28.1). The more total variables we are testing, the more of them will have small \\(p\\)-values by chance. This is the multiplicity problem.\n\n\n\nFigure 28.1: A spurious correlation resulting from data snooping.\n\n\nTo quantify this issue, consider the case when all \\(m\\) variables under consideration are null. Then, the chance that any one of them has a \\(p\\)-value below 0.05 is 0.05. So, the expected number of variables with one or more stars is \\(0.05m\\). For example, if we have 100 variables, then we expect to see 5 variables with stars on average, even though none of the variables are actually relevant to the response! The growth of the quantity \\(0.05m\\) with \\(m\\) confirms that the multiplicity problem grows more severe as the number of hypotheses tested increases.\nAnother way of thinking about the multiplicity problem is in the context of selection bias. The process of scanning across all variables and selecting those with small \\(p\\)-values is a selection event. Once the selection event has occurred, one must consider the null distribution of a \\(p\\)-value conditionally on the fact that it was selected. Since the selection event favors small \\(p\\)-values, the null distribution of a \\(p\\)-value conditional on selection is no longer uniform; it becomes skewed toward zero. Interpreting \\(p\\)-values (and their accompanying stars) “at face value” is misleading because it ignores the crucial selection step. Other terms for this include “data snooping” and “p-hacking.”\nThe multiplicity problem is not limited to regression. In the next two sections, we develop some definitions to describe the multiplicity problem more formally and generally."
  },
  {
    "objectID": "multiple-testing-intro.html#sec-global-multiple-testing",
    "href": "multiple-testing-intro.html#sec-global-multiple-testing",
    "title": "28  Introduction",
    "section": "28.2 Global testing and multiple testing",
    "text": "28.2 Global testing and multiple testing\nSuppose we have \\(m\\) null hypotheses \\(H_{01}, \\dots, H_{0m}\\). Let $p_1, , \\(p_m\\) be the corresponding \\(p\\)-values.\n\nDefinition 28.1 A \\(p\\)-value \\(p_j\\) for a null hypothesis \\(H_{0j}\\) is valid if \\[\n\\mathbb{P}_{H_{0j}}[p_j \\leq t] \\leq t \\quad \\text{for all } t \\in [0, 1].\n\\tag{28.1}\\]\n\nThis definition covers the uniform distribution, as well as distributions that are stochastically larger than uniform. Distributions of the latter kind are often obtained from resampling-based tests, such as permutation tests. In the remainder of this chapter, we will assume that all \\(p\\)-values are valid.\nGiven a set of \\(p\\)-values, there are several inferential goals potentially of interest. These can be subdivided first into global testing and multiple testing.\n\nDefinition 28.2 A global testing procedure is a test of the global null hypothesis \\[\nH_0 \\equiv \\bigcap_{j = 1}^m H_{0j}.\n\\] In other words, it is a function \\(\\phi: (p_1, \\dots, p_m) \\mapsto [0,1]\\). A global test has level \\(\\alpha\\) if it controls the Type-I error at this level: \\[\n\\mathbb{E}_{H_0}[\\phi(p_1, \\dots, p_m)] \\leq \\alpha.\n\\tag{28.2}\\]\n\nA global testing procedure determines whether any of the null hypotheses can be rejected. In regression modeling, a global test would be a test of the hypothesis \\(H_0: \\beta_1 = \\cdots = \\beta_m = 0\\).\n\nDefinition 28.3 A multiple testing procedure is a mapping from the set of \\(p\\)-values to a set of hypotheses to reject: \\[\n\\mathcal{M}: (p_1, \\dots, p_m) \\mapsto \\hat{S} \\subseteq \\{1, \\dots, m\\}.\n\\]\n\nA multiple testing procedure determines which of the null hypotheses can be rejected. In regression modeling, a multiple testing procedure would be a method for selecting which variables have nonzero coefficients, the problem we discussed in the beginning of this section."
  },
  {
    "objectID": "multiple-testing-intro.html#subsec-multiple-testing-goals",
    "href": "multiple-testing-intro.html#subsec-multiple-testing-goals",
    "title": "28  Introduction",
    "section": "28.3 Multiple testing goals",
    "text": "28.3 Multiple testing goals\nLet us define \\[\n\\mathcal{H}_0 \\equiv \\{j \\in \\{1, \\dots, m\\}: H_{0j} \\text{ is true}\\} \\quad \\text{and} \\quad \\mathcal{H}_1 \\equiv \\{j \\in \\{1, \\dots, m\\}: H_{0j} \\text{ is false}\\}.\n\\]\nIn other words, \\(\\mathcal{H}_0\\) is the set of indices of the true null hypotheses, and \\(\\mathcal{H}_1\\) is the set of indices of the false null hypotheses. There are two primary notions of Type-I error that multiple testing procedures seek to control: the family-wise error rate (FWER) and the false discovery rate (FDR).\n\n28.3.1 Definitions of Type-I error rate and power\n\nDefinition 28.4 The family-wise error rate (FWER) of a multiple testing procedure \\(\\mathcal{M}: (p_1, \\dots, p_m) \\mapsto \\hat{S}\\) is the probability that it makes any false rejections: \\[\n\\text{FWER}(\\mathcal{M}) \\equiv \\mathbb{P}[\\hat{S} \\cap \\mathcal{H}_0 \\neq \\varnothing].\n\\] A multiple testing procedure controls the FWER at level \\(\\alpha\\) if \\[\n\\text{FWER}(\\mathcal{M}) \\leq \\alpha.\n\\]\n\n\nDefinition 28.5 The false discovery proportion (FDP) of a rejection set \\(\\hat{S}\\) is the proportion of these rejections that are false: \\[\n\\text{FDP}(\\hat{S}) \\equiv \\frac{|\\hat{S} \\cap \\mathcal{H}_0|}{|\\hat{S}|}, \\quad \\text{where} \\quad \\frac{0}{0} \\equiv 0.\n\\] The false discovery rate (FDR) of a multiple testing procedure \\(\\mathcal{M}: (p_1, \\dots, p_m) \\mapsto \\hat{S}\\) is its expected false discovery proportion: \\[\n\\text{FDR}(\\mathcal{M}) \\equiv \\mathbb{E}[\\text{FDP}(\\hat{S})] = \\mathbb{E}\\left[\\frac{|\\hat{S} \\cap \\mathcal{H}_0|}{|\\hat{S}|}\\right].\n\\tag{28.3}\\] A multiple testing procedure controls the FDR at level \\(q\\) if \\[\n\\text{FDR}(\\mathcal{M}) \\leq q.\n\\]\n\nRegardless of what error rate a multiple testing procedure is intended to control, we would like it to have high power: \\[\n\\text{power}(\\mathcal{M}) \\equiv \\mathbb{E}\\left[\\frac{|\\hat{S} \\cap \\mathcal{H}_1|}{|\\mathcal{H}_1|}\\right].\n\\]\n\n\n28.3.2 Relationship between the FWER and FDR\nNote that the FWER is a probability, while the FDR is an expected proportion. This distinction is highlighted by using the different symbols \\(\\alpha\\) and \\(q\\) for the nominal FWER and FDR levels, respectively. The FWER is a more stringent error rate than the FDR, because it can only be low if no false discoveries are made most of the time; the FDR can be low if false discoveries are a small proportion of the total number of discoveries most of the time.\n\nProposition 28.1 For any multiple testing procedure \\(\\mathcal{M}\\), we have \\(\\text{FDR}(\\mathcal{M}) \\leq \\text{FWER}(\\mathcal{M})\\). Therefore, a multiple testing procedure controlling the FWER at level \\(\\alpha\\) also controls the FDR at level \\(\\alpha\\).\n\n\nProof. \\[\n\\text{FDR} \\equiv \\mathbb{E}\\left[\\frac{|\\hat{S} \\cap \\mathcal{H}_0|}{|\\hat{S}|}\\right] \\leq \\mathbb{E}\\left[1(|\\hat{S} \\cap \\mathcal{H}_0| &gt; 0)\\right] \\equiv \\text{FWER}.\n\\]\n\nThe FWER was the error rate of choice in the 20th century, when limitations on data collection permitted only small handfuls of hypotheses to be tested. In the 21st century, the internet and other new technologies permitted much larger-scale collection of data, leading to much larger sets of hypotheses being tested (e.g., tens of thousands). In this context, the less stringent FDR rate became more popular. In many cases, an initial large-scale FDR-controlling procedure is viewed as an exploratory analysis, whose goal is to nominate a smaller number of hypotheses for confirmation with follow-up experiments. The purpose of controlling the FDR in this context is to limit resources wasted on following up false leads."
  },
  {
    "objectID": "global-testing.html#sec-bonferroni-global-test",
    "href": "global-testing.html#sec-bonferroni-global-test",
    "title": "29  Global testing",
    "section": "29.1 Bonferroni global test (Bonferroni, 1936 and Dunn, 1961)",
    "text": "29.1 Bonferroni global test (Bonferroni, 1936 and Dunn, 1961)\n\n29.1.1 Test definition and validity\nThe motivation for the Bonferroni global test is to find the strongest signal among the \\(p\\)-values and reject the global null if this signal is strong enough. It makes sense that such a strategy would be powerful against sparse alternatives. We define the Bonferroni test via \\[\n\\phi(p_1, \\dots, p_m) \\equiv 1\\left(\\min_{1 \\leq j \\leq m} p_j \\leq \\alpha/m\\right).\n\\]\nThe Bonferroni global test rejects if any of the \\(p\\)-values cross the multiplicity-adjusted or Bonferroni-adjusted significance threshold of \\(\\alpha/m\\). This test can be viewed as a modified version of the naive test (29.1), but with the significance threshold \\(\\alpha\\) adjusted downward to \\(\\alpha/m\\). The more hypotheses we test, the more stringent the significance threshold must be.\n\nProposition 29.1 The Bonferroni test controls the FWER at level \\(\\alpha\\) for any joint dependence structure among the \\(p\\)-values.\n\n\nProof. We can verify the Type-I error control of the Bonferroni test via a union bound: \\[\n\\mathbb{P}_{H_0}\\left[\\min_{1 \\leq j \\leq m} p_j \\leq \\alpha/m\\right] \\leq \\sum_{j = 1}^m \\mathbb{P}_{H_{0j}}\\left[p_j \\leq \\alpha/m\\right] = m \\cdot \\alpha/m = \\alpha.\n\\]\n\n\n\n29.1.2 The impact of \\(p\\)-value dependence\nWhile the Bonferroni global test is valid for arbitrary \\(p\\)-value dependence structures, the underlying union bound may be loose for certain dependence structures. In particular, the Bonferroni bound derived above is tightest for independent \\(p\\)-values. Intuitively, the smallest \\(p\\)-value has the highest chance of being small if each \\(p\\)-value has its own independent source of randomness. Mathematically, let us compute the Type-I error of the Bonferroni global test under independence: \\[\n\\mathbb{P}_{H_0}\\left[\\min_{1 \\leq j \\leq m} p_j \\leq \\alpha/m\\right] = 1 - (1-\\alpha/m)^m \\approx \\alpha.\n\\] Therefore, the Bonferroni test exhausts essentially its entire level under independence. On the other hand, under perfect dependence (i.e., \\(p_1 = \\cdots = p_m\\) almost surely), the Bonferroni test is quite conservative: \\[\n\\mathbb{P}_{H_0}\\left[\\min_{1 \\leq j \\leq m} p_j \\leq \\alpha/m\\right] = \\mathbb{P}_{H_{01}}\\left[p_1 \\leq \\alpha/m\\right] = \\alpha/m.\n\\] In this special case, the level is \\(m\\) times lower than it should be, because no multiplicity adjustment is needed if the \\(p\\)-values are perfectly dependent."
  },
  {
    "objectID": "global-testing.html#sec-fisher-combination-test",
    "href": "global-testing.html#sec-fisher-combination-test",
    "title": "29  Global testing",
    "section": "29.2 Fisher combination test (Fisher, 1925)",
    "text": "29.2 Fisher combination test (Fisher, 1925)\nIf, on the other hand, we expect the signal to be spread out over many non-null hypotheses, the valuable evidence against the alternative is missed if only the minimum \\(p\\)-value is considered. In such circumstances, the Fisher combination test may be more powerful than the Bonferroni global test.\n\n29.2.1 Test definition and validity\nThe Fisher combination test is based on the observation that \\[\n\\text{if } p \\sim U[0,1], \\quad \\text{then} \\quad -2\\log p \\sim \\chi^2_2.\n\\] Therefore, if \\(p_1, \\dots, p_m\\) are independent uniform random variables, then we have \\[\n-2\\sum_{j = 1}^m \\log p_j \\sim \\chi^2_{2m}.\n\\] This leads to the Fisher combination test: \\[\n\\phi(p_1, \\dots, p_m) \\equiv 1\\left(-2\\sum_{j = 1}^m \\log p_j \\geq \\chi^2_{2m}(1-\\alpha)\\right).\n\\tag{29.2}\\]\n\nProposition 29.2 The Fisher combination test controls Type-I error at level \\(\\alpha\\) (28.2) if the \\(p\\)-values are independent.\n\n\nProof. Under the null, the \\(p\\)-values are stochastically larger than uniform (29.2). Therefore, \\(-2\\sum_{j = 1}^m \\log p_j\\) is stochastically larger than \\(\\chi^2_{2m}\\), from which the conclusion follows.\n\n\n\n29.2.2 Discussion\nThe Fisher exact test has a similar flavor to another chi-squared test. Suppose \\(X_j \\sim N(\\mu_j, 1)\\), and we would like to test \\(H_j: \\mu_j = 0\\). Under the global null, we have \\[\n\\text{if } X_1, \\dots, X_m \\overset{\\text{i.i.d.}}\\sim N(0,1), \\text{ then } \\sum_{j = 1}^m X_j^2 \\sim \\chi^2_m.\n\\tag{29.3}\\] It turns out that the tests based on (29.2) and (29.3) are quite similar. This helps us build intuition for what the Fisher combination test is doing: it’s averaging the strengths of the signal across hypotheses.\nThe independence assumption of the Fisher combination test makes it significantly less broadly applicable than the Bonferroni global test. However, one common application of the Fisher combination test is meta-analysis: the combination of information across multiple studies of the same hypothesis (or very related hypotheses). In this setting, the \\(p\\)-values are independent across studies, and the Fisher combination test is a natural choice because the strength of the signal is roughly the same across studies since they are studying very related hypotheses."
  },
  {
    "objectID": "multiple-testing-chapter.html#sec-bonferroni-fwer",
    "href": "multiple-testing-chapter.html#sec-bonferroni-fwer",
    "title": "30  Multiple testing",
    "section": "30.1 The Bonferroni procedure for FWER control",
    "text": "30.1 The Bonferroni procedure for FWER control\nWe discussed the Bonferroni test for the global null. This test can be extended to an FWER-controlling procedure:\n\\[\n\\hat S \\equiv \\{j: p_j \\leq \\alpha/m\\}\n\\tag{30.1}\\]\n\nProposition 30.1 The Bonferroni procedure controls the FWER at level \\(\\alpha\\) for arbitrary \\(p\\)-value dependence structures.\n\n\nProof. We have\n\\[\n\\mathbb{P}[\\hat S \\cap \\mathcal H_0 \\neq \\varnothing] = \\mathbb{P}\\left[\\min_{j \\in \\mathcal H_0} p_j \\leq \\alpha/m\\right] \\leq \\sum_{j \\in \\mathcal H_0} \\mathbb{P}[p_j \\leq \\alpha/m] = \\frac{|\\mathcal H_0|}{m}\\alpha \\leq \\alpha.\n\\]\nThis completes the proof.\n\nNote that the FWER is actually controlled at the level \\(\\frac{|\\mathcal H_0|}{m}\\alpha \\leq \\alpha\\), making the Bonferroni test conservative to the extent that \\(|\\mathcal H_0| &lt; m\\). The null proportion \\(\\frac{|\\mathcal H_0|}{m}\\) has such an effect on the performance of many multiple testing procedures. Not all global tests can be extended to FWER-controlling procedures in this way. For example, the Fisher combination test does not single out any of the hypotheses, as it only aggregates the \\(p\\)-values. By contrast, the Bonferroni test searches for \\(p\\)-values that are individually very small, allowing it to double as an FWER-controlling procedure."
  },
  {
    "objectID": "multiple-testing-chapter.html#sec-bh-fdr",
    "href": "multiple-testing-chapter.html#sec-bh-fdr",
    "title": "30  Multiple testing",
    "section": "30.2 The Benjamini-Hochberg procedure for FDR control",
    "text": "30.2 The Benjamini-Hochberg procedure for FDR control\nDesigning procedures with FDR control, as well as verifying the latter property, is typically harder than for FWER control. It is harder to decouple the effects of the individual hypotheses, as the denominator \\(|S|\\) in the FDR definition (28.3) couples them together. Both the FDR criterion and the most popular FDR-controlling procedure were proposed by Benjamini and Hochberg in 1995.\n\n30.2.1 Procedure\nTo define the BH procedure, consider thresholding the \\(p\\)-values at \\(t \\in [0,1]\\). We would expect \\(\\mathbb{E}[|\\{j: p_j \\leq t\\} \\cap \\mathcal H_0|] = |\\mathcal H_0|t\\) false discoveries among \\(\\{j: p_j \\leq t\\}\\). Since \\(|\\mathcal H_0|\\) is unknown, we can bound it from above by \\(mt\\). This leads to the FDP estimate:\n\\[\n\\widehat{\\text{FDP}}(t) \\equiv \\frac{mt}{|\\{j: p_j \\leq t\\}|}\n\\tag{30.2}\\]\nThe BH procedure is then defined via:\n\\[\n\\hat S \\equiv \\{j: p_j \\leq \\widehat t\\}, \\quad \\text{where} \\quad \\widehat t = \\max\\{t \\in [0,1]: \\widehat{\\text{FDP}}(t) \\leq q\\}\n\\tag{30.3}\\]\nIn words, we choose the most liberal \\(p\\)-value threshold for which the estimated FDP is below the nominal level \\(q\\). Note that the set over which the above maximum is taken is always nonempty because it at least contains 0: \\(\\widehat{\\text{FDP}}(0) = \\frac{0}{0} \\equiv 0\\).\n\n\n30.2.2 FDR control under independence\nBenjamini and Hochberg established that their procedure controls the FDR if the \\(p\\)-values are independent. Here we present an alternative argument due to Storey, Taylor, and Siegmund (2004).\n\nProposition 30.2 The BH procedure controls the FDR at level \\(q\\) assuming that the \\(p\\)-values are independent.\n\n\nProof. We have\n\\[\n\\begin{split}\n\\text{FDR} &= \\mathbb{E}\\left[\\text{FDP}(\\widehat t)\\right] = \\mathbb{E}\\left[\\frac{|\\{j \\in \\mathcal H_0: p_j \\leq \\widehat t\\}|}{|\\{j: p_j \\leq \\widehat t\\}|}\\right] \\\\\n&= \\mathbb{E}\\left[\\frac{|\\{j \\in \\mathcal H_0: p_j \\leq \\widehat t\\}|}{m \\widehat t}\\widehat{\\text{FDP}}(\\widehat t)\\right] \\leq q \\cdot \\mathbb{E}\\left[\\frac{|\\{j \\in \\mathcal H_0: p_j \\leq \\widehat t\\}|}{m \\widehat t}\\right].\n\\end{split}\n\\]\nTo prove that the last expectation is bounded above by 1, note that\n\\[\nM(t) \\equiv \\frac{|\\{j \\in \\mathcal H_0: p_j \\leq t\\}|}{m t}\n\\tag{30.4}\\]\nis a backwards martingale with respect to the filtration\n\\[\n\\mathcal F_t = \\sigma(\\{p_j: j \\in \\mathcal H_1\\}, |\\{j \\in \\mathcal H_0: p_j \\leq t'\\}| \\text{ for } t' \\geq t),\n\\tag{30.5}\\]\nwith \\(t\\) running backwards from 1 to 0. Indeed, for \\(s &lt; t\\) we have\n\\[\n\\mathbb{E}[M(s)|\\mathcal F_t] = \\mathbb{E}\\left[\\left.\\frac{|\\{j \\in \\mathcal H_0: p_j \\leq s\\}|}{m s} \\right| \\mathcal F_t\\right] = \\frac{\\frac{s}{t}|\\{j \\in \\mathcal H_0: p_j \\leq t\\}|}{m s} = \\frac{|\\{j \\in \\mathcal H_0: p_j \\leq t\\}|}{m t} = M(t).\n\\]\nThe threshold \\(\\widehat t\\) is a stopping time with respect to this filtration, so by the optional stopping theorem, we have\n\\[\n\\mathbb{E}\\left[\\frac{|\\{j \\in \\mathcal H_0: p_j \\leq \\widehat t\\}|}{m \\widehat t}\\right] = \\mathbb{E}[M(\\widehat t)] \\leq \\mathbb{E}[M(1)] = \\frac{|\\mathcal H_0|}{m} \\leq 1.\n\\]\nThis completes the proof.\n\n\n\n30.2.3 FDR control under dependence\nUnder dependence, the BH procedure’s FDR can be bounded by a multiple of the nominal FDR level.\n\nProposition 30.3 The BH procedure controls the FDR at level \\(q(1 + \\frac{1}{2} + \\cdots + \\frac{1}{m})\\) regardless of the \\(p\\)-value dependency structure.\n\n\nProof. We have\n\\[\n\\begin{split}\n\\text{FDP}(\\hat S) &= \\sum_{k = 1}^m \\frac{|\\hat S \\cap \\mathcal H_0|}{k}1(|\\hat S| = k) \\\\\n&= \\sum_{k = 1}^m \\sum_{j \\in \\mathcal H_0} \\frac{1}{k}1(j \\in \\hat S, |\\hat S| = k) \\\\\n&= \\sum_{k = 1}^m \\sum_{j \\in \\mathcal H_0} \\frac{1}{k}1\\left(p_j \\leq \\frac{qk}{m}, |\\hat S| = k\\right) \\\\\n&\\leq \\sum_{j \\in \\mathcal H_0} \\sum_{l = 1}^m \\frac{1}{l} 1\\left(p_j \\in \\left[\\frac{q(l-1)}{m}, \\frac{ql}{m}\\right]\\right).\n\\end{split}\n\\]\nIt follows that\n\\[\n\\text{FDR} = \\mathbb{E}[\\text{FDP}(\\hat S)] \\leq \\frac{|\\mathcal H_0|}{m}q\\left(1 + \\frac{1}{2} + \\cdots + \\frac{1}{m}\\right).\n\\]\nThis completes the proof."
  },
  {
    "objectID": "multiple-testing-chapter.html#sec-additional-topics",
    "href": "multiple-testing-chapter.html#sec-additional-topics",
    "title": "30  Multiple testing",
    "section": "30.3 Additional topics",
    "text": "30.3 Additional topics\n\n30.3.1 Weighted multiple testing procedures\nSometimes, we may have more prior evidence against certain null hypotheses than others, which we wish to incorporate in the global testing or multiple testing procedure to boost power. A common approach to doing so is to weight the \\(p\\)-values. Letting \\(w_1, \\dots, w_m\\) be \\(p\\)-value weights averaging to 1, define weighted \\(p\\)-values \\(\\tilde{p}_j\\) via:\n\\[\n\\tilde{p}_j \\equiv \\frac{p_j}{w_j}\n\\tag{30.6}\\]\nNote that \\(p\\)-values corresponding to hypotheses with large (small) weights will be made more (less) significant. We can then attempt to apply the above global testing and multiple testing procedures on the weighted \\(p\\)-values \\(\\tilde{p}_j\\) rather than the original \\(p\\)-values \\(p_j\\). As it turns out, in many cases these weighted procedures retain the Type-I error guarantees of their unweighted counterparts.\n\nProposition 30.4 The weighted variants of the Bonferroni global test, the Bonferroni FWER procedure, and the BH FDR procedure all control their respective Type-I error rates under the same conditions as their unweighted counterparts (arbitrary dependence for the Bonferroni procedures and independence for BH).\n\n\nProof. Here, we prove the statement just for the Bonferroni global test; the remaining proofs are left as exercises. The weighted Bonferroni global test is as follows:\n\\[\n\\phi(p_1, \\dots, p_m) \\equiv 1\\left(\\min_{1 \\leq j \\leq m} \\frac{p_j}{w_j} \\leq \\frac{\\alpha}{m}\\right).\n\\]\nIt follows that\n\\[\n\\mathbb{E}_{H_0}[\\phi(p_1, \\dots, p_m)] \\leq \\sum_{j = 1}^m \\frac{\\alpha}{m} w_j = \\alpha.\n\\]\nThe last equality follows from the fact that the weights \\(w_j\\) average to 1 by assumption.\nThis completes the proof."
  },
  {
    "objectID": "exponential-dispersion-models.html#footnotes",
    "href": "exponential-dispersion-models.html#footnotes",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "",
    "text": "In this class, we will always have \\(\\Theta = \\mathbb R\\).↩︎\nIf you are not familiar with measure theory, you can view \\(\\nu\\) as specifying the support of a distribution (the set of values it can take). For example, for binary \\(y\\), \\(\\nu\\) would indicate that \\(y\\) is supported on \\(\\{0,1\\}\\), and the “density” \\(f_{\\theta, \\phi}\\) would be a probability mass function.↩︎"
  },
  {
    "objectID": "exponential-dispersion-models.html#unit-deviance-properties",
    "href": "exponential-dispersion-models.html#unit-deviance-properties",
    "title": "19  Exponential dispersion model (EDM) distributions",
    "section": "19.6 Unit deviance properties",
    "text": "19.6 Unit deviance properties\nThe unit deviance \\(d(y, \\mu)\\) is a convex, nonnegative function, achieving a minimum of zero for \\(y = \\mu\\)."
  },
  {
    "objectID": "parameter-estimation.html#footnotes",
    "href": "parameter-estimation.html#footnotes",
    "title": "21  Parameter estimation",
    "section": "",
    "text": "The Fisher information is the expectation of the Hessian, but for canonical links, the Hessian is non-random, so the two coincide.↩︎"
  },
  {
    "objectID": "glm-inference.html#sec-likelihood-ratio-inference",
    "href": "glm-inference.html#sec-likelihood-ratio-inference",
    "title": "22  Inference in GLMs",
    "section": "22.3 Likelihood ratio inference",
    "text": "22.3 Likelihood ratio inference\n\n22.3.1 Testing one or more coefficients (\\(\\phi_0\\) known)\nLet \\(\\ell(\\boldsymbol{y}, \\boldsymbol{\\mu}) = -\\frac{D(\\boldsymbol{y}, \\boldsymbol{\\mu})}{2\\phi_0} + C\\) be the GLM log-likelihood (recall equation (21.6)). Let \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{\\beta}_S^0\\) be a null hypothesis about some subset of variables \\(S \\subset \\{0, 1, \\dots, p-1\\}\\), and let \\(\\boldsymbol{\\widehat{\\mu}}_{\\text{-}S}\\) be the maximum likelihood estimate under the null hypothesis. Likelihood ratio inference is based on the following asymptotic chi-square distribution:\n\\[\n2(\\ell(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}}) - \\ell(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}}_{\\text{-}S})) = \\frac{D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}}_{\\text{-}S}) - D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}})}{\\phi_0} \\overset{\\cdot}{\\sim} \\chi^2_{|S|}.\n\\tag{22.4}\\]\nThis approximation holds either in large samples (large-sample asymptotics) or in small samples but with small dispersion (small-dispersion asymptotics). The latter has to do with the fact that under small-dispersion asymptotics,\n\\[\n\\frac{d(y_i, \\mu_i)}{\\phi_0/w_i} \\overset{\\cdot}{\\sim} \\chi^2_1,\n\\]\nso\n\\[\n\\frac{D(\\boldsymbol{y}, \\boldsymbol{\\mu})}{\\phi_0} = \\sum_{i = 1}^n \\frac{d(y_i, \\mu_i)}{\\phi_0/w_i} \\overset{\\cdot}{\\sim} \\chi^2_n.\n\\]\nSuppose we wish to test the null hypothesis \\(H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{\\beta}_S^0\\). Then, based on the approximation (22.4), we can define the likelihood ratio test:\n\\[\n\\phi(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv 1\\left(\\frac{D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}}_{\\text{-}S}) - D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}})}{\\phi_0} &gt; \\chi^2_{|S|}(1-\\alpha)\\right).\n\\]\n\n\n22.3.2 Confidence interval for a single coefficient\nWe can obtain a confidence interval for \\(\\beta_j\\) by inverting the likelihood ratio test. Let \\(\\boldsymbol{\\widehat{\\mu}}_{\\text{-}j}(\\beta_j^0)\\) be the fitted mean vector under the constraint \\(\\beta_j = \\beta_j^0\\). Then, inverting the likelihood ratio test gives us the confidence interval:\n\\[\n\\text{CI}(\\beta_j) \\equiv \\left\\{\\beta_j: \\frac{D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}}_{\\text{-}j}(\\beta_j)) - D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}})}{\\phi_0} \\leq \\chi^2_{|S|}(1-\\alpha)\\right\\}.\n\\]\nLikelihood ratio-based confidence intervals tend to be more accurate than Wald intervals, especially when the parameter is near the edge of the parameter space, but they require more computation because \\(\\boldsymbol{\\widehat{\\mu}}_{\\text{-}j}(\\beta_j)\\) must be computed on a large grid of \\(\\beta_j\\) values. If we wanted to create confidence regions for groups of parameters, this would become computationally intensive due to the curse of dimensionality.\n\n\n22.3.3 Goodness of fit testing (\\(\\phi_0\\) known)\nFor \\(\\phi_0\\) known, we can also construct a goodness of fit test. To this end, we compare the deviances of the GLM and saturated model:\n\\[\n\\frac{D(\\boldsymbol{y}, \\boldsymbol{\\widehat{\\mu}}) - D(\\boldsymbol{y}, \\boldsymbol{y})}{\\phi_0} = \\frac{D(\\boldsymbol{y}; \\boldsymbol{\\widehat{\\mu}})}{\\phi_0} \\overset{\\cdot}{\\sim} \\chi^2_{n-p}.\n\\]\nNote that the goodness of fit test is a significance test with respect to the saturated model (22.1), which has \\(n\\) free parameters. Therefore, the number of free parameters increases with the sample size, so large-sample asymptotics cannot justify this test. Instead, we must rely on small-dispersion asymptotics. In particular, the likelihood ratio test relies on the saddlepoint approximation for small dispersions. To verify whether the saddlepoint approximation is accurate, we can apply the rules of thumb from Section 19.6.2.2 for each observation \\(y_i\\), when it is drawn from the distribution fitted under the GLM (rather than the saturated model). For instance, we can check that \\(m \\hat \\mu_i \\geq 3\\) and \\(m(1-\\hat \\mu_i) \\geq 3\\) in the case of grouped logistic regression of \\(\\hat \\mu_i \\geq 3\\) for Poisson regression. Here, \\(\\hat \\mu_i\\) are the fitted means under the GLM.\n\n\n22.3.4 Likelihood ratio inference for \\(\\phi_0\\) unknown\nIf \\(\\phi_0\\) is unknown, we can estimate it as discussed above and construct an \\(F\\)-statistic as follows:\n\\[\nF \\equiv \\frac{(D(\\boldsymbol{y}; \\boldsymbol{\\widehat{\\mu}}_{\\text{-}S}) - D(\\boldsymbol{y}; \\boldsymbol{\\widehat{\\mu}}))/|S|}{\\widetilde{\\phi}_0}.\n\\]\nIn normal linear model theory, the null distribution of \\(F\\) is exactly \\(F_{|S|, n-p}\\). For GLMs, the null distribution of \\(F\\) is approximately \\(F_{|S|, n-p}\\). We can use this \\(F\\) distribution to construct hypothesis tests for groups of coefficients, or invert it to get a confidence interval for a single coefficient. We cannot construct a goodness of fit test in the case that \\(\\phi_0\\) is unknown because the residual degrees of freedom would be used up to estimate \\(\\phi_0\\) rather than to carry out inference."
  },
  {
    "objectID": "parameter-estimation.html#sec-glm-residuals",
    "href": "parameter-estimation.html#sec-glm-residuals",
    "title": "21  Parameter estimation",
    "section": "21.4 Estimation of \\(\\phi_0\\) and GLM residuals",
    "text": "21.4 Estimation of \\(\\phi_0\\) and GLM residuals\nWhile sometimes the parameter \\(\\phi_0\\) is known (e.g., for binomial or Poisson GLMs), in other cases \\(\\phi_0\\) must be estimated (e.g., for the normal linear model). Recall from the linear model that we estimated \\(\\sigma^2 = \\phi_0\\) by taking the sum of the squares of the residuals: \\(\\widehat \\sigma^2 = \\frac{1}{n-p}\\|\\boldsymbol{y} - \\boldsymbol{\\widehat \\mu}\\|^2\\). However, it’s unclear in the GLM context exactly how to define a residual. In fact, there are two common ways of doing so, called deviance residuals and Pearson residuals. Deviance residuals are defined in terms of the unit deviance:\n\\[\n  r^D_i \\equiv \\text{sign}(y_i - \\widehat \\mu_i)\\sqrt{w_i d(y_i, \\widehat \\mu_i)}.\n\\]\nOn the other hand, Pearson residuals are defined as variance-normalized residuals:\n\\[\n  r^P_i \\equiv \\frac{y_i - \\widehat \\mu_i}{\\sqrt{V(\\widehat \\mu_i)/w_i}}.\n\\]\nThese residuals can be viewed as residuals from the (converged) weighted linear regression model (21.8). In the normal case, these residuals coincide, but in the general case, they do not. Based on these two notions of GLM residuals, we can define two estimators of \\(\\phi_0\\). One, based on the deviance residuals, is the mean deviance estimator of dispersion:\n\\[\n\\widetilde \\phi^D_0 \\equiv \\frac{1}{n-p}\\|r^D\\|^2 \\equiv \\frac{1}{n-p}\\sum_{i = 1}^n w_i d(y_i, \\widehat \\mu_i) \\equiv \\frac{1}{n-p}D(\\boldsymbol{y}; \\boldsymbol{\\widehat \\mu});\n\\]\nrecall that the total deviance \\(D(\\boldsymbol{y}; \\boldsymbol{\\widehat \\mu})\\) is a generalization of the residual sum of squares. The other, based on the Pearson residuals, is called the Pearson estimator of dispersion:\n\\[\n\\begin{split}\n\\widetilde \\phi^P_0 &\\equiv \\frac{1}{n-p}X^2 \\\\\n&\\equiv \\frac{1}{n-p}\\|r^P\\|^2 \\\\\n&\\equiv \\frac{1}{n-p}\\sum_{i = 1}^n w_i \\frac{(y_i - \\widehat \\mu_i)^2}{V(\\mu_i)}.\n\\end{split}\n\\tag{21.10}\\]\n\\(X^2\\) is known as the Pearson \\(X^2\\) statistic. The deviance-based estimator can be more accurate than the Pearson estimator under small-dispersion asymptotics. However, the Pearson estimator is more robust when only the first two moments of the EDM model are correct and in the absence of small-dispersion asymptotics. For these reasons, the Pearson estimator is generally preferred."
  }
]