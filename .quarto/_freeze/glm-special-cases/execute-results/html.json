{
  "hash": "d05f1a9026468b8a782e563fd00cb9d0",
  "result": {
    "markdown": "# Generalized linear models: Special cases {#sec-glms-special-cases}\n\nChapter 4 developed a general theory for GLMs. In Chapter 5, we specialize this theory to several important cases, including logistic regression and Poisson regression.\n\n## Logistic regression {#sec-logistic-regression}\n\n### Model definition and interpretation {#sec-logistic-model}\n\n#### Model definition. \nRecall from Chapter 4 that the logistic regression model is\n$$\nm_i y_i \\overset{\\text{ind}} \\sim \\text{Bin}(m_i, \\pi_i); \\quad \\text{logit}(\\pi_i) = \\log\\frac{\\pi_i}{1-\\pi_i} = \\boldsymbol{x}^T_{i*}\\boldsymbol{\\beta}.\n$$\nHere we use the canonical logit link function, although other link functions are possible. We also set the offsets to 0. The interpretation of the parameter $\\beta_j$ is that a unit increase in $x_j$—other predictors held constant—is associated with an (additive) increase of $\\beta_j$ on the log-odds scale or a multiplicative increase of $e^{\\beta_j}$ on the odds scale. Note that logistic regression data come in two formats: *ungrouped* and *grouped*. For ungrouped data, we have $m_1 = \\dots = m_n = 1$, so $y_i \\in \\{0,1\\}$ are Bernoulli random variables. For grouped data, we can have several independent Bernoulli observations per predictor $\\boldsymbol{x}_{i*}$, which give rise to binomial proportions $y_i \\in [0,1]$. This happens most often when all the predictors are discrete. You can always convert grouped data into ungrouped data, but not necessarily vice versa. We'll discuss below that the grouped and ungrouped formulations of logistic regression have the same MLE and standard errors but different deviances.\n\n#### Generative model equivalent.\nConsider the following generative model for $(\\boldsymbol{x}, y) \\in \\mathbb{R}^{p-1} \\times \\{0,1\\}$:\n$$\ny \\sim \\text{Ber}(\\pi); \\quad \\boldsymbol{x}|y \\sim \\begin{cases} N(\\boldsymbol{\\mu}_0, \\boldsymbol{V}) \\quad \\text{if } y = 0 \\\\ N(\\boldsymbol{\\mu}_1, \\boldsymbol{V}) \\quad \\text{if } y = 1 \\end{cases}.\n$$\nThen, we can derive that $y|\\boldsymbol{x}$ follows a logistic regression model (called a *discriminative* model because it conditions on $\\boldsymbol{x}$). Indeed,\n$$\n\\begin{aligned}\n\\text{logit}(p(y = 1|\\boldsymbol{x})) &= \\log\\frac{p(y = 1)p(\\boldsymbol{x}|y = 1)}{p(y = 0)p(\\boldsymbol{x}|y = 0)} \\\\\n&= \\log\\frac{\\pi \\exp\\left(-\\frac12(\\boldsymbol{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{V}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_1)\\right)}{(1-\\pi) \\exp\\left(-\\frac12(\\boldsymbol{x} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{V}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_0)\\right)} \\\\\n&= \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{V}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0) \\\\\n&\\equiv \\beta_0 + \\boldsymbol{x}^T \\boldsymbol{\\beta}_{-0}.\n\\end{aligned}\n$$\nThis is another natural route to motivating the logistic regression model.\n\n#### Special case: $2 \\times 2$ contingency table. {#sec-2x2-contingency}\n\nSuppose that $x \\in \\{0,1\\}$, and consider the logistic regression model $\\text{logit}(\\pi_i) = \\beta_0 + \\beta_1 x_i$. For example, suppose that $x \\in \\{0,1\\}$ encodes treatment (1) and control (0) in a clinical trial, and $y_i \\in \\{0,1\\}$ encodes success (1) and failure (0). We make $n$ observations of $(x_i, y_i)$ in this ungrouped setup. The parameter $e^{\\beta_1}$ can be interpreted as the *odds ratio*:\n$$\ne^{\\beta_1} = \\frac{\\mathbb{P}[y = 1|x=1]/\\mathbb{P}[y = 0|x=1]}{\\mathbb{P}[y = 1|x=0]/\\mathbb{P}[y = 0|x=0]}.\n$$\nThis parameter is the multiple by which the odds of success increase when going from control to treatment. We can summarize such data via the $2 \\times 2$ *contingency table* (@tbl-2-by-2-table). A grouped version of this data would be $\\{(x_1, y_1) = (0, 7/24), (x_2, y_2) = (1, 9/21)\\}$. The null hypothesis $H_0: \\beta_1 = 0 \\Longleftrightarrow H_0: e^{\\beta_1} = 1$ states that the success probability in both rows of the table is the same.\n\n|         | Success | Failure | Total |\n|---------|---------|---------|-------|\n| Treatment| 9       | 12      | 21    |\n| Control  | 7       | 17      | 24    |\n| Total    | 16      | 29      | 45    |\n\n: An example of a $2 \\times 2$ contingency table. {#tbl-2-by-2-table}\n\n### Logistic regression with case-control studies {#sec-logistic-case-control}\n\nIn a prospective study (e.g. a clinical trial), we assign treatment or control (i.e., $x$) to individuals, and then observe a binary outcome (i.e., $y$). Sometimes, the outcome $y$ takes a long time to measure or has a highly imbalanced distribution in the population (e.g., the development of lung cancer). In this case, an appealing study design is the *retrospective study*, where individuals are sampled based on their *response values* (e.g., presence of lung cancer) rather than their treatment/exposure status (e.g., smoking). It turns out that a logistic regression model is appropriate for such retrospective study designs as well. \n\nIndeed, suppose that $y|\\boldsymbol{x}$ follows a logistic regression model. Let's try to figure out the distribution of $y|\\boldsymbol{x}$ in the retrospectively gathered sample. Letting $z \\in \\{0,1\\}$ denote the indicator that an observation is sampled, define $\\rho_1 \\equiv \\mathbb{P}[z = 1|y = 1]$ and $\\rho_0 \\equiv \\mathbb{P}[z = 1|y = 0]$, and assume that $\\mathbb{P}[z = 1, y, \\boldsymbol{x}] = \\mathbb{P}[z = 1 | y]$. The latter assumption states that the predictors $\\boldsymbol{x}$ were not used in the retrospective sampling process. Then,\n\n$$\n\\text{logit}(\\mathbb{P}[y = 1|z = 1, \\boldsymbol{x}]) = \\log \\frac{\\rho_1 \\mathbb{P}[y = 1|\\boldsymbol{x}]}{\\rho_0 \\mathbb{P}[y = 0|\\boldsymbol{x}]} = \\log \\frac{\\rho_1}{\\rho_0} + \\text{logit}(\\mathbb{P}[y = 1|\\boldsymbol{x}]) = \\left(\\log \\frac{\\rho_1}{\\rho_0} + \\beta_0\\right) + \\boldsymbol{x}^T \\boldsymbol{\\beta}_{-0}.\n$$\n\nThus, conditioning on retrospective sampling changes only the intercept term, but preserves the coefficients of $\\boldsymbol{x}$. Therefore, we can carry out inference for $\\boldsymbol{\\beta}_{-0}$ in the same way regardless of whether the study design is prospective or retrospective.\n\n## Estimation and inference {#sec-estimation-inference}\n\n### Score and Fisher information {#sec-score-fisher}\n\nRecall from Chapter 4 that\n\n$$\n\\boldsymbol{U}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} (\\boldsymbol{y} - \\boldsymbol{\\mu}) \\quad \\text{and} \\quad \\boldsymbol{I}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X},\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\boldsymbol{W} &\\equiv \\text{diag}\\left(\\frac{w_i}{V(\\mu_i)(d\\eta_i/d\\mu_i)^2}\\right), \\\\\n\\boldsymbol{M} &\\equiv \\text{diag}\\left(\\frac{\\partial\\eta_i}{\\partial \\mu_i}\\right).\n\\end{aligned}\n$$\n\nSince logistic regression uses a canonical link function, we get the following simplifications:\n\n$$\n\\begin{aligned}\n\\boldsymbol{W} &= \\text{diag}\\left(w_i V(\\mu_i)\\right) = \\text{diag}\\left(m_i \\pi_i(1-\\pi_i)\\right), \\\\\n\\boldsymbol{M} &= \\text{diag}\\left(\\frac{1}{\\pi_i(1-\\pi_i)}\\right).\n\\end{aligned}\n$$\n\nHere we have substituted the notation $\\boldsymbol{\\pi}$ for $\\boldsymbol{\\mu}$, and recall that for logistic regression, $\\phi_0 = 1$, $w_i = m_i$, and $V(\\pi_i) = \\pi_i(1-\\pi_i)$. Therefore, the score equations for logistic regression are\n\n$$\n0 = \\boldsymbol{X}^T \\text{diag}\\left(m_i\\right)(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}) \\quad \\Longleftrightarrow \\quad \\sum_{i = 1}^n m_i x_{ij}(y_i-\\widehat{\\pi}_i) = 0, \\quad j = 0, \\dots, p-1.\n$$ {#eq-logistic-score-equations}\n\nWe can solve these equations using IRLS. The Fisher information is\n\n$$\n\\boldsymbol{I}(\\boldsymbol{\\beta}) = \\boldsymbol{X}^T \\text{diag}\\left(m_i \\pi_i(1-\\pi_i)\\right) \\boldsymbol{X}.\n$$\n\n### Wald inference {#sec-wald-inference}\n\nUsing the results in the previous paragraph, we can carry out Wald inference based on the normal approximation\n\n$$\n\\boldsymbol{\\widehat \\beta} \\overset \\cdot \\sim N\\left(\\boldsymbol \\beta, \\left(\\boldsymbol X^T\\text{diag}(m_i \\widehat \\pi_i(1-\\widehat \\pi_i))\\boldsymbol X\\right)^{-1}\\right).\n$$\n\nThis approximation holds for $\\sum_{i = 1}^n m_i \\rightarrow \\infty$. \n\n#### Example: $2 \\times 2$ contingency table. {#sec-2x2-contingency-table}\n\nSuppose we have a $2 \\times 2$ contingency table. The grouped logistic regression formulation of these data is\n\n$$\ny_0 \\sim \\frac{1}{m_0}\\text{Bin}(m_0, \\pi_0); \\quad y_1 \\sim \\frac{1}{m_1}\\text{Bin}(m_1, \\pi_1); \\quad \\text{logit}(\\pi_i) = \\beta_0 + \\beta_1 x_i.\n$$\n\nIn this case, we have $n = p = 2$, so the grouped logistic regression model is saturated. Therefore, we have\n\n$$\n\\hat \\pi_0 = y_0, \\quad \\text{and} \\quad \\hat \\pi_1 = y_1, \\quad \\text{so} \\quad \\hat \\beta_1 = \\log \\frac{\\hat \\pi_1 / (1 - \\hat \\pi_1)}{\\hat \\pi_0 / (1 - \\hat \\pi_0)} = \\log \\frac{y_1 / (1 - y_1)}{y_0 / (1 - y_0)}.\n$$\n\nThe squared Wald standard error for $\\hat \\beta_1$ is\n\n$$\n\\begin{split}\n\\text{SE}^2(\\widehat \\beta_1) &\\equiv \\left[\\left(\\boldsymbol X^T\\text{diag}(m_i \\widehat \\pi_i(1-\\widehat \\pi_i))\\boldsymbol X\\right)^{-1}\\right]_{22} \\\\\n&= \\left[\\left(\\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}^T\\begin{pmatrix} m_0y_0(1-y_0) & 0 \\\\ 0 & m_1y_1(1-y_1) \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\\right)^{-1}\\right]_{22} \\\\\n&= \\left[\\left(\\begin{pmatrix} m_0 y_0 (1-y_0) + m_1 y_1 (1-y_1) & m_1 y_1(1-y_1) \\\\ m_1 y_1(1-y_1) & m_1 y_1(1-y_1) \\end{pmatrix}\\right)^{-1}\\right]_{22} \\\\\n&= \\frac{m_0 y_0 (1-y_0) + m_1 y_1 (1-y_1)}{m_0y_0(1-y_0) \\cdot m_1y_1(1-y_1)} \\\\\n&= \\frac{1}{m_0y_0(1-y_0)} + \\frac{1}{m_1y_1(1-y_1)}.\n\\end{split}\n$$\n\nTherefore, the Wald test for $H_0: \\beta_1 = 0$ rejects if\n\n$$\n\\left|\\frac{\\hat \\beta_1}{\\text{SE}(\\hat \\beta_1)}\\right| = \\left|\\frac{\\log \\frac{y_1 / (1 - y_1)}{y_0 / (1 - y_0)}}{\\sqrt{\\frac{1}{m_0y_0(1-y_0)} + \\frac{1}{m_1y_1(1-y_1)}}}\\right| > z_{1-\\alpha/2}.\n$$\n\n#### Hauck-Donner effect. {#sec-hauck-donner-effect}\n\nUnfortunately, Wald inference in finite samples does not always perform very well. The Wald test above is known to be conservative if one or more of the mean parameters (in this case, $\\pi_i$) tends to the edge of the parameter space (in this case, $\\pi_i \\rightarrow 0$ or $\\pi_i \\rightarrow 1$). This is called the *Hauck-Donner effect*. As an example, consider testing $H_0: \\beta_0 = 0$ in the intercept-only model\n\n$$\nmy \\sim \\text{Bin}(m, \\pi); \\quad \\text{logit}(\\pi) = \\beta_0.\n$$\n\nThe Wald test statistic is $z \\equiv \\widehat \\beta/\\text{SE} = \\text{logit}(y)\\sqrt{my(1-y)}$. This test statistic actually tends to *decrease* as $y \\rightarrow 1$ (see @fig-hauck-donner), since the standard error grows faster than the estimate itself. So the test statistic becomes less significant as we go further away from the null! A similar situation arises in the $2 \\times 2$ contingency table example above, where the Wald test for $H_0: \\beta_1 = 0$ becomes less significant as $y_0 \\rightarrow 0$ and $y_1 \\rightarrow 1$. As a limiting case of this, the Wald test is undefined if $y_0 = 0$ and $y_1 = 1$. This situation is a special case of *perfect separability* in logistic regression: when a hyperplane in covariate space separates observations with $y_i = 0$ from those with $y_i = 1$. Some of the maximum likelihood coefficient estimates are infinite in this case, causing the Wald test to be undefined since it uses these coefficient estimates as test statistics.\n\n![The Hauck-Donner effect: The Wald statistic for testing $H_0: \\pi = 0.5$ within the model $my \\sim \\text{Bin}(m, \\pi)$ decreases as the proportion $y$ approaches 1. Here, $m = 25$.](figures/hauck-donner.pdf){#fig-hauck-donner width=50%}\n\n### Likelihood ratio inference {#sec-likelihood-ratio-inference}\n\n#### The Bernoulli and binomial deviance. {#sec-bernoulli-binomial-deviance}\n\nLet's first compute the deviance of a Bernoulli or binomial model. These deviances are the same because these two models have the same natural parameter and log-partition function. The unit deviance is\n\n$$\nt(y, \\pi) = y \\log \\pi + (1-y)\\log(1-\\pi).\n$$\n\nHence, we have\n\n$$\nt(y, y) = y \\log y + (1-y) \\log(1-y).\n$$\n\nHence, the unit deviance is\n\n$$\nd(y, \\mu) \\equiv 2(t(y,y)-t(y,\\pi)) = 2\\left(y \\log \\frac{y}{\\pi} + (1-y)\\log \\frac{1-y}{1-\\pi}\\right).\n$$\n\nThe total deviance, therefore, is\n\n$$\nD(\\boldsymbol y, \\hat{\\boldsymbol \\pi}) \\equiv \\sum_{i = 1}^n w_i d(y_i, \\widehat \\pi_i) = 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{y_i}{\\widehat \\pi_i} + m_i(1-y_i) \\log\\frac{1-y_i}{1-\\widehat \\pi_i}\\right).\n$$ {#eq-logistic-deviance}\n\n#### Comparing the deviances of grouped and ungrouped logistic regression models. {#sec-comparing-deviances}\n\nLet us pause to compare the total deviances of grouped and ungrouped logistic regression models. Consider the following grouped and ungrouped models:\n\n$$\ny^{\\text{grp}}_i \\overset{\\text{ind}} \\sim \\frac{1}{m_i}\\text{Bin}(m_i, \\pi_i) \\quad \\text{and} \\quad y^{\\text{ungrp}}_{ik} \\overset{\\text{ind}} \\sim \\text{Ber}(\\pi_i), \\quad k = 1, \\dots, m_i, \\quad \\text{where} \\quad \\text{logit}(\\pi_i) = \\boldsymbol x_{i*}^T \\boldsymbol \\beta.\n$$\n\nThe relationship between the grouped and ungrouped observations is that\n\n$$\ny^{\\text{grp}}_i = \\frac{1}{m_i}\\sum_{k = 1}^{m_i} y^{\\text{ungrp}}_{ik} \\equiv \\bar y^{\\text{ungrp}}_i.\n$$\n\nSince the grouped and ungrouped logistic regression models have the same likelihoods, it follows that they have the same maximum likelihood estimates $\\widehat{\\boldsymbol \\beta}$ and $\\widehat{\\boldsymbol \\pi}$. However, the total deviances of the two models are different. The total deviance of the grouped model is given by ([-@eq-logistic-deviance]):\n\n$$\nD(\\boldsymbol y^{\\text{grp}}, \\hat{\\boldsymbol \\pi}) = 2\\sum_{i = 1}^n \\left(m_i y^{\\text{grp}}_i \\log \\frac{y^{\\text{grp}}_i}{\\widehat \\pi_i} + m_i(1-y^{\\text{grp}}_i) \\log\\frac{1-y^{\\text{grp}}_i}{1-\\widehat \\pi_i}\\right).\n$$ {#eq-total-deviance-grouped}\n\nOn the other hand, the total deviance of the ungrouped model is\n\n$$\n\\begin{split}\nD(\\boldsymbol y^{\\text{ungrp}}, \\hat{\\boldsymbol \\pi}) &= 2\\sum_{i = 1}^n \\sum_{k = 1}^{m_i} \\left(y^{\\text{ungrp}}_{ik} \\log \\frac{y^{\\text{ungrp}}_{ik}}{\\widehat \\pi_i} + (1-y^{\\text{ungrp}}_{ik}) \\log\\frac{1-y^{\\text{ungrp}}_{ik}}{1-\\widehat \\pi_i}\\right) \\\\\n&= 2\\sum_{i = 1}^n \\sum_{k = 1}^{m_i} \\left(y^{\\text{ungrp}}_{ik} \\log \\frac{1}{\\widehat \\pi_i} + (1-y^{\\text{ungrp}}_{ik}) \\log\\frac{1}{1-\\widehat \\pi_i}\\right) \\\\\n&= 2\\sum_{i = 1}^n \\left(m_i y^{\\text{grp}}_i \\log \\frac{1}{\\widehat \\pi_i} + m_i(1-y^{\\text{grp}}_i) \\log\\frac{1}{1-\\widehat \\pi_i}\\right).\n\\end{split}\n$$ {#eq-total-deviance-ungrouped}\n\nIn the second line, we used the fact that $y \\log y \\rightarrow 0$ and $(1-y)\\log(1-y) \\rightarrow 0$ as $y \\rightarrow 0$ or $y \\rightarrow 1$. Comparing the grouped ([-@eq-total-deviance-grouped]) and ungrouped ([-@eq-total-deviance-ungrouped]) total deviances, we see that these are given by related, but different expressions. Because small dispersion asymptotics applies to the grouped model but not the ungrouped model, we have that\n\n$$\n\\text{under small-dispersion asymptotics,} \\quad D(\\boldsymbol y^{\\text{grp}}, \\hat{\\boldsymbol \\pi}) \\overset \\cdot \\sim \\chi^2_{n-p} \\quad \\text{but} \\quad D(\\boldsymbol y^{\\text{ungrp}}, \\hat{\\boldsymbol \\pi}) \\not \\sim \\chi^2_{n-p}.\n$$\n\n#### Likelihood ratio inference for one or more coefficients. {#sec-likelihood-ratio-test}\n\nLetting $\\boldsymbol{\\widehat \\pi}_0$ and $\\boldsymbol{\\widehat \\pi}_1$ be the MLEs from two nested models, we can then express the likelihood ratio statistic as\n\n$$\nD(\\boldsymbol y, \\boldsymbol{\\widehat \\pi}_0) - D(\\boldsymbol y, \\boldsymbol{\\widehat \\pi}_1) = 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{\\widehat \\pi_{i1}}{\\widehat \\pi_{i0}} + m_i(1-y_i) \\log\\frac{1-\\widehat \\pi_{i1}}{1-\\widehat \\pi_{i0}}\\right).\n$$\n\nNote that this expression holds for grouped or ungrouped logistic regression models. We can then construct a likelihood ratio test in the usual way. Likelihood ratio inference can be justified by either large-sample or small-dispersion asymptotics.\n\n## Goodness of fit testing {#sec-goodness-of-fit}\n\nIn grouped logistic regression, we can also use the likelihood ratio test to test goodness of fit. To do so, we compare the total deviance of the fitted model [-@eq-logistic-deviance] to a chi-squared quantile. In particular, the deviance-based goodness of fit test rejects when:\n\n$$\nD(\\boldsymbol{y}, \\hat{\\boldsymbol{\\pi}}) = 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{y_i}{\\widehat \\pi_i} + m_i(1-y_i) \\log\\frac{1-y_i}{1-\\widehat \\pi_i}\\right) > \\chi^2_{n-p}(1-\\alpha).\n$$ {#eq-goodness-of-fit}\n\nThis test is justified by small-dispersion asymptotics based on the saddlepoint approximation, which is decent when $\\min(m_i \\pi_i, (1-m_i)\\pi_i) \\geq 3$ for each $i$.\n\n## Example: $2 \\times 2$ table {#sec-example-2x2-table}\n\nLet us revisit the example of the $2 \\times 2$ table model, within which we would like to test $H_0: \\beta_1 = 0$. Note that we can view this as a goodness of fit test of the intercept-only model in a grouped logistic regression model since the alternative model is the saturated model (it has two observations and two parameters). To compute the likelihood ratio statistic, we first need to fit the intercept-only model. The score equations [-@eq-logistic-score-equations] reduce to:\n\n$$\nm_0 (y_0 - \\hat \\pi) + m_1 (y_1 - \\hat \\pi) = 0 \\quad \\Longrightarrow \\quad \\hat \\pi_0 = \\hat \\pi_1 = \\hat \\pi = \\frac{m_0 y_0 + m_1 y_1}{m_0 + m_1}.\n$$\n\nTherefore, the deviance-based test of $H_0: \\beta_1 = 0$ rejects when:\n\n$$\n\\begin{split}\nD(\\boldsymbol{y}, \\boldsymbol{\\widehat \\pi}) &= 2\\sum_{i = 1}^n \\left(m_i y_i \\log \\frac{y_i}{\\widehat \\pi_i} + m_i(1-y_i) \\log\\frac{1-y_i}{1-\\widehat \\pi_i}\\right) \\\\\n&= \\left(m_0 y_0 \\log\\frac{y_0}{\\hat \\pi} + m_0(1-y_0)\\log\\frac{1-y_0}{1-\\hat \\pi}\\right) + \\left(m_1 y_1 \\log\\frac{y_1}{\\hat \\pi} + m_1(1-y_1)\\log\\frac{1-y_1}{1-\\hat \\pi}\\right) \\\\\n&> \\chi^2_{1}(1-\\alpha).\n\\end{split}\n$$\n\nLikelihood ratio inference can give nontrivial conclusions in cases when Wald inference cannot, e.g. in the case of perfect separability. In the above example, suppose $y_0 = 0$ and $y_1 = 1$, giving perfect separability. Then, we can use the fact that $y \\log y \\rightarrow 0$ and $(1-y)\\log(1-y) \\rightarrow 0$ as $y \\rightarrow 0$ or $y \\rightarrow 1$ to see that:\n\n$$\nD(\\boldsymbol{y}, \\boldsymbol{\\widehat \\pi}) = 2\\left(m_0 \\log\\frac{1}{1-\\hat \\pi} + m_1 \\log\\frac{1}{\\hat \\pi}\\right) = 2\\left(m_0 \\log \\frac{m_0 + m_1}{m_0} + m_1 \\log \\frac{m_0 + m_1}{m_1}\\right).\n$$ {#eq-perfect-separability}\n\nThis gives us a finite value, which we can compare to $\\chi^2_{1}(1-\\alpha)$ to test $H_0: \\beta_1 = 0$. Even though the likelihood ratio statistic is still defined, we do still have to be careful because the data may suggest that the parameters are too close to the boundary of the parameter space. However, the rate at which the test breaks down as the parameters approach this boundary is slower than the rate at which the Wald test breaks down.\n\n### Score-based inference {#sec-score-based-inference}\n\nHere we present only the score-based goodness-of-fit test. Recalling Section @sec-score-tests-1, the score statistic for goodness of fit is Pearson's $X^2$ statistic:\n\n$$\nX^2 = \\sum_{i = 1}^n \\frac{w_i (y_i - \\widehat \\mu_i)^2}{V(\\widehat \\mu_i)} = \\sum_{i = 1}^n \\frac{m_i(y_i - \\widehat \\pi_i)^2}{\\widehat \\pi_i(1-\\widehat \\pi_i)}.\n$$ {#eq-pearson-statistic}\n\nThis test is justified by small-dispersion asymptotics based on the central limit theorem, which is decent when $\\min(m_i \\pi_i, (1-m_i)\\pi_i) \\geq 5$ for each $i$.\n\n### Fisher's exact test {#sec-fisher-exact-test}\n\nAs an alternative to asymptotic tests for logistic regression, in the case of $2 \\times 2$ tables, there is an *exact* test of $H_0: \\beta_1 = 0$. Suppose we have:\n\n$$\ns_1 = m_1y_1 \\sim \\text{Bin}(m_1, \\pi_1) \\quad \\text{and} \\quad s_2 = m_2y_2 \\sim \\text{Bin}(m_2, \\pi_2).\n$$ {#eq-binomial}\n\nThe trick is to conduct inference *conditional on* $s_1 + s_2$. Note that under $H_0: \\pi_1 = \\pi_2$, we have:\n\n$$\n\\begin{split}\n\\mathbb{P}[s_1 = t | s_1+s_2 = v] &= \\mathbb{P}[s_1 = t | s_1 + s_2 = v] \\\\\n&= \\frac{\\mathbb{P}[s_1 = t, s_2 = v-t]}{\\mathbb{P}[s_1 + s_2 = v]} \\\\\n&= \\frac{{m_1 \\choose t}\\pi^{t}(1-\\pi)^{m_1 - t}{m_2 \\choose v-t}\\pi^{v-t}(1-\\pi)^{m_2 - (v-t)}}{{m_1 + m_2 \\choose v}\\pi^v (1-\\pi)^{m_1 + m_2 - v}} \\\\\n&= \\frac{{m_1 \\choose t}{m_2 \\choose v-t}}{{m_1 + m_2 \\choose v}}.\n\\end{split}\n$$ {#eq-exact-test}\n\nTherefore, a finite-sample $p$-value to test $H_0: \\pi_1 = \\pi_2$ versus $H_1: \\pi_1 > \\pi_2$ is $\\mathbb{P}[s_1 \\geq t | s_1 + s_2]$, which can be computed exactly based on the formula above.\n\n# Poisson regression {#sec-poisson-regression}\n\n## Model definition and interpretation {#sec-model-definition}\n\nThe Poisson regression model (with offsets) is:\n\n$$\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\log \\mu_i = o_i + \\boldsymbol{x}_{i*}^T \\boldsymbol{\\beta}.\n$$ {#eq-poisson-with-offsets}\n\nBecause the log of the mean is linear in the predictors, Poisson regression models are often called *loglinear models*. To interpret the coefficients, note that a unit increase in $x_j$ (while keeping the other variables fixed) is associated with an increase in the predicted mean by a factor of $e^{\\beta_j}$.\n\n## Example: Modeling rates {#sec-example-modeling-rates}\n\nOne cool feature of the Poisson model is that rates can be easily modeled with the help of offsets. Let's say that the count $y_i$ is collected over the course of a time interval of length $t_i$, or a spatial region with area $t_i$, or a population of size $t_i$, etc. Then, it is meaningful to model:\n\n$$\ny_i \\overset{\\text{ind}} \\sim \\text{Poi}(\\pi_i t_i), \\quad \\log \\pi_i = \\boldsymbol{x}^T_{i*}\\boldsymbol{\\beta},\n$$\n\nwhere $\\pi_i$ represents the rate of events per day / per square mile / per capita, etc. In other words:\n\n$$\ny_i \\overset{\\text{ind}} \\sim \\text{Poi}(\\mu_i), \\quad \\log \\mu_i = \\log t_i + \\boldsymbol{x}^T_{i*}\\boldsymbol{\\beta},\n$$\n\nwhich is exactly equation [-@eq-poisson-with-offsets] with offsets $o_i = \\log t_i$. For example, in single-cell RNA-sequencing, $y_i$ is the number of reads aligning to a gene in cell $i$ and $t_i$ is the total number of reads measured in the cell, a quantity called the *sequencing depth*. We might use a Poisson regression model to carry out a *differential expression analysis* between two cell types.\n\n## Estimation and inference {#sec-estimation-inference}\n\n### Score, Fisher information, and Wald inference {#sec-wald-inference}\n\nWe found in Chapter @ch-glm-theory that the score and Fisher information for Poisson regression are:\n\n$$\n\\boldsymbol{U}(\\boldsymbol{ \\beta}) = \\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{\\mu}),\n$$\n\nand:\n\n$$\n\\boldsymbol{I}(\\boldsymbol{\\beta}) = \\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X} = \\boldsymbol{X}^T \\text{diag}(V(\\mu_i))\\boldsymbol{X} = \\boldsymbol{X}^T \\text{diag}(\\mu_i)\\boldsymbol{X},\n$$\n\nrespectively. Hence, the normal equations for the MLE are:\n\n$$\n\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{\\hat \\mu}).\n$$\n\nWald inference is based on the approximation:\n\n$$\n\\boldsymbol{\\hat \\beta} \\overset \\cdot \\sim N(\\boldsymbol{\\beta}, (\\boldsymbol{X}^T \\text{diag}(\\hat \\mu_i)\\boldsymbol{X})^{-1}).\n$$\n\n### Likelihood ratio inference {#sec-likelihood-ratio-inference}\n\nFor likelihood ratio inference, we first derive the total deviance. The unit deviance of a Poisson distribution is:\n\n$$\nd(y, \\mu) = y \\log \\frac{y}{\\mu} - (y - \\mu).\n$$\n\nHence, the total deviance is:\n\n$$\nD(\\boldsymbol{y}, \\boldsymbol{\\mu}) = \\sum_{i = 1}^n d(y_i, \\mu_i) = \\sum_{i = 1}^n \\left(y_i \\log \\frac{y_i}{\\mu_i} - (y_i - \\mu_i)\\right).\n$$\n\nThe residual deviance is then:\n\n$$\nD(\\boldsymbol{y}, \\boldsymbol{\\hat\\mu}) = \\sum_{i = 1}^n \\left(y_i \\log \\frac{y_i}{\\hat \\mu_i} - (y_i - \\hat \\mu_i)\\right) = \\sum_{i = 1}^n y_i \\log \\frac{y_i}{\\hat \\mu_i}.\n$$\n\nThe last equality holds for any model containing the intercept, since by the normal equations we have $\\sum_{i = 1}^n (y_i - \\hat \\mu_i) = \\boldsymbol{1}^T (\\boldsymbol{y} - \\boldsymbol{\\hat \\mu}) = 0$. We can carry out a likelihood ratio test for $H_0: \\boldsymbol{\\beta_S} = \\boldsymbol{0}$ via:\n\n$$\nD(\\boldsymbol{y}, \\boldsymbol{\\hat \\mu^0}) - D(\\boldsymbol{y}, \\boldsymbol{\\hat \\mu}) = \\sum_{i = 1}^n y_i \\log \\frac{\\hat \\mu_i}{\\hat \\mu^0_{i}} \\overset{\\cdot}\\sim \\chi^2_{|S|}.\n$$\n\nWe can carry out a goodness-of-fit test via:\n\n$$\nD(\\boldsymbol{y}, \\boldsymbol{\\hat\\mu}) = \\sum_{i = 1}^n y_i \\log \\frac{y_i}{\\hat \\mu_i} \\overset{\\cdot}\\sim \\chi^2_{n - p}.\n$$\n\n### Score-based inference {#sec-score-based-inference}\n\nRecalling equation [-@eq-score-test-univariate], the score test for $H_0: \\beta_j = 0$ is based on:\n\n$$\n\\frac{\\boldsymbol{x}_{j*}^T (\\boldsymbol{y} - \\boldsymbol{\\widehat \\mu}^0)}{\\sqrt{\\left([(\\boldsymbol{X}^T \\text{diag}(\\hat \\mu^0_{i})\\boldsymbol{X})^{-1}]_{jj}\\right)^{-1}}} \\overset{\\cdot}\\sim N(0, 1).\n$$\n\nOn the other hand, the score test for goodness-of-fit is based on the Pearson $X^2$ statistic:\n\n$$\nX^2 \\equiv \\sum_{i = 1}^n \\frac{(y_i - \\hat \\mu_i)^2}{\\hat \\mu_i} \\overset{\\cdot}\\sim \\chi^2_{n - p}.\n$$\n\n## Relationship between Poisson and multinomial distributions {#sec-poisson-multinomial}\n\nSuppose that $y_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i)$ for $i = 1, \\dots, n$. Then,\n\n$$\n\\begin{split}\n\\mathbb{P}\\left[y_1 = m_1, \\dots, y_n = m_n \\left| \\sum_{i}y_i = m\\right.\\right] &= \\frac{\\mathbb{P}[y_1 = m_1, \\dots, y_n = m_n]}{\\mathbb{P}[\\sum_{i}y_i = m]} \\\\\n&= \\frac{\\prod_{i = 1}^n e^{-\\mu_i}\\frac{\\mu_i^{m_i}}{m_i!}}{e^{-\\sum_i \\mu_i}\\frac{(\\sum_i \\mu_i)^m}{m!}} \\\\\n&= {m \\choose m_1, \\dots, m_n} \\prod_{i = 1}^n \\pi_i^{m_i}; \\quad \\pi_i \\equiv \\frac{\\mu_i}{\\sum_{i' = 1}^n \\mu_{i'}}.\n\\end{split}\n$$\n\nWe recognize the last expression as the probability mass function of the multinomial distribution with parameters $(\\pi_1, \\dots, \\pi_n)$ summing to one. In words, the joint distribution of a set of independent Poisson distributions conditional on their sum is a multinomial distribution.\n\n## Example: $2 \\times 2$ contingency tables {#sec-example-2x2-tables}\n\n### Poisson model for $2 \\times 2$ contingency tables {#sec-poisson-2x2}\n\nLet's say that we have two binary random variables $x_1, x_2 \\in \\{0,1\\}$ with joint distribution $\\mathbb{P}(x_1 = j, x_2 = k) = \\pi_{jk}$ for $j,k \\in \\{0,1\\}$. We collect a total of $n$ samples from this joint distribution and summarize the counts in a $2 \\times 2$ table, where $y_{jk}$ is the number of times we observed $(x_1, x_2) = (j,k)$, so that:\n\n$$\n(y_{00}, y_{01}, y_{10}, y_{11})|n \\sim \\text{Mult}(n, (\\pi_{00}, \\pi_{01}, \\pi_{10}, \\pi_{11})).\n$$\n\nOur primary question is whether these two random variables are independent, i.e.\n\n$$\n\\pi_{jk} = \\pi_{j+}\\pi_{+k}, \\quad \\text{where} \\quad \\pi_{j+} \\equiv \\mathbb{P}[x_1 = j] = \\pi_{j1} + \\pi_{j2}; \\quad \\pi_{+k} \\equiv \\mathbb{P}[x_2 = k] = \\pi_{1k} + \\pi_{2k}.\n$$ {#eq-null-product-formulation}\n\nWe can express this equivalently as:\n\n$$\n\\pi_{00}\\pi_{11} = \\pi_{01}\\pi_{10}.\n$$\n\nIn other words, we can express the independence hypothesis concisely as:\n\n$$\nH_0: \\frac{\\pi_{11}\\pi_{00}}{\\pi_{10}\\pi_{01}} = 1.\n$$ {#eq-independence-2}\n\nLet's arbitrarily assume that, additionally, $n \\sim \\text{Poi}(\\mu_{++})$. Then, by the relationship between Poisson and multinomial distributions, we have:\n\n$$\ny_{jk} \\overset{\\text{ind}} \\sim \\text{Poi}(\\mu_{++}\\pi_{jk}).\n$$\n\nLet $i \\in \\{1,2,3,4\\}$ index the four pairs $(x_1, x_2) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}$, so that:\n\n$$\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_{12}x_{i1} x_{i2}, \\quad i = 1, \\dots, 4,\n$$ {#eq-2x2-poisson-reg}\n\nwhere:\n\n$$\n\\beta_0 = \\log \\mu_{++} + \\log \\pi_{00}; \\quad \\beta_1 = \\log \\frac{\\pi_{10}}{\\pi_{00}}; \\quad \\beta_2 = \\log \\frac{\\pi_{01}}{\\pi_{00}}; \\quad \\beta_{12} = \\log\\frac{\\pi_{11}\\pi_{00}}{\\pi_{10}\\pi_{01}}.\n$$ {#eq-2x2-poisson-reg-coefs}\n\nNote that the independence hypothesis [-@eq-independence-2] reduces to the hypothesis $H_0: \\beta_{12} = 0$:\n\n$$\nH_0: \\frac{\\pi_{11}\\pi_{00}}{\\pi_{10}\\pi_{01}} = 1 \\quad \\Longleftrightarrow \\quad H_0: \\beta_{12} = 0.\n$$\n\nSo the presence of an interaction in the Poisson regression is equivalent to a lack of independence between $x_1$ and $x_2$. We can test the latter hypothesis using our standard tools for Poisson regression.\n\nFor example, we can use the Pearson $X^2$ goodness-of-fit test. To apply this test, we must find the fitted means under the null hypothesis model:\n\n$$\ny_i \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_i); \\quad \\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}, \\quad i = 1, \\dots, 4.\n$$ {#eq-2x2-poisson-reg-null}\n\nThe normal equations give us the following:\n\n$$\ny_{++} \\equiv \\sum_{j, k = 0}^1 y_{jk} = \\sum_{j, k = 0}^1 \\hat \\mu_{jk} \\equiv \\hat \\mu_{++}; \\quad y_{+1} \\equiv \\sum_{j = 0}^1 y_{j1} = \\sum_{j = 0}^1 \\hat \\mu_{j1} \\equiv \\hat \\mu_{+1}; \\quad y_{1+} \\equiv \\sum_{k = 0}^1 y_{1k} = \\sum_{k = 0}^1 \\hat \\mu_{1k} \\equiv \\hat \\mu_{1+}.\n$$\n\nBy combining these equations, we arrive at:\n\n$$\n\\hat \\mu_{++} = y_{++}; \\quad \\hat \\mu_{j+} = y_{j+} \\text{ for all } j \\in \\{0, 1\\}; \\quad \\hat \\mu_{+k} = y_{+k} \\text{ for all } k \\in \\{0, 1\\}.\n$$\n\nTherefore, the fitted means under the null hypothesis model [-@eq-2x2-poisson-reg-null] are:\n\n$$\n\\hat \\mu_{jk} = \\hat \\mu_{++}\\hat \\pi_{jk} = \\hat \\mu_{++}\\hat \\pi_{j+}\\hat \\pi_{+k} = y_{++}\\frac{y_{j+}}{y_{++}}\\frac{y_{+k}}{y_{++}} = \\frac{y_{j+}y_{+k}}{y_{++}}.\n$$\n\nHence, we have:\n\n$$\nX^2 = \\sum_{j,k = 0}^1 \\frac{(y_{jk} - \\widehat \\mu_{jk})^2}{\\widehat \\mu_{jk}} = \\sum_{j, k = 0}^1 \\frac{(y_{jk} - y_{j+}y_{+k}/y_{++})^2}{y_{j+}y_{+k}/y_{++}}.\n$$\n\nAlternatively, we can use the likelihood ratio test, which gives:\n\n$$\nG^2 = 2\\sum_{j,k = 0}^1 y_{jk}\\log\\frac{y_{jk}}{\\widehat \\mu_{jk}} = 2\\sum_{j, k = 0}^1 y_{jk}\\log\\frac{y_{jk}}{y_{j+}y_{+k}/y_{++}}.\n$$\n\nWe would compare both $X^2$ and $G^2$ to a $\\chi^2_1$ distribution.\n\n### Inference is the same regardless of conditioning on margins {#sec-inference-conditioning-margins}\n\nNow, our data might actually have been collected such that $n \\sim \\text{Poi}(\\mu_{++})$, or maybe $n$ was fixed in advance. Is the Poisson inference proposed above actually valid in the latter case? In fact, it is! To see this, let us consider the log likelihoods of the two models:\n\n$$\np_{\\boldsymbol{\\mu}}(\\boldsymbol{y}) = p_{\\mu_{++}}(y_{++} = n)p_{\\boldsymbol{\\pi}}(\\boldsymbol{y} | y_{++} = n),\n$$\n\nso:\n\n$$\n\\log p_{\\boldsymbol{\\mu}}(\\boldsymbol{y}) = \\log p_{\\mu_{++}}(y_{++} = n) + \\log p_{\\boldsymbol{\\pi}}(\\boldsymbol{y} | y_{++} = n) = C + \\log p_{\\boldsymbol{\\pi}}(\\boldsymbol{y} | y_{++} = n).\n$$\n\nIn other words, the log-likelihoods of the Poisson and multinomial models, as a function of $\\boldsymbol{\\pi}$, differ from each other by a constant. Therefore, any likelihood-based inference in these models is equivalent. The same argument shows that conditioning on the row or column totals (as opposed to the overall total) also yields the same exact inference. Therefore, regardless of the sampling mechanism, we can always conduct an independence test in a $2 \\times 2$ table via a Poisson regression.\n\n### Equivalence among Poisson and logistic regressions {#sec-poisson-logistic-equivalence}\n\nWe've talked about two ways to view a $2 \\times 2$ contingency table. In the logistic regression view, we thought about one variable as a predictor and the other as a response, seeking to test whether the predictor has an impact on the response. In the Poisson regression view, we thought about the two variables symmetrically, seeking to test independence. It turns out that these two perspectives are equivalent. Recall that we have derived in equations [-@eq-2x2-poisson-reg] and [-@eq-2x2-poisson-reg-coefs] that $x_1 \\perp \\!\\!\\! \\perp x_2$ if and only if $\\beta_{12} = 0$ in the Poisson regression:\n\n$$\n\\log y_i \\overset{\\text{ind}}{\\sim} \\text{Poi}(\\mu_i), \\quad \\log \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_{12} x_{i1}x_{i2}, \\quad i = 1, \\dots, 4.\n$$\n\nHowever, we have:\n\n$$\n\\beta_{12} = \\log \\frac{\\pi_{11}\\pi_{00}}{\\pi_{01}\\pi_{10}} = \\log \\frac{\\pi_{11}/\\pi_{01}}{\\pi_{01}/\\pi_{00}} = \\log \\frac{\\mathbb{P}[x_2 = 1 \\mid x_1 = 1] / \\mathbb{P}[x_2 = 0 \\mid x_1 = 1]}{\\mathbb{P}[x_2 = 1 \\mid x_1 = 0] / \\mathbb{P}[x_2 = 0 \\mid x_1 = 0]}.\n$$\n\nRecalling the logistic regression of $x_2$ on $x_1$:\n\n$$\n\\text{logit } \\mathbb{P}[x_2 = 1 \\mid x_1] = \\tilde{\\beta_0} + \\tilde{\\beta_1} x_1,\n$$ {#eq-2x2-logistic-reg}\n\nand that $\\tilde{\\beta_1}$ is the log odds ratio, we conclude that:\n\n$$\n\\beta_{12} = \\tilde{\\beta_1},\n$$\n\nso $x_1 \\perp \\!\\!\\! \\perp x_2$ if and only if $\\tilde{\\beta_1} = 0$. Due to the equivalence between Poisson and multinomial distributions, the hypothesis tests and confidence intervals for the log odds ratio $\\beta_{12}$ (or $\\tilde{\\beta_1}$) obtained from Poisson and logistic regressions will be the same.\n\n## Example: Poisson models for $J \\times K$ contingency tables {#sec-poisson-jk-tables}\n\nSuppose now that $x_1 \\in \\{1, \\dots, J\\}$ and $x_2 \\in \\{1, \\dots, K\\}$. Then, we denote $\\mathbb{P}[x_1 = j, x_2 = k] = \\pi_{jk}$. We still are interested in testing for independence between $j$ and $k$, which amounts to a goodness-of-fit test for the Poisson model:\n\n$$\ny_{jk} \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_{jk}); \\quad \\log \\mu_{jk} = \\beta_0 + \\beta^1_j + \\beta^2_k.\n$$\n\nThe score (Pearson) and deviance-based goodness-of-fit statistics for this test are:\n\n$$\nX^2 = \\sum_{j = 1}^J \\sum_{k = 1}^K \\frac{(y_{jk} - \\widehat \\mu_{jk})^2}{\\widehat \\mu_{jk}} \\quad \\text{and} \\quad G^2 = 2\\sum_{j = 1}^J \\sum_{k = 1}^K y_{jk}\\log \\frac{y_{jk}}{\\widehat \\mu_{jk}},\n$$\n\nwhere $\\widehat \\mu_{jk} = \\widehat y_{++}\\frac{y_{j+}}{y_{++}}\\frac{y_{+k}}{y_{++}}$. Like with the $2 \\times 2$ case, the test is the same regardless of whether we condition on the row sums, column sums, total count, or if we do not condition at all. The degrees of freedom in the full model is $JK$, while the degrees of freedom in the partial model is $J + K - 1$, so the degrees of freedom for the goodness-of-fit test is $JK - J - K + 1 = (J - 1)(K - 1)$. Pearson erroneously concluded that the test had $JK - 1$ degrees of freedom, which, when Fisher corrected it, created a lot of animosity between these two statisticians.\n\n## Example: Poisson models for $J \\times K \\times L$ contingency tables {#sec-poisson-jkl-tables}\n\nThese ideas can be extended to multi-way tables, for example, three-way tables. If we have $x_1 \\in \\{1, \\dots, J\\}$, $x_2 \\in \\{1, \\dots, K\\}$, $x_3 \\in \\{1, \\dots, L\\}$, then we might be interested in testing several kinds of null hypotheses:\n\n- Mutual independence: $H_0: x_1 \\perp \\!\\!\\! \\perp x_2 \\perp \\!\\!\\! \\perp x_3$.\n- Joint independence: $H_0: x_1 \\perp \\!\\!\\! \\perp (x_2, x_3)$.\n- Conditional independence: $H_0: x_1 \\perp \\!\\!\\! \\perp x_2 \\mid x_3$.\n\nThese three null hypotheses can be shown to be equivalent to the Poisson regression model:\n\n$$\ny_{jkl} \\overset{\\text{ind}}\\sim \\text{Poi}(\\mu_{jkl}),\n$$\n\nwhere:\n\n$$\n\\log \\mu_{jkl} = \\beta_0 + \\beta^1_j + \\beta^2_k + \\beta^3_l \\quad \\text{(mutual independence)};\n$$\n\n$$\n\\log \\mu_{jkl} = \\beta_0 + \\beta^1_j + \\beta^2_k + \\beta^3_l + \\beta^{2,3}_{kl} \\quad \\text{(joint independence)};\n$$\n\n$$\n\\log \\mu_{jkl} = \\beta_0 + \\beta^1_j + \\beta^2_k + \\beta^3_l + \\beta^{1,3}_{jl} + \\beta^{2,3}_l \\quad \\text{(conditional independence)}.\n$$\n\n## Negative binomial regression {#sec-nb-regression}\n\n### Overdispersion\n\nA pervasive issue with Poisson regression is *overdispersion*: that the variances of observations are greater than the corresponding means. A common cause of overdispersion is omitted variable bias. Suppose that $y \\sim \\text{Poi}(\\mu)$, where $\\log \\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$. However, we omitted variable $x_2$ and are considering the GLM based on $\\log \\mu = \\beta_0 + \\beta_1 x_1$. If $\\beta_2 \\neq 0$ and $x_2$ is correlated with $x_1$, then we have a confounding issue. Let's consider the more benign situation that $x_2$ is independent of $x_1$. Then, we have\n\n$$\n\\mathbb{E}[y|x_1] = \\mathbb{E}[\\mathbb{E}[y|x_1, x_2]|x_1] = \\mathbb{E}[e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}|x_1] = e^{\\beta_0 + \\beta_1 x_1}\\mathbb{E}[e^{\\beta_2 x_2}] = e^{\\beta'_0 + \\beta_1 x_1}.\n$$ {#eq-mean-variance}\n\nSo in the model for the mean of $y$, the impact of omitted variable $x_2$ seems only to have impacted the intercept. Let's consider the variance of $y$:\n\n$$\n\\text{Var}[y|x_1] = \\mathbb{E}[\\text{Var}[y|x_1, x_2]|x_1] + \\text{Var}[\\mathbb{E}[y|x_1, x_2]|x_1] = e^{\\beta'_0 + \\beta_1 x_1} + e^{2(\\beta'_0 + \\beta_1 x_1)}\\text{Var}[e^{\\beta_2 x_2}] > e^{\\beta'_0 + \\beta_1 x_1} = \\mathbb{E}[y|x_1].\n$$ {#eq-var-greater-mean}\n\nSo indeed, the variance is larger than what we would have expected under the Poisson model.\n\n### Hierarchical Poisson regression\n\nLet's say that $y|\\boldsymbol{x} \\sim \\text{Poi}(\\lambda)$, where $\\lambda|\\boldsymbol{x}$ is random due to the fluctuations of the omitted variables. A common distribution used to model nonnegative random variables is the *gamma* distribution $\\Gamma(\\mu, k)$, parameterized by a mean $\\mu > 0$ and a *shape* $k > 0$. This distribution has probability density function\n\n$$\nf(\\lambda; k, \\mu) = \\frac{(k/\\mu)^k}{\\Gamma(k)}e^{-k\\lambda/\\mu}\\lambda^{k-1},\n$$ {#eq-gamma-pdf}\n\nwith mean and variance given by\n\n$$\n\\mathbb{E}[\\lambda] = \\mu; \\quad \\text{Var}[\\lambda] = \\mu^2/k.\n$$ {#eq-gamma-moments}\n\nTherefore, it makes sense to augment the Poisson regression model as follows:\n\n$$\n\\lambda|\\boldsymbol{x} \\sim \\Gamma(\\mu, k), \\quad \\log \\mu = \\boldsymbol{x}^T \\boldsymbol{\\beta}, \\quad y | \\lambda \\sim \\text{Poi}(\\lambda).\n$$ {#eq-nb-hierarchical}\n\n### Negative binomial distribution\n\nA simpler way to write the hierarchical model [-@eq-nb-hierarchical] would be to marginalize out $\\lambda$. Doing so leaves us with a count distribution called the *negative binomial distribution*:\n\n$$\n\\lambda \\sim \\Gamma(\\mu, k),\\  y | \\lambda \\sim \\text{Poi}(\\lambda) \\quad \\Longrightarrow \\quad y \\sim \\text{NegBin}(\\mu, k).\n$$ {#eq-nb-definition}\n\nThe negative binomial probability mass function is\n\n$$\np(y; \\mu, k) = \\int_0^\\infty \\frac{(k/\\mu)^k}{\\Gamma(k)}e^{-k\\lambda/\\mu}\\lambda^{k-1}e^{-\\lambda}\\frac{\\lambda^y}{y!}d\\lambda = \\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left(\\frac{\\mu}{\\mu + k}\\right)^{y}\\left(\\frac{k}{\\mu + k}\\right)^{k}.\n$$ {#eq-nb-pmf}\n\nThis random variable has mean and variance given by\n\n$$\n\\mathbb{E}[y] = \\mathbb{E}[\\lambda] = \\mu \\quad \\text{and} \\quad \\text{Var}[y] = \\mathbb{E}[\\lambda] + \\text{Var}[\\lambda] = \\mu + \\frac{\\mu^2}{k}.\n$$ {#eq-nb-moments}\n\nAs we send $k \\rightarrow \\infty$, the distribution of $\\lambda$ tends to a point mass and the negative binomial distribution tends to $\\text{Poi}(\\mu)$.\n\n### Negative binomial as exponential dispersion model\n\nLet us see whether we can express the negative binomial model as an exponential dispersion model. First, let us write out the probability mass function:\n\n$$\np(y; \\mu, k) = \\exp\\left(y \\log \\frac{\\mu}{\\mu + k} - k \\log \\frac{\\mu + k}{k}\\right)\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}.\n$$ {#eq-nb-edm}\n\nUnfortunately, we run into difficulties expressing this probability mass function in EDM form, because there is not a neat decoupling between the natural parameter and the dispersion parameter. Indeed, for unknown $k$, the negative binomial model is *not* an EDM. However, we can still express the negative binomial model as an EDM (in fact, a one-parameter exponential family) if we treat $k$ as known. In particular, we can read off that\n\n$$\n\\theta = \\log \\frac{\\mu}{\\mu + k}, \\quad \\psi(\\theta) = k\\log \\frac{\\mu + k}{k} = -k\\log(1-e^{\\theta}).\n$$ {#eq-neg-bin-exp-fam}\n\nAn alternate parameterization of the negative binomial model is via $\\gamma = 1/k$. With this parameterization, we have\n\n$$\n\\mathbb{E}[y] = \\mu \\quad \\text{and} \\quad \\text{Var}[y] = \\mu + \\gamma \\mu^2.\n$$ {#eq-nb-alt-param}\n\nHere, $\\gamma$ acts as a kind of dispersion parameter, as the variance of $y$ grows with $\\gamma$. Note that the relationship between $\\text{Var}[y]$ and $\\gamma$ is not exactly proportional, as it is in EDMs. Nevertheless, the $\\gamma$ parameter is often called the negative binomial *dispersion*. Note that setting $\\gamma = 0$ recovers the Poisson distribution.\n\n### Negative binomial regression\n\nLet's revisit the hierarchical model [-@eq-nb-hierarchical], writing it more succinctly in terms of the negative binomial distribution:\n\n$$\ny_i \\overset{\\text{ind}}{\\sim} \\text{NegBin}(\\mu_i, \\gamma), \\quad \\log \\mu_i = \\boldsymbol{x}^T \\boldsymbol{\\beta}.\n$$ {#eq-nb-regression}\n\nNotice that we typically assume that all observations share the same dispersion parameter $\\gamma$. Reading off from equation [-@eq-neg-bin-exp-fam], we see that the canonical link function for the negative binomial distribution is $\\mu \\mapsto \\log \\frac{\\mu}{\\mu + k}$. However, typically for negative binomial regression we use the log link $g(\\mu) = \\log \\mu$ instead. This is the link of Poisson regression, and leads to more interpretable coefficient estimates. This is our first example of a non-canonical link!\n\n### Score and Fisher information\n\nRecall from Chapter 4 that\n\n$$\n\\boldsymbol{U}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{M} \\boldsymbol{W} (\\boldsymbol{y} - \\boldsymbol{\\mu}) \\quad \\text{and} \\quad \\boldsymbol{I}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi_0}\\boldsymbol{X}^T \\boldsymbol{W} \\boldsymbol{X},\n$$ {#eq-score-fisher-info}\n\nwhere\n\n$$\n\\boldsymbol{W} \\equiv \\text{diag}\\left(\\frac{w_i}{V(\\mu_i)(d\\eta_i/d\\mu_i)^2}\\right) \\quad \\text{and} \\quad \\boldsymbol{M} \\equiv \\text{diag}\\left(\\frac{\\partial\\eta_i}{\\partial \\mu_i}\\right).\n$$ {#eq-w-m-matrix}\n\nIn our case, we have\n\n$$\nw_i = 1; \\quad V(\\mu_i) = \\mu_i + \\gamma \\mu_i^2; \\quad \\frac{\\partial\\eta_i}{\\partial \\mu_i} = \\frac{1}{\\mu_i}.\n$$ {#eq-w-v-deriv}\n\nPutting this together, we find that\n\n$$\n\\boldsymbol{W} = \\text{diag}\\left(\\frac{\\mu_i}{1 + \\gamma \\mu_i}\\right); \\quad \\boldsymbol{M} = \\text{diag}\\left(\\frac{1}{1 + \\gamma \\mu_i}\\right).\n$$ {#eq-w-m-simplified}\n\n### Estimation in negative binomial regression\n\nNegative binomial regression is an EDM when $\\gamma$ is known, but typically the dispersion parameter is unknown. Note that there is a dependency in $\\psi$ on $k$ (i.e. on $\\gamma$), which complicates things. It means that the estimate $\\boldsymbol{\\widehat{\\beta}}$ depends on the parameter $\\gamma$ (this does not happen, for example, in the normal linear model case).^[Having said that, the dependency between $\\boldsymbol{\\widehat{\\beta}}$ and $\\widehat{\\gamma}$ is weak, as the two are asymptotically independent parameters.] Therefore, estimation in negative binomial regression is typically an iterative procedure, where at each step $\\boldsymbol{\\beta}$ is estimated for the current value of $\\gamma$ and then $\\gamma$ is estimated based on the updated value of $\\boldsymbol{\\beta}$. Let's discuss each of these tasks in turn. Given a value of $\\widehat{\\gamma}$, we have the normal equations:\n\n$$\n\\boldsymbol{X}^T \\text{diag}\\left(\\frac{1}{1 + \\widehat{\\gamma} \\widehat{\\mu}_i}\\right)(\\boldsymbol{y} - \\boldsymbol{\\widehat{\\mu}}) = 0.\n$$ {#eq-nb-normal-eq}\n\nThis reduces to the Poisson normal equations when $\\widehat{\\gamma} = 0$. Solving these equations for a fixed value of $\\widehat{\\gamma}$ can be done via IRLS, as usual. Estimating $\\gamma$ for a fixed value of $\\boldsymbol{\\widehat{\\beta}}$ can be done in several ways, including setting to zero the derivative of the likelihood with respect to $\\gamma$. This results in a nonlinear equation (not given here) that can be solved iteratively.\n\n### Wald inference\n\nWald inference is based on\n\n$$\n\\widehat{\\text{Var}}[\\boldsymbol{\\widehat{\\beta}}] = (\\boldsymbol{X}^T \\boldsymbol{\\widehat{W}} \\boldsymbol{X})^{-1}, \\quad \\text{where} \\quad \\boldsymbol{\\widehat{W}} = \\text{diag}\\left(\\frac{\\widehat{\\mu}_i}{1 + \\widehat{\\gamma} \\widehat{\\mu}_i}\\right).\n$$ {#eq-wald-inference}\n\n### Likelihood ratio test inference\n\nThe negative binomial deviance is\n\n$$\nD(\\boldsymbol{y}; \\boldsymbol{\\widehat{\\mu}}) = 2\\sum_{i = 1}^n \\left(y_i \\log \\frac{y_i}{\\widehat{\\mu}_i} - \\left(y_i + \\frac{1}{\\widehat{\\gamma}}\\right)\\log \\frac{1 + \\widehat{\\gamma} y_i}{1 + \\widehat{\\gamma} \\widehat{\\mu}_i}\\right).\n$$ {#eq-nb-deviance}\n\nWe can use this for comparing nested models, **but not for goodness of fit testing!** The issue is that we have estimated the parameter $\\gamma$, whereas goodness of fit tests are applicable only when the dispersion parameter is known.\n\n### Testing for overdispersion\n\nIt is reasonable to want to test for overdispersion, i.e., to test the null hypothesis $H_0: \\gamma = 0$. This is somewhat of a tricky task because $\\gamma = 0$ is at the edge of the parameter space. We can do so using a likelihood ratio test. As it turns out, the likelihood ratio statistic $T^{\\text{LRT}}$ has asymptotic null distribution\n\n$$\nT^{\\text{LRT}} \\equiv 2(\\ell^{\\text{NB}} - \\ell^{\\text{Poi}}) \\overset \\cdot \\sim \\frac{1}{2}\\delta_0 + \\frac{1}{2}\\chi^2_1.\n$$ {#eq-lrt-null}\n\nHere, $\\delta_0$ is the delta mass at zero. The reason for this is that, under the null, we can view the estimated dispersion parameter as being symmetrically distributed around 0. However, since the dispersion parameter is nonnegative, this means it gets rounded up to 0 with probability 1/2. Therefore, the likelihood ratio test for $H_0: \\gamma = 0$ rejects when\n\n$$\nT^{\\text{LRT}} > \\chi^2_1(1-2\\alpha).\n$$ {#eq-lrt-reject}\n\nNote that the above test for overdispersion can be viewed as a goodness of fit test for the Poisson GLM. It is different from the usual GLM goodness of fit tests, because the saturated model against which the latter tests stays in the Poisson family. Nevertheless, significant results in standard goodness of fit tests for Poisson GLMs are often an indication of overdispersion. Or, they may indicate omitted variable bias (e.g., you forgot to include an interaction), so it's somewhat tricky.\n\n### Overdispersion in logistic regression\n\nNote that overdispersion is potentially an issue not only in Poisson regression models but in logistic regression models as well. Dealing with overdispersion in the latter case is more tricky, because the analog of the negative binomial model (the beta-binomial model) is not an exponential family. An alternate route to dealing with overdispersion is quasi-likelihood modeling, but this topic is beyond the scope of the course.\n\n## R demo {#sec-r-demo}\n\n### Contingency table analysis {#sec-contingency-table}\n\nLet's take a look at the UC Berkeley admissions data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(tidyr)\n\nucb_data <- UCBAdmissions |> as_tibble()\nucb_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 × 4\n   Admit    Gender Dept      n\n   <chr>    <chr>  <chr> <dbl>\n 1 Admitted Male   A       512\n 2 Rejected Male   A       313\n 3 Admitted Female A        89\n 4 Rejected Female A        19\n 5 Admitted Male   B       353\n 6 Rejected Male   B       207\n 7 Admitted Female B        17\n 8 Rejected Female B         8\n 9 Admitted Male   C       120\n10 Rejected Male   C       205\n# ℹ 14 more rows\n```\n:::\n:::\n\n\nIt contains data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex. Let's see whether there is an association between `Gender` and `Admit`. Let's first aggregate over department:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nucb_data_agg <- ucb_data |>\n  group_by(Admit, Gender) |>\n  summarise(n = sum(n), .groups = \"drop\")\nucb_data_agg\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  Admit    Gender     n\n  <chr>    <chr>  <dbl>\n1 Admitted Female   557\n2 Admitted Male    1198\n3 Rejected Female  1278\n4 Rejected Male    1493\n```\n:::\n:::\n\n\nLet's see what the admissions rates are by gender:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nucb_data_agg |>\n  group_by(Gender) |>\n  summarise(`Admission rate` = sum(n*(Admit == \"Admitted\"))/sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  Gender `Admission rate`\n  <chr>             <dbl>\n1 Female            0.304\n2 Male              0.445\n```\n:::\n:::\n\n\nThis suggests that men have substantially higher admission rates than women. Let's see if we can confirm this using either a Fisher's exact test or a Pearson chi-square test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first convert to 2x2 table format\nadmit_vs_gender <- ucb_data_agg |>\n  pivot_wider(names_from = Gender, values_from = n) |>\n  column_to_rownames(var = \"Admit\")\nadmit_vs_gender\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Female Male\nAdmitted    557 1198\nRejected   1278 1493\n```\n:::\n\n```{.r .cell-code}\n# Fisher exact test (note that the direction of the effect can be deduced)\nfisher.test(admit_vs_gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  admit_vs_gender\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.4781839 0.6167675\nsample estimates:\nodds ratio \n 0.5432254 \n```\n:::\n\n```{.r .cell-code}\n# Chi-square test\nchisq.test(admit_vs_gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  admit_vs_gender\nX-squared = 91.61, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\nAs a sanity check, let's run the Poisson regression underlying the chi-square test above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit <- glm(n ~ Admit + Gender + Admit*Gender,\n                family = \"poisson\",\n                data = ucb_data_agg)\nsummary(pois_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = n ~ Admit + Gender + Admit * Gender, family = \"poisson\", \n    data = ucb_data_agg)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               6.32257    0.04237 149.218   <2e-16 ***\nAdmitRejected             0.83049    0.05077  16.357   <2e-16 ***\nGenderMale                0.76584    0.05128  14.933   <2e-16 ***\nAdmitRejected:GenderMale -0.61035    0.06389  -9.553   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  4.8635e+02  on 3  degrees of freedom\nResidual deviance: -3.4062e-13  on 0  degrees of freedom\nAIC: 43.225\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nBased on all of these tests, there seems to be a very substantial difference in admissions rates based on gender. That is not good.\n\nBut perhaps, women tend to apply to more selective departments? Let's look into this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nucb_data |>\n  group_by(Dept) |>\n  summarise(admissions_rate = sum(n*(Admit == \"Admitted\"))/sum(n),\n            prop_female_applicants = sum(n*(Gender == \"Female\"))/sum(n)) |>\n  ggplot(aes(x = admissions_rate, y = prop_female_applicants)) +\n  geom_point() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Admissions rate\",\n       y = \"Proportion female applicants\")\n```\n\n::: {.cell-output-display}\n![](glm-special-cases_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=288}\n:::\n:::\n\n\nIndeed, it does seem that female applicants typically applied to more selective departments! This suggests that it is very important to control for department when evaluating the association between admissions and gender. To do this, we can run a test for conditional independence in the $J \\times K \\times L$ table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit <- glm(n ~ Admit + Dept + Gender + Admit:Dept + Gender:Dept,\n                family = \"poisson\",\n                data = ucb_data)\npchisq(sum(resid(pois_fit, \"pearson\")^2),\n  df = pois_fit$df.residual,\n  lower.tail = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.002840164\n```\n:::\n:::\n\n\nStill we find a significant effect! But what is the direction of the effect? The chi-square test does not tell us. We can simply compute the admissions rates by department and plot them:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nucb_data |>\n  group_by(Dept, Gender) |>\n  summarise(`Admission rate` = sum(n*(Admit == \"Admitted\"))/sum(n),\n            .groups = \"drop\") |>\n  pivot_wider(names_from = Gender, values_from = `Admission rate`) |>\n  ggplot(aes(x = Female, y = Male, label = Dept)) +\n  geom_point() +\n  ggrepel::geom_text_repel() +\n  geom_abline(color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(x = \"Female admission rate\",\n       y = \"Male admission rate\")\n```\n\n::: {.cell-output-display}\n![](glm-special-cases_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=288}\n:::\n:::\n\n\nNow the difference doesn't seem so huge, with most departments close to even and with department A heavily skewed towards admitting women!\n\n### Revisiting the crime data, again {#sec-crime-data}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\nHere we are again, face to face with the crime data, with one last chance to get the analysis right. Let's load and preprocess it, as before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read crime data\ncrime_data <- read_tsv(\"data/Statewide_crime.dat\")\n\n# read and transform population data\npopulation_data <- read_csv(\"data/state-populations.csv\")\npopulation_data <- population_data |>\n  filter(State != \"Puerto Rico\") |>\n  select(State, Pop) |>\n  rename(state_name = State, state_pop = Pop)\n\n# collate state abbreviations\nstate_abbreviations <- tibble(\n  state_name = state.name,\n  state_abbrev = state.abb\n) |>\n  add_row(state_name = \"District of Columbia\", state_abbrev = \"DC\")\n\n# add CrimeRate to crime_data\ncrime_data <- crime_data |>\n  mutate(STATE = ifelse(STATE == \"IO\", \"IA\", STATE)) |>\n  rename(state_abbrev = STATE) |>\n  filter(state_abbrev != \"DC\") |> # remove outlier\n  left_join(state_abbreviations, by = \"state_abbrev\") |>\n  left_join(population_data, by = \"state_name\") |>\n  select(state_abbrev, Violent, Metro, HighSchool, Poverty, state_pop)\n\ncrime_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 6\n   state_abbrev Violent Metro HighSchool Poverty state_pop\n   <chr>          <dbl> <dbl>      <dbl>   <dbl>     <dbl>\n 1 AK               593  65.6       90.2     8      724357\n 2 AL               430  55.4       82.4    13.7   4934193\n 3 AR               456  52.5       79.2    12.1   3033946\n 4 AZ               513  88.2       84.4    11.9   7520103\n 5 CA               579  94.4       81.3    10.5  39613493\n 6 CO               345  84.5       88.3     7.3   5893634\n 7 CT               308  87.7       88.8     6.4   3552821\n 8 DE               658  80.1       86.5     5.8    990334\n 9 FL               730  89.3       85.9     9.7  21944577\n10 GA               454  71.6       85.2    10.8  10830007\n# ℹ 40 more rows\n```\n:::\n:::\n\n\nLet's recall the logistic regression we ran on these data in Chapter 4:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbin_fit <- glm(Violent / state_pop ~ Metro + HighSchool + Poverty,\n  weights = state_pop,\n  family = \"binomial\",\n  data = crime_data\n)\n```\n:::\n\n\nWe had found very poor results from the goodness of fit test for this model. We have therefore omitted some important variables and/or we have serious overdispersion on our hands.\n\nWe haven't discussed in any detail how to deal with overdispersion in logistic regression models, so let's try a Poisson model instead. The natural way to model rates using Poisson distributions is via offsets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit <- glm(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),\n  family = \"poisson\",\n  data = crime_data\n)\nsummary(pois_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)), \n    family = \"poisson\", data = crime_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.609e+01  3.520e-01  -45.72   <2e-16 ***\nMetro       -2.585e-02  5.727e-04  -45.15   <2e-16 ***\nHighSchool   9.106e-02  3.450e-03   26.39   <2e-16 ***\nPoverty      6.077e-02  4.852e-03   12.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 15589  on 49  degrees of freedom\nResidual deviance: 11741  on 46  degrees of freedom\nAIC: 12135\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\nAgain, everything is significant, and again, the regression summary shows that we have a huge residual deviance. This was to be expected, given that $\\text{Bin}(m, \\pi) \\approx \\text{Poi}(m\\pi)$ for large $m$ and small $\\pi$. So, the natural thing to try is a negative binomial regression! Negative binomial regression is not implemented in the regular `glm` package, but `glm.nb()` from the `MASS` package is a dedicated function for this task. Let's see what we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_fit <- MASS::glm.nb(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),\n  data = crime_data\n)\nsummary(nb_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nMASS::glm.nb(formula = Violent ~ Metro + HighSchool + Poverty + \n    offset(log(state_pop)), data = crime_data, init.theta = 1.467747388, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -10.254088   5.273418  -1.944   0.0518 .\nMetro        -0.012188   0.008518  -1.431   0.1525  \nHighSchool    0.028052   0.052482   0.535   0.5930  \nPoverty      -0.026852   0.068449  -0.392   0.6948  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.4677) family taken to be 1)\n\n    Null deviance: 59.516  on 49  degrees of freedom\nResidual deviance: 55.487  on 46  degrees of freedom\nAIC: 732.58\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.468 \n          Std. Err.:  0.268 \n\n 2 x log-likelihood:  -722.575 \n```\n:::\n:::\n\n\nAha! Things are not looking so significant anymore! And the residual deviance is not as huge! Although, we must be careful! The residual deviance no longer has the usual $\\chi^2$ distribution because of the estimated dispersion parameter. So we don't really have an easy goodness of fit test. The estimated value of $\\gamma$ (confusingly called $\\theta$ in the summary) is significantly different from zero, indicating overdispersion. Let's formally test for overdispersion using the nonstandard likelihood ratio test discussed above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nT_LRT <- 2 * (as.numeric(logLik(nb_fit)) - as.numeric(logLik(pois_fit)))\np_LRT <- pchisq(T_LRT, df = 1, lower.tail = FALSE)/2\np_LRT\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nSo at the very least the NB model fits much better than the Poisson model. Let's do some inference based on this model. For example, we can get Wald confidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint.default(nb_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   2.5 %      97.5 %\n(Intercept) -20.58979658 0.081620714\nMetro        -0.02888413 0.004507747\nHighSchool   -0.07481066 0.130915138\nPoverty      -0.16100973 0.107305015\n```\n:::\n:::\n\n\nOr we can get LRT-based (i.e. profile) confidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(nb_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                   2.5 %       97.5 %\n(Intercept) -19.20209590 -0.860399348\nMetro        -0.03153902  0.006365841\nHighSchool   -0.06265118  0.115318303\nPoverty      -0.13930110  0.085200541\n```\n:::\n:::\n\n\nOr we can get confidence intervals for the predicted means:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(nb_fit,\n  newdata = crime_data |> column_to_rownames(var = \"state_abbrev\"),\n  type = \"response\",\n  se.fit = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$fit\n       AK        AL        AR        AZ        CA        CO        CT        DE \n 116.1520  617.7064  375.4895  700.6931 3257.5300  725.1538  436.7863  127.2572 \n       FL        GA        HI        ID        IL        IN        IA        KS \n2232.2308 1301.2937  157.1416  263.8572 1379.1847  954.3366  546.5503  439.0649 \n       KY        LA        MA        MD        ME        MI        MN        MO \n 541.5706  391.6745  747.7454  737.0032  274.2879 1322.9956  970.4078  871.2829 \n       MS        MT        NC        ND        NE        NH        NJ        NM \n 380.6756  199.4947 1313.0904  134.8128  305.0634  261.1975  966.9940  204.3311 \n       NV        NY        OH        OK        OR        PA        RI        SC \n 327.7316 1926.3861 1477.1713  495.9711  517.8397 1600.0813   96.3565  684.9102 \n       SD        TN        TX        UT        VA        VT        WA        WI \n 160.9225  867.0224 2423.0647  416.6648 1244.5168  148.1635 1012.1932  892.0644 \n       WV        WY \n 226.4515  100.1906 \n\n$se.fit\n       AK        AL        AR        AZ        CA        CO        CT        DE \n 21.00552 143.65071 130.44272 165.08459 910.57769 121.34777  85.53768  32.15169 \n       FL        GA        HI        ID        IL        IN        IA        KS \n427.89514 173.04544  31.73873  40.28262 239.43324 147.21049 104.05752  68.82044 \n       KY        LA        MA        MD        ME        MI        MN        MO \n133.28938 129.40665 150.23524 158.93816  92.04222 171.28409 216.32477 110.88843 \n       MS        MT        NC        ND        NE        NH        NJ        NM \n138.28105  65.60335 379.90855  26.74061  69.62560  66.73731 220.88371  59.26953 \n       NV        NY        OH        OK        OR        PA        RI        SC \n 64.30971 387.25204 241.24541  95.44911  81.97419 220.42078  33.97964 119.45174 \n       SD        TN        TX        UT        VA        VT        WA        WI \n 41.50215 169.68896 738.95321 107.62725 209.14651  51.32810 191.75629 137.35158 \n       WV        WY \n 71.55328  22.79279 \n\n$residual.scale\n[1] 1\n```\n:::\n:::\n\n\nWe can carry out some hypothesis tests as well, e.g. to test $H_0: \\beta_{\\text{Metro}} = 0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_fit_partial <- MASS::glm.nb(Violent ~ HighSchool + Poverty + offset(log(state_pop)),\n  data = crime_data\n)\nanova_fit <- anova(nb_fit_partial, nb_fit)\nanova_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: Violent\n                                                  Model    theta Resid. df\n1         HighSchool + Poverty + offset(log(state_pop)) 1.428675        47\n2 Metro + HighSchool + Poverty + offset(log(state_pop)) 1.467747        46\n     2 x log-lik.   Test    df LR stat.   Pr(Chi)\n1       -724.1882                                \n2       -722.5753 1 vs 2     1 1.612878 0.2040877\n```\n:::\n:::\n",
    "supporting": [
      "glm-special-cases_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}