geom_line() +
geom_point() +
theme_bw() +
theme(legend.title = element_blank())
# average across test points to get overall results
overall_results = training_results_summary %>%
group_by(K) %>%
summarise(mean_sq_bias = mean(bias^2),
mean_variance = mean(variance)) %>%
mutate(expected_test_error = mean_sq_bias + mean_variance + sigma^2)
# compute bias, variance, and ETE for each test point
training_results_summary = training_results %>%
mutate(true_fit = f(X1,X2)) %>%
pivot_longer(-c(X1,X2,true_fit, resample),
names_to = "K",
names_prefix = "y_hat_",
names_transform = list(K = as.integer),
values_to = "yhat") %>%
group_by(K, X1, X2) %>%
summarise(bias = mean(yhat - true_fit),
variance = var(yhat)) %>%
ungroup()
balance = c(1000, 500, 2000, 750, 1500)
default = c(1, 0, 1, 1, 0)
data = tibble(balance, default)
library(tidyverse)
data = tibble(balance, default)
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + theme_bw()
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) + theme_bw()
balance = c(1250, 500, 2000, 750, 1500)
data = tibble(balance, default)
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) + theme_bw()
balance = c(1250, 500, 2000, 1250, 1500)
data = tibble(balance, default)
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) + theme_bw()
balance = c(1250, 500, 2000, 1750, 1500)
data = tibble(balance, default)
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) + theme_bw()
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, colour = "red") + theme_bw()
glm_fit = glm(default ~ balance, data = data, family = "binomial")
glm_fit$fitted.values
data %>% cbind(fitted = glm_fit$fitted.values) %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + ggrepel::geom_text_repel(aes(label = fitted), colour = "red") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, colour = "red") + theme_bw()
data %>% cbind(fitted = round(glm_fit$fitted.values, 2)) %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + ggrepel::geom_text_repel(aes(label = fitted), colour = "red") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, colour = "red") + theme_bw()
balance
default
default = c(0,0,1,1,1)
data = tibble(balance, default)
data %>% ggplot(aes(x = balance, y = default)) + geom_point(colour = "dodgerblue") + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, colour = "red") + theme_bw()
default_data = ISLR2::Default %>% as_tibble()
default_data
default_data = default_data %>% mutate(default = as.numeric(default == "Yes"))
default_data
train_samples = sample(1:nrow(default_data), 0.8*nrow(default_data))
default_train = default_data %>% filter(row_number() %in% train_samples)
default_test = default_data %>% filter(!(row_number() %in% train_samples))
glm_fit = glm(default ~ student + balance + income,
family = "binomial",
data = default_train)
summary(glm_fit)
coef(glm_fit)
fitted_probabilities = predict(glm_fit,
newdata = tibble(student = "No",
balance = 1000,
income = 50000),
type = "response")
fitted_probabilities
fitted_probabilities = predict(glm_fit,
newdata = default_train,
type = "response")                # to get output on probability scale
fitted_probabilities
head(fitted_probabilities)
predictions = fitted_probabilities > 0.5
head(predictions)
predictions = as.numeric(fitted_probabilities > 0.5)
head(predictions)
fitted_probabilities = predict(glm_fit,
newdata = default_test,
type = "response")                # to get output on probability scale
head(fitted_probabilities)
predictions = as.numeric(fitted_probabilities > 0.5)
head(predictions)
default_test = default_test %>% mutate(predicted = predictions, est_prob = fitted_probabilities)
default_test
# first add predictions and estimated probabilities to the tibble
default_test = default_test %>%
mutate(predicted_default = predictions,
est_prob = fitted_probabilities)
default_test
# then calculate misclassification rate
default_test %>%
summarise(mean(default == predicted_default))
# then calculate misclassification rate
default_test %>%
summarise(mean(default != predicted_default))
default_test %>%
select(default, predicted_default) %>%
table()
# ROC curve
roc_data = roc(default_test %>% pull(default),
fitted_probabilities)
library(pROC)        # for ROC curves
# ROC curve
roc_data = roc(default_test %>% pull(default),
fitted_probabilities)
tibble(FPR = 1-roc_data$specificities,
TPR = roc_data$sensitivities) %>%
ggplot(aes(x = FPR, y = TPR)) +
geom_line() +
geom_abline(slope = 1, linetype = "dashed") +
geom_point(x = fpr, y = 1-fnr, colour = "red") +
theme_bw()
tibble(FPR = 1-roc_data$specificities,
TPR = roc_data$sensitivities) %>%
ggplot(aes(x = FPR, y = TPR)) +
geom_line() +
geom_abline(slope = 1, linetype = "dashed") +
#  geom_point(x = fpr, y = 1-fnr, colour = "red") +
theme_bw()
# print the AUC
roc_data$auc
# ROC curve
roc_data = roc(default_test %>% pull(default),
fitted_probabilities)
tibble(FPR = 1-roc_data$specificities,
TPR = roc_data$sensitivities) %>%
ggplot(aes(x = FPR, y = TPR)) +
geom_line() +
geom_abline(slope = 1, linetype = "dashed") +
#  geom_point(x = fpr, y = 1-fnr, colour = "red") +
theme_bw()
default_train %>%
ggplot(aes(x = balance, y = default))+
geom_jitter(height = .05) +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
se = FALSE) +
ylab("Prob(default=1)") +
theme_bw()
default_train %>%
ggplot(aes(x = balance, y = default))+
geom_jitter(height = .05) +
geom_smooth(method = "glm",
formula = "y~x",
method.args = list(family = "binomial"),
se = FALSE) +
ylab("Prob(default=1)") +
theme_bw()
options(scipen = 0, digits = 3)  # controls number of significant digits printed
set.seed(1) # for reproducibility (DO NOT CHANGE)
tree_fit = rpart(spam ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
control = rpart.control(minsplit = 1, minbucket = 1, cp = 0),
data = spam_train)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
spam_data = read_tsv("../../data/spam_data.tsv")
# extract training data
spam_train = spam_data %>%
filter(test == 0) %>%
select(-test)
# extract test data
spam_test = spam_data %>%
filter(test == 1) %>%
select(-test)
spam_train %>% summarise(spam_fraction = mean(spam))
avg_word_freq = spam_train %>%
summarise_at(vars(starts_with("word_freq")), mean)
avg_word_freq
avg_word_freq_long = avg_word_freq %>%
pivot_longer(cols = everything(),
names_to = "word",
names_prefix = "word_freq_",
values_to = "avg_freq")
avg_word_freq_long
avg_word_freq_long %>%
ggplot(aes(x = avg_freq)) +
geom_histogram() + theme_bw()
avg_word_freq_long %>%
arrange(desc(avg_freq)) %>%
head(3)
diff_avg_word_freq = spam_train %>%
group_by(spam) %>%
summarise_at(vars(starts_with("word_freq")), mean) %>%
pivot_longer(cols = -spam,
names_to = "word",
names_prefix = "word_freq_",
values_to = "avg_freq") %>%
pivot_wider(names_from = spam,
names_prefix = "avg_freq_",
values_from = avg_freq) %>%
mutate(diff_avg_freq = avg_freq_1-avg_freq_0) %>%
select(word, diff_avg_freq)
diff_avg_word_freq
diff_avg_word_freq %>%
ggplot(aes(x = diff_avg_freq)) +
geom_histogram() + theme_bw()
diff_avg_word_freq %>%
arrange(desc(diff_avg_freq)) %>%
head(3)
diff_avg_word_freq %>%
arrange(diff_avg_freq) %>%
head(3)
tree_fit = rpart(spam ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
data = spam_train)
rpart.plot(tree_fit)
set.seed(1) # for reproducibility (DO NOT CHANGE)
tree_fit = rpart(spam ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
control = rpart.control(minsplit = 1, minbucket = 1, cp = 0),
data = spam_train)
printcp(tree_fit)
cp_table = printcp(tree_fit) %>% as_tibble()
cp_table
tree_fit
diff_avg_word_freq %>%
arrange(desc(diff_avg_freq)) %>%
head(3)
Heart = read_csv("https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/
master/Notebooks/Data/Heart.csv") %>% select(-X1)
Heart = read_csv("https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/
master/Notebooks/Data/Heart.csv") # %>% select(-X1)
Heart
url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Heart.csv"
Heart = read_csv(url) # %>% select(-X1)
Heart
url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Heart.csv"
Heart = read_csv(url) %>% select(-...1)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)
spam_data = read_tsv("../../data/spam_data.tsv")
# extract training data
spam_train = spam_data %>%
filter(test == 0) %>%
select(-test)
# extract test data
spam_test = spam_data %>%
filter(test == 1) %>%
select(-test)
spam_count = spam_train %>% filter (spam == 1) %>% count()
spam_count
avg_word_freq = spam_train %>%
summarise_at (vars(starts_with("word_freq")), mean, na.rm = TRUE)
avg_word_freq
avg_word_freq_long <- avg_word_freq %>%
pivot_longer (cols = everything(), names_to = "word",
names_prefix = "word_freq_",
values_to = "avg_freq")
avg_word_freq_long
#identify top three most frequent words
avg_word_freq_long %>% arrange(desc(avg_freq))
#plot histogram of the word frequencies
avg_word_freq_long %>%
ggplot (aes(x = avg_freq)) + geom_histogram() +
labs (x = "Word frequencies", y = "Number of words in range of frequencies") +
theme_bw()
diff_avg_word_freq <- spam_train %>% group_by(spam) %>% ## group by spam variable
summarise_at (vars(starts_with("word_freq")), mean, na.rm = TRUE) %>% ## redo the mean calculation above
pivot_longer (cols = 2:49, names_to = "word", ## redo the pivot_longer
names_prefix = "word_freq_",
values_to = "avg_freq")  %>%
pivot_wider(names_from = spam, values_from = avg_freq, names_prefix="avg_freq.") %>% ## reshape the data using spam indicator
mutate(diff_avg_freq = avg_freq.1 - avg_freq.0) %>% ## calculate the difference, where .1 is frequency in spam emails
select(word, diff_avg_freq) ## pick only 2 variables
diff_avg_word_freq
#identify top three most overrepresented in spam emails
diff_avg_word_freq %>% arrange(desc(diff_avg_freq))
#identify top three most underrepresented in spam emails
diff_avg_word_freq %>% arrange(diff_avg_freq)
#plot histogram of the word frequencies
diff_avg_word_freq %>%
ggplot (aes(x = diff_avg_freq)) + geom_histogram() +
labs (x = "Word frequency differences", y = "Number of words in range of frequencies") +
theme_bw()
#control = rpart.control(minsplit = 20, minbucket = round(minsplit/3))
tree_fit = rpart (spam ~ .,
method = "class",
parms = list (split = "gini"),
data = spam_train)
rpart.plot (tree_fit)
set.seed(1) # for reproducibility (DO NOT CHANGE)
tree_fit_2 = rpart (spam ~ .,
method = "class",
parms = list (split = "gini"),
control =rpart.control(minsplit = 1,minbucket=1, cp=0),
data = spam_train)
#rpart.plot (tree_fit_2)
printcp(tree_fit_2) %>%
kable(format = "latex", row.names = NA,
booktabs = TRUE, digits = 2,
col.names = c("Complexity parameter", "Number of splits", "Training Error", "CV estimate", "Cross-validated SD"),
caption = "Complexity parameter table.") %>%
kable_styling(position = "center")
#printcp(tree_fit_2)
cp_table = printcp (tree_fit_2) %>% as_tibble()
cp_table %>% filter (nsplit >=2) %>%
ggplot (aes(x= nsplit+1, y = xerror,
ymin = xerror - xstd,
ymax = xerror+xstd)) + scale_x_log10() +
geom_point () + geom_line() +
geom_errorbar (width = 0.025) +
xlab ("Number of terminal nodes") + ylab ("CV error") +
geom_hline (aes(yintercept = min(xerror)), linetype = "dashed") +
theme_bw()
cp_table %>% filter (xerror - xstd < min(xerror)) %>% arrange(nsplit) %>% head(1)
optimal_tree_info = cp_table %>%
filter (xerror -xstd < min (xerror)) %>%
arrange(nsplit) %>%
head(1)
optimal_tree_info
optimal_tree = prune (tree_fit_2, cp = optimal_tree_info$CP)
set.seed(1) # for reproducibility (DO NOT CHANGE)
rf_fit = randomForest (factor(spam) ~ ., data = spam_train)
rf_fit$mtry
rf_fit$err.rate %>% head()
#plot the OOB error
tibble (oob_error = rf_fit$err.rate[,"OOB"],
trees = 1:500) %>%
ggplot (aes (x = trees, y = oob_error)) + geom_line() +
xlab ("Number of trees") + ylab ("Out-of-bag error")
theme_bw()
system.time(randomForest(spam ~., importance= FALSE, data = spam_train))["elapsed"]
system.time(randomForest(spam ~., importance= TRUE, data = spam_train))["elapsed"]
dim(spam_train)  #57 features (58 variables minus the response variable)
rf_100 = randomForest (factor(spam) ~ ., ntree = 100, importance = FALSE,
data = spam_train)
rf_200 = randomForest (factor(spam) ~ ., ntree = 200, importance = FALSE,
data = spam_train)
rf_300 = randomForest (factor(spam) ~ ., ntree = 300, importance = FALSE,
data = spam_train)
rf_400 = randomForest (factor(spam) ~ ., ntree = 400, importance = FALSE,
data = spam_train)
rf_500 = randomForest (factor(spam) ~ ., ntree = 500, importance = FALSE,
data = spam_train)
st_100 = system.time(randomForest (factor(spam) ~ ., ntree = 100, importance = FALSE,
data = spam_train))["elapsed"]   #1.73 seconds
st_200 = system.time(randomForest (factor(spam) ~ ., ntree = 200, importance = FALSE,
data = spam_train))["elapsed"] #3.13 seconds
st_300 = system.time(randomForest (factor(spam) ~ ., ntree = 300, importance = FALSE,
data = spam_train))["elapsed"] #4.47 seconds
st_400 = system.time(randomForest (factor(spam) ~ ., ntree = 400, importance = FALSE,
data = spam_train))["elapsed"] #5.59 seconds
st_500 = system.time(randomForest (factor(spam) ~ ., ntree = 500, importance = FALSE,
data = spam_train))["elapsed"] #7.06 seconds
tibble(ntree = c(st_100, st_200, st_300, st_400, st_500), runtime = c(100, 200, 300, 400, 500)) %>% ggplot (aes (x = ntree, y = runtime)) + geom_point () +
labs(x = "Number of trees",
y = "Running time",
title = "Running time vs. number of trees") +
theme_bw () +
theme(plot.title = element_text(hjust = 0.5))
mvalues = seq.int (1, 57, length.out = 5) #1 15 29 43 57
oob_errors = numeric(length(mvalues))
ntree = 300
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(factor(spam) ~ ., mtry = m, data = spam_train)
oob_errors[idx] = rf_fit$err.rate[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
labs (x = m, y = "Out of Bag Error (OOB Error)") +
scale_x_continuous(breaks = mvalues) +
theme_bw()
set.seed(1) # for reproducibility (DO NOT CHANGE)
rf_fit_tuned = randomForest (factor(spam) ~., importance = TRUE, mtry = 15,
data=spam_train )
tibble(oob_error_new = rf_fit_tuned$err.rate[,"OOB"],
trees = 1:500) %>%
ggplot(aes(x = trees, y = oob_error_new)) + geom_line() +
labs(x = "Number of trees",
y = "Out-of-bag error") +
theme_bw()
varImpPlot(rf_fit_tuned, n.var = 10)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 1
gbm_fit_1 = gbm(spam ~ .,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 2
gbm_fit_2 = gbm(spam ~ .,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 2,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 3
gbm_fit_3 = gbm(spam ~ .,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 2,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
# extract CV errors
ntrees = 1000
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_fit_1$cv.error, depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_fit_2$cv.error, depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_fit_3$cv.error, depth = 3)
)
# plot CV errors
minima = cv_errors %>% group_by (depth) %>%
summarize (min_error = min(cv_err)) %>% mutate (depth = factor(depth))
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() + geom_hline (aes(yintercept = min_error, color = depth), data = minima, linetype = "dashed")+
theme_bw()
gbm.perf(gbm_fit_2)
gbm_fit_tuned = gbm_fit_2
optimal_num_trees = gbm.perf(gbm_fit_2, plot.it = FALSE)
summary(gbm_fit_tuned, n.trees = optimal_num_trees, plotit = FALSE) %>%
head(10) %>%
kable(format = "latex", row.names = NA,
booktabs = TRUE, digits = 2,
col.names = c("Variable Name", "Relative influence"),
caption = "Top ten variables of relative influence for the optimal
boosting model.") %>%
kable_styling (position = "center")
save.image(file='dung-files.RData')
getwd()
tibble(decision_tree = pred, random_forest = rf_predictions, boosting = gbm_probabilities)
options(scipen = 0, digits = 3)  # controls number of significant digits printed
pred = predict(optimal_tree, newdata = spam_test, type = "class")
mean(pred != spam_test$spam)  #misclassification error = 0.0833
rf_predictions = predict(rf_fit_tuned, newdata = spam_test, type = "response")
mean(rf_predictions != spam_test$spam)  #0.0482
gbm_probabilities = predict(gbm_fit_tuned, n.trees = optimal_num_trees,
type = "response",
newdata = spam_test)
gbm_predictions = as.numeric(gbm_probabilities > 0.5) #threshold the probabilities at 0.5
mean(gbm_predictions != spam_test$spam) # calculate the misclassification error = 0.0495
tibble(decision_tree = pred, random_forest = rf_predictions, boosting = gbm_probabilities)
nba_data_raw = read_csv("~/Downloads/2017-18_teamBoxScore.csv")
library(tidyverse)
nba_data_raw = read_csv("~/Downloads/2017-18_teamBoxScore.csv")
nba_data_raw
nba_data_raw %>% count(seasTyp)
nba_data_raw %>% select(teamAbbr, teamConf, teamLoc, teamRslt)
nba_data_raw %>% select(teamAbbr, teamConf, teamLoc, teamRslt) %>% mutate(game = ceiling(row_number()/2))
nba_data_raw %>% select(teamAbbr, teamConf, teamLoc, teamRslt) %>% mutate(game = ceiling(row_number()/2)) %>% relocate(game)
nba_data_raw %>% select(teamAbbr, teamConf, teamLoc, teamRslt) %>% mutate(game = ceiling(row_number()/2)) %>% relocate(game) %>% rename(team = teamAbbr, conference = teamConf, location = teamLoc, result = teamRslt)
nba_data_raw %>% select(teamAbbr, teamConf, teamLoc, teamRslt) %>% mutate(game = ceiling(row_number()/2)) %>% relocate(game) %>% rename(team = teamAbbr, conference = teamConf, location = teamLoc, result = teamRslt) %>% write_tsv("nba_data.tsv")
getwd()
setwd("~/code/teaching/stat-961-fall-2021/data")
nba_data_raw %>% select(teamAbbr, teamConf, teamLoc, teamRslt) %>% mutate(game = ceiling(row_number()/2)) %>% relocate(game) %>% rename(team = teamAbbr, conference = teamConf, location = teamLoc, result = teamRslt) %>% write_tsv("nba_data.tsv")
nba_data = read_tsv("../../data/nba_data.tsv")
setwd("~/code/teaching/stat-961-fall-2021/homework/homework-5")
nba_data = read_tsv("../../data/nba_data.tsv")
# create predictor data
predictors = nba_data %>%
select(-conference, -result) %>%
pivot_wider(names_from = team, values_from = location) %>%
mutate_at(vars(-game), function(val)(case_when(
val == "Home" ~ 1,
val == "Away" ~ -1,
TRUE ~ 0)))
# create response data
responses = nba_data %>%
filter(location == "Home") %>%
mutate(win = ifelse(result == "Win", 1, 0)) %>%
select(game, win)
# join predictor and response data
nba_data_wide = full_join(responses, predictors, by = "game") %>%
select(-game, -PHO)
bt_fit = glm(win ~ ., family = "binomial", data = nba_data_wide)
records = nba_data %>%
group_by(team, location) %>%
summarise(record = mean(result == "Win")) %>%
ungroup() %>%
pivot_wider(names_from = "location", values_from = "record")
coefficients = coef(bt_fit) %>%
as.data.frame() %>%
rownames_to_column(var = "team") %>%
rename(coefficient = 2) %>%
filter(team != "(Intercept)") %>%
add_row(team = "PHO", coefficient = 0)
table = full_join(records, coefficients, by = "team") %>%
arrange(desc(coefficient))
table %>%
kableExtra::kable(format = "latex", row.names = NA,
booktabs = TRUE, digits = 2) %>%
kableExtra::save_kable("figures/nba-summary.png")
data("jobsatisfaction", package = "coin")
install.packages("coin")
data("jobsatisfaction", package = "coin")
jobsatisfaction
reshape2::melt(jobsatisfaction)
job_satisfaction_data = reshape2::melt(jobsatisfaction) %>% rename(Count = value)
job_satisfaction_data
write_tsv(job_satisfaction_data, file = "job_satisfaction.tsv")
setwd("~/code/teaching/stat-961-fall-2021/data")
write_tsv(job_satisfaction_data, file = "job_satisfaction.tsv")
