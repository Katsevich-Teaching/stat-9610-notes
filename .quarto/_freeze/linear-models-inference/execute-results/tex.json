{
  "hash": "da1e5a88527ca6f3a67e98f4b667bf2d",
  "result": {
    "markdown": "# Linear models: Inference\n\nWe now understand the least squares estimator $\\boldsymbol{\\widehat{\\beta}}$ from geometric and algebraic points of view. In Chapter 2, we will switch to a probabilistic perspective to derive inferential statements for linear models, in the form of hypothesis tests and confidence intervals. In order to facilitate this, we will assume that the error terms are normally distributed:\n\n$$\n\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\quad \\text{where} \\ \\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I}_n).\n$$\n\n## Building blocks for linear model inference\n\n*See also Agresti 3.1.1, 3.1.2, 3.1.4*\n\nFirst we put in place some building blocks: The multivariate normal distribution (Section [-@sec-mvrnorm]), the distributions of linear regression estimates and residuals (Section [-@sec-lin-reg-dist]), and estimation of the noise variance $\\sigma^2$ (Section [-@sec-noise-estimation]).\n\n### The multivariate normal distribution {#sec-mvrnorm}\n\nRecall that a random vector $\\boldsymbol{w} \\in \\mathbb{R}^d$ has a multivariate normal distribution with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ if it has probability density\n\n$$\np(\\boldsymbol{w}) = \\frac{1}{\\sqrt{(2\\pi)^{d}\\text{det}(\\boldsymbol{\\Sigma})}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{w} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{w} - \\boldsymbol{\\mu})\\right).\n$$\n\nThese random vectors have lots of special properties, including:\n\n- **Linear transformation**: If $\\boldsymbol{w} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, then $\\boldsymbol{A} \\boldsymbol{w} + \\boldsymbol{b} \\sim N(\\boldsymbol{A} \\boldsymbol{\\mu} + \\boldsymbol{b}, \\boldsymbol{A} \\boldsymbol{\\Sigma} \\boldsymbol{A}^\\top)$.\n- **Independence**: If \n$$\n\\begin{pmatrix}\\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\end{pmatrix} \\sim N\\left(\\begin{pmatrix}\\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix} , \\begin{pmatrix}\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{12}^\\top & \\boldsymbol{\\Sigma}_{22}\\end{pmatrix}\\right),\n$$ \nthen $\\boldsymbol{w}_1 \\perp\\!\\!\\!\\perp \\boldsymbol{w}_2$ if and only if $\\boldsymbol{\\Sigma}_{12} = \\boldsymbol{0}$.\n\nAn important distribution related to the multivariate normal is the $\\chi^2_d$ (chi-squared with $d$ degrees of freedom) distribution, defined as\n\n$$\n\\chi^2_d \\equiv \\sum_{j = 1}^d w_j^2 \\quad \\text{for} \\quad w_1, \\dots, w_d \\overset{\\text{i.i.d.}}{\\sim} N(0, 1).\n$$\n\n### The distributions of linear regression estimates and residuals {#sec-lin-reg-dist}\n\n*See also Dunn and Smyth 2.8.2*\n\nThe most important distributional result in linear regression is that\n\n$$\n\\boldsymbol{\\widehat{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}).\n$$ {#eq-beta-hat-dist}\n\nIndeed, by the linear transformation property of the multivariate normal distribution,\n\n$$\n\\begin{split}\n\\boldsymbol{y} \\sim N(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2 \\boldsymbol{I}_n) &\\Longrightarrow \\boldsymbol{\\widehat{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{y} \\sim N((\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{X} \\boldsymbol{\\beta}, (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\sigma^2 \\boldsymbol{I}_n \\boldsymbol{X}(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}) \\\\\n&= N(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}).\n\\end{split}\n$$\n\nNext, let's consider the joint distribution of $\\boldsymbol{\\widehat{\\mu}} = \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}$ and $\\boldsymbol{\\widehat{\\epsilon}} = \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}$. We have\n\n$$\n\\begin{split}\n\\begin{pmatrix} \\boldsymbol{\\widehat{\\mu}} \\\\ \\boldsymbol{\\widehat{\\epsilon}} \\end{pmatrix} = \\begin{pmatrix} \\boldsymbol{H} \\boldsymbol{y} \\\\ (\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{y} \\end{pmatrix} = \\begin{pmatrix} \\boldsymbol{H} \\\\ \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\boldsymbol{y} &\\sim N\\left(\\begin{pmatrix} \\boldsymbol{H} \\\\ \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\boldsymbol{X} \\boldsymbol{\\beta}, \\begin{pmatrix} \\boldsymbol{H} \\\\ \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\cdot \\sigma^2 \\boldsymbol{I} \\begin{pmatrix} \\boldsymbol{H} & \\boldsymbol{I} - \\boldsymbol{H} \\end{pmatrix}\\right) \\\\\n&= N\\left(\\begin{pmatrix} \\boldsymbol{X} \\boldsymbol{\\beta} \\\\ \\boldsymbol{0} \\end{pmatrix}, \\begin{pmatrix} \\sigma^2 \\boldsymbol{H} & \\boldsymbol{0} \\\\ \\boldsymbol{0} & \\sigma^2(\\boldsymbol{I} - \\boldsymbol{H}) \\end{pmatrix} \\right).\n\\end{split}\n$$\n\nIn other words,\n\n$$\n\\boldsymbol{\\widehat{\\mu}} \\sim N(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2 \\boldsymbol{H}) \\quad \\text{and} \\quad \\boldsymbol{\\widehat{\\epsilon}} \\sim N(\\boldsymbol{0}, \\sigma^2(\\boldsymbol{I} - \\boldsymbol{H})), \\quad \\text{with} \\quad \\boldsymbol{\\widehat{\\mu}} \\perp\\!\\!\\!\\perp \\boldsymbol{\\widehat{\\epsilon}}.\n$$ {#eq-fit-and-error-dist}\n\nThe statistical independence between $\\boldsymbol{\\widehat{\\mu}}$ and $\\boldsymbol{\\widehat{\\epsilon}}$ is a result of the fact that these two quantities are projections of $\\boldsymbol{y}$ onto two orthogonal subspaces: $C(\\boldsymbol{X})$ and $C(\\boldsymbol{X})^\\perp$ (Figure [-@fig-orthogonality-fit-residuals]).\n\n![The fitted vector $\\boldsymbol{\\widehat{\\mu}}$ and the residual vector $\\boldsymbol{\\widehat{\\epsilon}}$ are projections of $\\boldsymbol{y}$ onto orthogonal subspaces.](figures/orthogonality-fit-residuals.jpg){#fig-orthogonality-fit-residuals width=50%}\n\nSince $\\boldsymbol{\\widehat{\\beta}}$ is a deterministic function of $\\boldsymbol{\\widehat{\\mu}}$ (in particular, $\\boldsymbol{\\widehat{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{\\widehat{\\mu}}$), it also follows that\n\n$$\n\\boldsymbol{\\widehat{\\beta}} \\perp\\!\\!\\!\\perp \\boldsymbol{\\widehat{\\epsilon}}.\n$$ {#eq-beta-ind-eps}\n\n### Estimation of the noise variance $\\sigma^2$ {#sec-noise-estimation}\n\n*See also Dunn and Smyth 2.4.2, 2.5.3*\n\nWe can't quite do inference for $\\boldsymbol{\\beta}$ based on the distributional result [-@eq-beta-hat-dist] because the noise variance $\\sigma^2$ is unknown to us. Intuitively, since $\\sigma^2 = \\mathbb{E}[\\epsilon_i^2]$, we can get an estimate of $\\sigma^2$ by looking at the quantity $\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2$. To get the distribution of this quantity, we need the following lemma:\n\n::: {#lem-normal-projection}\nLet $\\boldsymbol{w} \\sim N(\\boldsymbol{0}, \\boldsymbol{P})$ for some projection matrix $\\boldsymbol{P}$. Then, $\\|\\boldsymbol{w}\\|^2 \\sim \\chi^2_d$, where $d = \\text{trace}(\\boldsymbol{P})$ is the dimension of the subspace onto which $\\boldsymbol{P}$ projects.\n:::\n\n::: {.proof}\nLet $\\boldsymbol{P} = \\boldsymbol{U} \\boldsymbol{D} \\boldsymbol{U}^\\top$ be an eigenvalue decomposition of $\\boldsymbol{P}$, where $\\boldsymbol{U}$ is orthogonal and $\\boldsymbol{D}$ is a diagonal matrix with $D_{ii} \\in \\{0,1\\}$. We have $\\boldsymbol{w} \\overset{d}{=} \\boldsymbol{U} \\boldsymbol{D} \\boldsymbol{z}$ for $\\boldsymbol{z} \\sim N(0, \\boldsymbol{I}_n)$. Therefore,\n\n$$\n\\|\\boldsymbol{w}\\|^2 = \\|\\boldsymbol{D} \\boldsymbol{z}\\|^2 = \\sum_{i: D_{ii} = 1} z_i^2 \\sim \\chi^2_d, \\quad \\text{where } d = |\\{i: D_{ii} = 1\\}| = \\text{trace}(D) = \\text{trace}(\\boldsymbol{P}).\n$$\n:::\n\nRecall that $\\boldsymbol{I} - \\boldsymbol{H}$ is a projection onto the $(n-p)$-dimensional space $C(\\boldsymbol{X})^\\perp$, so by Lemma [-@lem-normal-projection] and equation [-@eq-fit-and-error-dist] we have\n\n$$\n\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2 \\sim \\sigma^2 \\chi^2_{n-p}.\n$$ {#eq-eps-norm-dist}\n\nFrom this result, it follows that $\\mathbb{E}[\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2] = n-p$, so\n\n$$\n\\widehat{\\sigma}^2 \\equiv \\frac{1}{n-p}\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2\n$$ {#eq-unbiased-noise-estimate}\n\nis an unbiased estimate for $\\sigma^2$. Why does the denominator need to be $n-p$ rather than $n$ for the estimator above to be unbiased? The reason for this is that the residuals $\\boldsymbol{\\widehat{\\epsilon}}$ are the projection of the true noise vector $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ onto the $(n-p)$-dimensional subspace $C(\\boldsymbol{X})^\\perp$ (Figure [-@fig-residuals-as-noise-projection]). To see this, note that\n\n$$\n\\boldsymbol{\\widehat{\\epsilon}} = (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{y} = (\\boldsymbol{I} - \\boldsymbol{H})(\\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) = (\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{\\epsilon}.\n$$\n\nTherefore, the norm of the residual vector will be smaller than that of the noise vector, especially to the extent that $p$ is close to $n$.\n\n![The residual vector $\\boldsymbol{\\widehat{\\epsilon}}$ is the projection of the noise vector $\\boldsymbol{\\epsilon}$ onto $C(\\boldsymbol{X})^\\perp$.](figures/residuals-as-noise-projection.jpg){#fig-residuals-as-noise-projection width=50%}\n\n## Hypothesis Testing\n\n*See also Agresti 3.2.1, 3.2.2, 3.2.4, 3.2.8*\n\nTypically, two types of null hypotheses are tested in a regression setting: those involving one-dimensional parameters and those involving multi-dimensional parameters. For example, consider the null hypotheses $H_0: \\beta_j = 0$ and $H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{0}$ for $S \\subseteq \\{0, 1, \\dots, p-1\\}$, respectively. We discuss tests of these two kinds of hypotheses in Sections [-@sec-one-dim-testing] and [-@sec-multi-dim-testing], and then discuss the power of these tests in Section [-@sec-power].\n\n### Testing a One-Dimensional Parameter {#sec-one-dim-testing}\n\n*See also Dunn and Smyth 2.8.3*\n\n#### $t$-test for a Single Coefficient\n\nThe most common question to ask in a linear regression context is: Is the $j$th predictor associated with the response when controlling for the other predictors? In the language of hypothesis testing, this corresponds to the null hypothesis:\n\n$$\nH_0: \\beta_j = 0\n$$ {#eq-one-dim-null}\n\nAccording to [-@eq-beta-hat-dist], we have $\\widehat{\\beta}_j \\sim N(0, \\sigma^2/s_j^2)$, where, as we learned in Chapter 1:\n\n$$\ns_j^{2} \\equiv [(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}_{jj}]^{-1} = \\|\\boldsymbol{x}_{*j}^\\perp\\|^2.\n$$\n\nTherefore,\n\n$$\n\\frac{\\widehat{\\beta}_j}{\\sigma/s_j} \\sim N(0,1),\n$$ {#eq-oracle-z-stat}\n\nand we are tempted to define a level $\\alpha$ test of the null hypothesis [-@eq-one-dim-null] based on this normal distribution. While this is infeasible since we don't know $\\sigma^2$, we can substitute in the unbiased estimate [-@eq-unbiased-noise-estimate] derived in Section [-@sec-noise-estimation]. Then,\n\n$$\n\\text{SE}(\\widehat{\\beta}_j) \\equiv \\frac{\\widehat{\\sigma}}{s_j}\n$$\n\nis the standard error of $\\widehat{\\beta}_j$, which is an approximation to the standard deviation of $\\widehat{\\beta}_j$. Dividing $\\widehat{\\beta}_j$ by its standard error gives us the $t$-statistic:\n\n$$\nt_j \\equiv \\frac{\\widehat{\\beta}_j}{\\text{SE}(\\widehat{\\beta}_j)} = \\frac{\\widehat{\\beta}_j}{\\sqrt{\\frac{1}{n-p}\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2}/s_j}.\n$$\n\nThis statistic is *pivotal*, in the sense that it has the same distribution for any $\\boldsymbol{\\beta}$ such that $\\beta_j = 0$. Indeed, we can rewrite it as:\n\n$$\nt_j = \\frac{\\frac{\\widehat{\\beta}}{\\sigma/s_j}}{\\sqrt{\\frac{\\sigma^{-2}\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2}{n-p}}}.\n$$\n\nRecalling the independence of $\\boldsymbol{\\widehat{\\beta}}$ and $\\boldsymbol{\\widehat{\\epsilon}}$ [-@eq-beta-ind-eps], the scaled chi-square distribution of $\\|\\boldsymbol{\\widehat{\\epsilon}}\\|^2$ [-@eq-eps-norm-dist], and the standard normal distribution of $\\frac{\\widehat{\\beta}}{\\sigma/s_j}$ [-@eq-oracle-z-stat], we find that:\n\n$$\n\\text{Under } H_0:\\beta_j = 0, \\quad t_j \\sim \\frac{N(0,1)}{\\sqrt{\\frac{1}{n-p}\\chi^2_{n-p}}}, \\quad \\text{with numerator and denominator independent.}\n$$\n\nThe latter distribution is called the *$t$ distribution with $n-p$ degrees of freedom* and is denoted $t_{n-p}$. This paves the way for the two-sided $t$-test:\n\n$$\n\\phi_t(\\boldsymbol{X}, \\boldsymbol{y}) = \\mathbbm{1}(|t_j| > t_{n-p}(1-\\alpha/2)),\n$$\n\nwhere $t_{n-p}(1-\\alpha/2)$ denotes the $1-\\alpha/2$ quantile of $t_{n-p}$. Note that, by the law of large numbers,\n\n$$\n\\frac{1}{n-p}\\chi^2_{n-p} \\overset{P}{\\rightarrow} 1 \\quad \\text{as} \\quad n - p \\rightarrow \\infty,\n$$\n\nso for large $n-p$ we have $t_{j} \\sim t_{n-p} \\approx N(0,1)$. Hence, the $t$-test is approximately equal to the following $z$-test:\n\n$$\n\\phi_t(\\boldsymbol{X}, \\boldsymbol{y}) \\approx \\phi_z(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv \\mathbbm{1}(|t_j| > z(1-\\alpha/2)),\n$$\n\nwhere $z(1-\\alpha/2)$ is the $1-\\alpha/2$ quantile of $N(0,1)$. The $t$-test can also be defined in a one-sided fashion if power against one-sided alternatives is desired.\n\n#### Example: One-Sample Model\n\nConsider the intercept-only linear regression model $y = \\beta_0 + \\epsilon$, and let's apply the $t$-test derived above to test the null hypothesis $H_0: \\beta_0 = 0$. We have $\\widehat{\\beta}_0 = \\bar{y}$. Furthermore, we have\n\n$$\n\\text{SE}^2(\\widehat{\\beta}_0) = \\frac{\\widehat{\\sigma}^2}{n}, \\quad \\text{where} \\quad \\widehat{\\sigma}^2 = \\frac{1}{n-1}\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}_n\\|^2.\n$$\n\nHence, we obtain the $t$ statistic:\n\n$$\nt = \\frac{\\widehat{\\beta}_0}{\\text{SE}(\\widehat{\\beta}_0)} = \\frac{\\sqrt{n} \\bar{y}}{\\sqrt{\\frac{1}{n-1}\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}_n\\|^2}}.\n$$\n\nAccording to the theory above, this test statistic has a null distribution of $t_{n-1}$.\n\n#### Example: Two-Sample Model\n\nSuppose we have $x_1 \\in \\{0,1\\}$, in which case the linear regression $y = \\beta_0 + \\beta_1 x_1 + \\epsilon$ becomes a two-sample model. We can rewrite this model as:\n\n$$\ny_i \\sim \\begin{cases}\nN(\\beta_0, \\sigma^2) \\quad &\\text{for } x_i = 0; \\\\\nN(\\beta_0 + \\beta_1, \\sigma^2) \\quad &\\text{for } x_i = 1.\n\\end{cases}\n$$\n\nIt is often of interest to test the null hypothesis $H_0: \\beta_1 = 0$, i.e., that the two groups have equal means. Let's define:\n\n$$\n\\bar{y}_0 \\equiv \\frac{1}{n_0}\\sum_{i: x_i = 0} y_i, \\quad \\bar{y}_1 \\equiv \\frac{1}{n_1}\\sum_{i: x_i = 1} y_i, \\quad \\text{where} \\quad n_0 = |\\{i: x_i = 0\\}| \\text{ and } n_1 = |\\{i: x_i = 1\\}|.\n$$\n\nThen, we have seen before that $\\widehat{\\beta}_0 = \\bar{y}_0$ and $\\widehat{\\beta}_1 = \\bar{y}_1 - \\bar{y}_0$. We can compute that:\n\n$$\ns_1^2 \\equiv \\|\\boldsymbol{x}_{*1}^{\\perp}\\|^2 = \\|\\boldsymbol{x}_{*1} - \\frac{n_1}{n}\\boldsymbol{1}\\|^2 = n_1\\frac{n_0^2}{n^2} + n_0\\frac{n_1^2}{n^2} = \\frac{n_0 n_1}{n} = \\frac{1}{\\frac{1}{n_0} + \\frac{1}{n_1}}\n$$\n\nand\n\n$$\n\\widehat{\\sigma}^2 = \\frac{1}{n-2}\\left(\\sum_{i: x_i = 0}(y_i - \\bar{y}_0)^2 + \\sum_{i: x_i = 1}(y_i - \\bar{y}_1)^2\\right).\n$$\n\nTherefore, we arrive at a $t$-statistic of:\n\n$$\nt = \\frac{\\sqrt{\\frac{1}{\\frac{1}{n_0} + \\frac{1}{n_1}}}(\\bar{y}_1 - \\bar{y}_0)}{\\sqrt{\\frac{1}{n-2}\\left(\\sum_{i: x_i = 0}(y_i - \\bar{y}_0)^2 + \\sum_{i: x_i = 1}(y_i - \\bar{y}_1)^2\\right)}}.\n$$\n\nUnder the null hypothesis, this statistic has a distribution of $t_{n-2}$.\n\n#### $t$-test for a Contrast Among Coefficients\n\nGiven a vector $\\boldsymbol{c} \\in \\mathbb{R}^p$, the quantity $\\boldsymbol{c}^T \\boldsymbol{\\beta}$ is sometimes called a *contrast*. For example, suppose $\\boldsymbol{c} = (1,-1, 0, \\dots, 0)$. Then, $\\boldsymbol{c}^T \\boldsymbol{\\beta} = \\beta_1 - \\beta_2$ is the difference in effects of the first and second predictors. We are sometimes interested in testing whether such a contrast is equal to zero, i.e., $H_0: \\boldsymbol{c}^T \\boldsymbol{\\beta} = 0$. While this hypothesis can involve two or more of the predictors, the parameter $\\boldsymbol{c}^T \\boldsymbol{\\beta}$ is still one-dimensional, and therefore we can still apply a $t$-test. Going back to the distribution $\\boldsymbol{\\widehat{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1})$, we find that:\n\n$$\n\\boldsymbol{c}^T\\boldsymbol{\\widehat{\\beta}} \\sim N(\\boldsymbol{c}^T\\boldsymbol{\\beta}, \\sigma^2\\boldsymbol{c}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{c}).\n$$\n\nTherefore, under the null hypothesis that $\\boldsymbol{c}^T \\boldsymbol{\\beta} = 0$, we can derive that:\n\n$$\n\\frac{\\boldsymbol{c}^T \\boldsymbol{\\widehat{\\beta}}}{\\widehat{\\sigma} \\sqrt{\\boldsymbol{c}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{c}}} \\sim t_{n-p},\n$$ {#eq-contrasts-t-dist}\n\ngiving us another $t$-test. Note that the $t$-tests described above can be recovered from this more general formulation by setting $\\boldsymbol{c} = \\boldsymbol{e}_j$, the indicator vector with the $j$th coordinate equal to 1 and all others equal to zero.\n\n### Testing a Multi-Dimensional Parameter {#sec-multi-dim-testing}\n\n*See also Dunn and Smyth 2.10.1*\n\n#### $F$-Test for a Group of Coefficients\n\nNow we move on to the case of testing a multi-dimensional parameter: $H_0: \\boldsymbol{\\beta}_S = \\boldsymbol{0}$ for some $S \\subseteq \\{0, 1, \\dots, p-1\\}$. In other words, we would like to test\n\n$$\nH_0: \\boldsymbol{y} = \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\beta}_{-S} + \\boldsymbol{\\epsilon} \\quad \\text{versus} \\quad H_1: \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n$$\n\nTo test this hypothesis, let us fit least squares coefficients $\\boldsymbol{\\widehat{\\beta}}_{-S}$ and $\\boldsymbol{\\widehat{\\beta}}$ for the partial model as well as the full model. If the partial model fits well, then the residuals $\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}$ from this model will not be much larger than the residuals $\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}$ from the full model. To quantify this intuition, let us recall our analysis of variance decomposition from Chapter 1:\n\n$$\n\\|\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}\\|^2 = \\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}\\|^2 + \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2.\n$$\n\nLet's consider the ratio\n\n$$\n\\frac{\\|\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}\\|^2 - \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2} = \\frac{\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}\\|^2}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2},\n$$\n\nwhich is the relative increase in the residual sum of squares when going from the full model to the partial model. Let us rewrite this ratio in terms of projection matrices. Let $\\boldsymbol{H}$ be the projection matrix for the full model, and let $\\boldsymbol{H}_{\\text{-}S}$ be the projection matrix for the partial model. Note that $\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}$ is the projection matrix onto the $|S|$-dimensional space $C(\\boldsymbol{X}) \\cap C(\\boldsymbol{X}_{\\text{-}S})^\\perp$ (@fig-f-test-geometry).\n\n![Geometry of the $F$-test. Orthogonality relationships stem from $C(\\boldsymbol{X}_{*,\\text{-}S}) \\perp C(\\boldsymbol{X}) \\cap C(\\boldsymbol{X}_{*, \\text{-}S})^\\perp \\perp C(\\boldsymbol{X})^\\perp$.](figures/F-test-geometry.png){#fig-f-test-geometry width=75%}\n\nWe have\n\n$$\n\\frac{\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}\\|^2}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2} = \\frac{\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2}{\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{y}\\|^2},\n$$\n\nso the numerator and denominator are the squared norms of the projections of $\\boldsymbol{y}$ onto $C(\\boldsymbol{X}) \\cap C(\\boldsymbol{X}_{*, \\text{-}S})^\\perp$ and $C(\\boldsymbol{X})^\\perp$, respectively (@fig-f-test-geometry). Under the null hypothesis, we have $\\boldsymbol{y} = \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\beta}_{-S} + \\boldsymbol{\\epsilon}$, and\n\n$$\n(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{X}_{*,\\text{-}S} \\boldsymbol{\\beta}_{-S} = (\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{X}_{*,\\text{-}S} \\boldsymbol{\\beta}_{-S} = 0\n$$\n\nbecause $\\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\beta}_{-S} \\in C(\\boldsymbol{X}_{*, \\text{-}S}) \\perp C(\\boldsymbol{X}) \\cap C(\\boldsymbol{X}_{*, \\text{-}S})^T \\perp C(\\boldsymbol{X})^\\perp$. It follows that\n\n$$\n\\frac{\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2}{\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{y}\\|^2} = \\frac{\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{\\epsilon}\\|^2}{\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{\\epsilon}\\|^2}.\n$$\n\nSince the projection matrices in the numerator and denominator project onto orthogonal subspaces, we have $(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{\\epsilon} \\perp\\!\\!\\!\\perp (\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{\\epsilon}$, with $\\|(\\boldsymbol{H} - \\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{\\epsilon}\\|^2 \\sim \\sigma^2 \\chi^2_{|S|}$ and $\\|(\\boldsymbol{I} - \\boldsymbol{H}) \\boldsymbol{\\epsilon}\\|^2 \\sim \\sigma^2 \\chi^2_{n-p}$. Renormalizing numerator and denominator to have expectation 1 under the null, we arrive at the $F$-statistic\n\n$$\nF \\equiv \\frac{(\\|\\boldsymbol{y} - \\boldsymbol{X}_{*, \\text{-}S} \\boldsymbol{\\widehat{\\beta}}_{-S}\\|^2 - \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2)/|S|}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2/(n-p)}.\n$$\n\nWe have derived that under the null hypothesis,\n\n$$\nF \\sim \\frac{\\chi^2_{|S|}/|S|}{\\chi^2_{n-p}/(n-p)}, \\quad \\text{with numerator and denominator independent.}\n$$\n\nThis distribution is called the $F$-distribution with $|S|$ and $n-p$ degrees of freedom, and is denoted $F_{|S|, n-p}$. Denoting by $F_{|S|, n-p}(1-\\alpha)$ the $1-\\alpha$ quantile of this distribution, we arrive at the $F$-test\n\n$$\n\\phi_F(\\boldsymbol{X}, \\boldsymbol{y}) \\equiv \\mathbbm{1}(F > F_{|S|, n-p}(1-\\alpha)).\n$$\n\nNote that the $F$-test searches for deviations of $\\boldsymbol{\\beta}_{S}$ in all directions, and does not have one-sided variants like the $t$-test.\n\n#### Example: Testing for Any Significant Coefficients Except the Intercept\n\nSuppose $\\boldsymbol{x}_{*,0} = \\boldsymbol{1}_n$ is an intercept term. Then, consider the null hypothesis $H_0: \\beta_1 = \\cdots = \\beta_{p-1} = 0$. In other words, the null hypothesis is the intercept-only model, and the alternative hypothesis is the regression model with an intercept and $p-1$ additional predictors. In this case, $S = \\{1, \\dots, p-1\\}$ and $-S = \\{0\\}$. The corresponding $F$ statistic is\n\n$$\nF \\equiv \\frac{(\\|\\boldsymbol{y} - \\bar{y} \\boldsymbol{1}\\|^2 - \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2)/(p-1)}{\\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}}\\|^2/(n-p)},\n$$\n\nwith null distribution $F_{p-1, n-p}$.\n\n#### Example: Testing for Equality of Group Means in $C$-Groups Model\n\nAs a further special case, consider the $C$-groups model from Chapter 1. Recall the ANOVA decomposition\n\n$$\n\\sum_{i = 1}^n (y_i - \\bar{y})^2 = \\sum_{i = 1}^n (\\bar{y}_{c(i)} - \\bar{y})^2 + \\sum_{i = 1}^n (y_i - \\bar{y}_{c(i)})^2 = \\text{SSB} + \\text{SSW}.\n$$\n\nThe $F$-statistic in this case becomes\n\n$$\nF = \\frac{\\sum_{i = 1}^n (\\bar{y}_{c(i)} - \\bar{y})^2/(C-1)}{\\sum_{i = 1}^n (y_i - \\bar{y}_{c(i)})^2/(n-C)} = \\frac{\\text{SSB}/(C-1)}{\\text{SSW}/(n-C)},\n$$\n\nwith null distribution $F_{C-1, n-C}$.\n\n## Power {#sec-power}\n\n*See also Agresti 3.2.5*\n\nSo far we've been focused on finding the null distributions of various test statistics in order to construct tests with Type-I error control. Now let's shift our attention to examining the power of these tests.\n\n### The power of a $t$-test\n\nConsider the $t$-test of the null hypothesis $H_0: \\beta_j = 0$. Suppose that, in reality, $\\beta_j \\neq 0$. What is the probability the $t$-test will reject the null hypothesis? To answer this question, recall that $\\widehat \\beta_j \\sim N(\\beta_j, \\sigma^2/s_j^2)$. Therefore,\n\n$$\nt = \\frac{\\widehat \\beta_j}{\\text{SE}(\\widehat \\beta_j)} = \\frac{\\beta_j}{\\text{SE}(\\widehat \\beta_j)} + \\frac{\\widehat \\beta_j - \\beta_j}{\\text{SE}(\\widehat \\beta_j)} \\overset{\\cdot}{\\sim} N\\left(\\frac{\\beta_j s_j}{\\sigma}, 1\\right)\n$$ {#eq-t-alt-dist-1}\n\nHere we have made the approximation $\\text{SE}(\\widehat \\beta_j) \\approx \\frac{\\sigma}{s_j}$, which is pretty good when $n-p$ is large. Therefore, the power of the two-sided $t$-test is\n\n$$\n\\mathbb{E}[\\phi_t] = \\mathbb{P}[\\phi_t = 1] \\approx \\mathbb{P}[|t| > z_{1-\\alpha/2}] \\approx \\mathbb{P}\\left[\\left|N\\left(\\frac{\\beta_j s_j}{\\sigma}, 1\\right)\\right| > z_{1-\\alpha/2}\\right]\n$$\n\nTherefore, the quantity $\\frac{\\beta_j s_j}{\\sigma}$ determines the power of the $t$-test. To understand $s_j$ a little better, let's assume that the rows $\\boldsymbol{x}_{i*}$ of the model matrix are drawn i.i.d. from some distribution $(x_0, \\dots, x_{p-1})$. Then we have roughly\n\n$$\n\\boldsymbol{x}_{*j}^\\perp \\approx \\boldsymbol{x}_{*j} - \\mathbb{E}[\\boldsymbol{x}_{*j}|\\boldsymbol{X}_{*, \\text{-}j}],\n$$\n\nso $x_{ij}^\\perp \\approx x_{ij} - \\mathbb{E}[x_{ij}|\\boldsymbol{x}_{i,\\text{-}j}]$. Hence,\n\n$$\ns_j^2 \\equiv \\|\\boldsymbol{x}_{*j}^\\perp\\|^2 \\approx n\\mathbb{E}[(x_j-\\mathbb{E}[x_j|\\boldsymbol{x}_{\\text{-}j}])^2] = n\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]].\n$$\n\nHence, we can rewrite the alternative distribution ([-@eq-t-alt-dist-1]) as\n\n$$\nt \\overset{\\cdot}{\\sim} N\\left(\\frac{\\beta_j \\cdot \\sqrt{n} \\cdot \\sqrt{\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]}}{\\sigma}, 1\\right)\n$$ {#eq-t-alt-dist-2}\n\nWe can see clearly now how the power of the $t$-test varies with the effect size $\\beta_j$, the sample size $n$, the degree of collinearity $\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]$, and the noise standard deviation $\\sigma$.\n\n### The power of an $F$-test\n\nNow let's turn our attention to computing the power of the $F$-test. We have\n\n$$\nF = \\frac{\\|\\boldsymbol{X}\\boldsymbol{\\widehat \\beta} - \\boldsymbol{X}_{*, \\text{-}S}\\boldsymbol{\\widehat \\beta}_{-S}\\|^2/|S|}{\\|\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\widehat \\beta}\\|^2/|n-p|} = \\frac{\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2/|S|}{\\|(\\boldsymbol{I} - \\boldsymbol{H})\\boldsymbol{y}\\|^2/|n-p|} \\approx \\frac{\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2/|S|}{\\sigma^2}.\n$$\n\nTo calculate the distribution of the numerator, we need to introduce the notion of a non-central chi-squared random variable.\n\n::: {#def-noncentral-chi-square}\nFor some vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$, suppose $\\boldsymbol{z} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{I}_d)$. Then, we define the distribution of $\\|\\boldsymbol{z}\\|^2$ as the noncentral chi-square random variable with $d$ degrees of freedom and noncentrality parameter $\\|\\boldsymbol{\\mu}\\|^2$ and denote this distribution by $\\chi^2_d(\\|\\boldsymbol{\\mu}\\|^2)$.\n:::\n\nThe following proposition states two useful facts about noncentral chi-square distributions.\n\n::: {#prp-noncentral-chi-square}\nThe following two relations hold:\n1. The mean of a $\\chi^2_d(\\|\\boldsymbol{\\mu}\\|^2)$ random variable is $d + \\|\\boldsymbol{\\mu}\\|^2$.\n2. If $\\boldsymbol{P}$ is a projection matrix and $\\boldsymbol{y} = \\boldsymbol{\\mu} + \\boldsymbol{\\epsilon}$, then $\\frac{1}{\\sigma^2}\\|\\boldsymbol{P} \\boldsymbol{y}\\|^2 \\sim \\chi^2_{\\text{tr}(\\boldsymbol{P})}\\left(\\frac{1}{\\sigma^2}\\|\\boldsymbol{P} \\boldsymbol{\\mu}\\|^2\\right).$\n:::\n\nIt therefore follows that\n\n$$\nF \\approx \\frac{\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S}) \\boldsymbol{y}\\|^2/|S|}{\\sigma^2} \\sim \\frac{1}{|S|}\\chi^2_{|S|}\\left(\\|(\\boldsymbol{H}-\\boldsymbol{H}_{\\text{-}S})\\boldsymbol{X} \\boldsymbol{\\beta}\\|^2\\right) = \\frac{1}{|S|}\\chi^2_{|S|}\\left(\\frac{1}{\\sigma^2}\\|\\boldsymbol{X}^\\perp_{*, S}\\boldsymbol{\\beta}_S\\|^2\\right).\n$$\n\nAssuming as before that the rows of $\\boldsymbol{X}$ are samples from a joint distribution, we can write\n\n$$\n\\|\\boldsymbol{X}^\\perp_{*, S}\\boldsymbol{\\beta}_S\\|^2 \\approx n\\boldsymbol{\\beta}_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S.\n$$\n\nTherefore,\n\n$$\nF \\overset{\\cdot}{\\sim} \\frac{1}{|S|}\\chi^2_{|S|}\\left(\\frac{n\\beta_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S}{\\sigma^2}\\right)\n$$\n\nwhich is similar in spirit to equation ([-@eq-t-alt-dist-2]). To get a better sense of what this relationship implies for the power of the $F$-test, we find from the first part of @prp-noncentral-chi-square that, under the alternative,\n\n$$\n\\mathbb{E}[F] \\approx \\mathbb{E}\\left[\\frac{1}{|S|}\\chi^2_{|S|}\\left(\\frac{n\\beta_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S}{\\sigma^2}\\right)\\right] = 1 + \\frac{n\\beta_S^T \\mathbb{E}[\\text{Var}[\\boldsymbol{x}_S|\\boldsymbol{x}_{\\text{-}S}]] \\boldsymbol{\\beta}_S}{|S| \\cdot \\sigma^2}.\n$$\n\nBy contrast, under the null, the mean of the $F$-statistic is 1. The $|S|$ term in the denominator above suggests that testing larger sets of variables explaining the same amount of variation in $\\boldsymbol{y}$ will hurt power. The test must accommodate for the fact that larger sets of variables will explain more of the variability in $y$ even under the null hypothesis.\n\n### Power of the $t$-test when predictors are added to the model\n\nAs we know, the outcome of a regression is a function of the predictors that are used. What happens to the $t$-test $p$-value for $H_0: \\beta_j = 0$ when a predictor is added to the model? To keep things simple, let's consider the\n\n$$\n\\text{true underlying model:}\\ y = \\beta_0 x_0 + \\beta_1 x_1 + \\epsilon.\n$$\n\nLet's consider the power of testing $H_0: \\beta_0 = 0$ in the regression models\n\n$$\n\\text{model 0:}\\ y = \\beta_0 x_0 + \\epsilon \\quad \\text{versus} \\quad \\text{model 1:}\\ y = \\beta_0 x_0 + \\beta_1 x_1 + \\epsilon.\n$$\n\nThere are four cases based on $\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}]$ and the value of $\\beta_1$ in the true model:\n\n1. $\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] \\neq 0$ and $\\beta_1 \\neq 0$. In this case, in model 0 we have omitted an important variable that is correlated with $\\boldsymbol{x}_{*0}$. Therefore, the meaning of $\\beta_0$ differs between model 0 and model 1, so it may not be meaningful to compare the $p$-values arising from these two models.\n2. $\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] \\neq 0$ and $\\beta_1 = 0$. In this case, we are adding a null predictor that is correlated with $x_{*0}$. Recall that the power of the $t$-test hinges on the quantity $\\frac{\\beta_j \\cdot \\sqrt{n} \\cdot \\sqrt{\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]}}{\\sigma}$. Adding the predictor $x_1$ has the effect of reducing the conditional predictor variance $\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]$, therefore reducing the power. This is a case of *predictor competition*.\n3. $\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] = 0$ and $\\beta_1 \\neq 0$. In this case, we are adding a non-null predictor that is orthogonal to $\\boldsymbol{x}_{*0}$. While the conditional predictor variance $\\mathbb{E}[\\text{Var}[x_j|\\boldsymbol{x}_{\\text{-}j}]]$ remains the same due to orthogonality, the residual variance $\\sigma^2$ is reduced when going from model 0 to model 1.\\footnote{If $\\beta_1$ is small enough, then the unbiased estimate of the residual variance may actually increase due to a reduction in the residual degrees of freedom in the denominator.} Therefore, in this case adding $x_1$ to the model increases the power for testing $H_0: \\beta_0 = 0$. This is a case of *predictor collaboration*.\n4. $\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] = 0$ and $\\beta_1 = 0$. In this case, we are adding an orthogonal null variable, which does not change the conditional predictor variance or the residual variance, and therefore keeps the power of the test the same.\n\nIn conclusion, adding a predictor can either increase or decrease the power of a $t$-test. Similar reasoning can be applied to the $F$-test.\n\n**Remark: Adjusting for covariates in randomized experiments.** Case 3 above, i.e., $\\text{cor}[\\boldsymbol{x}_{*0}, \\boldsymbol{x}_{*1}] = 0$ and $\\beta_1 \\neq 0$, arises in the context of randomized experiments in causal inference. In this case, $y$ represents the outcome, $x_0$ represents the treatment, and $x_1$ represents a covariate. Because the treatment is randomized, there is no correlation between $x_0$ and $x_1$. Therefore, it is not necessary to adjust for $x_1$ in order to get an unbiased estimate of the average treatment effect. However, it is known that adjusting for covariates can lead to more *precise* estimates of the treatment effect due to the phenomenon discussed in case 3 above. This point is also related to the discussion in Chapter 1 about the fact that if $x_0$ and $x_1$ are orthogonal, then the least squares coefficient $\\widehat \\beta_0$ is the same regardless of whether $x_1$ is included in the model. As we see here, either including $x_1$ in the model or adjusting $y$ for $x_1$ is necessary to get better power.\n\n## Confidence and prediction intervals\n\n*See also Agresti 3.3, Dunn and Smyth 2.8.4-2.8.5*\n\nIn addition to hypothesis testing, we often want to construct confidence intervals for the coefficients.\n\n### Confidence interval for a coefficient\n\nUnder $H_0: \\beta_j = 0$, we showed that $\\frac{\\widehat{\\beta_j}}{\\widehat{\\sigma}/s_j} \\sim t_{n-p}$. The same argument shows that for arbitrary $\\beta_j$, we have\n\n$$\n\\frac{\\widehat{\\beta_j} - \\beta_j}{\\widehat{\\sigma}/s_j} \\sim t_{n-p}.\n$$\n\nWe can use this relationship to construct a confidence interval for $\\beta_j$ as follows:\n\n$$\n\\begin{split}\n1-\\alpha = \\mathbb{P}[|t_{n-p}| \\leq t_{n-p}(1-\\alpha/2)] &= \\mathbb{P}\\left[\\left|\\frac{\\widehat{\\beta_j} - \\beta_j}{\\widehat{\\sigma}/s_j}\\right| \\leq t_{n-p}(1-\\alpha/2) \\right] \\\\\n&= \\mathbb{P}\\left[\\beta_j \\in \\left[\\widehat{\\beta_j} - \\frac{\\widehat{\\sigma}}{s_j}t_{n-p}(1-\\alpha/2), \\widehat{\\beta_j} + \\frac{\\widehat{\\sigma}}{s_j}t_{n-p}(1-\\alpha/2) \\right]\\right] \\\\\n&\\equiv \\mathbb{P}\\left[\\beta_j \\in \\left[\\widehat{\\beta_j} - \\text{SE}(\\widehat{\\beta_j})t_{n-p}(1-\\alpha/2), \\widehat{\\beta_j} + \\text{SE}(\\widehat{\\beta_j})t_{n-p}(1-\\alpha/2) \\right]\\right] \\\\\n&\\equiv \\mathbb{P}[\\beta_j \\in \\text{CI}(\\beta_j)].\n\\end{split}\n$$ {#eq-pointwise-interval-beta}\n\nThe confidence interval $\\text{CI}(\\beta_j)$ defined above therefore has $1-\\alpha$ coverage. Because of the duality between confidence intervals and hypothesis tests, the factors contributing to powerful tests ([-@sec-power]) also lead to shorter confidence intervals.\n\n### Confidence interval for $\\mathbb{E}[y|\\boldsymbol{x_0}]$\n\nSuppose now that we have a new predictor vector $\\boldsymbol{x_0} \\in \\mathbb{R}^p$. The mean of the response for this predictor vector is $\\mathbb{E}[y|\\boldsymbol{x_0}] = \\boldsymbol{x_0}^T \\boldsymbol{\\beta}$. Plugging in $\\boldsymbol{x_0}$ for $\\boldsymbol{c}$ in the relation, we obtain\n\n$$\n\\frac{\\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{x_0}^T \\boldsymbol{\\beta}}{\\widehat{\\sigma} \\sqrt{\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0}}} \\sim t_{n-p}.\n$$\n\nFrom this, we can derive that\n\n$$\n\\text{CI}(\\boldsymbol{x_0}^T \\boldsymbol{\\beta}) \\equiv \\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\text{SE}(\\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}}) \\cdot t_{n-p}(1-\\alpha/2) \\equiv \\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\widehat{\\sigma} \\sqrt{\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0}} \\cdot t_{n-p}(1-\\alpha/2)\n$$\n\nis a $1-\\alpha$ confidence interval for $\\boldsymbol{x_0}^T \\boldsymbol{\\beta}$. We see that the width of this confidence interval depends on $\\boldsymbol{x_0}$ through the quantity $\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0}$. Let's give this quantity a closer look, in the case when the regression contains an intercept, i.e., $\\boldsymbol{x_{*,0}} = \\boldsymbol{1}$. Then, we have $\\boldsymbol{x_0} = (1, \\boldsymbol{x^T_{0,\\text{-}0}})$. Then, defining $\\bar{x} \\in \\mathbb{R}^{p-1}$ as the vector of column-wise means of $\\boldsymbol{X_{*,\\text{-}0}}$, we can rewrite the regression as\n\n$$\ny = \\beta_0 + \\boldsymbol{x_{\\text{-}0}}^T \\boldsymbol{\\beta_{\\text{-}0}} + \\epsilon \\equiv \\beta'_0 + (\\boldsymbol{x_{\\text{-}0}}-\\bar{x})^T  \\boldsymbol{\\beta_{\\text{-}0}} + \\epsilon.\n$$\n\nTherefore, we seek a prediction interval for $\\boldsymbol{x_{0}}^T \\boldsymbol{\\beta} = \\beta'_0 + (\\boldsymbol{x_{0, \\text{-}0}}-\\bar{x})^T \\boldsymbol{\\beta_{\\text{-}0}}$. With this reformulation, we can compute\n\n$$\n\\begin{split}\n\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0} &= (1 \\ (\\boldsymbol{x_{0, \\text{-}0}}-\\bar{x})^T)\\begin{pmatrix}\\boldsymbol{1}^T \\boldsymbol{1} & 0 \\\\ 0 &\\boldsymbol{X_{*,\\text{-}0}}^T \\boldsymbol{X_{*,\\text{-}0}} \\end{pmatrix}^{-1}{1 \\choose \\boldsymbol{x_{0, \\text{-}0}}-\\bar{x}} \\\\\n&= \\frac{1}{n} + (\\boldsymbol{x_{0, \\text{-}0}}-\\bar{x})^T (\\boldsymbol{X_{*,\\text{-}0}}^T \\boldsymbol{X_{*,\\text{-}0}})^{-1}(\\boldsymbol{x_{0, \\text{-}0}}-\\bar{x}).\n\\end{split}\n$$\n\nHence, we see that this quantity grows larger as $\\boldsymbol{x_{0, \\text{-}0}}-\\bar{x}$ grows larger, and achieves its minimum when $\\boldsymbol{x_{0, \\text{-}0}}=\\bar{x}$. Let's look at the special case when $p = 2$, so there is just one predictor except the intercept. Then, we have $\\boldsymbol{X_{*,\\text{-}0}} = \\boldsymbol{x_{*,1}}-\\bar{x_1}$, so\n\n$$\n\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0} = \\frac{1}{n} + \\frac{(x_{01}-\\bar{x_1})^2}{\\|\\boldsymbol{x_{*,1}}-\\bar{x_1}\\|^2}.\n$$\n\n### Prediction interval for $y|\\boldsymbol{x_0}$\n\nInstead of creating a confidence interval for a point on the regression line, we may want to create a confidence interval for a new draw $y_0$ of $y$ for $\\boldsymbol{x} = \\boldsymbol{x_0}$, i.e., a *prediction interval*. Note that\n\n$$\ny_0 - \\boldsymbol{x_0}^T \\widehat{\\beta} = \\boldsymbol{x_0}^T \\beta + \\epsilon_0 - \\boldsymbol{x_0}^T \\widehat{\\beta} = \\epsilon_0 + \\boldsymbol{x_0}^T (\\beta-\\widehat{\\beta}) \\sim N(0, \\sigma^2 + \\sigma^2 \\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0}).\n$$\n\nTherefore, we have\n\n$$\n\\frac{y_0 - \\boldsymbol{x_0}^T \\widehat{\\beta}}{\\widehat{\\sigma}\\sqrt{1 + \\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0}}} \\sim t_{n-p},\n$$\n\nwhich leads to the $1-\\alpha$ prediction interval\n\n$$\n\\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\widehat{\\sigma} \\sqrt{1+\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{x_0}} \\cdot t_{n-p}(1-\\alpha/2) \\equiv \\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\text{SE}(\\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}}) \\cdot t_{n-p}(1-\\alpha/2).\n$$ {#eq-pointwise-contrast-interval}\n\n**Remark: Prediction with confidence in machine learning.**\n\nThe entire field of supervised machine learning is focused on accurately predicting $y_0$ from $\\boldsymbol{x_0}$, usually using nonlinear functions $\\widehat{f}(\\boldsymbol{x_0})$. In addition to providing a guess $\\widehat{y_0}$ for $y_0$, it is often useful to quantify the uncertainty in this guess. In other words, it is useful to come up with a prediction interval (or prediction region) $\\text{PI}(y_0)$ such that\n\n$$\n\\mathbb{P}[y_0 \\in \\text{PI}(y_0) \\mid \\boldsymbol{x_0}] \\geq 1-\\alpha.\n$$ {#eq-conditional-prediction-interval}\n\nFor example, in safety-critical applications of machine learning like self-driving cars, it is essential to have confidence in predictions. Unfortunately, beyond the realm of linear regression, it is hard to come up with intervals satisfying ([-@eq-conditional-prediction-interval]) for each point $\\boldsymbol{x_0}$. However, the emerging field of *conformal inference* provides guarantees on average over possible values of $\\boldsymbol{x}$:\n\n$$\n\\mathbb{P}[y \\in \\text{PI}(y)] = \\mathbb{E}[\\mathbb{P}[y \\in \\text{PI}(y) \\mid \\boldsymbol{x}]] \\geq 1-\\alpha.\n$$ {#eq-unconditional-prediction-interval}\n\nRemarkably, these guarantees place no assumption on the machine learning method used and require only that the data points on which $\\widehat{f}$ is trained are exchangeable (an even weaker condition than i.i.d.). While the unconditional guarantee ([-@eq-unconditional-prediction-interval]) is weaker than the conditional one ([-@eq-conditional-prediction-interval]), it can be obtained for modern machine learning and deep learning models.\n\n### Simultaneous intervals\n\nNote that the intervals in the preceding sections have *pointwise coverage*. For example, we have\n\n$$\n\\mathbb{P}[\\beta_j \\in \\text{CI}(\\beta_j)] \\geq 1-\\alpha \\quad \\text{for each } j.\n$$\n\nor\n\n$$\n\\mathbb{P}[\\boldsymbol{x_0}^T \\boldsymbol{\\beta} \\in \\text{CI}(\\boldsymbol{x_0}^T \\boldsymbol{\\beta})] \\geq 1-\\alpha \\quad \\text{for each } \\boldsymbol{x_0}.\n$$\n\nSometimes a stronger *simultaneous coverage* guarantee is desired, e.g.,\n\n$$\n\\mathbb{P}[\\beta_j \\in \\text{CI}^{\\text{sim}}(\\beta_j) \\ \\text{for each } j] \\geq 1-\\alpha\n$$ {#eq-simultaneous-coordinatewise}\n\nor\n\n$$\n\\mathbb{P}[\\boldsymbol{x_0}^T \\boldsymbol{\\beta} \\in \\text{CI}^{\\text{sim}}(\\boldsymbol{x_0}^T \\boldsymbol{\\beta}) \\ \\text{for each } \\boldsymbol{x_0}] \\geq 1-\\alpha.\n$$ {#eq-simultaneous-contrasts}\n\nSimultaneous confidence intervals are possible to construct as well. As a starting point, note that\n\n$$\n\\frac{\\frac{1}{p}\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2}{\\widehat{\\sigma}^2} \\sim F_{p, n-p}.\n$$\n\nHence, we have\n\n$$\n\\mathbb{P}[\\|\\boldsymbol{X} \\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 \\leq p \\widehat{\\sigma}^2 F_{p, n-p}(1-\\alpha)] \\geq 1-\\alpha.\n$$\n\nHence, the region\n\n$$\n\\text{CR}(\\boldsymbol{\\beta}) \\equiv \\{\\boldsymbol{\\beta}: (\\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{\\beta})^T \\boldsymbol{X}^T \\boldsymbol{X} (\\boldsymbol{\\widehat{\\beta}} - \\boldsymbol{\\beta})  \\leq p \\widehat{\\sigma}^2 F_{p, n-p}(1-\\alpha)\\} \\subseteq \\mathbb{R}^p\n$$\n\nis a $1-\\alpha$ confidence region for the vector $\\boldsymbol{\\beta}$:\n\n$$\n\\mathbb{P}[\\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})] \\geq 1-\\alpha.\n$$\n\nIt's easy to see that $\\text{CR}(\\boldsymbol{\\beta})$ is an ellipse centered at $\\boldsymbol{\\widehat{\\beta}}$.\n\n![Confidence region and simultaneous and pointwise confidence intervals.](figures/confidence-regions.jpg){#fig-confidence-region width=40%}\n\nSince the confidence region is for the entire vector $\\boldsymbol{\\beta}$, we can define simultaneous confidence intervals for each coordinate as follows:\n\n$$\n\\text{CI}^{\\text{sim}}(\\beta_j) \\equiv \\{\\beta_j: \\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})\\}.\n$$\n\nThen, these confidence intervals will satisfy the simultaneous coverage property ([-@eq-simultaneous-coordinatewise]). We will obtain a more explicit expression for $\\text{CI}^{\\text{sim}}(\\beta_j)$ shortly.\n\nSimilarly, we may define the simultaneous confidence regions\n\n$$\n\\text{CI}^{\\text{sim}}(\\boldsymbol{x_0}^T \\boldsymbol{\\beta}) \\equiv \\{\\boldsymbol{x_0}^T \\boldsymbol{\\beta}: \\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})\\}.\n$$\n\nLet us find a more explicit expression for the latter interval. For notational ease, let us define $\\boldsymbol{\\Sigma} \\equiv \\boldsymbol{X}^T \\boldsymbol{X}$. Then, note that if $\\boldsymbol{\\beta} \\in \\text{CR}(\\boldsymbol{\\beta})$, then by the Cauchy-Schwarz inequality we have\n\n$$\n\\begin{split}\n(\\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{x_0}^T \\boldsymbol{\\beta})^2 = \\|\\boldsymbol{x_0}^T (\\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\beta})\\|^2 &= \\|(\\boldsymbol{\\Sigma}^{-1/2}\\boldsymbol{x_0})^T \\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\beta})\\|^2 \\\\\n&\\leq \\|(\\boldsymbol{\\Sigma}^{-1/2}\\boldsymbol{x_0})\\|^2\\|\\boldsymbol{\\Sigma}^{1/2}(\\boldsymbol{\\widehat{\\beta}}-\\boldsymbol{\\beta})\\|^2 \\leq \\boldsymbol{x_0}^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{x_0} p \\widehat{\\sigma}^2 F_{p, n-p}(1-\\alpha),\n\\end{split}\n$$\n\ni.e.,\n\n$$\n\\boldsymbol{x_0}^T \\boldsymbol{\\beta} \\in \\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\widehat{\\sigma} \\sqrt{\\boldsymbol{x_0}^T (\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\boldsymbol{x_0}} \\sqrt{pF_{p, n-p}(1-\\alpha)} \\equiv \\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}} \\pm \\text{SE}(\\boldsymbol{x_0}^T \\boldsymbol{\\widehat{\\beta}})\\cdot\\sqrt{pF_{p, n-p}(1-\\alpha)}.\n$$ {#eq-simultaneous-fit-se}\n\nDefining the above interval as $\\text{CI}^{\\text{sim}}(\\boldsymbol{x_0}^T \\boldsymbol{\\beta})$ gives us the simultaneous coverage property ([-@eq-simultaneous-contrasts]). Comparing to equation ([-@eq-pointwise-contrast-interval]), we see that the simultaneous interval is the pointwise interval expanded by a factor of $\\sqrt{pF_{p, n-p}(1-\\alpha)}/t_{n-p}(1-\\alpha/2)$. Specializing to the case $\\boldsymbol{x_0} \\equiv \\boldsymbol{e_j}$, we get an expression for the simultaneous intervals for each coordinate:\n\n$$\n\\text{CI}^{\\text{sim}}(\\beta_j) \\equiv \\widehat{\\beta_j} \\pm \\widehat{\\sigma} \\sqrt{(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}_{jj}} \\sqrt{pF_{p, n-p}(1-\\alpha)} \\equiv \\text{SE}(\\widehat{\\beta_j})\\sqrt{pF_{p, n-p}(1-\\alpha)},\n$$ {#eq-simultaneous-coordinatewise-se}\n\nwhich again is the pointwise interval ([-@eq-pointwise-interval-beta]) expanded by a factor of $\\sqrt{pF_{p, n-p}(1-\\alpha)}/t_{n-p}(1-\\alpha/2)$. These simultaneous intervals are called *Working-Hotelling intervals*.\n\n## Practical considerations\n\n### Practical versus statistical significance\n\nYou can have a statistically significant effect that is not practically significant. The hypothesis testing framework is most useful in the case when the signal-to-noise ratio is relatively small. Otherwise, constructing a confidence interval for the effect size is a more meaningful approach.\n\n### Correlation versus causation, and Simpson's paradox\n\nCausation can be elusive for several reasons. One is reverse causation, where it is not clear whether $X$ causes $Y$ or $Y$ causes $X$. Another is confounding, where there is a third variable $Z$ that causes both $X$ and $Y$. For the latter reason, linear regression coefficients can be sensitive to the choice of other predictors to include and can be misleading if you omit important variables from the regression. A special and sometimes overlooked case of this is *Simpson's paradox*, where an important discrete variable is omitted. Consider the example in Figure [-@fig-simpson-paradox]. Sometimes this discrete variable may seem benign, such as the year in which the data was collected. Such variables might or might not be measured.\n\n![An example of Simpson's paradox (source: Wikipedia).](figures/kidney-stones.png){#fig-simpson-paradox width=100%}\n\n### Dealing with correlated predictors\n\nIt depends on the goal. If we're trying to tease apart effects of correlated predictors, then we have no choice but to proceed as usual despite lower power. Otherwise, we can test predictors in groups via the $F$-test to get higher power at the cost of lower \"resolution.\" Sometimes, it is recommended to simply remove predictors that are correlated with other predictors. This practice, however, is somewhat arbitrary and not recommended.\n\n### Model selection\n\nWe need to ask ourselves: Why do we want to do model selection? It can either be for prediction purposes or for inferential purposes. If it is for prediction purposes, then we can apply cross-validation to select a model and we don't need to think very hard about statistical significance. If it is for inference, then we need to be more careful. There are various classical model selection criteria (e.g., AIC, BIC), but it is not entirely clear what statistical guarantee we are getting for the resulting models. A simpler approach is to apply a $t$-test for each variable in the model, apply a multiple testing correction to the resulting $p$-values, and report the set of significant variables and the associated guarantee. Re-fitting the linear regression after model selection leads us into some dicey inferential territory due to selection bias. This is the subject of ongoing research, and the jury is still out on the best way of doing this.\n\n## R demo\n\n*See also Agresti 3.4.1, 3.4.3, Dunn and Smyth 2.6, 2.14*\n\nLet's put into practice what we've learned in this chapter by analyzing data about house prices.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(GGally)\n\nhouses_data <- read_tsv(\"data/Houses.dat\")\nhouses_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 x 7\n    case taxes  beds baths   new price  size\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1  3104     4     2     0  280.  2048\n 2     2  1173     2     1     0  146.   912\n 3     3  3076     4     2     0  238.  1654\n 4     4  1608     3     2     0  200   2068\n 5     5  1454     3     3     0  160.  1477\n 6     6  2997     3     2     1  500.  3153\n 7     7  4054     3     2     0  266.  1355\n 8     8  3002     3     2     1  290.  2075\n 9     9  6627     5     4     0  587   3990\n10    10   320     3     2     0   70   1160\n# i 90 more rows\n```\n:::\n:::\n\n\n\n### Exploration\n\nLet's first do a bit of exploration:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualize distribution of housing prices, superimposing the mean\nhouses_data |>\n  ggplot(aes(x = price)) +\n  geom_histogram(color = \"black\", bins = 30) +\n  geom_vline(aes(xintercept = mean(price)),\n    colour = \"red\",\n    linetype = \"dashed\"\n  )\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare median and mean price\nhouses_data |>\n  summarise(\n    mean_price = mean(price),\n    median_price = median(price)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  mean_price median_price\n       <dbl>        <dbl>\n1       155.         133.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a pairs plot of continuous variables\nhouses_data |>\n  select(price, size, taxes) |>\n  ggpairs()\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# see how price relates to beds\nhouses_data |>\n  ggplot(aes(x = factor(beds), y = price)) +\n  geom_boxplot(fill = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# see how price relates to baths\nhouses_data |>\n  ggplot(aes(x = factor(baths), y = price)) +\n  geom_boxplot(fill = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# see how price relates to new\nhouses_data |>\n  ggplot(aes(x = factor(new), y = price)) +\n  geom_boxplot(fill = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Hypothesis testing\n\nLet's run a linear regression and interpret the summary. But first, we must decide whether to model beds/baths as categorical or continuous? We should probably model these as categorical, given the potentially nonlinear trend observed in the box plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm(price ~ factor(beds) + factor(baths) + new + size,\n  data = houses_data\n)\nsummary(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = price ~ factor(beds) + factor(baths) + new + size, \n    data = houses_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-179.306  -32.037   -2.899   19.115  152.718 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -19.26307   18.01344  -1.069 0.287730    \nfactor(beds)3   -16.46430   15.04669  -1.094 0.276749    \nfactor(beds)4   -12.48561   21.12357  -0.591 0.555936    \nfactor(beds)5  -101.14581   55.83607  -1.811 0.073366 .  \nfactor(baths)2    2.39872   15.44014   0.155 0.876885    \nfactor(baths)3   -0.70410   26.45512  -0.027 0.978825    \nfactor(baths)4  273.20079   83.65764   3.266 0.001540 ** \nnew              66.94940   18.50445   3.618 0.000487 ***\nsize              0.10882    0.01234   8.822 7.46e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.17 on 91 degrees of freedom\nMultiple R-squared:  0.7653,\tAdjusted R-squared:  0.7446 \nF-statistic: 37.08 on 8 and 91 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nWe can read off the test statistics and $p$-values for each variable from the regression summary, as well as for the $F$-test against the constant model from the bottom of the summary.\n\nLet's use an $F$-test to assess whether the categorical `baths` variable is important.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_partial <- lm(price ~ factor(beds) + new + size,\n  data = houses_data\n)\nanova(lm_fit_partial, lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: price ~ factor(beds) + new + size\nModel 2: price ~ factor(beds) + factor(baths) + new + size\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     94 273722                                \n2     91 238289  3     35433 4.5104 0.005374 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nWhat if we had not coded `baths` as a factor?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_not_factor <- lm(price ~ factor(beds) + baths + new + size,\n  data = houses_data\n)\nanova(lm_fit_partial, lm_fit_not_factor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: price ~ factor(beds) + new + size\nModel 2: price ~ factor(beds) + baths + new + size\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     94 273722                           \n2     93 273628  1     94.33 0.0321 0.8583\n```\n:::\n:::\n\n\n\nIf we want to test for the equality of means across groups of a categorical predictor, without adjusting for other variables, we can use the ANOVA $F$-test. There are several equivalent ways of doing so:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# just use the summary function\nlm_fit_baths <- lm(price ~ factor(baths), data = houses_data)\nsummary(lm_fit_baths)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = price ~ factor(baths), data = houses_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-146.44  -45.88   -7.89   22.22  352.01 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       90.21      19.51   4.624 1.17e-05 ***\nfactor(baths)2    57.68      21.72   2.656  0.00927 ** \nfactor(baths)3   174.52      31.13   5.607 1.97e-07 ***\nfactor(baths)4   496.79      82.77   6.002 3.45e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 80.44 on 96 degrees of freedom\nMultiple R-squared:  0.3881,\tAdjusted R-squared:  0.369 \nF-statistic:  20.3 on 3 and 96 DF,  p-value: 2.865e-10\n```\n:::\n\n```{.r .cell-code}\n# use the anova function as before\nlm_fit_const <- lm(price ~ 1, data = houses_data)\nanova(lm_fit_const, lm_fit_baths)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: price ~ 1\nModel 2: price ~ factor(baths)\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n1     99 1015150                                  \n2     96  621130  3    394020 20.299 2.865e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n# use the aov function\naov_fit <- aov(price ~ factor(baths), data = houses_data)\nsummary(aov_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Df Sum Sq Mean Sq F value   Pr(>F)    \nfactor(baths)  3 394020  131340    20.3 2.86e-10 ***\nResiduals     96 621130    6470                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nWe can also use an $F$-test to test for the presence of an interaction with a multi-class categorical predictor.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_interaction <- lm(price ~ size * factor(beds), data = houses_data)\nsummary(lm_fit_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = price ~ size * factor(beds), data = houses_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-232.643  -25.938   -0.942   19.172  155.517 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          50.12619   48.22282   1.039 0.301310    \nsize                  0.05037    0.04210   1.197 0.234565    \nfactor(beds)3      -103.85734   52.20373  -1.989 0.049620 *  \nfactor(beds)4      -143.90213   67.31359  -2.138 0.035185 *  \nfactor(beds)5      -507.88205  144.10191  -3.524 0.000663 ***\nsize:factor(beds)3    0.07589    0.04368   1.738 0.085633 .  \nsize:factor(beds)4    0.09234    0.04704   1.963 0.052638 .  \nsize:factor(beds)5    0.21147    0.05957   3.550 0.000609 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53.35 on 92 degrees of freedom\nMultiple R-squared:  0.7421,\tAdjusted R-squared:  0.7225 \nF-statistic: 37.81 on 7 and 92 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nlm_fit_size <- lm(price ~ size + factor(beds), data = houses_data)\nanova(lm_fit_size, lm_fit_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: price ~ size + factor(beds)\nModel 2: price ~ size * factor(beds)\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     95 300953                                \n2     92 261832  3     39121 4.5819 0.004905 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nContrasts of regression coefficients can be tested using the `glht()` function from the `multcomp` package.\n\n### Confidence intervals\n\nWe can construct pointwise confidence intervals for each coefficient using `confint()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       2.5 %      97.5 %\n(Intercept)     -55.04455734  16.5184161\nfactor(beds)3   -46.35270691  13.4241025\nfactor(beds)4   -54.44498235  29.4737689\nfactor(beds)5  -212.05730801   9.7656895\nfactor(baths)2  -28.27123130  33.0686620\nfactor(baths)3  -53.25394742  51.8457394\nfactor(baths)4  107.02516067 439.3764122\nnew              30.19258305 103.7062177\nsize              0.08431972   0.1333284\n```\n:::\n:::\n\n\n\nTo create simultaneous confidence intervals, we need a somewhat more manual approach. We start with the coefficients and standard errors:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(summary(lm_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   Estimate  Std. Error     t value     Pr(>|t|)\n(Intercept)     -19.2630706 18.01344052 -1.06937209 2.877304e-01\nfactor(beds)3   -16.4643022 15.04669172 -1.09421410 2.767490e-01\nfactor(beds)4   -12.4856067 21.12356937 -0.59107467 5.559357e-01\nfactor(beds)5  -101.1458092 55.83607248 -1.81147786 7.336590e-02\nfactor(baths)2    2.3987153 15.44014266  0.15535578 8.768849e-01\nfactor(baths)3   -0.7041040 26.45511871 -0.02661504 9.788251e-01\nfactor(baths)4  273.2007864 83.65764044  3.26570036 1.540093e-03\nnew              66.9494004 18.50445029  3.61801617 4.872475e-04\nsize              0.1088241  0.01233621  8.82151661 7.460814e-14\n```\n:::\n:::\n\n\n\nThen we add lower and upper confidence interval endpoints based on the formula ([-@eq-simultaneous-coordinatewise-se]):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\nn <- nrow(houses_data)\np <- length(coef(lm_fit))\nf_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)\ncoef(summary(lm_fit)) |>\n  as.data.frame() |>\n  rownames_to_column(var = \"Variable\") |>\n  select(Variable, Estimate, `Std. Error`) |>\n  mutate(\n    CI_lower = Estimate - `Std. Error` * sqrt(p * f_quantile),\n    CI_upper = Estimate + `Std. Error` * sqrt(p * f_quantile)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Variable     Estimate  Std. Error      CI_lower    CI_upper\n1    (Intercept)  -19.2630706 18.01344052  -95.38917389  56.8630327\n2  factor(beds)3  -16.4643022 15.04669172  -80.05271036  47.1241059\n3  factor(beds)4  -12.4856067 21.12356937 -101.75533960  76.7841262\n4  factor(beds)5 -101.1458092 55.83607248 -337.11309238 134.8214739\n5 factor(baths)2    2.3987153 15.44014266  -62.85244495  67.6498756\n6 factor(baths)3   -0.7041040 26.45511871 -112.50535022 111.0971422\n7 factor(baths)4  273.2007864 83.65764044  -80.34245635 626.7440292\n8            new   66.9494004 18.50445029  -11.25174573 145.1505465\n9           size    0.1088241  0.01233621    0.05669037   0.1609578\n```\n:::\n:::\n\n\n\nNote that the simultaneous intervals are substantially larger.\n\nTo construct pointwise confidence intervals for the fit, we can use the `predict()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lm_fit, newdata = houses_data, interval = \"confidence\") |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        fit       lwr      upr\n1 193.52176 165.22213 221.8214\n2  79.98449  51.91430 108.0547\n3 150.64507 122.28397 179.0062\n4 191.71955 172.27396 211.1651\n5 124.30169  81.34488 167.2585\n6 376.74308 333.44559 420.0406\n```\n:::\n:::\n\n\n\nTo get pointwise prediction intervals, we switch `\"confidence\"` to `\"prediction\"`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(lm_fit, newdata = houses_data, interval = \"prediction\") |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        fit       lwr      upr\n1 193.52176  88.00908 299.0344\n2  79.98449 -25.46688 185.4359\n3 150.64507  45.11589 256.1743\n4 191.71955  88.22951 295.2096\n5 124.30169  13.95069 234.6527\n6 376.74308 266.25901 487.2271\n```\n:::\n:::\n\n\n\nTo construct simultaneous confidence intervals for the fit or predictions, we again need a slightly more manual approach. We call `predict()` again, but this time asking it for the standard errors rather than the confidence intervals:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(lm_fit, newdata = houses_data, se.fit = TRUE)\nhead(predictions$fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3         4         5         6 \n193.52176  79.98449 150.64507 191.71955 124.30169 376.74308 \n```\n:::\n\n```{.r .cell-code}\nhead(predictions$se.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3         4         5         6 \n14.246855 14.131352 14.277804  9.789472 21.625709 21.797212 \n```\n:::\n:::\n\n\n\nNow we can construct the simultaneous confidence intervals via the formula ([-@eq-simultaneous-fit-se]):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)\ntibble(\n  lower = predictions$fit - predictions$se.fit * sqrt(p * f_quantile),\n  upper = predictions$fit + predictions$se.fit * sqrt(p * f_quantile)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 x 2\n   lower upper\n   <dbl> <dbl>\n 1 133.   254.\n 2  20.3  140.\n 3  90.3  211.\n 4 150.   233.\n 5  32.9  216.\n 6 285.   469.\n 7  82.8  145.\n 8 188.   331.\n 9 371.   803.\n10  57.3  128.\n# i 90 more rows\n```\n:::\n:::\n\n\n\nIn the case of simple linear regression, we can plot these pointwise and simultaneous confidence intervals as bands:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# to produce confidence intervals for fits in general, use the predict() function\nn <- nrow(houses_data)\np <- 2\nalpha <- 0.05\nlm_fit <- lm(price ~ size, data = houses_data)\npredictions <- predict(lm_fit, se.fit = TRUE)\nt_quantile <- qt(1 - alpha / 2, df = n - p)\nf_quantile <- qf(1 - alpha, df1 = p, df2 = n - p)\nhouses_data |>\n  mutate(\n    fit = predictions$fit,\n    se = predictions$se.fit,\n    ptwise_width = t_quantile * se,\n    simultaneous_width = sqrt(p * f_quantile) * se\n  ) |>\n  ggplot(aes(x = size)) +\n  geom_point(aes(y = price)) +\n  geom_line(aes(y = fit), color = \"blue\") +\n  geom_line(aes(y = fit + ptwise_width, color = \"Pointwise\")) +\n  geom_line(aes(y = fit - ptwise_width, color = \"Pointwise\")) +\n  geom_line(aes(y = fit + simultaneous_width, color = \"Simultaneous\")) +\n  geom_line(aes(y = fit - simultaneous_width, color = \"Simultaneous\")) +\n  theme(legend.title = element_blank(), legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Predictor competition and collaboration\n\nLet's look at the power of detecting the association between `price` and `beds`. We can imagine that `beds` and `baths` are correlated:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouses_data |>\n  ggplot(aes(x = beds, y = baths)) +\n  geom_count()\n```\n\n::: {.cell-output-display}\n![](linear-models-inference_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nSo let's see how significant `beds` is, with and without `baths` in the model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_only_beds <- lm(price ~ factor(beds), data = houses_data)\nsummary(lm_fit_only_beds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = price ~ factor(beds), data = houses_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-234.35  -50.63  -15.69   24.56  365.86 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     105.94      21.48   4.931 3.43e-06 ***\nfactor(beds)3    44.69      24.47   1.827 0.070849 .  \nfactor(beds)4   105.70      32.35   3.268 0.001504 ** \nfactor(beds)5   246.71      69.62   3.544 0.000611 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 93.65 on 96 degrees of freedom\nMultiple R-squared:  0.1706,\tAdjusted R-squared:  0.1447 \nF-statistic: 6.583 on 3 and 96 DF,  p-value: 0.0004294\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_only_baths <- lm(price ~ factor(baths), data = houses_data)\nlm_fit_beds_baths <- lm(price ~ factor(beds) + factor(baths), data = houses_data)\nanova(lm_fit_only_baths, lm_fit_beds_baths)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: price ~ factor(baths)\nModel 2: price ~ factor(beds) + factor(baths)\n  Res.Df    RSS Df Sum of Sq     F  Pr(>F)  \n1     96 621130                             \n2     93 572436  3     48693 2.637 0.05424 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nWe see that the significance of `beds` dropped by two orders of magnitude. This is an example of predictor competition.\n\nOn the other hand, note that the variable `new` is not very correlated with `beds`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm(new ~ beds, data = houses_data)\nsummary(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = new ~ beds, data = houses_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.15762 -0.11000 -0.11000 -0.08619  0.91381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.03857    0.14950   0.258    0.797\nbeds         0.02381    0.04871   0.489    0.626\n\nResidual standard error: 0.3157 on 98 degrees of freedom\nMultiple R-squared:  0.002432,\tAdjusted R-squared:  -0.007747 \nF-statistic: 0.2389 on 1 and 98 DF,  p-value: 0.6261\n```\n:::\n:::\n\n\n\nbut we know it has a substantial impact on `price`. Let's look at the significance of the test that `beds` is not important when we add `new` to the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_only_new <- lm(price ~ new, data = houses_data)\nlm_fit_beds_new <- lm(price ~ new + factor(beds), data = houses_data)\nanova(lm_fit_only_new, lm_fit_beds_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: price ~ new\nModel 2: price ~ new + factor(beds)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     98 787781                                  \n2     95 619845  3    167936 8.5795 4.251e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nAdding `new` to the model made the $p$-value more significant by a factor of 10. This is an example of predictor collaboration.\n\n",
    "supporting": [
      "linear-models-inference_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}