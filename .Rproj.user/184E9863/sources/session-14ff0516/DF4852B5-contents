% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  letterpaper,
  oneside]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{lmodern}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{fancyhdr}
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT 9610 Lecture Notes},
  pdfauthor={Eugene Katsevich},
  pdflang={english},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{STAT 9610Lecture Notes}
\author{Eugene Katsevich}
\date{}

\begin{document}
\frontmatter
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, borderline west={3pt}{0pt}{shadecolor}, interior hidden, frame hidden, enhanced, boxrule=0pt, breakable]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\mainmatter
\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

\hypertarget{welcome}{%
\section*{Welcome}\label{welcome}}
\addcontentsline{toc}{section}{Welcome}

\markright{Welcome}

This is a set of lecture notes developed for the PhD statistics course
``STAT 9610: Statistical Methodology'\,' at the University of
Pennsylvania. Much of the content is adapted from Alan Agresti's book
\textit{Foundations of Linear and Generalized Linear Models} (2015).
These notes may contain typos and errors, and will be updated in
subsequent iterations of STAT 9610.

\hypertarget{preview-linear-and-generalized-linear-models}{%
\section*{Preview: Linear and generalized linear
models}\label{preview-linear-and-generalized-linear-models}}
\addcontentsline{toc}{section}{Preview: Linear and generalized linear
models}

\markright{Preview: Linear and generalized linear models}

\emph{See also Agresti 1.1, Dunn and Smyth 1.1-1.2, 1.5-1.6, 1.8-1.12}

The overarching statistical goal addressed in this class is to learn
about relationships between a response \(y\) and predictors
\(x_0, x_1, \dots, x_{p-1}\). This abstract formulation encompasses an
extremely wide variety of applications. The most widely used set of
statistical models to address such problems are \emph{generalized linear
models}, which are the focus of this class.

Let's start by recalling the \emph{linear model}, the most fundamental
of the generalized linear models. In this case, the response is
continuous (\(y \in \mathbb{R}\)) and modeled as:

\[
y = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} + \epsilon,
\]

where

\[
\epsilon \sim (0, \sigma^2), \quad \text{i.e.} \ \mathbb{E}[\epsilon] = 0 \ \text{and} \ \text{Var}[\epsilon] = \sigma^2.
\]

We view the predictors \(x_0, \dots, x_{p-1}\) as fixed, so the only
source of randomness in \(y\) is \(\epsilon\). Another way of writing
the linear model is:

\[
\mu \equiv \mathbb{E}[y] = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} \equiv \eta.
\]

Not all responses are continuous, however. In some cases, we have binary
responses (\(y \in \{0,1\}\)) or count responses (\(y \in \mathbb{Z}\)).
In these cases, there is a mismatch between the:

\[
\textit{linear predictor } \eta \equiv \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1}
\]

and the

\[
\textit{mean response } \mu \equiv \mathbb{E}[y].
\]

The linear predictor can take arbitrary real values
(\(\eta \in \mathbb{R}\)), but the mean response can lie in a restricted
range, depending on the response type. For example, \(\mu \in [0,1]\)
for binary \(y\) and \(\mu \in [0, \infty)\) for count \(y\).

For these kinds of responses, it makes sense to model a
\emph{transformation} of the mean as linear, rather than the mean
itself:

\[
g(\mu) = g(\mathbb{E}[y]) = \beta_0 x_0 + \cdots + \beta_{p-1} x_{p-1} = \eta.
\]

This transformation \(g\) is called the link function. For binary \(y\),
a common choice of link function is the \emph{logit link}, which
transforms a probability into a log-odds:

\[
\text{logit}(\pi) \equiv \log \frac{\pi}{1-\pi}.
\]

So the predictors contribute linearly on the log-odds scale rather than
on the probability scale. For count \(y\), a common choice of link
function is the \emph{log link}.

Models of the form

\[
g(\mu) = \eta
\]

are called \emph{generalized linear models} (GLMs). They specialize to
linear models for the identity link function, i.e., \(g(\mu) = \mu\).
The focus of this course is methodologies to learn about the
coefficients
\(\boldsymbol{\beta} \equiv (\beta_0, \dots, \beta_{p-1})^T\) of a GLM
based on a sample
\((\boldsymbol{X}, \boldsymbol{y}) \equiv \{(x_{i,0}, \dots, x_{i,p-1}, y_i)\}_{i = 1}^n\)
drawn from this distribution. Learning about the coefficient vector
helps us learn about the relationship between the response and the
predictors.

\hypertarget{course-outline}{%
\section*{Course outline}\label{course-outline}}
\addcontentsline{toc}{section}{Course outline}

\markright{Course outline}

This course is broken up into five units:

\begin{itemize}
\tightlist
\item
  \textbf{Chapter 1. Linear model: Estimation.} The \emph{least squares}
  point estimate \(\boldsymbol{\widehat \beta}\) of
  \(\boldsymbol{\beta}\) based on a dataset
  \((\boldsymbol{X}, \boldsymbol{y})\) under the linear model
  assumptions.
\item
  \textbf{Chapter 2. Linear model: Inference.} Under the additional
  assumption that \(\epsilon \sim N(0,\sigma^2)\), how to carry out
  statistical inference (hypothesis testing and confidence intervals)
  for the coefficients.
\item
  \textbf{Chapter 3. Linear model: Misspecification.} What to do when
  the linear model assumptions are not correct: What issues can arise,
  how to diagnose them, and how to fix them.
\item
  \textbf{Chapter 4. GLMs: General theory.} Estimation and inference for
  GLMs (generalizing Chapters 1 and 2). GLMs fit neatly into a unified
  theory based on \emph{exponential families}.
\item
  \textbf{Chapter 5. GLMs: Special cases.} Looking more closely at the
  most important special cases of GLMs, including logistic regression
  and Poisson regression.
\end{itemize}

\hypertarget{notation}{%
\section*{Notation}\label{notation}}
\addcontentsline{toc}{section}{Notation}

\markright{Notation}

We will use the following notations in this course. Vector and matrix
quantities will be bolded, whereas scalar quantities will not be.
Capital letters will be used for matrices, and lowercase for vectors and
scalars. No notational distinction will be made between random
quantities and their realizations. The letters \(i = 1, \dots, n\) and
\(j = 0, \dots, p-1\) will index samples and predictors, respectively.
The predictors \(\{x_{ij}\}_{i,j}\) will be gathered into an
\(n \times p\) matrix \(\boldsymbol{X}\). The rows of \(\boldsymbol{X}\)
correspond to samples, with the \(i\)th row denoted
\(\boldsymbol{x}_{i*}\). The columns of \(\boldsymbol{X}\) correspond to
predictors, with the \(j\)th column denoted \(\boldsymbol{x}_{*j}\). The
responses \(\{y_i\}_i\) will be gathered into an \(n \times 1\) response
vector \(\boldsymbol{y}\). The notation \(\equiv\) will be used for
definitions.

\bookmarksetup{startatroot}

\hypertarget{linear-models-estimation}{%
\chapter{Linear models: Estimation}\label{linear-models-estimation}}

\hypertarget{types-of-predictors-interpreting-linear-model-coefficients}{%
\section{Types of predictors; interpreting linear model
coefficients}\label{types-of-predictors-interpreting-linear-model-coefficients}}

\emph{See also Agresti 1.2, Dunn and Smyth 1.4, 1.7, 2.7}

The types of predictors \(x_j\) (e.g.~binary or continuous) has less of
an effect on the regression than the type of response, but it is still
important to pay attention to the former.

\textbf{Intercepts.} It is common to include an \emph{intercept} in a
linear regression model, a predictor \(x_0\) such that \(x_{i0} = 1\)
for all \(i\). When an intercept is present, we index it as the 0th
predictor. The simplest kind of linear model is the \emph{intercept-only
model} or the \emph{one-sample model}:
\begin{equation}\protect\hypertarget{eq-one-sample-model}{}{
y = \beta_0 + \epsilon.
}\label{eq-one-sample-model}\end{equation} The parameter \(\beta_0\) is
the mean of the response.

\textbf{Binary predictors.} In addition to an intercept, suppose we have
a binary predictor \(x_1 \in \{0,1\}\) (e.g.~\(x_1 = 1\) for patients
who took blood pressure medication and \(x_1 = 0\) for those who
didn't). This leads to the following linear model:
\begin{equation}\protect\hypertarget{eq-two-sample-model}{}{
y = \beta_0 + \beta_1 x_1 + \epsilon.
}\label{eq-two-sample-model}\end{equation} Here, \(\beta_0\) is the mean
response (say blood pressure) for observations with \(x_1 = 0\) and
\(\beta_0 + \beta_1\) is the mean response for observations with
\(x_1 = 1\). Therefore, the parameter \(\beta_1\) is the difference in
mean response between observations with \(x_1 = 1\) and \(x_1 = 0\).
This parameter is sometimes called the \emph{effect} or \emph{effect
size} of \(x_1\), though a causal relationship might or might not be
present. The model (Equation~\ref{eq-two-sample-model}) is sometimes
called the \emph{two-sample model}, because the response data can be
split into two ``samples'': those corresponding to \(x_1 = 0\) and those
corresponding to \(x_1 = 1\).

\textbf{Categorical predictors.} A binary predictor is a special case of
a categorical predictor: A predictor taking two or more discrete values.
Suppose we have a predictor \(w \in \{w_0, w_1, \dots, w_{C-1}\}\),
where \(C \geq 2\) is the number of categories and
\(w_0, \dots, w_{C-1}\) are the \emph{levels} of \(w\). E.g. suppose
\(\{w_0, \dots, w_{C-1}\}\) is the collection of U.S. states, so that
\(C = 50\). If we want to regress a response on the categorical
predictor \(w\), we cannot simply set \(x_1 = w\) in the context of the
linear regression (Equation~\ref{eq-two-sample-model}). Indeed, \(w\)
does not necessarily take numerical values. Instead, we need to add a
predictor \(x_j\) for each of the levels of \(w\). In particular, define
\(x_j \equiv \mathbb 1(w = w_j)\) for \(j = 1, \dots, C-1\) and consider
the regression
\begin{equation}\protect\hypertarget{eq-C-sample-model}{}{
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_{C-1}x_{C-1} + \epsilon.
}\label{eq-C-sample-model}\end{equation} Here, category 0 is the
\emph{base category}, and \(\beta_0\) represents the mean response in
the base category. The coefficient \(\beta_j\) represents the difference
in mean response between the \(j\)th category and the base category.

\textbf{Quantitative predictors.} A quantitative predictor is one that
can take on any real value. For example, suppose that
\(x_1 \in \mathbb{R}\), and consider the linear model
\begin{equation}\protect\hypertarget{eq-simple-linear-regression}{}{
y = \beta_0 + \beta_1 x_1 + \epsilon.
}\label{eq-simple-linear-regression}\end{equation} Now, the
interpretation of \(\beta_1\) is that an increase in \(x_1\) by 1 is
associated with an increase in \(y\) by \(\beta_1\). We must be careful
to avoid saying ``an increase in \(x_1\) by 1 \emph{causes} \(y\) to
increase by \(\beta_1\)'' unless we make additional causal assumptions.
Note that the units of \(x_1\) matter. If \(x_1\) is the height of a
person, then the value and the interpretation of \(\beta_1\) changes
depending on whether that height is measured in feet or in meters.

\textbf{Ordinal predictors.} There is an awkward category of predictor
in between categorical and continuous called \emph{ordinal}. An ordinal
predictor is one that takes a discrete number of values, but these
values have an intrinsic ordering,
e.g.~\(x_1 \in \{\texttt{small}, \texttt{medium}, \texttt{large}\}\). It
can be treated as categorical at the cost of losing the ordering
information, or as continuous if one is willing to assign quantitative
values to each category.

\textbf{Multiple predictors.} A linear regression need not contain just
one predictor (aside from an intercept). For example, let's say \(x_1\)
and \(x_2\) are two predictors. Then, a linear model with both
predictors is
\begin{equation}\protect\hypertarget{eq-multiple-regression}{}{
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon.
}\label{eq-multiple-regression}\end{equation} When there are multiple
predictors, the interpretation of coefficients must be revised somewhat.
For example, \(\beta_1\) in the above regression is the effect of an
increase in \(x_1\) by 1 \emph{while holding \(x_2\) constant} or
\emph{while adjusting for \(x_2\)} or \emph{while controlling for
\(x_2\)}. If \(y\) is blood pressure, \(x_1\) is a binary predictor
indicating blood pressure medication taken and \(x_2\) is sex, then
\(\beta_1\) is the effect of the medication on blood pressure while
controlling for sex. In general, the coefficient of a predictor depends
on what other predictors are in the model. As an extreme case, suppose
the medication has no actual effect, but that men generally have higher
blood pressure and higher rates of taking the medication. Then, the
coefficient \(\beta_1\) in the single regression model
(Equation~\ref{eq-two-sample-model}) would be nonzero but the
coefficient in the multiple regression model
(Equation~\ref{eq-multiple-regression}) would be equal to zero. In this
case, sex acts as a \emph{confounder}.

\textbf{Interactions.} Note that the multiple regression model
(Equation~\ref{eq-multiple-regression}) has the built-in assumption that
the effect of \(x_1\) on \(y\) is the same for any fixed value of
\(x_2\) (and vice versa). In some cases, the effect of one variable on
the response may depend on the value of another variable. In this case,
it's appropriate to add another predictor called an \emph{interaction}.
Suppose \(x_2\) is quantitative (e.g.~years of job experience) and
\(x_2\) is binary (e.g.~sex, with \(x_2 = 1\) meaning male). Then, we
can define a third predictor \(x_3\) as the product of the first two,
i.e.~\(x_3 = x_1x_2\). This gives the regression model
\begin{equation}\protect\hypertarget{eq-interaction}{}{
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon.
}\label{eq-interaction}\end{equation} Now, the effect of adding another
year of job experience is \(\beta_1\) for females and
\(\beta_1 + \beta_3\) for males. The coefficient \(\beta_3\) is the
difference in the effect of job experience between males and females.

\hypertarget{model-matrices-model-vector-spaces-and-identifiability}{%
\section{Model matrices, model vector spaces, and
identifiability}\label{model-matrices-model-vector-spaces-and-identifiability}}

\emph{See also Agresti 1.3-1.4, Dunn and Smyth 2.1, 2.2, 2.5.1}

The matrix \(\boldsymbol{X}\) is called the \emph{model matrix} or the
\emph{design matrix}. Concatenating the linear model equations across
observations gives us an equivalent formulation: \[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}; \quad \mathbb{E}[\boldsymbol{\epsilon}] = \boldsymbol{0}, \ \text{Var}[\boldsymbol{\epsilon}] = \sigma^2 \boldsymbol{I_n}
\] or \[
\mathbb{E}[\boldsymbol{y}] = \boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{\mu}.
\] As \(\boldsymbol{\beta}\) varies in \(\mathbb{R}^p\), the set of
possible vectors \(\boldsymbol{\mu} \in \mathbb{R}^n\) is defined \[
C(\boldsymbol{X}) \equiv \{\boldsymbol{\mu} = \boldsymbol{X} \boldsymbol{\beta}: \boldsymbol{\beta} \in \mathbb{R}^p\}.
\] \(C(\boldsymbol{X})\), called the \emph{model vector space}, is a
subspace of \(\mathbb{R}^n\):
\(C(\boldsymbol{X}) \subseteq \mathbb{R}^n\). Since \[
\boldsymbol{X} \boldsymbol{\beta} = \beta_0 \boldsymbol{x_{*0}} + \cdots + \beta_{p-1} \boldsymbol{x_{*p-1}},
\] the model vector space is the column space of the matrix
\(\boldsymbol{X}\) (Figure 1).

\begin{figure}

{\centering \includegraphics{figures/model-vector-space.png}

}

\caption{The model vector space.}

\end{figure}

The \emph{dimension} of \(C(\boldsymbol{X})\) is the rank of
\(\boldsymbol{X}\), i.e.~the number of linearly independent columns of
\(\boldsymbol{X}\). If \(\text{rank}(\boldsymbol{X}) < p\), this means
that there are two different vectors \(\boldsymbol{\beta}\) and
\(\boldsymbol{\beta'}\) such that
\(\boldsymbol{X} \boldsymbol{\beta} = \boldsymbol{X} \boldsymbol{\beta'}\).
Therefore, we have two values of the parameter vector that give the same
model for \(\boldsymbol{y}\). This makes \(\boldsymbol{\beta}\)
\emph{not identifiable}, and makes it impossible to reliably determine
\(\boldsymbol{\beta}\) based on the data. For this reason, we will
generally assume that \(\boldsymbol{\beta}\) is \emph{identifiable},
i.e.~\(\boldsymbol{X} \boldsymbol{\beta} \neq \boldsymbol{X} \boldsymbol{\beta'}\)
if \(\boldsymbol{\beta} \neq \boldsymbol{\beta'}\). This is equivalent
to the assumption that \(\text{rank}(\boldsymbol{X}) = p\). Note that
this cannot hold when \(p > n\), so for the majority of the course we
will assume that \(p \leq n\). In this case,
\(\text{rank}(\boldsymbol{X}) = p\) if and only if \(\boldsymbol{X}\)
has \emph{full-rank}.

As an example when \(p \leq n\) but when \(\boldsymbol{\beta}\) is still
not identifiable, consider the case of a categorical predictor. Suppose
the categories of \(w\) were \(\{w_1, \dots, w_{C-1}\}\), i.e.~the
baseline category \(w_0\) did not exist. In this case, the model
(Equation~\ref{eq-C-sample-model}) would not be identifiable because
\(x_0 = 1 = x_1 + \cdots + x_{C-1}\) and thus
\(x_{*0} = 1 = x_{*1} + \cdots + x_{*,C-1}\). Indeed, this means that
one of the predictors can be expressed as a linear combination of the
others, so \(\boldsymbol{X}\) cannot have full rank. A simpler way of
phrasing the problem is that we are describing \(C-1\) intrinsic
parameters (the means in each of the \(C-1\) groups) with \(C\) model
parameters. There must therefore be some redundancy. For this reason, if
we include an intercept term in the model then we must designate one of
our categories as the baseline and exclude its indicator from the model.

\hypertarget{least-squares-estimation}{%
\section{Least squares estimation}\label{least-squares-estimation}}

\hypertarget{algebraic-perspective}{%
\subsection{Algebraic perspective}\label{algebraic-perspective}}

\emph{See also Agresti 2.1.1, Dunn and Smyth 2.4.1, 2.5.2}

Now, suppose that we are given a dataset
\((\boldsymbol{X}, \boldsymbol{y})\). How do we go about estimating
\(\boldsymbol{\beta}\) based on this data? The canonical approach is the
\emph{method of least squares}: \[
\boldsymbol{\widehat{\beta}} \equiv \underset{\boldsymbol{\beta}}{\arg \min}\ \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}\|^2.
\] The quantity \[
\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2 = \|\boldsymbol{y} - \boldsymbol{\widehat{\mu}}\|^2 = \sum_{i = 1}^n (y_i - \widehat{\mu}_i)^2
\] is called the \emph{residual sum of squares (RSS)}, and it measures
the lack of fit of the linear regression model. We therefore want to
choose \(\boldsymbol{\widehat{\beta}}\) to minimize this lack of fit.
Letting
\(L(\boldsymbol{\beta}) = \frac{1}{2}\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}\|^2\),
we can do some calculus to derive that \[
\frac{\partial}{\partial \boldsymbol{\beta}}L(\boldsymbol{\beta}) = -\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}).
\] Setting this vector of partial derivatives equal to zero, we arrive
at the \emph{normal equations}:
\begin{equation}\protect\hypertarget{eq-normal-equations}{}{
-\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}) = 0 \quad \Longleftrightarrow \quad \boldsymbol{X}^T \boldsymbol{X} \boldsymbol{\widehat{\beta}} = \boldsymbol{X}^T \boldsymbol{y}.
}\label{eq-normal-equations}\end{equation} If \(\boldsymbol{X}\) is full
rank, the matrix \(\boldsymbol{X}^T \boldsymbol{X}\) is invertible and
we can therefore conclude that
\begin{equation}\protect\hypertarget{eq-beta-hat}{}{
\boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y}.
}\label{eq-beta-hat}\end{equation}

\hypertarget{probabilistic-perspective}{%
\subsection{Probabilistic perspective}\label{probabilistic-perspective}}

\emph{See also Agresti 2.7.1}

\textbf{Least squares as maximum likelihood estimation.}

Note that if \(\boldsymbol{\epsilon}\) is assumed to be
\(N(0,\sigma^2 \boldsymbol{I_n})\), then the least squares solution
would also be the maximum likelihood solution. Indeed, for
\(y_i \sim N(\mu_i, \sigma^2)\), the log-likelihood is \[
\log \left[\prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)\right] = \text{constant} - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \mu_i)^2.
\]


\backmatter

\end{document}
