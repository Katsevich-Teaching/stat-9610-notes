<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 9610Lecture Notes - 4&nbsp; Generalized linear models: General theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./glm-special-cases.html" rel="next">
<link href="./linear-models-misspecification.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./glm-general-theory.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Generalized linear models: General theory</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 9610<br>Lecture Notes</a> 
        <div class="sidebar-tools-main">
    <a href="./STAT-9610-br-Lecture-Notes.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-models-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear models: Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-models-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-models-misspecification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear models: Misspecification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-general-theory.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Generalized linear models: General theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-special-cases.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models: Special cases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multiple-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multiple testing</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-edm" id="toc-sec-edm" class="nav-link active" data-scroll-target="#sec-edm"><span class="header-section-number">4.1</span> Exponential dispersion model (EDM) distributions</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">4.1.1</span> Definition</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">4.1.2</span> Examples</a></li>
  <li><a href="#moments-of-exponential-dispersion-model-distributions" id="toc-moments-of-exponential-dispersion-model-distributions" class="nav-link" data-scroll-target="#moments-of-exponential-dispersion-model-distributions"><span class="header-section-number">4.1.3</span> Moments of exponential dispersion model distributions</a></li>
  <li><a href="#relationships-among-the-mean-variance-and-natural-parameter" id="toc-relationships-among-the-mean-variance-and-natural-parameter" class="nav-link" data-scroll-target="#relationships-among-the-mean-variance-and-natural-parameter"><span class="header-section-number">4.1.4</span> Relationships among the mean, variance, and natural parameter</a></li>
  <li><a href="#the-unit-deviance" id="toc-the-unit-deviance" class="nav-link" data-scroll-target="#the-unit-deviance"><span class="header-section-number">4.1.5</span> The unit deviance</a></li>
  <li><a href="#small-dispersion-approximations-to-an-edm" id="toc-small-dispersion-approximations-to-an-edm" class="nav-link" data-scroll-target="#small-dispersion-approximations-to-an-edm"><span class="header-section-number">4.1.6</span> Small-dispersion approximations to an EDM</a></li>
  </ul></li>
  <li><a href="#sec-glm-def" id="toc-sec-glm-def" class="nav-link" data-scroll-target="#sec-glm-def"><span class="header-section-number">4.2</span> Generalized linear models and examples</a>
  <ul class="collapse">
  <li><a href="#definition-1" id="toc-definition-1" class="nav-link" data-scroll-target="#definition-1"><span class="header-section-number">4.2.1</span> Definition</a></li>
  </ul></li>
  <li><a href="#sec-glm-max-lik" id="toc-sec-glm-max-lik" class="nav-link" data-scroll-target="#sec-glm-max-lik"><span class="header-section-number">4.3</span> Parameter estimation in GLMs</a>
  <ul class="collapse">
  <li><a href="#sec-glm-likelihood" id="toc-sec-glm-likelihood" class="nav-link" data-scroll-target="#sec-glm-likelihood"><span class="header-section-number">4.3.1</span> The GLM likelihood, score, and Fisher information</a></li>
  <li><a href="#sec-mle-glm" id="toc-sec-mle-glm" class="nav-link" data-scroll-target="#sec-mle-glm"><span class="header-section-number">4.3.2</span> Maximum likelihood estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
  <li><a href="#sec-irls" id="toc-sec-irls" class="nav-link" data-scroll-target="#sec-irls"><span class="header-section-number">4.3.3</span> Iteratively reweighted least squares</a></li>
  </ul></li>
  <li><a href="#sec-glm-inf" id="toc-sec-glm-inf" class="nav-link" data-scroll-target="#sec-glm-inf"><span class="header-section-number">4.4</span> Inference in GLMs</a>
  <ul class="collapse">
  <li><a href="#sec-preliminaries" id="toc-sec-preliminaries" class="nav-link" data-scroll-target="#sec-preliminaries"><span class="header-section-number">4.4.1</span> Preliminaries</a></li>
  <li><a href="#sec-wald-inference" id="toc-sec-wald-inference" class="nav-link" data-scroll-target="#sec-wald-inference"><span class="header-section-number">4.4.2</span> Wald inference</a></li>
  <li><a href="#sec-score-inference" id="toc-sec-score-inference" class="nav-link" data-scroll-target="#sec-score-inference"><span class="header-section-number">4.4.3</span> Score-based inference</a></li>
  </ul></li>
  <li><a href="#sec-r-demo" id="toc-sec-r-demo" class="nav-link" data-scroll-target="#sec-r-demo"><span class="header-section-number">4.5</span> R demo</a>
  <ul class="collapse">
  <li><a href="#sec-crime-data" id="toc-sec-crime-data" class="nav-link" data-scroll-target="#sec-crime-data"><span class="header-section-number">4.5.1</span> Crime data</a></li>
  <li><a href="#sec-noisy-miner-data" id="toc-sec-noisy-miner-data" class="nav-link" data-scroll-target="#sec-noisy-miner-data"><span class="header-section-number">4.5.2</span> Noisy miner data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="ch-glm-theory" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Generalized linear models: General theory</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Chapters 1-3 focused on the most common class of models used in applications: linear models. Despite their versatility, linear models do not apply in all situations. In particular, they are not designed to deal with binary or count responses. In Chapter 4, we introduce <em>generalized linear models</em> (GLMs), a generalization of linear models that encompasses a wide variety of incredibly useful models, including logistic regression and Poisson regression.</p>
<p>We’ll start Chapter 4 by introducing exponential dispersion models (Section <a href="#sec-edm"><span>Section&nbsp;4.1</span></a>), a generalization of the Gaussian distribution that serves as the backbone of GLMs. Then we formally define a GLM, demonstrating logistic regression and Poisson regression as special cases (Section <a href="#sec-glm-def"><span>Section&nbsp;4.2</span></a>). Next, we discuss maximum likelihood inference in GLMs (Section <a href="#sec-glm-max-lik"><span>Section&nbsp;4.3</span></a>). Finally, we discuss how to carry out statistical inference in GLMs (Section <a href="#sec-glm-inf"><span>Section&nbsp;4.4</span></a>).</p>
<section id="sec-edm" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-edm"><span class="header-section-number">4.1</span> Exponential dispersion model (EDM) distributions</h2>
<section id="definition" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">4.1.1</span> Definition</h3>
<p>Let’s start with the Gaussian distribution. If <span class="math inline">\(y \sim N(\mu, \sigma^2)\)</span>, then it has the following density with respect to the Lebesgue measure <span class="math inline">\(\nu\)</span> on <span class="math inline">\(\mathbb{R}\)</span>:</p>
<p><span class="math display">\[
f_{\mu, \sigma^2}(y) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y-\mu)^2\right) = \exp\left(\frac{\mu y - \frac{1}{2}\mu^2}{\sigma^2}\right) \cdot \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac1{2\sigma^2} y^2\right).
\]</span></p>
<p>We can consider a more general class of densities with respect to any measure <span class="math inline">\(\nu\)</span>:</p>
<p><span id="eq-edm-def"><span class="math display">\[
f_{\theta, \phi}(y) \equiv \exp\left(\frac{\theta y - \psi(\theta)}{\phi}\right)h(y, \phi), \quad \theta \in \Theta \subseteq \mathbb{R}, \ \phi &gt; 0.
\tag{4.1}\]</span></span></p>
<p>Here <span class="math inline">\(\theta\)</span> is called the <em>natural parameter</em>, <span class="math inline">\(\psi\)</span> is called the <em>log-partition function</em>, <span class="math inline">\(\Theta \equiv \{\theta: \psi(\theta) &lt; \infty\}\)</span> is called the natural parameter space, <span class="math inline">\(\phi &gt; 0\)</span> is called the <em>dispersion parameter</em>, and <span class="math inline">\(h\)</span> is called the <em>base density</em>. The distribution with density <span class="math inline">\(f_{\theta, \phi}\)</span> with respect to a measure <span class="math inline">\(\nu\)</span> on <span class="math inline">\(\mathbb{R}\)</span> is called an <em>exponential dispersion model</em> (EDM). Sometimes, we parameterize this distribution using its mean and dispersion, writing</p>
<p><span class="math display">\[
y \sim \text{EDM}(\mu, \phi).
\]</span></p>
<p>When <span class="math inline">\(\phi = 1\)</span>, the distribution becomes a <em>one-parameter natural exponential family</em>. The support of an EDM distribution remains fixed as <span class="math inline">\((\theta, \phi)\)</span> vary.</p>
</section>
<section id="examples" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="examples"><span class="header-section-number">4.1.2</span> Examples</h3>
<section id="normal-distribution" class="level4" data-number="4.1.2.1">
<h4 data-number="4.1.2.1" class="anchored" data-anchor-id="normal-distribution"><span class="header-section-number">4.1.2.1</span> Normal distribution</h4>
<p>As derived above, <span class="math inline">\(y \sim N(\mu, \sigma^2)\)</span> is an EDM with</p>
<p><span class="math display">\[
\theta = \mu, \quad \psi(\theta) = -\frac 12 \theta^2, \quad \phi = \sigma^2, \quad h(y, \phi) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac1{2\sigma^2} y^2\right).
\]</span></p>
</section>
<section id="bernoulli-distribution" class="level4" data-number="4.1.2.2">
<h4 data-number="4.1.2.2" class="anchored" data-anchor-id="bernoulli-distribution"><span class="header-section-number">4.1.2.2</span> Bernoulli distribution</h4>
<p>Suppose <span class="math inline">\(y \sim \text{Ber}(\mu)\)</span>. Then, we have</p>
<p><span class="math display">\[
f(y) = \mu^{y}(1-\mu)^{1-y} = \exp\left(y \log \frac{\mu}{1-\mu} + \log(1-\mu) \right).
\]</span></p>
<p>Therefore, we have <span class="math inline">\(\theta = \log \frac{\mu}{1-\mu}\)</span>, so that <span class="math inline">\(\log(1-\mu) = -\log(1+e^\theta)\)</span>. It follows that</p>
<p><span class="math display">\[
\theta = \log \frac{\mu}{1-\mu}, \quad \psi(\theta) = \log(1+e^\theta), \quad \phi = 1, \quad h(y) = 1.
\]</span></p>
<p>Hence, the Bernoulli distribution is an EDM, as well as a one-parameter exponential family. Note that <span class="math inline">\(\text{Ber}(0)\)</span> and <span class="math inline">\(\text{Ber}(1)\)</span> are not included in this class of EDMs, because there is no <span class="math inline">\(\theta \in \Theta = \mathbb{R}\)</span> that gives rise to <span class="math inline">\(\mu = 0\)</span> or <span class="math inline">\(\mu = 1\)</span>. Hence, <span class="math inline">\(\mu \in (0,1)\)</span>, and the support of any Bernoulli EDM is <span class="math inline">\(\{0,1\}\)</span>.</p>
</section>
<section id="binomial-distribution" class="level4" data-number="4.1.2.3">
<h4 data-number="4.1.2.3" class="anchored" data-anchor-id="binomial-distribution"><span class="header-section-number">4.1.2.3</span> Binomial distribution</h4>
<p>Consider the binomial proportion <span class="math inline">\(y\)</span>: <span class="math inline">\(my \sim \text{Bin}(m, \mu)\)</span>. We have</p>
<p><span class="math display">\[
f(y) = {m \choose my}\mu^{my}(1-\mu)^{m(1-y)} = \exp\left(m\left(y \log \frac{\mu}{1-\mu} + \log(1-\mu)\right)\right){m \choose my},
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\theta = \log\frac{\mu}{1-\mu}, \quad \psi(\theta) = \frac{e^{\theta}}{1 + e^{\theta}}, \quad \phi = 1/m, \quad h(y, \phi) = {m \choose my}.
\]</span></p>
<p>Note that <span class="math inline">\(\text{Bin}(m, 0)\)</span> and <span class="math inline">\(\text{Bin}(m, 1)\)</span> are not included in this class of EDMs, for the same reason as above. Hence, <span class="math inline">\(\mu \in (0,1)\)</span>, and the support of any binomial EDM is <span class="math inline">\(\{0,\frac{1}{m}, \frac{2}{m}, \dots, 1\}\)</span>.</p>
</section>
<section id="poisson-distribution" class="level4" data-number="4.1.2.4">
<h4 data-number="4.1.2.4" class="anchored" data-anchor-id="poisson-distribution"><span class="header-section-number">4.1.2.4</span> Poisson distribution</h4>
<p>Suppose <span class="math inline">\(y \sim \text{Poi}(\mu)\)</span>. We have</p>
<p><span class="math display">\[
f(y) = e^{-\mu}\frac{\mu^y}{y!} = \exp(y \log \mu - \mu)\frac{1}{y!}.
\]</span></p>
<p>Therefore, we have <span class="math inline">\(\theta = \log \mu\)</span>, so that <span class="math inline">\(\mu = e^\theta\)</span>. It follows that</p>
<p><span class="math display">\[
\theta = \log \mu, \quad \psi(\theta) = e^\theta,\quad \phi = 1, \quad h(y) = \frac{1}{y!}.
\]</span></p>
<p>Hence, the Poisson distribution is an EDM, as well as a one-parameter exponential family. Note that <span class="math inline">\(\text{Poi}(0)\)</span> is not included in this class of EDMs, because there is no <span class="math inline">\(\theta \in \Theta = \mathbb{R}\)</span> that gives rise to <span class="math inline">\(\mu = 0\)</span>. Hence, <span class="math inline">\(\mu \in (0,\infty)\)</span>, and the support of any Poisson EDM is <span class="math inline">\(\mathbb{N}\)</span>.</p>
<p>Many other examples fall into this class, including the negative binomial, gamma, and inverse-Gaussian distributions. We will see at least some of these in the next chapter.</p>
</section>
</section>
<section id="moments-of-exponential-dispersion-model-distributions" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="moments-of-exponential-dispersion-model-distributions"><span class="header-section-number">4.1.3</span> Moments of exponential dispersion model distributions</h3>
<p>It turns out that the derivatives of the log-partition function <span class="math inline">\(\psi\)</span> give the moments of <span class="math inline">\(y\)</span>. Indeed, let’s start with the relationship</p>
<p><span class="math display">\[
\int f_{\theta, \phi}(y)d\nu(y) = \int \exp\left(\frac{\theta y - \psi(\theta)}{\phi}\right)h(y, \phi) d\nu(y) = 1.
\]</span></p>
<p>Differentiating in <span class="math inline">\(\theta\)</span> and interchanging the derivative and the integral, we obtain</p>
<p><span class="math display">\[
0 = \frac{d}{d\theta} \int f_{\theta, \phi}(y)dy = \int \frac{y - \dot \psi(\theta)}{\phi}f_{\theta, \phi}(y) dy,
\]</span></p>
<p>from which it follows that</p>
<p><span id="eq-psi-dot"><span class="math display">\[
\dot \psi(\theta) = \int \dot \psi(\theta)f_{\theta, \phi}(y)dy = \int y f_{\theta, \phi}(y)dy = \mathbb{E}[y] \equiv \mu.
\tag{4.2}\]</span></span></p>
<p>Thus, the first derivative of the log partition function is the mean of <span class="math inline">\(y\)</span>. Differentiating again, we get</p>
<p><span class="math display">\[
\phi \cdot \ddot \psi(\theta) = \phi \int \ddot \psi(\theta) f_{\theta, \phi}(y) d\nu(y) = \int (y - \dot \psi(\theta))^2 f_{\theta, \phi}(y) dy = \int (y - \mu)^2f_{\theta, \phi}(y) d\nu(y) = \text{Var}[y].
\]</span></p>
<p>Thus, the second derivative of the log-partition function multiplied by the dispersion parameter is the variance of <span class="math inline">\(y\)</span>.</p>
</section>
<section id="relationships-among-the-mean-variance-and-natural-parameter" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="relationships-among-the-mean-variance-and-natural-parameter"><span class="header-section-number">4.1.4</span> Relationships among the mean, variance, and natural parameter</h3>
<section id="relationship-between-the-mean-and-the-natural-parameter" class="level4" data-number="4.1.4.1">
<h4 data-number="4.1.4.1" class="anchored" data-anchor-id="relationship-between-the-mean-and-the-natural-parameter"><span class="header-section-number">4.1.4.1</span> Relationship between the mean and the natural parameter</h4>
<p>The log-partition function <span class="math inline">\(\psi\)</span> induces a connection (<a href="#eq-psi-dot"><span>4.2</span></a>) between the natural parameter <span class="math inline">\(\theta\)</span> and the mean <span class="math inline">\(\mu\)</span>. Because</p>
<p><span id="eq-dmu-dtheta"><span class="math display">\[
\frac{d\mu}{d\theta} = \frac{d}{d\theta}\dot \psi(\theta) = \ddot \psi(\theta) = \frac{1}{\phi}\text{Var}[y] &gt; 0,
\tag{4.3}\]</span></span></p>
<p>it follows that <span class="math inline">\(\mu\)</span> is a strictly increasing function of <span class="math inline">\(\theta\)</span>, so in particular the mapping between <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\theta\)</span> is bijective. Therefore, we can think of equivalently parameterizing the distribution via <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\theta\)</span>.</p>
</section>
<section id="relationship-between-the-mean-and-variance" class="level4" data-number="4.1.4.2">
<h4 data-number="4.1.4.2" class="anchored" data-anchor-id="relationship-between-the-mean-and-variance"><span class="header-section-number">4.1.4.2</span> Relationship between the mean and variance</h4>
<p>Note that the mean of an EDM, together with the dispersion parameter, determines its variance (since it determines the natural parameter <span class="math inline">\(\theta\)</span>). Define</p>
<p><span class="math display">\[
V(\mu) \equiv \frac{d\mu}{d\theta},
\]</span></p>
<p>so that <span class="math inline">\(\text{Var}[y] = \phi V(\mu)\)</span>. For example, a Poisson random variable with mean <span class="math inline">\(\mu\)</span> has variance <span class="math inline">\(\mu\)</span> and a Bernoulli random variable with mean <span class="math inline">\(\mu\)</span> has <span class="math inline">\(V(\mu) = \mu(1-\mu)\)</span>. The mean-variance relationship turns out to characterize the EDM, i.e.&nbsp;an EDM with mean equal to its variance is the Poisson distribution. For all EDMs except the normal distribution, the variance depends nontrivially on the mean. Therefore, heteroskedasticity is a natural feature of EDMs (rather than a pathology that needs to be corrected for).</p>
</section>
</section>
<section id="the-unit-deviance" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="the-unit-deviance"><span class="header-section-number">4.1.5</span> The unit deviance</h3>
<p>It’s possible to rewrite the EDM distribution in terms of <span class="math inline">\(\mu\)</span> rather than in terms of <span class="math inline">\(\theta\)</span>. Take the quantity in the numerator of the exponential in the EDM density (<a href="#eq-edm-def"><span>4.1</span></a>) and call it <span class="math inline">\(t(y, \mu)\)</span>:</p>
<p><span class="math display">\[
t(y, \mu) \equiv \theta y - \psi(\theta).
\]</span></p>
<p>Let’s consider the shape of this function by taking the first two derivatives with respect to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \theta}t(y, \mu) = y - \mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{\partial^2}{\partial \theta^2}t(y, \mu) = -V(\mu) &lt; 0.
\]</span></p>
<p>Hence, <span class="math inline">\(t(y, \mu)\)</span> has a unique global maximum at <span class="math inline">\(\mu = y\)</span>. We can then define the <em>unit deviance</em> <span class="math inline">\(d(y, \mu)\)</span> as twice the distance between the value <span class="math inline">\(t(y, \mu)\)</span> and the optimal value <span class="math inline">\(t(y,y)\)</span>:</p>
<p><span class="math display">\[
d(y, \mu) \equiv 2(t(y, y) - t(y, \mu)).
\]</span></p>
<p>The unit deviance is nonnegative, and minimized by <span class="math inline">\(\mu = y\)</span>. For the normal distribution, the unit deviance is <span class="math inline">\(d(y, \mu) = (y - \mu)^2\)</span>. The unit deviance can therefore be viewed as a “distance” between the mean <span class="math inline">\(\mu\)</span> and the observation <span class="math inline">\(y\)</span>.</p>
<section id="example" class="level4" data-number="4.1.5.1">
<h4 data-number="4.1.5.1" class="anchored" data-anchor-id="example"><span class="header-section-number">4.1.5.1</span> Example</h4>
<p>For the Poisson distribution, we have <span class="math inline">\(t(y, \mu) = y\log \mu - \mu\)</span>, so</p>
<p><span class="math display">\[
d(y, \mu) \equiv 2(t(y, y) - t(y, \mu)) = 2(y\log y - y - y \log \mu + \mu) = 2\left(y\log \frac{y}{\mu} - (y - \mu)\right).
\]</span></p>
<p>See Figure <a href="#fig-poisson-unit-deviance">Figure&nbsp;<span>4.1</span></a> for an example of the shape of this function.</p>
<div id="fig-poisson-unit-deviance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><embed src="figures/poisson-unit-deviance.pdf" class="img-fluid" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;4.1: The Poisson unit deviance for <span class="math inline">\(y = 4\)</span>.</figcaption>
</figure>
</div>
<p>Note that the Poisson deviance is asymmetric about <span class="math inline">\(\mu = y\)</span>. This is a consequence of the nontrivial mean-variance relationship for the Poisson distribution. In particular, the Poisson distribution’s variance grows with its mean. Therefore, an observation of <span class="math inline">\(y = 4\)</span> is less likely to have come from a Poisson distribution with mean <span class="math inline">\(\mu = 2\)</span> than from a Poisson distribution with mean <span class="math inline">\(\mu = 6\)</span>.</p>
</section>
</section>
<section id="small-dispersion-approximations-to-an-edm" class="level3" data-number="4.1.6">
<h3 data-number="4.1.6" class="anchored" data-anchor-id="small-dispersion-approximations-to-an-edm"><span class="header-section-number">4.1.6</span> Small-dispersion approximations to an EDM</h3>
<p>If the dispersion <span class="math inline">\(\phi\)</span> is small, then that means that <span class="math inline">\(y\)</span> is a fairly precise estimate of <span class="math inline">\(\mu\)</span>, similar to an average of multiple independent samples from a mean-<span class="math inline">\(\mu\)</span> distribution. Consider, for example, that <span class="math inline">\(\frac{1}{m}\text{Bin}(m, \mu)\)</span> is the mean of <span class="math inline">\(m\)</span> i.i.d. draws from <span class="math inline">\(\text{Ber}(\mu)\)</span>. In this case, we can use either the normal approximation or the saddlepoint approximation to approximate the EDM density. For the sake of this section, we will abuse notation by denoting by <span class="math inline">\(f_{\mu, \phi}\)</span> the EDM with mean <span class="math inline">\(\mu\)</span> and dispersion <span class="math inline">\(\phi\)</span>.</p>
<section id="the-normal-approximation" class="level4" data-number="4.1.6.1">
<h4 data-number="4.1.6.1" class="anchored" data-anchor-id="the-normal-approximation"><span class="header-section-number">4.1.6.1</span> The normal approximation</h4>
<section id="the-approximation" class="level5" data-number="4.1.6.1.1">
<h5 data-number="4.1.6.1.1" class="anchored" data-anchor-id="the-approximation"><span class="header-section-number">4.1.6.1.1</span> The approximation</h5>
<p>For small values of <span class="math inline">\(\phi\)</span>, we can hope to approximate <span class="math inline">\(f_{\mu, \phi}\)</span> with a normal distribution. Recall that the mean and variance of this distribution are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\phi \cdot V(\mu)\)</span>, respectively. The central limit theorem gives</p>
<p><span class="math display">\[
\frac{y - \mu}{\sqrt{\phi \cdot V(\mu)}} \rightarrow_d N(0,1) \quad \text{as} \quad \phi \rightarrow 0,
\]</span></p>
<p>so</p>
<p><span class="math display">\[
y \overset \cdot \sim N(\mu, \phi \cdot V(\mu)) \equiv \tilde f^{\text{normal}}_{\mu, \phi}.
\]</span></p>
<p>For example, we have</p>
<p><span class="math display">\[
\text{Poi}(\mu) \approx N(\mu, \mu).
\]</span></p>
<p>For the normal EDM, note that the normal approximation is exact. One consequence of the normal approximation is</p>
<p><span class="math display">\[
\frac{(y - \mu)^2}{\phi \cdot V(\mu)} \overset \cdot \sim \chi^2_1.
\]</span></p>
<p>This fact will be useful to us as we carry out inference for GLMs.</p>
</section>
<section id="approximation-accuracy" class="level5" data-number="4.1.6.1.2">
<h5 data-number="4.1.6.1.2" class="anchored" data-anchor-id="approximation-accuracy"><span class="header-section-number">4.1.6.1.2</span> Approximation accuracy</h5>
<p>We have</p>
<p><span class="math display">\[
\tilde f^{\text{normal}}_{\mu, \phi}(y) = f_{\mu, \phi}(y) + O(\sqrt{\phi}).
\]</span></p>
<p>In practice, the rule of thumb for the applicability of this approximation is that</p>
<p><span class="math display">\[
\tau \equiv \frac{\phi \cdot V(\mu)}{(\mu - \text{boundary})^2} \leq \frac{1}{5}.
\]</span></p>
<p>Here, “boundary” represents the nearest boundary of the parameter space to <span class="math inline">\(\mu\)</span>. For example, if <span class="math inline">\(y \sim \frac{1}{m}\text{Bin}(m, \mu)\)</span>, then we have</p>
<p><span class="math display">\[
\tau = \frac{\frac{1}{m} \cdot \mu \cdot (1-\mu)}{\min(\mu, 1-\mu)^2} = \frac{1}{m} \cdot \max\left(\frac{\mu}{1-\mu}, \frac{1-\mu}{\mu}\right) \approx \frac{1}{m} \cdot \max\left(\frac 1 \mu, \frac 1 {1-\mu}\right),
\]</span></p>
<p>so <span class="math inline">\(\tau \leq 1/5\)</span> roughly if <span class="math inline">\(m \mu \leq 5\)</span> and <span class="math inline">\(m (1-\mu) \leq 5\)</span>. For Poisson distributions, we always have <span class="math inline">\(\tau = 1\)</span>, but for some reason small-dispersion asymptotics still applies as <span class="math inline">\(\mu \rightarrow \infty\)</span> as opposed to <span class="math inline">\(\tau \rightarrow 0\)</span>. The criterion <span class="math inline">\(\tau \leq 1/5\)</span> is satisfied when <span class="math inline">\(\mu \leq 5\)</span>.</p>
</section>
</section>
<section id="the-saddlepoint-approximation" class="level4" data-number="4.1.6.2">
<h4 data-number="4.1.6.2" class="anchored" data-anchor-id="the-saddlepoint-approximation"><span class="header-section-number">4.1.6.2</span> The saddlepoint approximation</h4>
<section id="the-approximation-1" class="level5" data-number="4.1.6.2.1">
<h5 data-number="4.1.6.2.1" class="anchored" data-anchor-id="the-approximation-1"><span class="header-section-number">4.1.6.2.1</span> The approximation</h5>
<p>Another approximation to the EDM density is the saddlepoint approximation, which tends to be more accurate than the normal approximation. The reason the normal approximation may be inaccurate is that the quality of the central limit approximation degrades as one enters the tails of the distribution. In particular, the normal approximation to <span class="math inline">\(f_{\mu, \phi}(y)\)</span> may be poor if <span class="math inline">\(\mu\)</span> is far from <span class="math inline">\(y\)</span>. The saddlepoint approximation is built on the observation that the EDM density for <span class="math inline">\(f_{\mu, \phi}(y)\)</span> can be written in terms of the density <span class="math inline">\(f_{y, \phi}(y)\)</span>; the latter density is by definition evaluated at its mean. Indeed,</p>
<p><span id="eq-deviance-form"><span class="math display">\[
\begin{split}
f_{\mu, \phi}(y) &amp;\equiv \exp\left(\frac{\theta y - \psi(\theta)}{\phi}\right)h(y, \phi) \\
&amp;= \exp\left(\frac{t(y, \mu)}{\phi}\right)h(y, \phi) \\
&amp;= \exp\left(\frac{-2(t(y, y) - t(y, \mu)) + 2t(y,y)}{2\phi}\right)h(y, \phi) \\
&amp;= \exp\left(-\frac{d(y, \mu)}{2\phi}\right)\exp\left(\frac{t(y,y)}{\phi}\right) h(y, \phi) \\
&amp;= \exp\left(-\frac{d(y, \mu)}{2\phi}\right)f_{y, \phi}(y).
\end{split}
\tag{4.4}\]</span></span></p>
<p>Now, we apply the central limit theorem to approximate <span class="math inline">\(f_{y, \phi}(y)\)</span>:</p>
<p><span class="math display">\[
f_{y, \phi}(y) \approx \frac{1}{\sqrt{2\pi \phi V(y)}}.
\]</span></p>
<p>Substituting this approximation into (<a href="#eq-deviance-form"><span>4.4</span></a>), we obtain the <em>saddlepoint approximation</em>:</p>
<p><span class="math display">\[
f_{\mu, \phi}(y) \approx \frac{1}{\sqrt{2\pi \phi V(y)}}\exp\left(-\frac{d(y, \mu)}{2\phi}\right) \equiv \widetilde f^{\text{saddle}}_{\mu, \phi}(y).
\]</span></p>
<p>For the normal EDM, note that the normal approximation is exact. For the Poisson distribution, we get</p>
<p><span class="math display">\[
\widetilde f^{\text{saddle}}_{\mu, \phi}(y) = \frac{1}{\sqrt{2\pi y}}\exp\left(y \log \frac y \mu - (y - \mu)\right).
\]</span></p>
<p>The approximation can be shown to lead to the following consequence:</p>
<p><span class="math display">\[
\frac{d(y, \mu)}{\phi} \overset \cdot \sim \chi^2_1.
\]</span></p>
<p>Here, we are using the unit deviance rather than the squared distance to measure the deviation of <span class="math inline">\(\mu\)</span> from <span class="math inline">\(y\)</span>. This fact will be useful to us as we carry out inference for GLMs.</p>
</section>
<section id="approximation-accuracy-1" class="level5" data-number="4.1.6.2.2">
<h5 data-number="4.1.6.2.2" class="anchored" data-anchor-id="approximation-accuracy-1"><span class="header-section-number">4.1.6.2.2</span> Approximation accuracy</h5>
<p>We have still used a normal approximation, but this time we have used it to approximate <span class="math inline">\(f_{y, \phi}(y)\)</span> instead of <span class="math inline">\(f_{\mu, \phi}(y)\)</span>. Since the normal approximation is applied to a distribution (<span class="math inline">\(f_{y, \phi}\)</span>) at its mean, we expect it to be more accurate than a normal approximation applied to a distribution (<span class="math inline">\(f_{\mu, \phi}\)</span>) at a point potentially far from its mean. The saddlepoint approximation yields an approximation to the density that is <em>multiplicative</em> rather than <em>additive</em>, and of order <span class="math inline">\(O(\phi)\)</span> rather than <span class="math inline">\(O(\sqrt{\phi})\)</span>:</p>
<p><span class="math display">\[
\tilde f^{\text{saddle}}_{\mu, \phi}(y) = f_{\mu, \phi}(y) \cdot (1 + O(\phi)).
\]</span></p>
<p>In practice, the rule of thumb for the applicability of this approximation is that <span class="math inline">\(\tau \leq 1/3\)</span>; the looser requirement on <span class="math inline">\(\tau\)</span> reflects the greater accuracy of the saddlepoint approximation. This translates to <span class="math inline">\(m\mu \geq 3\)</span> and <span class="math inline">\(m(1-\mu) \geq 3\)</span> for the binomial and <span class="math inline">\(\mu \geq 3\)</span> for the Poisson.</p>
</section>
</section>
<section id="comparing-the-two-approximations" class="level4" data-number="4.1.6.3">
<h4 data-number="4.1.6.3" class="anchored" data-anchor-id="comparing-the-two-approximations"><span class="header-section-number">4.1.6.3</span> Comparing the two approximations</h4>
<p>The saddlepoint approximation is more accurate than the normal approximation, as discussed above. However, the accuracy of the saddlepoint approximation relies on the assumption that the entire parametric form of the EDM is correctly specified. On the other hand, the accuracy of the normal distribution requires only that the first two moments of the EDM are correctly specified.</p>
</section>
</section>
</section>
<section id="sec-glm-def" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-glm-def"><span class="header-section-number">4.2</span> Generalized linear models and examples</h2>
<p>In this class, the focus is on building models that tie a vector of predictors <span class="math inline">\((\boldsymbol{x_{i*}})\)</span> to a response <span class="math inline">\(y_i\)</span>. For linear regression, the mean of <span class="math inline">\(y\)</span> was modeled as a linear combination of the predictors <span class="math inline">\(\boldsymbol{x_{i*}^T} \boldsymbol{\beta}\)</span>: <span class="math inline">\(\mu_i = \boldsymbol{x_{i*}^T} \boldsymbol{\beta}\)</span>. More generally, we might want to model <em>a function</em> of the mean <span class="math inline">\(\eta_i = g(\mu_i)\)</span> as a linear combination of the predictors; <span class="math inline">\(g\)</span> is called the <em>link function</em> and <span class="math inline">\(\eta_i\)</span> the <em>linear predictor</em>. Pairing a link function with an EDM gives us a <em>generalized linear model</em> (GLM):</p>
<section id="definition-1" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="definition-1"><span class="header-section-number">4.2.1</span> Definition</h3>
<p>We define <span class="math inline">\(\{(y_i, \boldsymbol{x_{i*}})\}_{i = 1}^n\)</span> as following a generalized linear model based on the exponential dispersion model <span class="math inline">\(f_{\theta, \phi}\)</span>, monotonic and differentiable link function <span class="math inline">\(g\)</span>, and observation weights <span class="math inline">\(w_i\)</span> if</p>
<p><span id="eq-glm-def"><span class="math display">\[
y_i \overset{\text{ind}} \sim \text{EDM}(\mu_i, \phi_0/w_i), \quad \eta_i \equiv g(\mu_i) = o_i + \boldsymbol{x^T_{i*}}\boldsymbol{\beta}.
\tag{4.5}\]</span></span></p>
<p>The offset terms <span class="math inline">\(o_i\)</span> and observation weights <span class="math inline">\(w_i\)</span> are both known in advance. The free parameters in a GLM are the coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> and, possibly, the parameter <span class="math inline">\(\phi_0\)</span> controlling the dispersion. We will see examples where <span class="math inline">\(\phi_0\)</span> is known (e.g.&nbsp;Poisson regression) and those where <span class="math inline">\(\phi_0\)</span> is unknown (e.g.&nbsp;linear regression).</p>
<p>The “default” choice for the link function <span class="math inline">\(g\)</span> is the <em>canonical link function</em></p>
<p><span class="math display">\[
g(\mu) = \dot \psi^{-1}(\mu),
\]</span></p>
<p>which, given the relationship (<a href="#eq-psi-dot"><span>4.2</span></a>), gives <span class="math inline">\(\eta = \dot \psi^{-1}(\mu) = \theta\)</span>, i.e.&nbsp;the linear predictor coincides with the natural parameter. As discussed in the context of equation (<a href="#eq-dmu-dtheta"><span>4.3</span></a>), <span class="math inline">\(\dot \psi^{-1}\)</span> is a valid link function because it is monotonic and differentiable. Canonical link functions are very commonly used with EDMs because they lead to various nice properties that general EDMs do not enjoy (e.g.&nbsp;concave log-likelihood).</p>
<section id="example-linear-regression-model" class="level4" data-number="4.2.1.1">
<h4 data-number="4.2.1.1" class="anchored" data-anchor-id="example-linear-regression-model"><span class="header-section-number">4.2.1.1</span> Example: Linear regression model</h4>
<p>The linear regression model is a special case of a GLM, with <span class="math inline">\(\phi_0 = \sigma^2\)</span> (unknown), <span class="math inline">\(w_i = 1\)</span>, <span class="math inline">\(o_i = 0\)</span>, and identity (canonical) link function:</p>
<p><span class="math display">\[
y_i \overset{\text{ind}}\sim N(\mu_i, \sigma^2); \quad \eta_i = \mu_i = \boldsymbol{x_{i*}^T} \boldsymbol{\beta}.
\]</span></p>
</section>
<section id="example-weighted-linear-regression-model" class="level4" data-number="4.2.1.2">
<h4 data-number="4.2.1.2" class="anchored" data-anchor-id="example-weighted-linear-regression-model"><span class="header-section-number">4.2.1.2</span> Example: Weighted linear regression model</h4>
<p>If each observation <span class="math inline">\(y_i\)</span> is the mean of <span class="math inline">\(m_i\)</span> independent repeated observations, then we get a weighted linear regression model, with <span class="math inline">\(\phi_0 = \sigma^2\)</span> (unknown), <span class="math inline">\(w_i = m_i\)</span>, <span class="math inline">\(o_i = 0\)</span>, and identity (canonical) link function:</p>
<p><span class="math display">\[
y_i \overset{\text{ind}}\sim N(\mu_i, {\textstyle \frac{\sigma^2}{m_i}}); \quad \eta_i = \mu_i = \boldsymbol{x_{i*}^T} \boldsymbol{\beta}.
\]</span></p>
</section>
<section id="example-ungrouped-logistic-regression-model" class="level4" data-number="4.2.1.3">
<h4 data-number="4.2.1.3" class="anchored" data-anchor-id="example-ungrouped-logistic-regression-model"><span class="header-section-number">4.2.1.3</span> Example: Ungrouped logistic regression model</h4>
<p>The <em>ungrouped logistic regression model</em> is the GLM based on the Bernoulli EDM with <span class="math inline">\(\phi_0 = 1\)</span> (known), <span class="math inline">\(w_i = 1\)</span>, <span class="math inline">\(o_i = 0\)</span>, and the canonical link function:</p>
<p><span class="math display">\[
y_i \overset{\text{ind}}\sim \text{Ber}(\mu_i); \quad \eta_i = \theta_i = \log\frac{\mu_i}{1-\mu_i} = \boldsymbol{x_{i*}^T} \boldsymbol{\beta}.
\]</span></p>
<p>Thus the canonical link function for logistic regression is the <em>logistic link function</em> <span class="math inline">\(g(\mu) = \log \frac{\mu}{1-\mu}\)</span>.</p>
</section>
<section id="example-grouped-logistic-regression-model" class="level4" data-number="4.2.1.4">
<h4 data-number="4.2.1.4" class="anchored" data-anchor-id="example-grouped-logistic-regression-model"><span class="header-section-number">4.2.1.4</span> Example: Grouped logistic regression model</h4>
<p>Suppose <span class="math inline">\(y_i\)</span> is a binomial proportion based on <span class="math inline">\(m_i\)</span> trials. The <em>grouped logistic regression model</em> is the GLM based on the binomial EDM with <span class="math inline">\(\phi_0 = 1\)</span> (known), <span class="math inline">\(w_i = 1/m_i\)</span>, <span class="math inline">\(o_i = 0\)</span>, and the canonical link function:</p>
<p><span class="math display">\[
m_i y_i \sim \text{Bin}(m_i, \mu_i); \quad \eta_i = \log \frac{\mu_i}{1-\mu_i} = o_i + \boldsymbol{x^T_{i*}}\boldsymbol{\beta}.
\]</span></p>
<p>Note that a binomial proportion <span class="math inline">\(y_i\)</span> based on <span class="math inline">\(m_i\)</span> trials and a success probability of <span class="math inline">\(\mu_i\)</span> can be equivalently represented as <span class="math inline">\(m_i\)</span> independent Bernoulli draws with the same success probability <span class="math inline">\(\mu_i\)</span>. Therefore, any grouped logistic regression model can be equivalently represented as an ungrouped logistic regression model with <span class="math inline">\(\sum_{i = 1}^n m_i\)</span> observations. We will see that, despite this equivalence, grouped logistic regression models have some useful properties that ungrouped logistic regression models do not.</p>
</section>
<section id="example-poisson-regression-model" class="level4" data-number="4.2.1.5">
<h4 data-number="4.2.1.5" class="anchored" data-anchor-id="example-poisson-regression-model"><span class="header-section-number">4.2.1.5</span> Example: Poisson regression model</h4>
<p><em>Poisson regression</em> is the Poisson EDM with <span class="math inline">\(\phi_0 = 1\)</span> (known), <span class="math inline">\(w_i = 1\)</span>, <span class="math inline">\(o_i = 0\)</span>, and the canonical link function:</p>
<p><span class="math display">\[
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \eta_i = \theta_i = \log \mu_i = \boldsymbol{x_{i*}^T} \boldsymbol{\beta}.
\]</span></p>
<p>Thus the canonical link function for Poisson regression is the <em>log link function</em> <span class="math inline">\(g(\mu) = \log \mu\)</span>.</p>
</section>
</section>
</section>
<section id="sec-glm-max-lik" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-glm-max-lik"><span class="header-section-number">4.3</span> Parameter estimation in GLMs</h2>
<section id="sec-glm-likelihood" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="sec-glm-likelihood"><span class="header-section-number">4.3.1</span> The GLM likelihood, score, and Fisher information</h3>
<p>The log-likelihood of a GLM is:</p>
<p><span id="eq-glm-log-likelihood"><span class="math display">\[
\log \mathcal L(\boldsymbol{\beta}) = \sum_{i = 1}^n \frac{\theta_i y_i - \psi(\theta_i)}{\phi_0/w_i} + \sum_{i = 1}^n \log h(y_i, \phi_0/w_i).
\tag{4.6}\]</span></span></p>
<p>Let’s differentiate this with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, using the chain rule:</p>
<p><span class="math display">\[
\begin{split}
  \frac{\partial \log \mathcal L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &amp;= \frac{\partial \log \mathcal L(\boldsymbol{\beta})}{\partial \boldsymbol{\theta}}\frac{\partial \boldsymbol{\theta}}{\partial \boldsymbol{\mu}} \frac{\partial \boldsymbol{\mu}}{\partial \boldsymbol{\eta}}\frac{\partial \boldsymbol{\eta}}{\partial \boldsymbol{\beta}} \\
  &amp;=  (\boldsymbol{y} - \boldsymbol{\mu})^T \text{diag}(\phi_0/w_i)^{-1} \cdot \text{diag}(\ddot{\psi}(\theta_i))^{-1} \cdot \text{diag}\left(\frac{\partial\mu_i}{\partial \eta_i}\right) \cdot \boldsymbol{X}\\
  &amp;= \frac{1}{\phi_0}(\boldsymbol{y} - \boldsymbol{\mu})^T \text{diag}\left(\frac{w_i}{V(\mu_i)(d\eta_i/d\mu_i)^2}\right)\cdot \text{diag}\left(\frac{\partial\eta_i}{\partial \mu_i}\right) \cdot \boldsymbol{X} \\
  &amp;\equiv \frac{1}{\phi_0}(\boldsymbol{y} - \boldsymbol{\mu})^T \boldsymbol{W} \boldsymbol{M} \boldsymbol{X}.
\end{split}
\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{W} \equiv \text{diag}(W_i)\)</span> is a diagonal matrix of <em>working weights</em> and <span class="math inline">\(\boldsymbol{M} \equiv \text{diag}\left(\frac{\partial\eta_i}{\partial \mu_i}\right) = \text{diag}(g'(\mu_i))\)</span> is a diagonal matrix of link derivatives. Transposing, we get the score vector:</p>
<p><span id="eq-glm-score"><span class="math display">\[
\boldsymbol{U}(\boldsymbol{\beta}) = \frac{1}{\phi_0}\boldsymbol{X}^T \boldsymbol{M} \boldsymbol{W} (\boldsymbol{y} - \boldsymbol{\mu}).
\tag{4.7}\]</span></span></p>
<p>To get the Fisher information matrix, note first that:</p>
<p><span id="eq-glm-variance-y"><span class="math display">\[
\text{Var}[\boldsymbol{y}] = \text{diag}\left(\phi_0\frac{V(\mu_i)}{w_i}\right) = \phi_0 \boldsymbol{W}^{-1} \boldsymbol{M}^{-2}
\tag{4.8}\]</span></span></p>
<p>we can compute the covariance matrix of the score vector:</p>
<p><span id="eq-glm-fisher-info"><span class="math display">\[
\begin{split}
\boldsymbol{I}(\boldsymbol{\beta}) = \text{Var}[\boldsymbol{U}(\boldsymbol{\beta})] &amp;= \frac{1}{\phi^2_0}\boldsymbol{X}^T \boldsymbol{M} \boldsymbol{W} \text{Var}[\boldsymbol{y}] \boldsymbol{M} \boldsymbol{W} \boldsymbol{X} \\
&amp;= \frac{1}{\phi^2_0}\boldsymbol{X}^T \boldsymbol{M} \boldsymbol{W} \phi_0 \boldsymbol{W}^{-1}\boldsymbol{M}^{-2} \boldsymbol{M} \boldsymbol{W} \boldsymbol{X} \\
&amp;= \frac{1}{\phi_0}\boldsymbol{X}^T \boldsymbol{W} \boldsymbol{X}.
\end{split}
\tag{4.9}\]</span></span></p>
</section>
<section id="sec-mle-glm" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="sec-mle-glm"><span class="header-section-number">4.3.2</span> Maximum likelihood estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span></h3>
<p>To estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>, we can set the score vector to zero:</p>
<p><span class="math display">\[
\frac{1}{\phi_0}\boldsymbol{X}^T \widehat{\boldsymbol{M}} \widehat{\boldsymbol{W}} (\boldsymbol{y} - \widehat{\boldsymbol{\mu}}) = 0 \quad \Longleftrightarrow \quad \boldsymbol{X}^T \text{diag}\left(\frac{w_i}{V(\widehat \mu_i)g'(\widehat \mu_i)}\right)(\boldsymbol{y} - \widehat{\boldsymbol{\mu}}) = 0.
\]</span></p>
<p>These equations are called the <em>normal equations</em>. Unfortunately, unlike least squares, the normal equations cannot be solved analytically for <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>. They are solved numerically instead; see Section <a href="#sec-irls"><span>4.3.3</span></a>. Note that <span class="math inline">\(\phi_0\)</span> cancels from the normal equations, and therefore the coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> can be estimated without estimating the dispersion. Recall that we have seen this phenomenon for least squares. Also note that the normal equations simplify when the canonical link function is used, so that <span class="math inline">\(\eta_i = \theta_i\)</span>. Assuming additionally that <span class="math inline">\(w_i = 1\)</span>, we get:</p>
<p><span class="math display">\[
\boldsymbol{\widehat M} \boldsymbol{\widehat W} = \text{diag}\left(\frac{\widehat{\partial \mu_i/\partial \theta_i}}{V(\widehat \mu_i)}\right) = \frac{\ddot{\psi}(\widehat \theta_i)}{\ddot{\psi}(\widehat \theta_i)} = 1,
\]</span></p>
<p>so the normal equations reduce to:</p>
<p><span id="eq-canonical-normal-equations"><span class="math display">\[
\boldsymbol{X}^T (\boldsymbol{y} - \widehat{\boldsymbol{\mu}}) = 0.
\tag{4.10}\]</span></span></p>
<p>We recognize these as the normal equation for linear regression. Since both ungrouped logistic regression and Poisson regression also use canonical links and have unit weights, the simplified normal equations <a href="#eq-canonical-normal-equations"><span>4.10</span></a> apply to the latter regressions as well.</p>
<p>In the linear regression case, we interpreted the normal equations <a href="#eq-canonical-normal-equations"><span>4.10</span></a> as an orthogonality statement: <span class="math inline">\(\boldsymbol{y} - \widehat{\boldsymbol{\mu}} \perp C(\boldsymbol{X})\)</span>. In the case of GLMs, the <span class="math inline">\(C(\boldsymbol{X}) \equiv \{\boldsymbol{\mu} = \mathbb{E}[\boldsymbol{y}]: \boldsymbol{\beta} \in \mathbb{R}^p\}\)</span> is no longer a linear space. In fact, it is a nonlinear transformation of the column space of <span class="math inline">\(\boldsymbol{X}\)</span> (a <span class="math inline">\(p\)</span>-dimensional manifold in <span class="math inline">\(\mathbb{R}^n\)</span>):</p>
<p><span class="math display">\[
C(\boldsymbol{X}) \equiv \{\boldsymbol{\mu} = \mathbb{E}[\boldsymbol{y}]: \boldsymbol{\beta} \in \mathbb{R}^p\} = \{g^{-1}(\boldsymbol{X} \boldsymbol{\beta}): \boldsymbol{\beta} \in \mathbb{R}^p\}.
\]</span></p>
<p>Therefore, we cannot view the mapping <span class="math inline">\(\boldsymbol{y} \mapsto \boldsymbol{\widehat \mu}\)</span> as a linear projection. Nevertheless, it is possible to interpret <span class="math inline">\(\boldsymbol{\widehat \mu}\)</span> as the “closest” point (in some sense) to <span class="math inline">\(\boldsymbol{y}\)</span> in <span class="math inline">\(C(\boldsymbol{X})\)</span>. To see this, recall the deviance form of the EDM density <a href="#eq-deviance-form"><span>4.4</span></a>. Taking a logarithm and summing over <span class="math inline">\(i = 1, \dots, n\)</span>, we find the following expression for the negative log likelihood:</p>
<p><span id="eq-glm-likelihood-via-deviance"><span class="math display">\[
-\log \mathcal L(\boldsymbol{\beta}) = \sum_{i = 1}^n \frac{d(y_i, \mu_i)}{2\phi_i} + C = \frac{\sum_{i = 1}^n w_id(y_i, \mu_i)}{2\phi_0} + C  \equiv \frac{D(\boldsymbol{y}, \boldsymbol{\mu})}{2\phi_0} + C \equiv \frac12 D^*(\boldsymbol{y}, \boldsymbol{\mu}) + C.
\tag{4.11}\]</span></span></p>
<p><span class="math inline">\(D(\boldsymbol{y}, \boldsymbol{\mu})\)</span> is called the <em>deviance</em> or the <em>total deviance</em>, and it can be interpreted as a kind of distance between the mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> and the observation vector <span class="math inline">\(\boldsymbol{y}\)</span>. For example, in the linear model case, <span class="math inline">\(D(\boldsymbol{y}, \boldsymbol{\mu}) = \|\boldsymbol{y} - \boldsymbol{\mu}\|^2\)</span>. The quantity <span class="math inline">\(D^*(\boldsymbol{y}, \boldsymbol{\mu})\)</span> is called the <em>scaled deviance.</em> In the linear model case, <span class="math inline">\(D(\boldsymbol{y}, \boldsymbol{\mu}) = \frac{\|\boldsymbol{y} - \boldsymbol{\mu}\|^2}{\sigma^2}\)</span>. Therefore, maximizing the GLM log likelihood is equivalent to minimizing the deviance:</p>
<p><span class="math display">\[
\boldsymbol{\widehat \beta} = \underset{\boldsymbol{\beta}}{\arg \min}\ D(\boldsymbol{y}, \boldsymbol{\mu}(\boldsymbol{\beta})), \quad \text{so that} \quad \boldsymbol{\widehat \mu} = \underset{\boldsymbol{\mu} \in C(\boldsymbol{X})}{\arg \min}\ D(\boldsymbol{y}, \boldsymbol{\mu}).
\]</span></p>
</section>
<section id="sec-irls" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="sec-irls"><span class="header-section-number">4.3.3</span> Iteratively reweighted least squares</h3>
<section id="log-concavity-of-glm-likelihood" class="level4" data-number="4.3.3.1">
<h4 data-number="4.3.3.1" class="anchored" data-anchor-id="log-concavity-of-glm-likelihood"><span class="header-section-number">4.3.3.1</span> Log-concavity of GLM likelihood</h4>
<p>Before talking about maximizing the GLM log-likelihood, we investigate the concavity of this function. We claim that, in the case when the canonical link is used, <span class="math inline">\(\log \mathcal L(\boldsymbol{\beta})\)</span> is a concave function of <span class="math inline">\(\boldsymbol{\beta}\)</span>, which implies that this function is “easy to optimize”, i.e., has no local maxima.</p>
<div id="prp-log-concavity" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.1 </strong></span><strong>Proposition:</strong> If <span class="math inline">\(g\)</span> is the canonical link function, then the function <span class="math inline">\(\log \mathcal L(\boldsymbol{\beta})\)</span> defined in <a href="#eq-glm-log-likelihood"><span>4.6</span></a> is concave in <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>It suffices to show that <span class="math inline">\(\psi\)</span> is a convex function since then <span class="math inline">\(\log \mathcal L(\boldsymbol{\beta})\)</span> would be the sum of a linear function of <span class="math inline">\(\boldsymbol{\beta}\)</span> and the composition of a concave function with a linear function. To verify that <span class="math inline">\(\psi\)</span> is convex, it suffices to recall that <span class="math inline">\(\ddot{\psi}(\theta) = \frac{1}{\phi}\text{Var}_\theta[y] &gt; 0\)</span>.</p>
</div>
<p>Proposition <a href="#prp-log-concavity"><span>4.1</span></a> gives us confidence that an iterative algorithm will converge to the global maximum of the likelihood. We present such an iterative algorithm next.</p>
</section>
<section id="sec-newton-raphson" class="level4" data-number="4.3.3.2">
<h4 data-number="4.3.3.2" class="anchored" data-anchor-id="sec-newton-raphson"><span class="header-section-number">4.3.3.2</span> Newton-Raphson</h4>
<p>We can maximize the log-likelihood <a href="#eq-glm-log-likelihood"><span>4.6</span></a> via the Newton-Raphson algorithm, which involves the gradient and Hessian of the function we’d like to maximize. The gradient is the score vector <a href="#eq-glm-score"><span>4.7</span></a>, while the Hessian is the Fisher information <a href="#eq-glm-fisher-info"><span>4.9</span></a>. The Newton-Raphson iteration is therefore:</p>
<p><span id="eq-nr-iteration"><span class="math display">\[
\begin{split}
\boldsymbol{\widehat \beta}^{(t+1)} &amp;= \boldsymbol{\widehat \beta}^{(t)} - (\nabla^2_{\boldsymbol{\beta}} \log \mathcal L(\boldsymbol{\widehat \beta}^{(t)}))^{-1} \nabla_{\boldsymbol{\beta}} \log \mathcal L(\boldsymbol{\widehat \beta}^{(t)}) \\
&amp;= \boldsymbol{\widehat \beta}^{(t)} + (\boldsymbol{X}^T \boldsymbol{\widehat W}^{(t)}\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{\widehat W}^{(t)}\boldsymbol{\widehat M}^{(t)}(\boldsymbol{y} - \boldsymbol{\widehat \mu}^{(t)}).
\end{split}
\tag{4.12}\]</span></span></p>
<p>See Figure <a href="#fig-newton-raphson"><span>4.2</span></a>.</p>
<div id="fig-newton-raphson" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/newton-raphson.jpeg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;4.2: Newton-Raphson iteratively approximates the log likelihood via a quadratic function and maximizing that function.</figcaption>
</figure>
</div>
</section>
<section id="sec-irls-interpretation" class="level4" data-number="4.3.3.3">
<h4 data-number="4.3.3.3" class="anchored" data-anchor-id="sec-irls-interpretation"><span class="header-section-number">4.3.3.3</span> Iteratively reweighted least squares (IRLS)</h4>
<p>A nice interpretation of the Newton-Raphson algorithm is as a sequence of weighted least squares fits, known as the iteratively reweighted least squares (IRLS) algorithm. Suppose that we have a current estimate <span class="math inline">\(\boldsymbol{\widehat \beta}^{(t)}\)</span>, and suppose we are looking for a vector <span class="math inline">\(\boldsymbol{\beta}\)</span> near <span class="math inline">\(\boldsymbol{\widehat \beta}^{(t)}\)</span> that fits the model even better. We have:</p>
<p><span class="math display">\[
\mathbb{E}_{\boldsymbol{\beta}}[\boldsymbol{y}] = g^{-1}(\boldsymbol{X} \boldsymbol{\beta}) \approx g^{-1}(\boldsymbol{X} \boldsymbol{\widehat \beta}^{(t)}) + \text{diag}(\partial \mu_i/\partial \eta_i)(\boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{X} \boldsymbol{\widehat \beta}^{(t)}) = \boldsymbol{\widehat \mu}^{(t)} + (\boldsymbol{\widehat M}^{(t)})^{-1}(\boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{X} \boldsymbol{\widehat \beta}^{(t)})
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\text{Var}_{\boldsymbol{\beta}}[\boldsymbol{y}] \approx \phi_0 (\boldsymbol{\widehat W}^{(t)})^{-1}(\boldsymbol{\widehat M}^{(t)})^{-2},
\]</span></p>
<p>recalling equation <a href="#eq-glm-variance-y"><span>4.8</span></a>. Thus, up to the first two moments, near <span class="math inline">\(\boldsymbol{\beta} = \boldsymbol{\widehat \beta}^{(t)}\)</span> the distribution of <span class="math inline">\(\boldsymbol{y}\)</span> is approximately:</p>
<p><span class="math display">\[
\boldsymbol{y} = \boldsymbol{\widehat \mu}^{(t)} + (\boldsymbol{\widehat M}^{(t)})^{-1}(\boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{X} \boldsymbol{\widehat \beta}^{(t)}) + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim N\left(\boldsymbol{0}, \phi_0 (\boldsymbol{\widehat W}^{(t)})^{-1}(\boldsymbol{\widehat M}^{(t)})^{-2}\right),
\]</span></p>
<p>or, equivalently:</p>
<p><span id="eq-second-order-approximation"><span class="math display">\[
\boldsymbol{z}^{(t)} \equiv \boldsymbol{\widehat M}^{(t)}(\boldsymbol{y} - \boldsymbol{\widehat \mu}^{(t)}) + \boldsymbol{X} \boldsymbol{\widehat \beta}^{(t)} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}', \quad \boldsymbol{\epsilon}' \sim N(\boldsymbol{0}, \phi_0 (\boldsymbol{\widehat W}^{(t)})^{-1}).
\tag{4.13}\]</span></span></p>
<p>The regression of the <em>adjusted response variable</em> <span class="math inline">\(\boldsymbol{z}^{(t)}\)</span> on <span class="math inline">\(\boldsymbol{X}\)</span> leaves us with a weighted linear regression (hence the name <em>working weights</em> for <span class="math inline">\(W_i\)</span>), whose maximum likelihood estimate is:</p>
<p><span id="eq-irls-iteration"><span class="math display">\[
\boldsymbol{\widehat \beta}^{(t+1)} = (\boldsymbol{X}^T \boldsymbol{\widehat W}^{(t)} \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{\widehat W}^{(t)} \boldsymbol{z}^{(t)},
\tag{4.14}\]</span></span></p>
<p>which we define as our next iterate. It’s easy to verify that the IRLS iteration <a href="#eq-irls-iteration"><span>4.14</span></a> is equivalent to the Newton-Raphson iteration <a href="#eq-nr-iteration"><span>4.12</span></a>. Note that we have derived these algorithms for canonical links; they each can be derived for non-canonical links but need not be equivalent in this more general case.</p>
</section>
<section id="sec-glm-residuals" class="level4" data-number="4.3.3.4">
<h4 data-number="4.3.3.4" class="anchored" data-anchor-id="sec-glm-residuals"><span class="header-section-number">4.3.3.4</span> Estimation of <span class="math inline">\(\phi_0\)</span> and GLM residuals</h4>
<p>While sometimes the parameter <span class="math inline">\(\phi_0\)</span> is known (e.g., for binomial or Poisson GLMs), in other cases <span class="math inline">\(\phi_0\)</span> must be estimated (e.g., for the normal linear model). Recall from the linear model that we estimated <span class="math inline">\(\sigma^2 = \phi_0\)</span> by taking the sum of the squares of the residuals: <span class="math inline">\(\widehat \sigma^2 = \frac{1}{n-p}\|\boldsymbol{y} - \boldsymbol{\widehat \mu}\|^2\)</span>. However, it’s unclear in the GLM context exactly how to define a residual. In fact, there are two common ways of doing so, called <em>deviance residuals</em> and <em>Pearson residuals</em>. Deviance residuals are defined in terms of the unit deviance:</p>
<p><span class="math display">\[
  r^D_i \equiv \text{sign}(y_i - \widehat \mu_i)\sqrt{w_i d(y_i, \widehat \mu_i)}.
\]</span></p>
<p>On the other hand, Pearson residuals are defined as variance-normalized residuals:</p>
<p><span class="math display">\[
  r^P_i \equiv \frac{y_i - \widehat \mu_i}{\sqrt{V(\widehat \mu_i)/w_i}}.
\]</span></p>
<p>These residuals can be viewed as residuals from the (converged) weighted linear regression model <a href="#eq-second-order-approximation"><span>4.13</span></a>. In the normal case, these residuals coincide, but in the general case, they do not. Based on these two notions of GLM residuals, we can define two estimators of <span class="math inline">\(\phi_0\)</span>. One, based on the deviance residuals, is the <em>mean deviance estimator of dispersion</em>:</p>
<p><span class="math display">\[
\widetilde \phi^D_0 \equiv \frac{1}{n-p}\|r^D\|^2 \equiv \frac{1}{n-p}\sum_{i = 1}^n w_i d(y_i, \widehat \mu_i) \equiv \frac{1}{n-p}D(\boldsymbol{y}; \boldsymbol{\widehat \mu});
\]</span></p>
<p>recall that the total deviance <span class="math inline">\(D(\boldsymbol{y}; \boldsymbol{\widehat \mu})\)</span> is a generalization of the residual sum of squares. The other, based on the Pearson residuals, is called the <em>Pearson estimator of dispersion</em>:</p>
<p><span id="eq-pearson-estimator-dispersion"><span class="math display">\[
\widetilde \phi^P_0 \equiv \frac{1}{n-p}X^2 \equiv \frac{1}{n-p}\|r^P\|^2 \equiv \frac{1}{n-p}\sum_{i = 1}^n w_i \frac{(y_i - \widehat \mu_i)^2}{V(\mu_i)}.
\tag{4.15}\]</span></span></p>
<p><span class="math inline">\(X^2\)</span> is known as the Pearson <span class="math inline">\(X^2\)</span> statistic. The deviance-based estimator can be more accurate than the Pearson estimator under small-dispersion asymptotics. However, the Pearson estimator is more robust when only the first two moments of the EDM model are correct and in the absence of small-dispersion asymptotics. For these reasons, the Pearson estimator is generally preferred.</p>
</section>
</section>
</section>
<section id="sec-glm-inf" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-glm-inf"><span class="header-section-number">4.4</span> Inference in GLMs</h2>
<section id="sec-preliminaries" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="sec-preliminaries"><span class="header-section-number">4.4.1</span> Preliminaries</h3>
<section id="sec-inferential-goals" class="level4" data-number="4.4.1.1">
<h4 data-number="4.4.1.1" class="anchored" data-anchor-id="sec-inferential-goals"><span class="header-section-number">4.4.1.1</span> Inferential goals</h4>
<p>There are two types of inferential goals: hypothesis testing and confidence interval/region construction.</p>
<section id="sec-hypothesis-testing" class="level5" data-number="4.4.1.1.1">
<h5 data-number="4.4.1.1.1" class="anchored" data-anchor-id="sec-hypothesis-testing"><span class="header-section-number">4.4.1.1.1</span> Hypothesis testing</h5>
<ol type="1">
<li><strong>Single coefficient</strong>: <span class="math inline">\(H_0: \beta_j = \beta_j^0\)</span> versus <span class="math inline">\(H_1: \beta_j \neq \beta_j^0\)</span> for some <span class="math inline">\(\beta_j^0 \in \mathbb{R}\)</span>.</li>
<li><strong>Group of coefficients</strong>: <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span> versus <span class="math inline">\(H_1: \boldsymbol{\beta}_S \neq \boldsymbol{\beta}_S^0\)</span> for some <span class="math inline">\(S \subset \{0,\dots,p-1\}\)</span> and some <span class="math inline">\(\boldsymbol{\beta}_S^0 \in \mathbb{R}^{|S|}\)</span>.</li>
<li><strong>Goodness of fit</strong>: The goodness of fit null hypothesis is that the GLM <a href="#eq-glm-def">Equation&nbsp;<span>4.5</span></a> is correctly specified. Consider the <em>saturated model</em>:</li>
</ol>
<p><span id="eq-saturated-model"><span class="math display">\[
y_i \overset{\text{ind}} \sim \text{EDM}(\mu_i, \phi_0/w_i) \quad \text{for} \quad i = 1,\dots,n.
\tag{4.16}\]</span></span></p>
<p>Let</p>
<p><span class="math display">\[
\mathcal{M}^{\text{GLM}} \equiv \{\boldsymbol{\mu}: \mu_i = \boldsymbol{x}_{i*}^T \boldsymbol{\beta} + o_i \text{ for some } \boldsymbol{\beta} \in \mathbb{R}^p\}
\]</span></p>
<p>be the set of mean vectors consistent with the GLM. Then, the goodness of fit testing problem is <span class="math inline">\(H_0: \boldsymbol{\mu} \in \mathcal{M}^{\text{GLM}}\)</span> versus <span class="math inline">\(H_1: \boldsymbol{\mu} \notin \mathcal{M}^{\text{GLM}}\)</span>.</p>
</section>
<section id="sec-confidence-interval-region" class="level5" data-number="4.4.1.1.2">
<h5 data-number="4.4.1.1.2" class="anchored" data-anchor-id="sec-confidence-interval-region"><span class="header-section-number">4.4.1.1.2</span> Confidence interval/region construction</h5>
<ol type="1">
<li><strong>Confidence interval for a single coefficient</strong>: Here, the goal is to produce a confidence interval <span class="math inline">\(\text{CI}(\beta_j)\)</span> for a coefficient <span class="math inline">\(\beta_j\)</span>.</li>
<li><strong>Confidence region for a group of coefficients</strong>: Here, the goal is to produce a confidence region <span class="math inline">\(\text{CR}(\boldsymbol{\beta}_S)\)</span> for a group of coefficients <span class="math inline">\(\boldsymbol{\beta}_S\)</span>.</li>
<li><strong>Confidence interval for a fitted value</strong>: In GLMs, fitted values can either be considered for parameters on the linear scale (<span class="math inline">\(\eta_i = \boldsymbol{x}_{i*}^T \boldsymbol{\beta} + o_i\)</span>) or the mean scale (<span class="math inline">\(\mu_i = g^{-1}(\boldsymbol{x}_{i*}^T \boldsymbol{\beta} + o_i)\)</span>). The goal, then, is to produce confidence intervals <span class="math inline">\(\text{CI}(\eta_i)\)</span> or <span class="math inline">\(\text{CI}(\mu_i)\)</span> for <span class="math inline">\(\eta_i\)</span> or <span class="math inline">\(\mu_i\)</span>, respectively.</li>
</ol>
</section>
</section>
<section id="sec-inferential-tools" class="level4" data-number="4.4.1.2">
<h4 data-number="4.4.1.2" class="anchored" data-anchor-id="sec-inferential-tools"><span class="header-section-number">4.4.1.2</span> Inferential tools</h4>
<p>Inference in GLMs is based on asymptotic likelihood theory. These asymptotics can be based on <em>large-sample asymptotics</em> or <em>small-dispersion asymptotics</em>. Large-sample asymptotics are applicable for testing hypotheses and estimating parameters within models where the number of parameters is fixed while the sample size grows. Small-dispersion asymptotics are applicable for testing hypotheses and estimating parameters within models where the dispersion is small, regardless of the sample size. Large-sample asymptotics apply to testing and estimating coefficients in GLMs <a href="#eq-glm-def">Equation&nbsp;<span>4.5</span></a> with a fixed number of parameters as the sample size grows, but not to testing goodness of fit. Indeed, goodness-of-fit tests refer to the saturated model <a href="#eq-saturated-model">Equation&nbsp;<span>4.16</span></a>, whose number of parameters grows with <span class="math inline">\(n\)</span>. Small-dispersion asymptotics, on the other hand, apply to goodness-of-fit testing.</p>
<p>Hypothesis tests (and, by inversion, confidence intervals) can be constructed in three asymptotically equivalent ways: Wald tests, likelihood ratio tests (LRT), and score tests. These tests can be justified using either large-sample or small-dispersion asymptotics, depending on the context. Despite their asymptotic equivalence, in finite samples, some tests may be preferable to others (though for normal linear models, these tests are equivalent in finite samples as well). See Figure <a href="#fig-trinity-comparison">Figure&nbsp;<span>4.3</span></a>.</p>
<div id="fig-trinity-comparison" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/trinity-comparison.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;4.3: A comparison of the three asymptotic methods for GLM inference.</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-wald-inference" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="sec-wald-inference"><span class="header-section-number">4.4.2</span> Wald inference</h3>
<p>Wald inference is based on the following asymptotic normality statement:</p>
<p><span id="eq-wald-approximation"><span class="math display">\[
\boldsymbol{\widehat \beta} \overset{\cdot}{\sim} N(\boldsymbol{\beta}, \boldsymbol{I}^{-1}(\boldsymbol{\beta})) = N(\boldsymbol{\beta}, \phi_0(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\beta}) \boldsymbol{X})^{-1}),
\tag{4.17}\]</span></span></p>
<p>recalling our derivation of the Fisher information from equation <a href="#eq-glm-fisher-info">Equation&nbsp;<span>4.9</span></a>. This approximation can be justified via <em>large-sample asymptotics</em> or <em>small-dispersion asymptotics</em>. Wald inference is easy to carry out, and for this reason, it is considered the default type of inference. However, as we will see in Unit 5, it also tends to be the least accurate in small samples. Furthermore, Wald tests are usually not applied for testing goodness of fit.</p>
<section id="sec-wald-test-single-coeff" class="level4" data-number="4.4.2.1">
<h4 data-number="4.4.2.1" class="anchored" data-anchor-id="sec-wald-test-single-coeff"><span class="header-section-number">4.4.2.1</span> Wald test for <span class="math inline">\(\beta_j = \beta_j^0\)</span> (known <span class="math inline">\(\phi_0\)</span>)</h4>
<p>Based on the Wald approximation <a href="#eq-wald-approximation">Equation&nbsp;<span>4.17</span></a>, under the null hypothesis, we have:</p>
<p><span class="math display">\[
\widehat \beta_j \overset{\cdot}{\sim} N(\beta_j^0, \phi_0[(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\beta}) \boldsymbol{X})^{-1}]_{jj}) \approx N(\beta_j^0, \phi_0[(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{jj}) \equiv N(0, \text{SE}(\beta_j)^2),
\]</span></p>
<p>where we have used a plug-in estimator of the variance. This leads us to the Wald <span class="math inline">\(z\)</span>-test:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}\left(\left|\frac{\widehat \beta_j - \beta_j^0}{\text{SE}(\beta_j)}\right| &gt; z_{1-\alpha/2}\right).
\]</span></p>
<p>Since a one-dimensional parameter is being tested, we can make the test one-sided if desired.</p>
</section>
<section id="sec-wald-test-group-coeff" class="level4" data-number="4.4.2.2">
<h4 data-number="4.4.2.2" class="anchored" data-anchor-id="sec-wald-test-group-coeff"><span class="header-section-number">4.4.2.2</span> Wald test for <span class="math inline">\(\boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span> (known <span class="math inline">\(\phi_0\)</span>)</h4>
<p>Extending the reasoning above, we have under the null hypothesis that:</p>
<p><span class="math display">\[
\boldsymbol{\widehat \beta}_S \overset{\cdot}{\sim} N(\boldsymbol{\beta}_S^0, \phi_0[(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\beta}) \boldsymbol{X})^{-1}]_{S,S}) \approx N(\boldsymbol{\beta}_S^0, \phi_0[(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{S,S}),
\]</span></p>
<p>and therefore:</p>
<p><span class="math display">\[
\frac{1}{\phi_0} (\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S^0)^T \left([(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{S,S}\right)^{-1}(\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S^0) \overset{\cdot}{\sim} \chi^2_{|S|}.
\]</span></p>
<p>Hence, we have the Wald <span class="math inline">\(\chi^2\)</span> test:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}\left(\frac{1}{\phi_0} (\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S^0)^T \left([(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{S,S}\right)^{-1}(\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S^0) &gt; \chi^2_{|S|}(1-\alpha)\right).
\]</span></p>
</section>
<section id="sec-wald-ci-single-coeff" class="level4" data-number="4.4.2.3">
<h4 data-number="4.4.2.3" class="anchored" data-anchor-id="sec-wald-ci-single-coeff"><span class="header-section-number">4.4.2.3</span> Wald confidence interval for <span class="math inline">\(\beta_j\)</span> (known <span class="math inline">\(\phi_0\)</span>)</h4>
<p>Inverting the Wald test for <span class="math inline">\(\beta_j\)</span>, we get a Wald confidence interval:</p>
<p><span id="eq-conf-int-beta"><span class="math display">\[
\text{CI}(\beta_j) \equiv \widehat \beta_j \pm z_{1-\alpha/2} \cdot \text{SE}(\beta_j), \quad \text{where} \quad \text{SE}(\beta_j) \equiv \sqrt{\phi_0[(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{jj}}.
\tag{4.18}\]</span></span></p>
</section>
<section id="sec-wald-cr-group-coeff" class="level4" data-number="4.4.2.4">
<h4 data-number="4.4.2.4" class="anchored" data-anchor-id="sec-wald-cr-group-coeff"><span class="header-section-number">4.4.2.4</span> Wald confidence region for <span class="math inline">\(\boldsymbol{\beta}_S\)</span> (known <span class="math inline">\(\phi_0\)</span>)</h4>
<p>By inverting the test of <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span>, we get the Wald confidence region:</p>
<p><span class="math display">\[
\text{CR}(\boldsymbol{\beta}_S) \equiv \left\{\boldsymbol{\beta}_S: \frac{1}{\phi_0} (\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S)^T \left([(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{S,S}\right)^{-1}(\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S) \leq \chi^2_{|S|}(1-\alpha)\right\}.
\]</span></p>
<p>If <span class="math inline">\(S = \{0, 1, \dots, p-1\}\)</span>, we are left with:</p>
<p><span class="math display">\[
\text{CR}(\boldsymbol{\beta}_S) \equiv \left\{\boldsymbol{\beta}: \frac{1}{\phi_0} (\boldsymbol{\widehat \beta} - \boldsymbol{\beta})^T \boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X} (\boldsymbol{\widehat \beta} - \boldsymbol{\beta}) \leq \chi^2_{p}(1-\alpha)\right\}.
\]</span></p>
</section>
<section id="sec-wald-ci-fitted-values" class="level4" data-number="4.4.2.5">
<h4 data-number="4.4.2.5" class="anchored" data-anchor-id="sec-wald-ci-fitted-values"><span class="header-section-number">4.4.2.5</span> Wald confidence intervals for <span class="math inline">\(\eta_i\)</span> and <span class="math inline">\(\mu_i\)</span> (known <span class="math inline">\(\phi_0\)</span>)</h4>
<p>Given the Wald approximation <a href="#eq-wald-approximation">Equation&nbsp;<span>4.17</span></a>, we have:</p>
<p><span class="math display">\[
\widehat \eta_i \equiv o_i + \boldsymbol{x}_{i*}^T \boldsymbol{\widehat \beta} \overset{\cdot}{\sim} N(\eta_i, \phi_0 \cdot \boldsymbol{x}_{i*}^T (\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1} \boldsymbol{x}_{i*}) \equiv N(\eta_i, \text{SE}(\eta_i)^2).
\]</span></p>
<p>Hence, the Wald interval for <span class="math inline">\(\eta_i\)</span> is:</p>
<p><span class="math display">\[
\text{CI}(\eta_i) \equiv o_i + \boldsymbol{x}_{i*}^T \boldsymbol{\widehat\beta} \pm z_{1-\alpha/2} \cdot \text{SE}(\eta_i), \quad \text{SE}(\eta_i) \equiv \sqrt{\phi_0 \boldsymbol{x}_{i*}^T (\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1} \boldsymbol{x}_{i*}}.
\]</span></p>
<p>A confidence interval for <span class="math inline">\(\mu_i \equiv \mathbb{E}_{\boldsymbol{\beta}}[y_i] = g^{-1}(\eta_i)\)</span> can be obtained by applying the monotonic function <span class="math inline">\(g^{-1}\)</span> to the endpoints of the confidence interval for <span class="math inline">\(\eta_i\)</span>. Note that the resulting confidence interval may be asymmetric. We can get a symmetric interval by applying the delta method, but this interval would be less accurate because it involves the delta method approximation in addition to the Wald approximation.</p>
</section>
<section id="sec-wald-inference-unknown-dispersion" class="level4" data-number="4.4.2.6">
<h4 data-number="4.4.2.6" class="anchored" data-anchor-id="sec-wald-inference-unknown-dispersion"><span class="header-section-number">4.4.2.6</span> Wald inference when <span class="math inline">\(\phi_0\)</span> is unknown</h4>
<p>When <span class="math inline">\(\phi_0\)</span> is unknown, we need to plug in an estimate <span class="math inline">\(\widetilde \phi_0\)</span> (e.g.&nbsp;the deviance-based or Pearson-based estimate). Now our standard errors are <span class="math inline">\(\text{SE}(\beta_j) \equiv \sqrt{\widetilde \phi_0 \cdot [(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{jj}}\)</span>, and our test statistic for <span class="math inline">\(H_0: \beta_j = \beta_j^0\)</span> is:</p>
<p><span class="math display">\[
\frac{\widehat \beta_j - \beta_j^0}{\sqrt{\widetilde \phi_0}\sqrt{[(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{jj}}}.
\]</span></p>
<p>Unlike linear regression, it is not the case in general that <span class="math inline">\(\boldsymbol{\widehat \beta}\)</span> and <span class="math inline">\(\widetilde \phi_0\)</span> are independent. Nevertheless, they are <em>asymptotically independent</em>. Therefore, the above statistic is <em>approximately</em> distributed as <span class="math inline">\(t_{n-p}\)</span>. Hence, the test for <span class="math inline">\(H_0: \beta_j = \beta_j^0\)</span> is:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}\left(\left|\frac{\widehat \beta_j - \beta_j^0}{\text{SE}(\beta_j)}\right| &gt; t_{n-p}(1-\alpha/2)\right).
\]</span></p>
<p>Likewise, we would replace <span class="math inline">\(z_{1-\alpha}\)</span> by <span class="math inline">\(t_{n-p}(1-\alpha/2)\)</span> for all tests and confidence intervals concerning univariate quantities. For multivariate quantities, we will get approximate <span class="math inline">\(F\)</span> distributions instead of approximate <span class="math inline">\(\chi^2\)</span> distributions. For example:</p>
<p><span class="math display">\[
\frac{\frac{1}{|S|}(\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S^0)^T \left([(\boldsymbol{X}^T \boldsymbol{W}(\boldsymbol{\widehat \beta}) \boldsymbol{X})^{-1}]_{S,S}\right)^{-1}(\boldsymbol{\widehat \beta}_S - \boldsymbol{\beta}_S^0)}{\widetilde \phi_0} \overset{\cdot}{\sim} F_{|S|, n-p}.
\]</span> ### Likelihood ratio inference {#sec-likelihood-ratio-inference}</p>
</section>
<section id="sec-likelihood-ratio-coeff-known" class="level4" data-number="4.4.2.7">
<h4 data-number="4.4.2.7" class="anchored" data-anchor-id="sec-likelihood-ratio-coeff-known"><span class="header-section-number">4.4.2.7</span> Testing one or more coefficients (<span class="math inline">\(\phi_0\)</span> known)</h4>
<p>Let <span class="math inline">\(\ell(\boldsymbol{y}, \boldsymbol{\mu}) = -\frac{D(\boldsymbol{y}, \boldsymbol{\mu})}{2\phi_0} + C\)</span> be the GLM log-likelihood (recall equation <a href="#eq-glm-likelihood-via-deviance">Equation&nbsp;<span>4.11</span></a>). Let <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span> be a null hypothesis about some subset of variables <span class="math inline">\(S \subset \{0, 1, \dots, p-1\}\)</span>, and let <span class="math inline">\(\boldsymbol{\widehat{\mu}}_{\text{-}S}\)</span> be the maximum likelihood estimate under the null hypothesis. Likelihood ratio inference is based on the following asymptotic chi-square distribution:</p>
<p><span id="eq-likelihood-ratio-approximation"><span class="math display">\[
2(\ell(\boldsymbol{y}, \boldsymbol{\widehat{\mu}}) - \ell(\boldsymbol{y}, \boldsymbol{\widehat{\mu}}_{\text{-}S})) = \frac{D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}}_{\text{-}S}) - D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}})}{\phi_0} \overset{\cdot}{\sim} \chi^2_{|S|}.
\tag{4.19}\]</span></span></p>
<p>This approximation holds either in large samples (large-sample asymptotics) or in small samples but with small dispersion (small-dispersion asymptotics). The latter has to do with the fact that under small-dispersion asymptotics,</p>
<p><span class="math display">\[
\frac{d(y_i, \mu_i)}{\phi_0/w_i} \overset{\cdot}{\sim} \chi^2_1,
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\frac{D(\boldsymbol{y}, \boldsymbol{\mu})}{\phi_0} = \sum_{i = 1}^n \frac{d(y_i, \mu_i)}{\phi_0/w_i} \overset{\cdot}{\sim} \chi^2_n.
\]</span></p>
<p>Suppose we wish to test the null hypothesis <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span>. Then, based on the approximation <a href="#eq-likelihood-ratio-approximation">Equation&nbsp;<span>4.19</span></a>, we can define the likelihood ratio test:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}\left(\frac{D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}}_{\text{-}S}) - D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}})}{\phi_0} &gt; \chi^2_{|S|}(1-\alpha)\right).
\]</span></p>
</section>
<section id="sec-likelihood-ratio-ci-single-coeff" class="level4" data-number="4.4.2.8">
<h4 data-number="4.4.2.8" class="anchored" data-anchor-id="sec-likelihood-ratio-ci-single-coeff"><span class="header-section-number">4.4.2.8</span> Confidence interval for a single coefficient</h4>
<p>We can obtain a confidence interval for <span class="math inline">\(\beta_j\)</span> by inverting the likelihood ratio test. Let <span class="math inline">\(\boldsymbol{\widehat{\mu}}_{\text{-}j}(\beta_j^0)\)</span> be the fitted mean vector under the constraint <span class="math inline">\(\beta_j = \beta_j^0\)</span>. Then, inverting the likelihood ratio test gives us the confidence interval:</p>
<p><span class="math display">\[
\text{CI}(\beta_j) \equiv \left\{\beta_j: \frac{D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}}_{\text{-}j}(\beta_j)) - D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}})}{\phi_0} \leq \chi^2_{|S|}(1-\alpha)\right\}.
\]</span></p>
<p>Likelihood ratio-based confidence intervals tend to be more accurate than Wald intervals, especially when the parameter is near the edge of the parameter space, but they require more computation because <span class="math inline">\(\boldsymbol{\widehat{\mu}}_{\text{-}j}(\beta_j)\)</span> must be computed on a large grid of <span class="math inline">\(\beta_j\)</span> values. If we wanted to create <em>confidence regions</em> for groups of parameters, this would become computationally intensive due to the curse of dimensionality.</p>
</section>
<section id="sec-likelihood-ratio-goodness-of-fit" class="level4" data-number="4.4.2.9">
<h4 data-number="4.4.2.9" class="anchored" data-anchor-id="sec-likelihood-ratio-goodness-of-fit"><span class="header-section-number">4.4.2.9</span> Goodness of fit testing (<span class="math inline">\(\phi_0\)</span> known)</h4>
<p>For <span class="math inline">\(\phi_0\)</span> known, we can also construct a goodness of fit test. To this end, we compare the deviances of the GLM and saturated model:</p>
<p><span class="math display">\[
\frac{D(\boldsymbol{y}, \boldsymbol{\widehat{\mu}}) - D(\boldsymbol{y}, \boldsymbol{y})}{\phi_0} = \frac{D(\boldsymbol{y}; \boldsymbol{\widehat{\mu}})}{\phi_0} \overset{\cdot}{\sim} \chi^2_{n-p}.
\]</span></p>
<p>Note that the goodness of fit test is a significance test with respect to the saturated model <a href="#eq-saturated-model">Equation&nbsp;<span>4.16</span></a>, which has <span class="math inline">\(n\)</span> free parameters. Therefore, the number of free parameters increases with the sample size, so large-sample asymptotics cannot justify this test. Instead, we must rely on small-dispersion asymptotics.</p>
</section>
<section id="sec-likelihood-ratio-unknown-dispersion" class="level4" data-number="4.4.2.10">
<h4 data-number="4.4.2.10" class="anchored" data-anchor-id="sec-likelihood-ratio-unknown-dispersion"><span class="header-section-number">4.4.2.10</span> Likelihood ratio inference for <span class="math inline">\(\phi_0\)</span> unknown</h4>
<p>If <span class="math inline">\(\phi_0\)</span> is unknown, we can estimate it as discussed above and construct an <span class="math inline">\(F\)</span>-statistic as follows:</p>
<p><span class="math display">\[
F \equiv \frac{(D(\boldsymbol{y}; \boldsymbol{\widehat{\mu}}_{\text{-}S}) - D(\boldsymbol{y}; \boldsymbol{\widehat{\mu}}))/|S|}{\widetilde{\phi}_0}.
\]</span></p>
<p>In normal linear model theory, the null distribution of <span class="math inline">\(F\)</span> is <em>exactly</em> <span class="math inline">\(F_{|S|, n-p}\)</span>. For GLMs, the null distribution of <span class="math inline">\(F\)</span> is <em>approximately</em> <span class="math inline">\(F_{|S|, n-p}\)</span>. We can use this <span class="math inline">\(F\)</span> distribution to construct hypothesis tests for groups of coefficients, or invert it to get a confidence interval for a single coefficient. We cannot construct a goodness of fit test in the case that <span class="math inline">\(\phi_0\)</span> is unknown because the residual degrees of freedom would be used up to estimate <span class="math inline">\(\phi_0\)</span> rather than to carry out inference.</p>
</section>
</section>
<section id="sec-score-inference" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="sec-score-inference"><span class="header-section-number">4.4.3</span> Score-based inference</h3>
<p>Score-based inference can be used for the same set of inferential tasks as likelihood ratio inference.</p>
<section id="sec-score-test-multiple-coeff" class="level4" data-number="4.4.3.1">
<h4 data-number="4.4.3.1" class="anchored" data-anchor-id="sec-score-test-multiple-coeff"><span class="header-section-number">4.4.3.1</span> Testing multiple coefficients (<span class="math inline">\(\phi_0\)</span> known)</h4>
<p>Let <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span> be a null hypothesis about a subset of variables <span class="math inline">\(S\)</span>, and let <span class="math inline">\(\boldsymbol{\widehat{\beta}}^0\)</span> be the maximum likelihood estimate under this null hypothesis. Score test inference is based on the asymptotic approximation:</p>
<p><span class="math display">\[
U(\boldsymbol{\widehat{\beta}}^0)^T \boldsymbol{I}(\boldsymbol{\widehat{\beta}}^0)^{-1} U(\boldsymbol{\widehat{\beta}}^0) \overset{\cdot}{\sim} \chi^2_{|S|},
\]</span></p>
<p>recalling that <span class="math inline">\(U(\boldsymbol{\beta}) = \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\)</span> is the score vector. This approximation can be justified either by small-dispersion asymptotics or large-sample asymptotics (both based on the central limit theorem). Since:</p>
<p><span class="math display">\[
\boldsymbol{\widehat{\beta}}^0_{\text{-}S} = \underset{\boldsymbol{\beta}_{\text{-}S}}{\arg \max}\ \ell(\boldsymbol{\beta}^0_{S}, \boldsymbol{\beta}_{\text{-}S}),
\]</span></p>
<p>it follows that:</p>
<p><span class="math display">\[
[U(\boldsymbol{\widehat{\beta}}^0)]_{\text{-}S} \equiv \frac{\partial \ell}{\partial \boldsymbol{\beta}_{\text{-}S}}(\boldsymbol{\widehat{\beta}}^0) = \frac{\partial \ell}{\partial \boldsymbol{\beta}_{\text{-}S}}(\boldsymbol{\beta}^0_{S}, \boldsymbol{\widehat{\beta}}_{\text{-}S}) = 0.
\]</span></p>
<p>Hence, we have:</p>
<p><span class="math display">\[
U(\boldsymbol{\widehat{\beta}}^0)^T \boldsymbol{I}(\boldsymbol{\widehat{\beta}}^0)^{-1} U(\boldsymbol{\widehat{\beta}}^0) = [U(\boldsymbol{\widehat{\beta}}^0)]_S^T [\boldsymbol{I}(\boldsymbol{\widehat{\beta}}^0)^{-1}]_{S,S} [U(\boldsymbol{\widehat{\beta}}^0)]_S.
\]</span></p>
<p>Recalling the expressions <a href="#eq-glm-score">Equation&nbsp;<span>4.7</span></a> and <a href="#eq-glm-fisher-info">Equation&nbsp;<span>4.9</span></a> for the score vector and information matrix, we have that:</p>
<p><span class="math display">\[
U(\boldsymbol{\widehat{\beta}}^0)^T \boldsymbol{I}(\boldsymbol{\widehat{\beta}}^0)^{-1} U(\boldsymbol{\widehat{\beta}}^0) = \frac{1}{\phi_0}(\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0)^T \boldsymbol{\widehat{W}}^0 \boldsymbol{\widehat{M}}^0 \boldsymbol{X}_{S,*}[(\boldsymbol{X}^T \boldsymbol{\widehat{W}}^0 \boldsymbol{X})^{-1}]_{S,S} \boldsymbol{X}_{S,*}^T \boldsymbol{\widehat{M}}^0 \boldsymbol{\widehat{W}}^0 (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0).
\]</span></p>
<p>Therefore, we arrive at the score test for <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{\beta}_S^0\)</span>:</p>
<p><span class="math display">\[
\small
\phi(\boldsymbol{X}, \boldsymbol{y}) = \mathbbm{1}\left((\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0)^T \boldsymbol{\widehat{W}}^0 \boldsymbol{\widehat{M}}^0 \boldsymbol{X}_{S,*}[(\boldsymbol{X}^T \boldsymbol{\widehat{W}}^0 \boldsymbol{X})^{-1}]_{S,S} \boldsymbol{X}_{S,*}^T \boldsymbol{\widehat{M}}^0 \boldsymbol{\widehat{W}}^0 (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0) &gt; \phi_0 \chi^2_{|S|}(1-\alpha)\right).
\]</span></p>
<p>The nice thing about a score test is that the model need only be fit under the null hypothesis. If there are several variables one is considering adding to a model, the model need not be refit upon the addition of each variable.</p>
</section>
<section id="sec-score-test-single-coeff" class="level4" data-number="4.4.3.2">
<h4 data-number="4.4.3.2" class="anchored" data-anchor-id="sec-score-test-single-coeff"><span class="header-section-number">4.4.3.2</span> Testing a single coefficient (<span class="math inline">\(\phi_0\)</span> known)</h4>
<p>If <span class="math inline">\(S = \{j\}\)</span>, the score test becomes univariate and can be based on the following normal approximation:</p>
<p><span id="eq-score-test-univariate"><span class="math display">\[
\frac{\boldsymbol{x}_{j,*}^T \boldsymbol{\widehat{M}}^0 \boldsymbol{\widehat{W}}^0 (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0)}{\sqrt{\phi_0 ([(\boldsymbol{X}^T \boldsymbol{\widehat{W}}^0 \boldsymbol{X})^{-1}]_{j, j})^{-1}}} \overset{\cdot}{\sim} N(0, 1).
\tag{4.20}\]</span></span></p>
<p>Unlike its multivariate counterparts, we can construct not just a two-sided test but also one-sided tests based on this normal approximation. For example, below is a right-sided score test for <span class="math inline">\(H_0: \beta_j = \beta_j^0\)</span>:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) = \mathbbm{1}\left(\frac{\boldsymbol{x}_{j,*}^T \boldsymbol{\widehat{M}}^0 \boldsymbol{\widehat{W}}^0 (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0)}{\sqrt{\phi_0 ([(\boldsymbol{X}^T \boldsymbol{\widehat{W}}^0 \boldsymbol{X})^{-1}]_{j, j})^{-1}}} &gt; z_{1-\alpha}\right).
\]</span></p>
</section>
<section id="sec-score-ci-single-coeff" class="level4" data-number="4.4.3.3">
<h4 data-number="4.4.3.3" class="anchored" data-anchor-id="sec-score-ci-single-coeff"><span class="header-section-number">4.4.3.3</span> Confidence interval for a single coefficient (<span class="math inline">\(\phi_0\)</span> known)</h4>
<p>Just as with the likelihood ratio test, it is possible to invert a score test for a single coefficient to obtain a confidence interval. It is uncommon to invert a multivariate test to obtain a confidence region for multiple coordinates of <span class="math inline">\(\boldsymbol{\beta}\)</span>, given the computationally expensive search across a grid of possible <span class="math inline">\(\boldsymbol{\beta}\)</span> values.</p>
</section>
<section id="sec-score-goodness-of-fit" class="level4" data-number="4.4.3.4">
<h4 data-number="4.4.3.4" class="anchored" data-anchor-id="sec-score-goodness-of-fit"><span class="header-section-number">4.4.3.4</span> Goodness of fit testing (<span class="math inline">\(\phi_0\)</span> known)</h4>
<p>We can view goodness of fit testing as testing a hypothesis about the coefficients in a GLM with <span class="math inline">\(\boldsymbol{X} = \boldsymbol{I}_{n \times n}\)</span>, which amounts to the saturated model that allows unrestricted means for each observation. Furthermore, the coefficient vector fitted under the null hypothesis amounts to fitting the GLM as usual. Therefore, we have:</p>
<p><span class="math display">\[
\begin{split}
U(\boldsymbol{\widehat{\beta}})^T \boldsymbol{I}(\boldsymbol{\widehat{\beta}})^{-1} U(\boldsymbol{\widehat{\beta}}) &amp;= \frac{1}{\phi_0}(\boldsymbol{y} - \boldsymbol{\widehat{\mu}})^T \boldsymbol{\widehat{W}} \boldsymbol{\widehat{M}} \boldsymbol{X} (\boldsymbol{X}^T \boldsymbol{\widehat{W}} \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{\widehat{M}} \boldsymbol{\widehat{W}} (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}) \\
&amp;= \frac{1}{\phi_0}(\boldsymbol{y} - \boldsymbol{\widehat{\mu}})^T \boldsymbol{\widehat{W}} \boldsymbol{\widehat{M}}^2 (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}) \\
&amp;= \frac{1}{\phi_0}(\boldsymbol{y} - \boldsymbol{\widehat{\mu}})^T \text{diag}\left( \frac{w_i}{V(\widehat{\mu}_i)} \right) (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}) \\
&amp;= \frac{1}{\phi_0}\sum_{i=1}^n \frac{w_i (y_i - \widehat{\mu}_i)^2}{V(\widehat{\mu}_i)} \equiv \frac{1}{\phi_0} X^2,
\end{split}
\]</span></p>
<p>where <span class="math inline">\(X^2\)</span> is the Pearson chi-square statistic. Therefore, the score test for goodness of fit is:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}\left(X^2 \phi_0 &gt; \chi^2_{n-p}(1-\alpha)\right).
\]</span></p>
<p>In the context of contingency table analysis (see the next chapter), this test reduces to the Pearson chi-square test of independence between two categorical variables. This test was proposed in 1900; it was only pointed out about a century later that this is a score test (Smyth 2003).</p>
</section>
<section id="sec-score-test-unknown-dispersion" class="level4" data-number="4.4.3.5">
<h4 data-number="4.4.3.5" class="anchored" data-anchor-id="sec-score-test-unknown-dispersion"><span class="header-section-number">4.4.3.5</span> Score test inference for <span class="math inline">\(\phi_0\)</span> unknown</h4>
<p>Score test inference for one or more coefficients <span class="math inline">\(\boldsymbol{\beta}_S\)</span> can be achieved by replacing <span class="math inline">\(\phi_0\)</span> with one of its estimators and replacing the normal and chi-square distributions with <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions, respectively. For example, the score test for a single coefficient <span class="math inline">\(\beta_j\)</span> is:</p>
<p><span class="math display">\[
\phi(\boldsymbol{X}, \boldsymbol{y}) = \mathbbm{1}\left(\frac{\boldsymbol{x}_{j,*}^T \boldsymbol{\widehat{M}}^0 \boldsymbol{\widehat{W}}^0 (\boldsymbol{y} - \boldsymbol{\widehat{\mu}}^0)}{\sqrt{\widetilde{\phi}_0 ([(\boldsymbol{X}^T \boldsymbol{\widehat{W}}^0 \boldsymbol{X})^{-1}]_{j, j})^{-1}}} &gt; t_{n-p}(1-\alpha)\right).
\]</span></p>
<p>The <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions are not exact in finite samples, but are better approximations than the normal and chi-square distributions. The score test for goodness of fit is not applicable in the case when <span class="math inline">\(\phi_0\)</span> is unknown, similarly to the likelihood ratio test. Indeed, note the relationship between the Pearson goodness of fit test, which rejects when <span class="math inline">\(\frac{1}{\phi_0}X^2 &gt; \chi^2_{n-p}(1-\alpha)\)</span>, and the Pearson estimator of the dispersion parameter: <span class="math inline">\(\widetilde{\phi}_0 \equiv \frac{X^2}{n-p}\)</span>. If we try to plug in the Pearson estimator for the dispersion into the Pearson goodness of fit test, we end up with a test statistic deterministically equal to <span class="math inline">\(n-p\)</span>. This reflects the fact that the residual degrees of freedom can either be used to estimate the dispersion or to test goodness of fit; they cannot be used for both.</p>
</section>
</section>
</section>
<section id="sec-r-demo" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="sec-r-demo"><span class="header-section-number">4.5</span> R demo</h2>
<section id="sec-crime-data" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="sec-crime-data"><span class="header-section-number">4.5.1</span> Crime data</h3>
<p>Let’s revisit the crime data from Homework 2, this time fitting a logistic regression to it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># read crime data</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>crime_data <span class="ot">&lt;-</span> <span class="fu">read_tsv</span>(<span class="st">"data/Statewide_crime.dat"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read and transform population data</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>population_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/state-populations.csv"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>population_data <span class="ot">&lt;-</span> population_data <span class="sc">|&gt;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(State <span class="sc">!=</span> <span class="st">"Puerto Rico"</span>) <span class="sc">|&gt;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(State, Pop) <span class="sc">|&gt;</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">state_name =</span> State, <span class="at">state_pop =</span> Pop)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># collate state abbreviations</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>state_abbreviations <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">state_name =</span> state.name,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">state_abbrev =</span> state.abb</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">state_name =</span> <span class="st">"District of Columbia"</span>, <span class="at">state_abbrev =</span> <span class="st">"DC"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># add CrimeRate to crime_data</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>crime_data <span class="ot">&lt;-</span> crime_data <span class="sc">|&gt;</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">STATE =</span> <span class="fu">ifelse</span>(STATE <span class="sc">==</span> <span class="st">"IO"</span>, <span class="st">"IA"</span>, STATE)) <span class="sc">|&gt;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">state_abbrev =</span> STATE) <span class="sc">|&gt;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(state_abbrev <span class="sc">!=</span> <span class="st">"DC"</span>) <span class="sc">|&gt;</span> <span class="co"># remove outlier</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(state_abbreviations, <span class="at">by =</span> <span class="st">"state_abbrev"</span>) <span class="sc">|&gt;</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(population_data, <span class="at">by =</span> <span class="st">"state_name"</span>) <span class="sc">|&gt;</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">CrimeRate =</span> Violent <span class="sc">/</span> state_pop) <span class="sc">|&gt;</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(state_abbrev, CrimeRate, Metro, HighSchool, Poverty, state_pop)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>crime_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 50 × 6
   state_abbrev CrimeRate Metro HighSchool Poverty state_pop
   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
 1 AK           0.000819   65.6       90.2     8      724357
 2 AL           0.0000871  55.4       82.4    13.7   4934193
 3 AR           0.000150   52.5       79.2    12.1   3033946
 4 AZ           0.0000682  88.2       84.4    11.9   7520103
 5 CA           0.0000146  94.4       81.3    10.5  39613493
 6 CO           0.0000585  84.5       88.3     7.3   5893634
 7 CT           0.0000867  87.7       88.8     6.4   3552821
 8 DE           0.000664   80.1       86.5     5.8    990334
 9 FL           0.0000333  89.3       85.9     9.7  21944577
10 GA           0.0000419  71.6       85.2    10.8  10830007
# ℹ 40 more rows</code></pre>
</div>
</div>
<p>We can fit a GLM using the <code>glm</code> command, specifying as additional arguments the observation weights as well as the exponential dispersion model. In this case, the weights are the state populations and the family is binomial:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>glm_fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(CrimeRate <span class="sc">~</span> Metro <span class="sc">+</span> HighSchool <span class="sc">+</span> Poverty,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">weights =</span> state_pop,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">"binomial"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> crime_data</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can print the summary table as usual:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = CrimeRate ~ Metro + HighSchool + Poverty, family = "binomial", 
    data = crime_data, weights = state_pop)

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.609e+01  3.520e-01  -45.72   &lt;2e-16 ***
Metro       -2.586e-02  5.727e-04  -45.15   &lt;2e-16 ***
HighSchool   9.106e-02  3.450e-03   26.39   &lt;2e-16 ***
Poverty      6.077e-02  4.852e-03   12.53   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 15590  on 49  degrees of freedom
Residual deviance: 11742  on 46  degrees of freedom
AIC: 12136

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>Amazingly, everything is very significant! This is because the weights for each observation (the state populations) are very high, effectively making the sample size very high. But frankly, this is a bit suspicious. Glancing at the bottom of the regression summary, we see a residual deviance of 11742 on 46 degrees of freedom. This part of the summary refers to the deviance-based goodness of fit test. Under the null hypothesis that the model fits well, we expect that the residual deviance has a distribution of <span class="math inline">\(\chi^2_{46}\)</span>, which has a mean of 46.</p>
<p>Let’s formally check the goodness of fit. We can extract the residual deviance and residual degrees of freedom from the GLM fit:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>glm_fit<span class="sc">$</span>deviance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 11742.28</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>glm_fit<span class="sc">$</span>df.residual</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 46</code></pre>
</div>
</div>
<p>We can then compute the chi-square <span class="math inline">\(p\)</span>-value:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute based on residual deviance from fit object</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(glm_fit<span class="sc">$</span>deviance,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">df =</span> glm_fit<span class="sc">$</span>df.residual,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower.tail =</span> <span class="cn">FALSE</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute residual deviance as sum of squares of residuals</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="fu">sum</span>(<span class="fu">resid</span>(glm_fit, <span class="st">"deviance"</span>)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">df =</span> glm_fit<span class="sc">$</span>df.residual,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower.tail =</span> <span class="cn">FALSE</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
<p>Wow, we get a <span class="math inline">\(p\)</span>-value of zero! Let’s try doing a score-based (i.e., Pearson) goodness of fit test:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="fu">sum</span>(<span class="fu">resid</span>(glm_fit, <span class="st">"pearson"</span>)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">df =</span> glm_fit<span class="sc">$</span>df.residual,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower.tail =</span> <span class="cn">FALSE</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
<p>Also zero. So we need to immediately stop using this model for inference about these data, since it fits the data very poorly. We will discuss how to build a better model for the crime data in the next unit. For now, we turn to analyzing a different dataset.</p>
</section>
<section id="sec-noisy-miner-data" class="level3" data-number="4.5.2">
<h3 data-number="4.5.2" class="anchored" data-anchor-id="sec-noisy-miner-data"><span class="header-section-number">4.5.2</span> Noisy miner data</h3>
<p><em>Credit: Generalized Linear Models With Examples in R textbook.</em></p>
<p>Let’s consider the noisy miner dataset. Noisy miners are a small but aggressive native Australian bird. We want to know how the number of these birds observed in a patch of land depends on various factors of that patch of land.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GLMsData)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"nminer"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>nminer <span class="sc">|&gt;</span> <span class="fu">as_tibble</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 31 × 8
   Miners  Eucs  Area Grazed Shrubs Bulokes Timber Minerab
    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;int&gt;   &lt;int&gt;
 1      0     2    22      0      1     120     16       0
 2      0    10    11      0      1      67     25       0
 3      1    16    51      0      1      85     13       3
 4      1    20    22      0      1      45     12       2
 5      1    19     4      0      1     160     14       8
 6      1    18    61      0      1      75      6       1
 7      1    12    16      0      1     100     12       8
 8      1    16    14      0      1     321     15       5
 9      0     3     5      0      1     275      8       0
10      1    12     6      1      0     227     10       4
# ℹ 21 more rows</code></pre>
</div>
</div>
<p>Since the response is a count, we can model it as a Poisson random variable. Let’s fit that GLM:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>glm_fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Minerab <span class="sc">~</span> . <span class="sc">-</span> Miners, <span class="at">family =</span> <span class="st">"poisson"</span>, <span class="at">data =</span> nminer)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = Minerab ~ . - Miners, family = "poisson", data = nminer)

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.886345   0.875737  -1.012    0.311    
Eucs         0.129309   0.021757   5.943 2.79e-09 ***
Area        -0.028736   0.013241  -2.170    0.030 *  
Grazed       0.140831   0.364622   0.386    0.699    
Shrubs       0.335828   0.375059   0.895    0.371    
Bulokes      0.001469   0.001773   0.828    0.408    
Timber      -0.006781   0.009074  -0.747    0.455    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 150.545  on 30  degrees of freedom
Residual deviance:  54.254  on 24  degrees of freedom
AIC: 122.41

Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
<p>We exclude <code>Miners</code> because this is just a binarized version of the response variable. Things look a bit better on the GOF front:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="fu">sum</span>(<span class="fu">resid</span>(glm_fit, <span class="st">"deviance"</span>)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">df =</span> glm_fit<span class="sc">$</span>df.residual,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower.tail =</span> <span class="cn">FALSE</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.000394186</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="fu">sum</span>(<span class="fu">resid</span>(glm_fit, <span class="st">"pearson"</span>)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">df =</span> glm_fit<span class="sc">$</span>df.residual,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower.tail =</span> <span class="cn">FALSE</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0001185197</code></pre>
</div>
</div>
<p>Still, there is some model misspecification, but for now, we still proceed with the rest of the analysis.</p>
<p>The standard errors shown in the summary are based on the Wald test. We can get Wald confidence intervals based on these standard errors by using the formula:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>glm_fit <span class="sc">|&gt;</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>() <span class="sc">|&gt;</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>() <span class="sc">|&gt;</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">|&gt;</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(<span class="st">`</span><span class="at">2.5 %</span><span class="st">`</span> <span class="ot">=</span> Estimate <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.025</span>)<span class="sc">*</span><span class="st">`</span><span class="at">Std. Error</span><span class="st">`</span>,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">`</span><span class="at">97.5 %</span><span class="st">`</span> <span class="ot">=</span> Estimate <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.025</span>)<span class="sc">*</span><span class="st">`</span><span class="at">Std. Error</span><span class="st">`</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   2.5 %       97.5 %
(Intercept) -2.602757559 -2.602757559
Eucs         0.086666177  0.086666177
Area        -0.054686818 -0.054686818
Grazed      -0.573814583 -0.573814583
Shrubs      -0.399274191 -0.399274191
Bulokes     -0.002007061 -0.002007061
Timber      -0.024565751 -0.024565751</code></pre>
</div>
</div>
<p>Or, we can simply use <code>confint.default()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint.default</span>(glm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   2.5 %       97.5 %
(Intercept) -2.602757559  0.830066560
Eucs         0.086666177  0.171951888
Area        -0.054686818 -0.002784651
Grazed      -0.573814583  0.855476296
Shrubs      -0.399274191  1.070929206
Bulokes     -0.002007061  0.004944760
Timber      -0.024565751  0.011002885</code></pre>
</div>
</div>
<p>Or, we might want LRT-based confidence intervals, which are given by <code>confint()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(glm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Waiting for profiling to be done...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                  2.5 %       97.5 %
(Intercept) -2.63176754  0.812111327
Eucs         0.08782624  0.173336323
Area        -0.05658079 -0.004456166
Grazed      -0.57858596  0.855903871
Shrubs      -0.38600748  1.090319407
Bulokes     -0.00214123  0.004838901
Timber      -0.02483241  0.010820749</code></pre>
</div>
</div>
<p>In this case, the two sets of confidence intervals seem fairly similar.</p>
<p>Now, we can get prediction intervals, either on the linear predictor scale or on the mean scale:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>pred_linear <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm_fit, <span class="at">newdata =</span> nminer[<span class="dv">31</span>,], <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>pred_mean <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm_fit, <span class="at">newdata =</span> nminer[<span class="dv">31</span>,], <span class="at">type =</span> <span class="st">"response"</span>, <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>pred_linear</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$fit
       31 
0.6556799 

$se.fit
[1] 0.2635664

$residual.scale
[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>pred_mean</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$fit
      31 
1.926452 

$se.fit
       31 
0.5077481 

$residual.scale
[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(pred_mean<span class="sc">$</span>fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       31 
0.6556799 </code></pre>
</div>
</div>
<p>We see that the prediction on the linear predictor scale is exactly the logarithm of the prediction on the mean scale. However, the standard error given on the mean scale uses the delta method. We prefer to directly transform the confidence interval from the linear scale using the inverse link function (in this case, the exponential):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using delta method</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(pred_mean<span class="sc">$</span>fit <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.025</span>)<span class="sc">*</span>pred_mean<span class="sc">$</span>se.fit,</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  pred_mean<span class="sc">$</span>fit <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>pred_mean<span class="sc">$</span>se.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       31        31 
0.9312839 2.9216197 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using transformation</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">c</span>(pred_linear<span class="sc">$</span>fit <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.025</span>)<span class="sc">*</span>pred_linear<span class="sc">$</span>se.fit,</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>      pred_linear<span class="sc">$</span>fit <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>pred_linear<span class="sc">$</span>se.fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      31       31 
1.149238 3.229285 </code></pre>
</div>
</div>
<p>In this case, the intervals obtained are somewhat different. We can plot confidence intervals for the fit in a univariate case (e.g., regressing <code>Minerab</code> on <code>Eucs</code>) using <code>geom_smooth()</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>nminer <span class="sc">|&gt;</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Eucs, <span class="at">y =</span> Minerab)) <span class="sc">+</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"glm"</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">family =</span> <span class="st">"poisson"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="glm-general-theory_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="384"></p>
</figure>
</div>
</div>
</div>
<p>We can also test the coefficients in the model. The Wald tests for individual coefficients were already given by the regression summary above. We might want to carry out likelihood ratio tests for individual coefficients instead. For example, let’s do this for <code>Eucs</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>glm_fit_partial <span class="ot">&lt;-</span> <span class="fu">glm</span>(Minerab <span class="sc">~</span> . <span class="sc">-</span> Miners <span class="sc">-</span> Eucs, <span class="at">family =</span> <span class="st">"poisson"</span>, <span class="at">data =</span> nminer)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(glm_fit_partial, glm_fit, <span class="at">test =</span> <span class="st">"LRT"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model 1: Minerab ~ (Miners + Eucs + Area + Grazed + Shrubs + Bulokes + 
    Timber) - Miners - Eucs
Model 2: Minerab ~ (Miners + Eucs + Area + Grazed + Shrubs + Bulokes + 
    Timber) - Miners
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1        25     95.513                          
2        24     54.254  1   41.259 1.333e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The <code>Eucs</code> variable is quite significant! We can manually carry out the LRT as a sanity check:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>deviance_partial <span class="ot">&lt;-</span> glm_fit_partial<span class="sc">$</span>deviance</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>deviance_full <span class="ot">&lt;-</span> glm_fit<span class="sc">$</span>deviance</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>lrt_stat <span class="ot">&lt;-</span> deviance_partial <span class="sc">-</span> deviance_full</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pchisq</span>(lrt_stat, <span class="at">df =</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(lrt_stat, p_value)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
  lrt_stat  p_value
     &lt;dbl&gt;    &lt;dbl&gt;
1     41.3 1.33e-10</code></pre>
</div>
</div>
<p>We can test groups of variables using the likelihood ratio test as well.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./linear-models-misspecification.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear models: Misspecification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./glm-special-cases.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models: Special cases</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>