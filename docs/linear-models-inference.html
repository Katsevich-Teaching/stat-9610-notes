<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 9610Lecture Notes - 2&nbsp; Linear models: Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./linear-models-misspecification.html" rel="next">
<link href="./linear-models-estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linear-models-inference.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: Inference</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 9610<br>Lecture Notes</a> 
        <div class="sidebar-tools-main">
    <a href="./STAT-9610-br-Lecture-Notes.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-models-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear models: Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-models-inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-models-misspecification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear models: Misspecification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-general-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Generalized linear models: General theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-special-cases.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models: Special cases</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multiple-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multiple testing</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#building-blocks-for-linear-model-inference" id="toc-building-blocks-for-linear-model-inference" class="nav-link active" data-scroll-target="#building-blocks-for-linear-model-inference"><span class="header-section-number">2.1</span> Building blocks for linear model inference</a>
  <ul class="collapse">
  <li><a href="#sec-mvrnorm" id="toc-sec-mvrnorm" class="nav-link" data-scroll-target="#sec-mvrnorm"><span class="header-section-number">2.1.1</span> The multivariate normal distribution</a></li>
  <li><a href="#sec-lin-reg-dist" id="toc-sec-lin-reg-dist" class="nav-link" data-scroll-target="#sec-lin-reg-dist"><span class="header-section-number">2.1.2</span> The distributions of linear regression estimates and residuals</a></li>
  <li><a href="#sec-noise-estimation" id="toc-sec-noise-estimation" class="nav-link" data-scroll-target="#sec-noise-estimation"><span class="header-section-number">2.1.3</span> Estimation of the noise variance <span class="math inline">\(\sigma^2\)</span></a></li>
  </ul></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing"><span class="header-section-number">2.2</span> Hypothesis Testing</a>
  <ul class="collapse">
  <li><a href="#sec-one-dim-testing" id="toc-sec-one-dim-testing" class="nav-link" data-scroll-target="#sec-one-dim-testing"><span class="header-section-number">2.2.1</span> Testing a One-Dimensional Parameter</a></li>
  <li><a href="#sec-multi-dim-testing" id="toc-sec-multi-dim-testing" class="nav-link" data-scroll-target="#sec-multi-dim-testing"><span class="header-section-number">2.2.2</span> Testing a Multi-Dimensional Parameter</a></li>
  </ul></li>
  <li><a href="#sec-power" id="toc-sec-power" class="nav-link" data-scroll-target="#sec-power"><span class="header-section-number">2.3</span> Power</a>
  <ul class="collapse">
  <li><a href="#the-power-of-a-t-test" id="toc-the-power-of-a-t-test" class="nav-link" data-scroll-target="#the-power-of-a-t-test"><span class="header-section-number">2.3.1</span> The power of a <span class="math inline">\(t\)</span>-test</a></li>
  <li><a href="#the-power-of-an-f-test" id="toc-the-power-of-an-f-test" class="nav-link" data-scroll-target="#the-power-of-an-f-test"><span class="header-section-number">2.3.2</span> The power of an <span class="math inline">\(F\)</span>-test</a></li>
  <li><a href="#power-of-the-t-test-when-predictors-are-added-to-the-model" id="toc-power-of-the-t-test-when-predictors-are-added-to-the-model" class="nav-link" data-scroll-target="#power-of-the-t-test-when-predictors-are-added-to-the-model"><span class="header-section-number">2.3.3</span> Power of the <span class="math inline">\(t\)</span>-test when predictors are added to the model</a></li>
  </ul></li>
  <li><a href="#confidence-and-prediction-intervals" id="toc-confidence-and-prediction-intervals" class="nav-link" data-scroll-target="#confidence-and-prediction-intervals"><span class="header-section-number">2.4</span> Confidence and prediction intervals</a>
  <ul class="collapse">
  <li><a href="#confidence-interval-for-a-coefficient" id="toc-confidence-interval-for-a-coefficient" class="nav-link" data-scroll-target="#confidence-interval-for-a-coefficient"><span class="header-section-number">2.4.1</span> Confidence interval for a coefficient</a></li>
  <li><a href="#confidence-interval-for-mathbbeyboldsymbolx_0" id="toc-confidence-interval-for-mathbbeyboldsymbolx_0" class="nav-link" data-scroll-target="#confidence-interval-for-mathbbeyboldsymbolx_0"><span class="header-section-number">2.4.2</span> Confidence interval for <span class="math inline">\(\mathbb{E}[y|\boldsymbol{x_0}]\)</span></a></li>
  <li><a href="#prediction-interval-for-yboldsymbolx_0" id="toc-prediction-interval-for-yboldsymbolx_0" class="nav-link" data-scroll-target="#prediction-interval-for-yboldsymbolx_0"><span class="header-section-number">2.4.3</span> Prediction interval for <span class="math inline">\(y|\boldsymbol{x_0}\)</span></a></li>
  <li><a href="#simultaneous-intervals" id="toc-simultaneous-intervals" class="nav-link" data-scroll-target="#simultaneous-intervals"><span class="header-section-number">2.4.4</span> Simultaneous intervals</a></li>
  </ul></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations"><span class="header-section-number">2.5</span> Practical considerations</a>
  <ul class="collapse">
  <li><a href="#practical-versus-statistical-significance" id="toc-practical-versus-statistical-significance" class="nav-link" data-scroll-target="#practical-versus-statistical-significance"><span class="header-section-number">2.5.1</span> Practical versus statistical significance</a></li>
  <li><a href="#correlation-versus-causation-and-simpsons-paradox" id="toc-correlation-versus-causation-and-simpsons-paradox" class="nav-link" data-scroll-target="#correlation-versus-causation-and-simpsons-paradox"><span class="header-section-number">2.5.2</span> Correlation versus causation, and Simpson’s paradox</a></li>
  <li><a href="#dealing-with-correlated-predictors" id="toc-dealing-with-correlated-predictors" class="nav-link" data-scroll-target="#dealing-with-correlated-predictors"><span class="header-section-number">2.5.3</span> Dealing with correlated predictors</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">2.5.4</span> Model selection</a></li>
  </ul></li>
  <li><a href="#r-demo" id="toc-r-demo" class="nav-link" data-scroll-target="#r-demo"><span class="header-section-number">2.6</span> R demo</a>
  <ul class="collapse">
  <li><a href="#exploration" id="toc-exploration" class="nav-link" data-scroll-target="#exploration"><span class="header-section-number">2.6.1</span> Exploration</a></li>
  <li><a href="#hypothesis-testing-1" id="toc-hypothesis-testing-1" class="nav-link" data-scroll-target="#hypothesis-testing-1"><span class="header-section-number">2.6.2</span> Hypothesis testing</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="header-section-number">2.6.3</span> Confidence intervals</a></li>
  <li><a href="#predictor-competition-and-collaboration" id="toc-predictor-competition-and-collaboration" class="nav-link" data-scroll-target="#predictor-competition-and-collaboration"><span class="header-section-number">2.6.4</span> Predictor competition and collaboration</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear models: Inference</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>We now understand the least squares estimator <span class="math inline">\(\boldsymbol{\widehat{\beta}}\)</span> from geometric and algebraic points of view. In Chapter 2, we will switch to a probabilistic perspective to derive inferential statements for linear models, in the form of hypothesis tests and confidence intervals. In order to facilitate this, we will assume that the error terms are normally distributed:</p>
<p><span class="math display">\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \text{where} \ \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_n).
\]</span></p>
<section id="building-blocks-for-linear-model-inference" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="building-blocks-for-linear-model-inference"><span class="header-section-number">2.1</span> Building blocks for linear model inference</h2>
<p><em>See also Agresti 3.1.1, 3.1.2, 3.1.4</em></p>
<p>First we put in place some building blocks: The multivariate normal distribution (Section <a href="#sec-mvrnorm"><span>2.1.1</span></a>), the distributions of linear regression estimates and residuals (Section <a href="#sec-lin-reg-dist"><span>2.1.2</span></a>), and estimation of the noise variance <span class="math inline">\(\sigma^2\)</span> (Section <a href="#sec-noise-estimation"><span>2.1.3</span></a>).</p>
<section id="sec-mvrnorm" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="sec-mvrnorm"><span class="header-section-number">2.1.1</span> The multivariate normal distribution</h3>
<p>Recall that a random vector <span class="math inline">\(\boldsymbol{w} \in \mathbb{R}^d\)</span> has a multivariate normal distribution with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> if it has probability density</p>
<p><span class="math display">\[
p(\boldsymbol{w}) = \frac{1}{\sqrt{(2\pi)^{d}\text{det}(\boldsymbol{\Sigma})}}\exp\left(-\frac{1}{2}(\boldsymbol{w} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{w} - \boldsymbol{\mu})\right).
\]</span></p>
<p>These random vectors have lots of special properties, including:</p>
<ul>
<li><strong>Linear transformation</strong>: If <span class="math inline">\(\boldsymbol{w} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>, then <span class="math inline">\(\boldsymbol{A} \boldsymbol{w} + \boldsymbol{b} \sim N(\boldsymbol{A} \boldsymbol{\mu} + \boldsymbol{b}, \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^\top)\)</span>.</li>
<li><strong>Independence</strong>: If <span class="math display">\[
\begin{pmatrix}\boldsymbol{w}_1 \\ \boldsymbol{w}_2 \end{pmatrix} \sim N\left(\begin{pmatrix}\boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix} , \begin{pmatrix}\boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{12}^\top &amp; \boldsymbol{\Sigma}_{22}\end{pmatrix}\right),
\]</span> then <span class="math inline">\(\boldsymbol{w}_1 \perp\!\!\!\perp \boldsymbol{w}_2\)</span> if and only if <span class="math inline">\(\boldsymbol{\Sigma}_{12} = \boldsymbol{0}\)</span>.</li>
</ul>
<p>An important distribution related to the multivariate normal is the <span class="math inline">\(\chi^2_d\)</span> (chi-squared with <span class="math inline">\(d\)</span> degrees of freedom) distribution, defined as</p>
<p><span class="math display">\[
\chi^2_d \equiv \sum_{j = 1}^d w_j^2 \quad \text{for} \quad w_1, \dots, w_d \overset{\text{i.i.d.}}{\sim} N(0, 1).
\]</span></p>
</section>
<section id="sec-lin-reg-dist" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="sec-lin-reg-dist"><span class="header-section-number">2.1.2</span> The distributions of linear regression estimates and residuals</h3>
<p><em>See also Dunn and Smyth 2.8.2</em></p>
<p>The most important distributional result in linear regression is that</p>
<p><span id="eq-beta-hat-dist"><span class="math display">\[
\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}).
\tag{2.1}\]</span></span></p>
<p>Indeed, by the linear transformation property of the multivariate normal distribution,</p>
<p><span class="math display">\[
\begin{split}
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}_n) &amp;\Longrightarrow \boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{y} \sim N((\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{\beta}, (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \sigma^2 \boldsymbol{I}_n \boldsymbol{X}(\boldsymbol{X}^\top \boldsymbol{X})^{-1}) \\
&amp;= N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}).
\end{split}
\]</span></p>
<p>Next, let’s consider the joint distribution of <span class="math inline">\(\boldsymbol{\widehat{\mu}} = \boldsymbol{X} \boldsymbol{\widehat{\beta}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\epsilon}} = \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\)</span>. We have</p>
<p><span class="math display">\[
\begin{split}
\begin{pmatrix} \boldsymbol{\widehat{\mu}} \\ \boldsymbol{\widehat{\epsilon}} \end{pmatrix} = \begin{pmatrix} \boldsymbol{H} \boldsymbol{y} \\ (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y} \end{pmatrix} = \begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\boldsymbol{y} &amp;\sim N\left(\begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\boldsymbol{X} \boldsymbol{\beta}, \begin{pmatrix} \boldsymbol{H} \\ \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\cdot \sigma^2 \boldsymbol{I} \begin{pmatrix} \boldsymbol{H} &amp; \boldsymbol{I} - \boldsymbol{H} \end{pmatrix}\right) \\
&amp;= N\left(\begin{pmatrix} \boldsymbol{X} \boldsymbol{\beta} \\ \boldsymbol{0} \end{pmatrix}, \begin{pmatrix} \sigma^2 \boldsymbol{H} &amp; \boldsymbol{0} \\ \boldsymbol{0} &amp; \sigma^2(\boldsymbol{I} - \boldsymbol{H}) \end{pmatrix} \right).
\end{split}
\]</span></p>
<p>In other words,</p>
<p><span id="eq-fit-and-error-dist"><span class="math display">\[
\boldsymbol{\widehat{\mu}} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{H}) \quad \text{and} \quad \boldsymbol{\widehat{\epsilon}} \sim N(\boldsymbol{0}, \sigma^2(\boldsymbol{I} - \boldsymbol{H})), \quad \text{with} \quad \boldsymbol{\widehat{\mu}} \perp\!\!\!\perp \boldsymbol{\widehat{\epsilon}}.
\tag{2.2}\]</span></span></p>
<p>The statistical independence between <span class="math inline">\(\boldsymbol{\widehat{\mu}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\epsilon}}\)</span> is a result of the fact that these two quantities are projections of <span class="math inline">\(\boldsymbol{y}\)</span> onto two orthogonal subspaces: <span class="math inline">\(C(\boldsymbol{X})\)</span> and <span class="math inline">\(C(\boldsymbol{X})^\perp\)</span> (Figure <a href="#fig-orthogonality-fit-residuals"><span>2.1</span></a>).</p>
<div id="fig-orthogonality-fit-residuals" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/orthogonality-fit-residuals.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2.1: The fitted vector <span class="math inline">\(\boldsymbol{\widehat{\mu}}\)</span> and the residual vector <span class="math inline">\(\boldsymbol{\widehat{\epsilon}}\)</span> are projections of <span class="math inline">\(\boldsymbol{y}\)</span> onto orthogonal subspaces.</figcaption>
</figure>
</div>
<p>Since <span class="math inline">\(\boldsymbol{\widehat{\beta}}\)</span> is a deterministic function of <span class="math inline">\(\boldsymbol{\widehat{\mu}}\)</span> (in particular, <span class="math inline">\(\boldsymbol{\widehat{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{\widehat{\mu}}\)</span>), it also follows that</p>
<p><span id="eq-beta-ind-eps"><span class="math display">\[
\boldsymbol{\widehat{\beta}} \perp\!\!\!\perp \boldsymbol{\widehat{\epsilon}}.
\tag{2.3}\]</span></span></p>
</section>
<section id="sec-noise-estimation" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="sec-noise-estimation"><span class="header-section-number">2.1.3</span> Estimation of the noise variance <span class="math inline">\(\sigma^2\)</span></h3>
<p><em>See also Dunn and Smyth 2.4.2, 2.5.3</em></p>
<p>We can’t quite do inference for <span class="math inline">\(\boldsymbol{\beta}\)</span> based on the distributional result <a href="#eq-beta-hat-dist"><span>2.1</span></a> because the noise variance <span class="math inline">\(\sigma^2\)</span> is unknown to us. Intuitively, since <span class="math inline">\(\sigma^2 = \mathbb{E}[\epsilon_i^2]\)</span>, we can get an estimate of <span class="math inline">\(\sigma^2\)</span> by looking at the quantity <span class="math inline">\(\|\boldsymbol{\widehat{\epsilon}}\|^2\)</span>. To get the distribution of this quantity, we need the following lemma:</p>
<div id="lem-normal-projection" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.1 </strong></span>Let <span class="math inline">\(\boldsymbol{w} \sim N(\boldsymbol{0}, \boldsymbol{P})\)</span> for some projection matrix <span class="math inline">\(\boldsymbol{P}\)</span>. Then, <span class="math inline">\(\|\boldsymbol{w}\|^2 \sim \chi^2_d\)</span>, where <span class="math inline">\(d = \text{trace}(\boldsymbol{P})\)</span> is the dimension of the subspace onto which <span class="math inline">\(\boldsymbol{P}\)</span> projects.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\boldsymbol{P} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{U}^\top\)</span> be an eigenvalue decomposition of <span class="math inline">\(\boldsymbol{P}\)</span>, where <span class="math inline">\(\boldsymbol{U}\)</span> is orthogonal and <span class="math inline">\(\boldsymbol{D}\)</span> is a diagonal matrix with <span class="math inline">\(D_{ii} \in \{0,1\}\)</span>. We have <span class="math inline">\(\boldsymbol{w} \overset{d}{=} \boldsymbol{U} \boldsymbol{D} \boldsymbol{z}\)</span> for <span class="math inline">\(\boldsymbol{z} \sim N(0, \boldsymbol{I}_n)\)</span>. Therefore,</p>
<p><span class="math display">\[
\|\boldsymbol{w}\|^2 = \|\boldsymbol{D} \boldsymbol{z}\|^2 = \sum_{i: D_{ii} = 1} z_i^2 \sim \chi^2_d, \quad \text{where } d = |\{i: D_{ii} = 1\}| = \text{trace}(D) = \text{trace}(\boldsymbol{P}).
\]</span></p>
</div>
<p>Recall that <span class="math inline">\(\boldsymbol{I} - \boldsymbol{H}\)</span> is a projection onto the <span class="math inline">\((n-p)\)</span>-dimensional space <span class="math inline">\(C(\boldsymbol{X})^\perp\)</span>, so by Lemma <a href="#lem-normal-projection"><span>2.1</span></a> and equation <a href="#eq-fit-and-error-dist"><span>2.2</span></a> we have</p>
<p><span id="eq-eps-norm-dist"><span class="math display">\[
\|\boldsymbol{\widehat{\epsilon}}\|^2 \sim \sigma^2 \chi^2_{n-p}.
\tag{2.4}\]</span></span></p>
<p>From this result, it follows that <span class="math inline">\(\mathbb{E}[\|\boldsymbol{\widehat{\epsilon}}\|^2] = n-p\)</span>, so</p>
<p><span id="eq-unbiased-noise-estimate"><span class="math display">\[
\widehat{\sigma}^2 \equiv \frac{1}{n-p}\|\boldsymbol{\widehat{\epsilon}}\|^2
\tag{2.5}\]</span></span></p>
<p>is an unbiased estimate for <span class="math inline">\(\sigma^2\)</span>. Why does the denominator need to be <span class="math inline">\(n-p\)</span> rather than <span class="math inline">\(n\)</span> for the estimator above to be unbiased? The reason for this is that the residuals <span class="math inline">\(\boldsymbol{\widehat{\epsilon}}\)</span> are the projection of the true noise vector <span class="math inline">\(\boldsymbol{\epsilon} \in \mathbb{R}^n\)</span> onto the <span class="math inline">\((n-p)\)</span>-dimensional subspace <span class="math inline">\(C(\boldsymbol{X})^\perp\)</span> (Figure <a href="#fig-residuals-as-noise-projection"><span>2.2</span></a>). To see this, note that</p>
<p><span class="math display">\[
\boldsymbol{\widehat{\epsilon}} = (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y} = (\boldsymbol{I} - \boldsymbol{H})(\boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) = (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{\epsilon}.
\]</span></p>
<p>Therefore, the norm of the residual vector will be smaller than that of the noise vector, especially to the extent that <span class="math inline">\(p\)</span> is close to <span class="math inline">\(n\)</span>.</p>
<div id="fig-residuals-as-noise-projection" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/residuals-as-noise-projection.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2.2: The residual vector <span class="math inline">\(\boldsymbol{\widehat{\epsilon}}\)</span> is the projection of the noise vector <span class="math inline">\(\boldsymbol{\epsilon}\)</span> onto <span class="math inline">\(C(\boldsymbol{X})^\perp\)</span>.</figcaption>
</figure>
</div>
</section>
</section>
<section id="hypothesis-testing" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="hypothesis-testing"><span class="header-section-number">2.2</span> Hypothesis Testing</h2>
<p><em>See also Agresti 3.2.1, 3.2.2, 3.2.4, 3.2.8</em></p>
<p>Typically, two types of null hypotheses are tested in a regression setting: those involving one-dimensional parameters and those involving multi-dimensional parameters. For example, consider the null hypotheses <span class="math inline">\(H_0: \beta_j = 0\)</span> and <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{0}\)</span> for <span class="math inline">\(S \subseteq \{0, 1, \dots, p-1\}\)</span>, respectively. We discuss tests of these two kinds of hypotheses in Sections <a href="#sec-one-dim-testing"><span>2.2.1</span></a> and <a href="#sec-multi-dim-testing"><span>2.2.2</span></a>, and then discuss the power of these tests in Section <a href="#sec-power"><span>2.3</span></a>.</p>
<section id="sec-one-dim-testing" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-one-dim-testing"><span class="header-section-number">2.2.1</span> Testing a One-Dimensional Parameter</h3>
<p><em>See also Dunn and Smyth 2.8.3</em></p>
<section id="t-test-for-a-single-coefficient" class="level4" data-number="2.2.1.1">
<h4 data-number="2.2.1.1" class="anchored" data-anchor-id="t-test-for-a-single-coefficient"><span class="header-section-number">2.2.1.1</span> <span class="math inline">\(t\)</span>-test for a Single Coefficient</h4>
<p>The most common question to ask in a linear regression context is: Is the <span class="math inline">\(j\)</span>th predictor associated with the response when controlling for the other predictors? In the language of hypothesis testing, this corresponds to the null hypothesis:</p>
<p><span id="eq-one-dim-null"><span class="math display">\[
H_0: \beta_j = 0
\tag{2.6}\]</span></span></p>
<p>According to <a href="#eq-beta-hat-dist"><span>2.1</span></a>, we have <span class="math inline">\(\widehat{\beta}_j \sim N(0, \sigma^2/s_j^2)\)</span>, where, as we learned in Chapter 1:</p>
<p><span class="math display">\[
s_j^{2} \equiv [(\boldsymbol{X}^T \boldsymbol{X})^{-1}_{jj}]^{-1} = \|\boldsymbol{x}_{*j}^\perp\|^2.
\]</span></p>
<p>Therefore,</p>
<p><span id="eq-oracle-z-stat"><span class="math display">\[
\frac{\widehat{\beta}_j}{\sigma/s_j} \sim N(0,1),
\tag{2.7}\]</span></span></p>
<p>and we are tempted to define a level <span class="math inline">\(\alpha\)</span> test of the null hypothesis <a href="#eq-one-dim-null"><span>2.6</span></a> based on this normal distribution. While this is infeasible since we don’t know <span class="math inline">\(\sigma^2\)</span>, we can substitute in the unbiased estimate <a href="#eq-unbiased-noise-estimate"><span>2.5</span></a> derived in Section <a href="#sec-noise-estimation"><span>2.1.3</span></a>. Then,</p>
<p><span class="math display">\[
\text{SE}(\widehat{\beta}_j) \equiv \frac{\widehat{\sigma}}{s_j}
\]</span></p>
<p>is the standard error of <span class="math inline">\(\widehat{\beta}_j\)</span>, which is an approximation to the standard deviation of <span class="math inline">\(\widehat{\beta}_j\)</span>. Dividing <span class="math inline">\(\widehat{\beta}_j\)</span> by its standard error gives us the <span class="math inline">\(t\)</span>-statistic:</p>
<p><span class="math display">\[
t_j \equiv \frac{\widehat{\beta}_j}{\text{SE}(\widehat{\beta}_j)} = \frac{\widehat{\beta}_j}{\sqrt{\frac{1}{n-p}\|\boldsymbol{\widehat{\epsilon}}\|^2}/s_j}.
\]</span></p>
<p>This statistic is <em>pivotal</em>, in the sense that it has the same distribution for any <span class="math inline">\(\boldsymbol{\beta}\)</span> such that <span class="math inline">\(\beta_j = 0\)</span>. Indeed, we can rewrite it as:</p>
<p><span class="math display">\[
t_j = \frac{\frac{\widehat{\beta}}{\sigma/s_j}}{\sqrt{\frac{\sigma^{-2}\|\boldsymbol{\widehat{\epsilon}}\|^2}{n-p}}}.
\]</span></p>
<p>Recalling the independence of <span class="math inline">\(\boldsymbol{\widehat{\beta}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\epsilon}}\)</span> <a href="#eq-beta-ind-eps"><span>2.3</span></a>, the scaled chi-square distribution of <span class="math inline">\(\|\boldsymbol{\widehat{\epsilon}}\|^2\)</span> <a href="#eq-eps-norm-dist"><span>2.4</span></a>, and the standard normal distribution of <span class="math inline">\(\frac{\widehat{\beta}}{\sigma/s_j}\)</span> <a href="#eq-oracle-z-stat"><span>2.7</span></a>, we find that:</p>
<p><span class="math display">\[
\text{Under } H_0:\beta_j = 0, \quad t_j \sim \frac{N(0,1)}{\sqrt{\frac{1}{n-p}\chi^2_{n-p}}}, \quad \text{with numerator and denominator independent.}
\]</span></p>
<p>The latter distribution is called the <em><span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-p\)</span> degrees of freedom</em> and is denoted <span class="math inline">\(t_{n-p}\)</span>. This paves the way for the two-sided <span class="math inline">\(t\)</span>-test:</p>
<p><span class="math display">\[
\phi_t(\boldsymbol{X}, \boldsymbol{y}) = \mathbbm{1}(|t_j| &gt; t_{n-p}(1-\alpha/2)),
\]</span></p>
<p>where <span class="math inline">\(t_{n-p}(1-\alpha/2)\)</span> denotes the <span class="math inline">\(1-\alpha/2\)</span> quantile of <span class="math inline">\(t_{n-p}\)</span>. Note that, by the law of large numbers,</p>
<p><span class="math display">\[
\frac{1}{n-p}\chi^2_{n-p} \overset{P}{\rightarrow} 1 \quad \text{as} \quad n - p \rightarrow \infty,
\]</span></p>
<p>so for large <span class="math inline">\(n-p\)</span> we have <span class="math inline">\(t_{j} \sim t_{n-p} \approx N(0,1)\)</span>. Hence, the <span class="math inline">\(t\)</span>-test is approximately equal to the following <span class="math inline">\(z\)</span>-test:</p>
<p><span class="math display">\[
\phi_t(\boldsymbol{X}, \boldsymbol{y}) \approx \phi_z(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}(|t_j| &gt; z(1-\alpha/2)),
\]</span></p>
<p>where <span class="math inline">\(z(1-\alpha/2)\)</span> is the <span class="math inline">\(1-\alpha/2\)</span> quantile of <span class="math inline">\(N(0,1)\)</span>. The <span class="math inline">\(t\)</span>-test can also be defined in a one-sided fashion if power against one-sided alternatives is desired.</p>
</section>
<section id="example-one-sample-model" class="level4" data-number="2.2.1.2">
<h4 data-number="2.2.1.2" class="anchored" data-anchor-id="example-one-sample-model"><span class="header-section-number">2.2.1.2</span> Example: One-Sample Model</h4>
<p>Consider the intercept-only linear regression model <span class="math inline">\(y = \beta_0 + \epsilon\)</span>, and let’s apply the <span class="math inline">\(t\)</span>-test derived above to test the null hypothesis <span class="math inline">\(H_0: \beta_0 = 0\)</span>. We have <span class="math inline">\(\widehat{\beta}_0 = \bar{y}\)</span>. Furthermore, we have</p>
<p><span class="math display">\[
\text{SE}^2(\widehat{\beta}_0) = \frac{\widehat{\sigma}^2}{n}, \quad \text{where} \quad \widehat{\sigma}^2 = \frac{1}{n-1}\|\boldsymbol{y} - \bar{y} \boldsymbol{1}_n\|^2.
\]</span></p>
<p>Hence, we obtain the <span class="math inline">\(t\)</span> statistic:</p>
<p><span class="math display">\[
t = \frac{\widehat{\beta}_0}{\text{SE}(\widehat{\beta}_0)} = \frac{\sqrt{n} \bar{y}}{\sqrt{\frac{1}{n-1}\|\boldsymbol{y} - \bar{y} \boldsymbol{1}_n\|^2}}.
\]</span></p>
<p>According to the theory above, this test statistic has a null distribution of <span class="math inline">\(t_{n-1}\)</span>.</p>
</section>
<section id="example-two-sample-model" class="level4" data-number="2.2.1.3">
<h4 data-number="2.2.1.3" class="anchored" data-anchor-id="example-two-sample-model"><span class="header-section-number">2.2.1.3</span> Example: Two-Sample Model</h4>
<p>Suppose we have <span class="math inline">\(x_1 \in \{0,1\}\)</span>, in which case the linear regression <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \epsilon\)</span> becomes a two-sample model. We can rewrite this model as:</p>
<p><span class="math display">\[
y_i \sim \begin{cases}
N(\beta_0, \sigma^2) \quad &amp;\text{for } x_i = 0; \\
N(\beta_0 + \beta_1, \sigma^2) \quad &amp;\text{for } x_i = 1.
\end{cases}
\]</span></p>
<p>It is often of interest to test the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>, i.e., that the two groups have equal means. Let’s define:</p>
<p><span class="math display">\[
\bar{y}_0 \equiv \frac{1}{n_0}\sum_{i: x_i = 0} y_i, \quad \bar{y}_1 \equiv \frac{1}{n_1}\sum_{i: x_i = 1} y_i, \quad \text{where} \quad n_0 = |\{i: x_i = 0\}| \text{ and } n_1 = |\{i: x_i = 1\}|.
\]</span></p>
<p>Then, we have seen before that <span class="math inline">\(\widehat{\beta}_0 = \bar{y}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1 = \bar{y}_1 - \bar{y}_0\)</span>. We can compute that:</p>
<p><span class="math display">\[
s_1^2 \equiv \|\boldsymbol{x}_{*1}^{\perp}\|^2 = \|\boldsymbol{x}_{*1} - \frac{n_1}{n}\boldsymbol{1}\|^2 = n_1\frac{n_0^2}{n^2} + n_0\frac{n_1^2}{n^2} = \frac{n_0 n_1}{n} = \frac{1}{\frac{1}{n_0} + \frac{1}{n_1}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\widehat{\sigma}^2 = \frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar{y}_0)^2 + \sum_{i: x_i = 1}(y_i - \bar{y}_1)^2\right).
\]</span></p>
<p>Therefore, we arrive at a <span class="math inline">\(t\)</span>-statistic of:</p>
<p><span class="math display">\[
t = \frac{\sqrt{\frac{1}{\frac{1}{n_0} + \frac{1}{n_1}}}(\bar{y}_1 - \bar{y}_0)}{\sqrt{\frac{1}{n-2}\left(\sum_{i: x_i = 0}(y_i - \bar{y}_0)^2 + \sum_{i: x_i = 1}(y_i - \bar{y}_1)^2\right)}}.
\]</span></p>
<p>Under the null hypothesis, this statistic has a distribution of <span class="math inline">\(t_{n-2}\)</span>.</p>
</section>
<section id="t-test-for-a-contrast-among-coefficients" class="level4" data-number="2.2.1.4">
<h4 data-number="2.2.1.4" class="anchored" data-anchor-id="t-test-for-a-contrast-among-coefficients"><span class="header-section-number">2.2.1.4</span> <span class="math inline">\(t\)</span>-test for a Contrast Among Coefficients</h4>
<p>Given a vector <span class="math inline">\(\boldsymbol{c} \in \mathbb{R}^p\)</span>, the quantity <span class="math inline">\(\boldsymbol{c}^T \boldsymbol{\beta}\)</span> is sometimes called a <em>contrast</em>. For example, suppose <span class="math inline">\(\boldsymbol{c} = (1,-1, 0, \dots, 0)\)</span>. Then, <span class="math inline">\(\boldsymbol{c}^T \boldsymbol{\beta} = \beta_1 - \beta_2\)</span> is the difference in effects of the first and second predictors. We are sometimes interested in testing whether such a contrast is equal to zero, i.e., <span class="math inline">\(H_0: \boldsymbol{c}^T \boldsymbol{\beta} = 0\)</span>. While this hypothesis can involve two or more of the predictors, the parameter <span class="math inline">\(\boldsymbol{c}^T \boldsymbol{\beta}\)</span> is still one-dimensional, and therefore we can still apply a <span class="math inline">\(t\)</span>-test. Going back to the distribution <span class="math inline">\(\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2(\boldsymbol{X}^T \boldsymbol{X})^{-1})\)</span>, we find that:</p>
<p><span class="math display">\[
\boldsymbol{c}^T\boldsymbol{\widehat{\beta}} \sim N(\boldsymbol{c}^T\boldsymbol{\beta}, \sigma^2\boldsymbol{c}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{c}).
\]</span></p>
<p>Therefore, under the null hypothesis that <span class="math inline">\(\boldsymbol{c}^T \boldsymbol{\beta} = 0\)</span>, we can derive that:</p>
<p><span id="eq-contrasts-t-dist"><span class="math display">\[
\frac{\boldsymbol{c}^T \boldsymbol{\widehat{\beta}}}{\widehat{\sigma} \sqrt{\boldsymbol{c}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{c}}} \sim t_{n-p},
\tag{2.8}\]</span></span></p>
<p>giving us another <span class="math inline">\(t\)</span>-test. Note that the <span class="math inline">\(t\)</span>-tests described above can be recovered from this more general formulation by setting <span class="math inline">\(\boldsymbol{c} = \boldsymbol{e}_j\)</span>, the indicator vector with the <span class="math inline">\(j\)</span>th coordinate equal to 1 and all others equal to zero.</p>
</section>
</section>
<section id="sec-multi-dim-testing" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="sec-multi-dim-testing"><span class="header-section-number">2.2.2</span> Testing a Multi-Dimensional Parameter</h3>
<p><em>See also Dunn and Smyth 2.10.1</em></p>
<section id="f-test-for-a-group-of-coefficients" class="level4" data-number="2.2.2.1">
<h4 data-number="2.2.2.1" class="anchored" data-anchor-id="f-test-for-a-group-of-coefficients"><span class="header-section-number">2.2.2.1</span> <span class="math inline">\(F\)</span>-Test for a Group of Coefficients</h4>
<p>Now we move on to the case of testing a multi-dimensional parameter: <span class="math inline">\(H_0: \boldsymbol{\beta}_S = \boldsymbol{0}\)</span> for some <span class="math inline">\(S \subseteq \{0, 1, \dots, p-1\}\)</span>. In other words, we would like to test</p>
<p><span class="math display">\[
H_0: \boldsymbol{y} = \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} + \boldsymbol{\epsilon} \quad \text{versus} \quad H_1: \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}.
\]</span></p>
<p>To test this hypothesis, let us fit least squares coefficients <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{-S}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\beta}}\)</span> for the partial model as well as the full model. If the partial model fits well, then the residuals <span class="math inline">\(\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\)</span> from this model will not be much larger than the residuals <span class="math inline">\(\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\)</span> from the full model. To quantify this intuition, let us recall our analysis of variance decomposition from Chapter 1:</p>
<p><span class="math display">\[
\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 = \|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 + \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2.
\]</span></p>
<p>Let’s consider the ratio</p>
<p><span class="math display">\[
\frac{\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2} = \frac{\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2},
\]</span></p>
<p>which is the relative increase in the residual sum of squares when going from the full model to the partial model. Let us rewrite this ratio in terms of projection matrices. Let <span class="math inline">\(\boldsymbol{H}\)</span> be the projection matrix for the full model, and let <span class="math inline">\(\boldsymbol{H}_{\text{-}S}\)</span> be the projection matrix for the partial model. Note that <span class="math inline">\(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}\)</span> is the projection matrix onto the <span class="math inline">\(|S|\)</span>-dimensional space <span class="math inline">\(C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{\text{-}S})^\perp\)</span> (<a href="#fig-f-test-geometry">Figure&nbsp;<span>2.3</span></a>).</p>
<div id="fig-f-test-geometry" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/F-test-geometry.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2.3: Geometry of the <span class="math inline">\(F\)</span>-test. Orthogonality relationships stem from <span class="math inline">\(C(\boldsymbol{X}_{*,\text{-}S}) \perp C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^\perp \perp C(\boldsymbol{X})^\perp\)</span>.</figcaption>
</figure>
</div>
<p>We have</p>
<p><span class="math display">\[
\frac{\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2} = \frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}\|^2},
\]</span></p>
<p>so the numerator and denominator are the squared norms of the projections of <span class="math inline">\(\boldsymbol{y}\)</span> onto <span class="math inline">\(C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^\perp\)</span> and <span class="math inline">\(C(\boldsymbol{X})^\perp\)</span>, respectively (<a href="#fig-f-test-geometry">Figure&nbsp;<span>2.3</span></a>). Under the null hypothesis, we have <span class="math inline">\(\boldsymbol{y} = \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} + \boldsymbol{\epsilon}\)</span>, and</p>
<p><span class="math display">\[
(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{X}_{*,\text{-}S} \boldsymbol{\beta}_{-S} = (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{X}_{*,\text{-}S} \boldsymbol{\beta}_{-S} = 0
\]</span></p>
<p>because <span class="math inline">\(\boldsymbol{X}_{*, \text{-}S} \boldsymbol{\beta}_{-S} \in C(\boldsymbol{X}_{*, \text{-}S}) \perp C(\boldsymbol{X}) \cap C(\boldsymbol{X}_{*, \text{-}S})^T \perp C(\boldsymbol{X})^\perp\)</span>. It follows that</p>
<p><span class="math display">\[
\frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}\|^2} = \frac{\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon}\|^2}{\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\|^2}.
\]</span></p>
<p>Since the projection matrices in the numerator and denominator project onto orthogonal subspaces, we have <span class="math inline">\((\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon} \perp\!\!\!\perp (\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\)</span>, with <span class="math inline">\(\|(\boldsymbol{H} - \boldsymbol{H}_{\text{-}S}) \boldsymbol{\epsilon}\|^2 \sim \sigma^2 \chi^2_{|S|}\)</span> and <span class="math inline">\(\|(\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{\epsilon}\|^2 \sim \sigma^2 \chi^2_{n-p}\)</span>. Renormalizing numerator and denominator to have expectation 1 under the null, we arrive at the <span class="math inline">\(F\)</span>-statistic</p>
<p><span class="math display">\[
F \equiv \frac{(\|\boldsymbol{y} - \boldsymbol{X}_{*, \text{-}S} \boldsymbol{\widehat{\beta}}_{-S}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2)/|S|}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2/(n-p)}.
\]</span></p>
<p>We have derived that under the null hypothesis,</p>
<p><span class="math display">\[
F \sim \frac{\chi^2_{|S|}/|S|}{\chi^2_{n-p}/(n-p)}, \quad \text{with numerator and denominator independent.}
\]</span></p>
<p>This distribution is called the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(|S|\)</span> and <span class="math inline">\(n-p\)</span> degrees of freedom, and is denoted <span class="math inline">\(F_{|S|, n-p}\)</span>. Denoting by <span class="math inline">\(F_{|S|, n-p}(1-\alpha)\)</span> the <span class="math inline">\(1-\alpha\)</span> quantile of this distribution, we arrive at the <span class="math inline">\(F\)</span>-test</p>
<p><span class="math display">\[
\phi_F(\boldsymbol{X}, \boldsymbol{y}) \equiv \mathbbm{1}(F &gt; F_{|S|, n-p}(1-\alpha)).
\]</span></p>
<p>Note that the <span class="math inline">\(F\)</span>-test searches for deviations of <span class="math inline">\(\boldsymbol{\beta}_{S}\)</span> in all directions, and does not have one-sided variants like the <span class="math inline">\(t\)</span>-test.</p>
</section>
<section id="example-testing-for-any-significant-coefficients-except-the-intercept" class="level4" data-number="2.2.2.2">
<h4 data-number="2.2.2.2" class="anchored" data-anchor-id="example-testing-for-any-significant-coefficients-except-the-intercept"><span class="header-section-number">2.2.2.2</span> Example: Testing for Any Significant Coefficients Except the Intercept</h4>
<p>Suppose <span class="math inline">\(\boldsymbol{x}_{*,0} = \boldsymbol{1}_n\)</span> is an intercept term. Then, consider the null hypothesis <span class="math inline">\(H_0: \beta_1 = \cdots = \beta_{p-1} = 0\)</span>. In other words, the null hypothesis is the intercept-only model, and the alternative hypothesis is the regression model with an intercept and <span class="math inline">\(p-1\)</span> additional predictors. In this case, <span class="math inline">\(S = \{1, \dots, p-1\}\)</span> and <span class="math inline">\(-S = \{0\}\)</span>. The corresponding <span class="math inline">\(F\)</span> statistic is</p>
<p><span class="math display">\[
F \equiv \frac{(\|\boldsymbol{y} - \bar{y} \boldsymbol{1}\|^2 - \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2)/(p-1)}{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\widehat{\beta}}\|^2/(n-p)},
\]</span></p>
<p>with null distribution <span class="math inline">\(F_{p-1, n-p}\)</span>.</p>
</section>
<section id="example-testing-for-equality-of-group-means-in-c-groups-model" class="level4" data-number="2.2.2.3">
<h4 data-number="2.2.2.3" class="anchored" data-anchor-id="example-testing-for-equality-of-group-means-in-c-groups-model"><span class="header-section-number">2.2.2.3</span> Example: Testing for Equality of Group Means in <span class="math inline">\(C\)</span>-Groups Model</h4>
<p>As a further special case, consider the <span class="math inline">\(C\)</span>-groups model from Chapter 1. Recall the ANOVA decomposition</p>
<p><span class="math display">\[
\sum_{i = 1}^n (y_i - \bar{y})^2 = \sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2 + \sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2 = \text{SSB} + \text{SSW}.
\]</span></p>
<p>The <span class="math inline">\(F\)</span>-statistic in this case becomes</p>
<p><span class="math display">\[
F = \frac{\sum_{i = 1}^n (\bar{y}_{c(i)} - \bar{y})^2/(C-1)}{\sum_{i = 1}^n (y_i - \bar{y}_{c(i)})^2/(n-C)} = \frac{\text{SSB}/(C-1)}{\text{SSW}/(n-C)},
\]</span></p>
<p>with null distribution <span class="math inline">\(F_{C-1, n-C}\)</span>.</p>
</section>
</section>
</section>
<section id="sec-power" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-power"><span class="header-section-number">2.3</span> Power</h2>
<p><em>See also Agresti 3.2.5</em></p>
<p>So far we’ve been focused on finding the null distributions of various test statistics in order to construct tests with Type-I error control. Now let’s shift our attention to examining the power of these tests.</p>
<section id="the-power-of-a-t-test" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="the-power-of-a-t-test"><span class="header-section-number">2.3.1</span> The power of a <span class="math inline">\(t\)</span>-test</h3>
<p>Consider the <span class="math inline">\(t\)</span>-test of the null hypothesis <span class="math inline">\(H_0: \beta_j = 0\)</span>. Suppose that, in reality, <span class="math inline">\(\beta_j \neq 0\)</span>. What is the probability the <span class="math inline">\(t\)</span>-test will reject the null hypothesis? To answer this question, recall that <span class="math inline">\(\widehat \beta_j \sim N(\beta_j, \sigma^2/s_j^2)\)</span>. Therefore,</p>
<p><span id="eq-t-alt-dist-1"><span class="math display">\[
t = \frac{\widehat \beta_j}{\text{SE}(\widehat \beta_j)} = \frac{\beta_j}{\text{SE}(\widehat \beta_j)} + \frac{\widehat \beta_j - \beta_j}{\text{SE}(\widehat \beta_j)} \overset{\cdot}{\sim} N\left(\frac{\beta_j s_j}{\sigma}, 1\right)
\tag{2.9}\]</span></span></p>
<p>Here we have made the approximation <span class="math inline">\(\text{SE}(\widehat \beta_j) \approx \frac{\sigma}{s_j}\)</span>, which is pretty good when <span class="math inline">\(n-p\)</span> is large. Therefore, the power of the two-sided <span class="math inline">\(t\)</span>-test is</p>
<p><span class="math display">\[
\mathbb{E}[\phi_t] = \mathbb{P}[\phi_t = 1] \approx \mathbb{P}[|t| &gt; z_{1-\alpha/2}] \approx \mathbb{P}\left[\left|N\left(\frac{\beta_j s_j}{\sigma}, 1\right)\right| &gt; z_{1-\alpha/2}\right]
\]</span></p>
<p>Therefore, the quantity <span class="math inline">\(\frac{\beta_j s_j}{\sigma}\)</span> determines the power of the <span class="math inline">\(t\)</span>-test. To understand <span class="math inline">\(s_j\)</span> a little better, let’s assume that the rows <span class="math inline">\(\boldsymbol{x}_{i*}\)</span> of the model matrix are drawn i.i.d. from some distribution <span class="math inline">\((x_0, \dots, x_{p-1})\)</span>. Then we have roughly</p>
<p><span class="math display">\[
\boldsymbol{x}_{*j}^\perp \approx \boldsymbol{x}_{*j} - \mathbb{E}[\boldsymbol{x}_{*j}|\boldsymbol{X}_{*, \text{-}j}],
\]</span></p>
<p>so <span class="math inline">\(x_{ij}^\perp \approx x_{ij} - \mathbb{E}[x_{ij}|\boldsymbol{x}_{i,\text{-}j}]\)</span>. Hence,</p>
<p><span class="math display">\[
s_j^2 \equiv \|\boldsymbol{x}_{*j}^\perp\|^2 \approx n\mathbb{E}[(x_j-\mathbb{E}[x_j|\boldsymbol{x}_{\text{-}j}])^2] = n\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]].
\]</span></p>
<p>Hence, we can rewrite the alternative distribution (<a href="#eq-t-alt-dist-1"><span>2.9</span></a>) as</p>
<p><span id="eq-t-alt-dist-2"><span class="math display">\[
t \overset{\cdot}{\sim} N\left(\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]}}{\sigma}, 1\right)
\tag{2.10}\]</span></span></p>
<p>We can see clearly now how the power of the <span class="math inline">\(t\)</span>-test varies with the effect size <span class="math inline">\(\beta_j\)</span>, the sample size <span class="math inline">\(n\)</span>, the degree of collinearity <span class="math inline">\(\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]\)</span>, and the noise standard deviation <span class="math inline">\(\sigma\)</span>.</p>
</section>
<section id="the-power-of-an-f-test" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="the-power-of-an-f-test"><span class="header-section-number">2.3.2</span> The power of an <span class="math inline">\(F\)</span>-test</h3>
<p>Now let’s turn our attention to computing the power of the <span class="math inline">\(F\)</span>-test. We have</p>
<p><span class="math display">\[
F = \frac{\|\boldsymbol{X}\boldsymbol{\widehat \beta} - \boldsymbol{X}_{*, \text{-}S}\boldsymbol{\widehat \beta}_{-S}\|^2/|S|}{\|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\widehat \beta}\|^2/|n-p|} = \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\|(\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y}\|^2/|n-p|} \approx \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\sigma^2}.
\]</span></p>
<p>To calculate the distribution of the numerator, we need to introduce the notion of a non-central chi-squared random variable.</p>
<div id="def-noncentral-chi-square" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 </strong></span>For some vector <span class="math inline">\(\boldsymbol{\mu} \in \mathbb{R}^d\)</span>, suppose <span class="math inline">\(\boldsymbol{z} \sim N(\boldsymbol{\mu}, \boldsymbol{I}_d)\)</span>. Then, we define the distribution of <span class="math inline">\(\|\boldsymbol{z}\|^2\)</span> as the noncentral chi-square random variable with <span class="math inline">\(d\)</span> degrees of freedom and noncentrality parameter <span class="math inline">\(\|\boldsymbol{\mu}\|^2\)</span> and denote this distribution by <span class="math inline">\(\chi^2_d(\|\boldsymbol{\mu}\|^2)\)</span>.</p>
</div>
<p>The following proposition states two useful facts about noncentral chi-square distributions.</p>
<div id="prp-noncentral-chi-square" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.1 </strong></span>The following two relations hold: 1. The mean of a <span class="math inline">\(\chi^2_d(\|\boldsymbol{\mu}\|^2)\)</span> random variable is <span class="math inline">\(d + \|\boldsymbol{\mu}\|^2\)</span>. 2. If <span class="math inline">\(\boldsymbol{P}\)</span> is a projection matrix and <span class="math inline">\(\boldsymbol{y} = \boldsymbol{\mu} + \boldsymbol{\epsilon}\)</span>, then <span class="math inline">\(\frac{1}{\sigma^2}\|\boldsymbol{P} \boldsymbol{y}\|^2 \sim \chi^2_{\text{tr}(\boldsymbol{P})}\left(\frac{1}{\sigma^2}\|\boldsymbol{P} \boldsymbol{\mu}\|^2\right).\)</span></p>
</div>
<p>It therefore follows that</p>
<p><span class="math display">\[
F \approx \frac{\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S}) \boldsymbol{y}\|^2/|S|}{\sigma^2} \sim \frac{1}{|S|}\chi^2_{|S|}\left(\|(\boldsymbol{H}-\boldsymbol{H}_{\text{-}S})\boldsymbol{X} \boldsymbol{\beta}\|^2\right) = \frac{1}{|S|}\chi^2_{|S|}\left(\frac{1}{\sigma^2}\|\boldsymbol{X}^\perp_{*, S}\boldsymbol{\beta}_S\|^2\right).
\]</span></p>
<p>Assuming as before that the rows of <span class="math inline">\(\boldsymbol{X}\)</span> are samples from a joint distribution, we can write</p>
<p><span class="math display">\[
\|\boldsymbol{X}^\perp_{*, S}\boldsymbol{\beta}_S\|^2 \approx n\boldsymbol{\beta}_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S.
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
F \overset{\cdot}{\sim} \frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{\sigma^2}\right)
\]</span></p>
<p>which is similar in spirit to equation (<a href="#eq-t-alt-dist-2"><span>2.10</span></a>). To get a better sense of what this relationship implies for the power of the <span class="math inline">\(F\)</span>-test, we find from the first part of <a href="#prp-noncentral-chi-square">Proposition&nbsp;<span>2.1</span></a> that, under the alternative,</p>
<p><span class="math display">\[
\mathbb{E}[F] \approx \mathbb{E}\left[\frac{1}{|S|}\chi^2_{|S|}\left(\frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{\sigma^2}\right)\right] = 1 + \frac{n\beta_S^T \mathbb{E}[\text{Var}[\boldsymbol{x}_S|\boldsymbol{x}_{\text{-}S}]] \boldsymbol{\beta}_S}{|S| \cdot \sigma^2}.
\]</span></p>
<p>By contrast, under the null, the mean of the <span class="math inline">\(F\)</span>-statistic is 1. The <span class="math inline">\(|S|\)</span> term in the denominator above suggests that testing larger sets of variables explaining the same amount of variation in <span class="math inline">\(\boldsymbol{y}\)</span> will hurt power. The test must accommodate for the fact that larger sets of variables will explain more of the variability in <span class="math inline">\(y\)</span> even under the null hypothesis.</p>
</section>
<section id="power-of-the-t-test-when-predictors-are-added-to-the-model" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="power-of-the-t-test-when-predictors-are-added-to-the-model"><span class="header-section-number">2.3.3</span> Power of the <span class="math inline">\(t\)</span>-test when predictors are added to the model</h3>
<p>As we know, the outcome of a regression is a function of the predictors that are used. What happens to the <span class="math inline">\(t\)</span>-test <span class="math inline">\(p\)</span>-value for <span class="math inline">\(H_0: \beta_j = 0\)</span> when a predictor is added to the model? To keep things simple, let’s consider the</p>
<p><span class="math display">\[
\text{true underlying model:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
\]</span></p>
<p>Let’s consider the power of testing <span class="math inline">\(H_0: \beta_0 = 0\)</span> in the regression models</p>
<p><span class="math display">\[
\text{model 0:}\ y = \beta_0 x_0 + \epsilon \quad \text{versus} \quad \text{model 1:}\ y = \beta_0 x_0 + \beta_1 x_1 + \epsilon.
\]</span></p>
<p>There are four cases based on <span class="math inline">\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}]\)</span> and the value of <span class="math inline">\(\beta_1\)</span> in the true model:</p>
<ol type="1">
<li><span class="math inline">\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] \neq 0\)</span> and <span class="math inline">\(\beta_1 \neq 0\)</span>. In this case, in model 0 we have omitted an important variable that is correlated with <span class="math inline">\(\boldsymbol{x}_{*0}\)</span>. Therefore, the meaning of <span class="math inline">\(\beta_0\)</span> differs between model 0 and model 1, so it may not be meaningful to compare the <span class="math inline">\(p\)</span>-values arising from these two models.</li>
<li><span class="math inline">\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] \neq 0\)</span> and <span class="math inline">\(\beta_1 = 0\)</span>. In this case, we are adding a null predictor that is correlated with <span class="math inline">\(x_{*0}\)</span>. Recall that the power of the <span class="math inline">\(t\)</span>-test hinges on the quantity <span class="math inline">\(\frac{\beta_j \cdot \sqrt{n} \cdot \sqrt{\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]}}{\sigma}\)</span>. Adding the predictor <span class="math inline">\(x_1\)</span> has the effect of reducing the conditional predictor variance <span class="math inline">\(\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]\)</span>, therefore reducing the power. This is a case of <em>predictor competition</em>.</li>
<li><span class="math inline">\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0\)</span> and <span class="math inline">\(\beta_1 \neq 0\)</span>. In this case, we are adding a non-null predictor that is orthogonal to <span class="math inline">\(\boldsymbol{x}_{*0}\)</span>. While the conditional predictor variance <span class="math inline">\(\mathbb{E}[\text{Var}[x_j|\boldsymbol{x}_{\text{-}j}]]\)</span> remains the same due to orthogonality, the residual variance <span class="math inline">\(\sigma^2\)</span> is reduced when going from model 0 to model 1. Therefore, in this case adding <span class="math inline">\(x_1\)</span> to the model increases the power for testing <span class="math inline">\(H_0: \beta_0 = 0\)</span>. This is a case of <em>predictor collaboration</em>.</li>
<li><span class="math inline">\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0\)</span> and <span class="math inline">\(\beta_1 = 0\)</span>. In this case, we are adding an orthogonal null variable, which does not change the conditional predictor variance or the residual variance, and therefore keeps the power of the test the same.</li>
</ol>
<p>In conclusion, adding a predictor can either increase or decrease the power of a <span class="math inline">\(t\)</span>-test. Similar reasoning can be applied to the <span class="math inline">\(F\)</span>-test.</p>
<p><strong>Remark: Adjusting for covariates in randomized experiments.</strong> Case 3 above, i.e., <span class="math inline">\(\text{cor}[\boldsymbol{x}_{*0}, \boldsymbol{x}_{*1}] = 0\)</span> and <span class="math inline">\(\beta_1 \neq 0\)</span>, arises in the context of randomized experiments in causal inference. In this case, <span class="math inline">\(y\)</span> represents the outcome, <span class="math inline">\(x_0\)</span> represents the treatment, and <span class="math inline">\(x_1\)</span> represents a covariate. Because the treatment is randomized, there is no correlation between <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_1\)</span>. Therefore, it is not necessary to adjust for <span class="math inline">\(x_1\)</span> in order to get an unbiased estimate of the average treatment effect. However, it is known that adjusting for covariates can lead to more <em>precise</em> estimates of the treatment effect due to the phenomenon discussed in case 3 above. This point is also related to the discussion in Chapter 1 about the fact that if <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_1\)</span> are orthogonal, then the least squares coefficient <span class="math inline">\(\widehat \beta_0\)</span> is the same regardless of whether <span class="math inline">\(x_1\)</span> is included in the model. As we see here, either including <span class="math inline">\(x_1\)</span> in the model or adjusting <span class="math inline">\(y\)</span> for <span class="math inline">\(x_1\)</span> is necessary to get better power.</p>
</section>
</section>
<section id="confidence-and-prediction-intervals" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="confidence-and-prediction-intervals"><span class="header-section-number">2.4</span> Confidence and prediction intervals</h2>
<p><em>See also Agresti 3.3, Dunn and Smyth 2.8.4-2.8.5</em></p>
<p>In addition to hypothesis testing, we often want to construct confidence intervals for the coefficients.</p>
<section id="confidence-interval-for-a-coefficient" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="confidence-interval-for-a-coefficient"><span class="header-section-number">2.4.1</span> Confidence interval for a coefficient</h3>
<p>Under <span class="math inline">\(H_0: \beta_j = 0\)</span>, we showed that <span class="math inline">\(\frac{\widehat{\beta_j}}{\widehat{\sigma}/s_j} \sim t_{n-p}\)</span>. The same argument shows that for arbitrary <span class="math inline">\(\beta_j\)</span>, we have</p>
<p><span class="math display">\[
\frac{\widehat{\beta_j} - \beta_j}{\widehat{\sigma}/s_j} \sim t_{n-p}.
\]</span></p>
<p>We can use this relationship to construct a confidence interval for <span class="math inline">\(\beta_j\)</span> as follows:</p>
<p><span id="eq-pointwise-interval-beta"><span class="math display">\[
\begin{split}
1-\alpha = \mathbb{P}[|t_{n-p}| \leq t_{n-p}(1-\alpha/2)] &amp;= \mathbb{P}\left[\left|\frac{\widehat{\beta_j} - \beta_j}{\widehat{\sigma}/s_j}\right| \leq t_{n-p}(1-\alpha/2) \right] \\
&amp;= \mathbb{P}\left[\beta_j \in \left[\widehat{\beta_j} - \frac{\widehat{\sigma}}{s_j}t_{n-p}(1-\alpha/2), \widehat{\beta_j} + \frac{\widehat{\sigma}}{s_j}t_{n-p}(1-\alpha/2) \right]\right] \\
&amp;\equiv \mathbb{P}\left[\beta_j \in \left[\widehat{\beta_j} - \text{SE}(\widehat{\beta_j})t_{n-p}(1-\alpha/2), \widehat{\beta_j} + \text{SE}(\widehat{\beta_j})t_{n-p}(1-\alpha/2) \right]\right] \\
&amp;\equiv \mathbb{P}[\beta_j \in \text{CI}(\beta_j)].
\end{split}
\tag{2.11}\]</span></span></p>
<p>The confidence interval <span class="math inline">\(\text{CI}(\beta_j)\)</span> defined above therefore has <span class="math inline">\(1-\alpha\)</span> coverage. Because of the duality between confidence intervals and hypothesis tests, the factors contributing to powerful tests (<a href="#sec-power"><span>2.3</span></a>) also lead to shorter confidence intervals.</p>
</section>
<section id="confidence-interval-for-mathbbeyboldsymbolx_0" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="confidence-interval-for-mathbbeyboldsymbolx_0"><span class="header-section-number">2.4.2</span> Confidence interval for <span class="math inline">\(\mathbb{E}[y|\boldsymbol{x_0}]\)</span></h3>
<p>Suppose now that we have a new predictor vector <span class="math inline">\(\boldsymbol{x_0} \in \mathbb{R}^p\)</span>. The mean of the response for this predictor vector is <span class="math inline">\(\mathbb{E}[y|\boldsymbol{x_0}] = \boldsymbol{x_0}^T \boldsymbol{\beta}\)</span>. Plugging in <span class="math inline">\(\boldsymbol{x_0}\)</span> for <span class="math inline">\(\boldsymbol{c}\)</span> in the relation, we obtain</p>
<p><span class="math display">\[
\frac{\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} - \boldsymbol{x_0}^T \boldsymbol{\beta}}{\widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}}} \sim t_{n-p}.
\]</span></p>
<p>From this, we can derive that</p>
<p><span class="math display">\[
\text{CI}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}) \cdot t_{n-p}(1-\alpha/2) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}} \cdot t_{n-p}(1-\alpha/2)
\]</span></p>
<p>is a <span class="math inline">\(1-\alpha\)</span> confidence interval for <span class="math inline">\(\boldsymbol{x_0}^T \boldsymbol{\beta}\)</span>. We see that the width of this confidence interval depends on <span class="math inline">\(\boldsymbol{x_0}\)</span> through the quantity <span class="math inline">\(\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}\)</span>. Let’s give this quantity a closer look, in the case when the regression contains an intercept, i.e., <span class="math inline">\(\boldsymbol{x_{*,0}} = \boldsymbol{1}\)</span>. Then, we have <span class="math inline">\(\boldsymbol{x_0} = (1, \boldsymbol{x^T_{0,\text{-}0}})\)</span>. Then, defining <span class="math inline">\(\bar{x} \in \mathbb{R}^{p-1}\)</span> as the vector of column-wise means of <span class="math inline">\(\boldsymbol{X_{*,\text{-}0}}\)</span>, we can rewrite the regression as</p>
<p><span class="math display">\[
y = \beta_0 + \boldsymbol{x_{\text{-}0}}^T \boldsymbol{\beta_{\text{-}0}} + \epsilon \equiv \beta'_0 + (\boldsymbol{x_{\text{-}0}}-\bar{x})^T  \boldsymbol{\beta_{\text{-}0}} + \epsilon.
\]</span></p>
<p>Therefore, we seek a prediction interval for <span class="math inline">\(\boldsymbol{x_{0}}^T \boldsymbol{\beta} = \beta'_0 + (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T \boldsymbol{\beta_{\text{-}0}}\)</span>. With this reformulation, we can compute</p>
<p><span class="math display">\[
\begin{split}
\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0} &amp;= (1 \ (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T)\begin{pmatrix}\boldsymbol{1}^T \boldsymbol{1} &amp; 0 \\ 0 &amp;\boldsymbol{X_{*,\text{-}0}}^T \boldsymbol{X_{*,\text{-}0}} \end{pmatrix}^{-1}{1 \choose \boldsymbol{x_{0, \text{-}0}}-\bar{x}} \\
&amp;= \frac{1}{n} + (\boldsymbol{x_{0, \text{-}0}}-\bar{x})^T (\boldsymbol{X_{*,\text{-}0}}^T \boldsymbol{X_{*,\text{-}0}})^{-1}(\boldsymbol{x_{0, \text{-}0}}-\bar{x}).
\end{split}
\]</span></p>
<p>Hence, we see that this quantity grows larger as <span class="math inline">\(\boldsymbol{x_{0, \text{-}0}}-\bar{x}\)</span> grows larger, and achieves its minimum when <span class="math inline">\(\boldsymbol{x_{0, \text{-}0}}=\bar{x}\)</span>. Let’s look at the special case when <span class="math inline">\(p = 2\)</span>, so there is just one predictor except the intercept. Then, we have <span class="math inline">\(\boldsymbol{X_{*,\text{-}0}} = \boldsymbol{x_{*,1}}-\bar{x_1}\)</span>, so</p>
<p><span class="math display">\[
\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0} = \frac{1}{n} + \frac{(x_{01}-\bar{x_1})^2}{\|\boldsymbol{x_{*,1}}-\bar{x_1}\|^2}.
\]</span></p>
</section>
<section id="prediction-interval-for-yboldsymbolx_0" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="prediction-interval-for-yboldsymbolx_0"><span class="header-section-number">2.4.3</span> Prediction interval for <span class="math inline">\(y|\boldsymbol{x_0}\)</span></h3>
<p>Instead of creating a confidence interval for a point on the regression line, we may want to create a confidence interval for a new draw <span class="math inline">\(y_0\)</span> of <span class="math inline">\(y\)</span> for <span class="math inline">\(\boldsymbol{x} = \boldsymbol{x_0}\)</span>, i.e., a <em>prediction interval</em>. Note that</p>
<p><span class="math display">\[
y_0 - \boldsymbol{x_0}^T \widehat{\beta} = \boldsymbol{x_0}^T \beta + \epsilon_0 - \boldsymbol{x_0}^T \widehat{\beta} = \epsilon_0 + \boldsymbol{x_0}^T (\beta-\widehat{\beta}) \sim N(0, \sigma^2 + \sigma^2 \boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}).
\]</span></p>
<p>Therefore, we have</p>
<p><span class="math display">\[
\frac{y_0 - \boldsymbol{x_0}^T \widehat{\beta}}{\widehat{\sigma}\sqrt{1 + \boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}}} \sim t_{n-p},
\]</span></p>
<p>which leads to the <span class="math inline">\(1-\alpha\)</span> prediction interval</p>
<p><span id="eq-pointwise-contrast-interval"><span class="math display">\[
\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{1+\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{x_0}} \cdot t_{n-p}(1-\alpha/2) \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}) \cdot t_{n-p}(1-\alpha/2).
\tag{2.12}\]</span></span></p>
<p><strong>Remark: Prediction with confidence in machine learning.</strong></p>
<p>The entire field of supervised machine learning is focused on accurately predicting <span class="math inline">\(y_0\)</span> from <span class="math inline">\(\boldsymbol{x_0}\)</span>, usually using nonlinear functions <span class="math inline">\(\widehat{f}(\boldsymbol{x_0})\)</span>. In addition to providing a guess <span class="math inline">\(\widehat{y_0}\)</span> for <span class="math inline">\(y_0\)</span>, it is often useful to quantify the uncertainty in this guess. In other words, it is useful to come up with a prediction interval (or prediction region) <span class="math inline">\(\text{PI}(y_0)\)</span> such that</p>
<p><span id="eq-conditional-prediction-interval"><span class="math display">\[
\mathbb{P}[y_0 \in \text{PI}(y_0) \mid \boldsymbol{x_0}] \geq 1-\alpha.
\tag{2.13}\]</span></span></p>
<p>For example, in safety-critical applications of machine learning like self-driving cars, it is essential to have confidence in predictions. Unfortunately, beyond the realm of linear regression, it is hard to come up with intervals satisfying (<a href="#eq-conditional-prediction-interval"><span>2.13</span></a>) for each point <span class="math inline">\(\boldsymbol{x_0}\)</span>. However, the emerging field of <em>conformal inference</em> provides guarantees on average over possible values of <span class="math inline">\(\boldsymbol{x}\)</span>:</p>
<p><span id="eq-unconditional-prediction-interval"><span class="math display">\[
\mathbb{P}[y \in \text{PI}(y)] = \mathbb{E}[\mathbb{P}[y \in \text{PI}(y) \mid \boldsymbol{x}]] \geq 1-\alpha.
\tag{2.14}\]</span></span></p>
<p>Remarkably, these guarantees place no assumption on the machine learning method used and require only that the data points on which <span class="math inline">\(\widehat{f}\)</span> is trained are exchangeable (an even weaker condition than i.i.d.). While the unconditional guarantee (<a href="#eq-unconditional-prediction-interval"><span>2.14</span></a>) is weaker than the conditional one (<a href="#eq-conditional-prediction-interval"><span>2.13</span></a>), it can be obtained for modern machine learning and deep learning models.</p>
</section>
<section id="simultaneous-intervals" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="simultaneous-intervals"><span class="header-section-number">2.4.4</span> Simultaneous intervals</h3>
<p>Note that the intervals in the preceding sections have <em>pointwise coverage</em>. For example, we have</p>
<p><span class="math display">\[
\mathbb{P}[\beta_j \in \text{CI}(\beta_j)] \geq 1-\alpha \quad \text{for each } j.
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\mathbb{P}[\boldsymbol{x_0}^T \boldsymbol{\beta} \in \text{CI}(\boldsymbol{x_0}^T \boldsymbol{\beta})] \geq 1-\alpha \quad \text{for each } \boldsymbol{x_0}.
\]</span></p>
<p>Sometimes a stronger <em>simultaneous coverage</em> guarantee is desired, e.g.,</p>
<p><span id="eq-simultaneous-coordinatewise"><span class="math display">\[
\mathbb{P}[\beta_j \in \text{CI}^{\text{sim}}(\beta_j) \ \text{for each } j] \geq 1-\alpha
\tag{2.15}\]</span></span></p>
<p>or</p>
<p><span id="eq-simultaneous-contrasts"><span class="math display">\[
\mathbb{P}[\boldsymbol{x_0}^T \boldsymbol{\beta} \in \text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \ \text{for each } \boldsymbol{x_0}] \geq 1-\alpha.
\tag{2.16}\]</span></span></p>
<p>Simultaneous confidence intervals are possible to construct as well. As a starting point, note that</p>
<p><span class="math display">\[
\frac{\frac{1}{p}\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X} \boldsymbol{\beta}\|^2}{\widehat{\sigma}^2} \sim F_{p, n-p}.
\]</span></p>
<p>Hence, we have</p>
<p><span class="math display">\[
\mathbb{P}[\|\boldsymbol{X} \boldsymbol{\widehat{\beta}} - \boldsymbol{X} \boldsymbol{\beta}\|^2 \leq p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha)] \geq 1-\alpha.
\]</span></p>
<p>Hence, the region</p>
<p><span class="math display">\[
\text{CR}(\boldsymbol{\beta}) \equiv \{\boldsymbol{\beta}: (\boldsymbol{\widehat{\beta}} - \boldsymbol{\beta})^T \boldsymbol{X}^T \boldsymbol{X} (\boldsymbol{\widehat{\beta}} - \boldsymbol{\beta})  \leq p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha)\} \subseteq \mathbb{R}^p
\]</span></p>
<p>is a <span class="math inline">\(1-\alpha\)</span> confidence region for the vector <span class="math inline">\(\boldsymbol{\beta}\)</span>:</p>
<p><span class="math display">\[
\mathbb{P}[\boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})] \geq 1-\alpha.
\]</span></p>
<p>It’s easy to see that <span class="math inline">\(\text{CR}(\boldsymbol{\beta})\)</span> is an ellipse centered at <span class="math inline">\(\boldsymbol{\widehat{\beta}}\)</span>.</p>
<div id="fig-confidence-region" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/confidence-regions.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2.4: Confidence region and simultaneous and pointwise confidence intervals.</figcaption>
</figure>
</div>
<p>Since the confidence region is for the entire vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, we can define simultaneous confidence intervals for each coordinate as follows:</p>
<p><span class="math display">\[
\text{CI}^{\text{sim}}(\beta_j) \equiv \{\beta_j: \boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\}.
\]</span></p>
<p>Then, these confidence intervals will satisfy the simultaneous coverage property (<a href="#eq-simultaneous-coordinatewise"><span>2.15</span></a>). We will obtain a more explicit expression for <span class="math inline">\(\text{CI}^{\text{sim}}(\beta_j)\)</span> shortly.</p>
<p>Similarly, we may define the simultaneous confidence regions</p>
<p><span class="math display">\[
\text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta}) \equiv \{\boldsymbol{x_0}^T \boldsymbol{\beta}: \boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\}.
\]</span></p>
<p>Let us find a more explicit expression for the latter interval. For notational ease, let us define <span class="math inline">\(\boldsymbol{\Sigma} \equiv \boldsymbol{X}^T \boldsymbol{X}\)</span>. Then, note that if <span class="math inline">\(\boldsymbol{\beta} \in \text{CR}(\boldsymbol{\beta})\)</span>, then by the Cauchy-Schwarz inequality we have</p>
<p><span class="math display">\[
\begin{split}
(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}}-\boldsymbol{x_0}^T \boldsymbol{\beta})^2 = \|\boldsymbol{x_0}^T (\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 &amp;= \|(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{x_0})^T \boldsymbol{\Sigma}^{1/2}(\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 \\
&amp;\leq \|(\boldsymbol{\Sigma}^{-1/2}\boldsymbol{x_0})\|^2\|\boldsymbol{\Sigma}^{1/2}(\boldsymbol{\widehat{\beta}}-\boldsymbol{\beta})\|^2 \leq \boldsymbol{x_0}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{x_0} p \widehat{\sigma}^2 F_{p, n-p}(1-\alpha),
\end{split}
\]</span></p>
<p>i.e.,</p>
<p><span id="eq-simultaneous-fit-se"><span class="math display">\[
\boldsymbol{x_0}^T \boldsymbol{\beta} \in \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \widehat{\sigma} \sqrt{\boldsymbol{x_0}^T (\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{x_0}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}} \pm \text{SE}(\boldsymbol{x_0}^T \boldsymbol{\widehat{\beta}})\cdot\sqrt{pF_{p, n-p}(1-\alpha)}.
\tag{2.17}\]</span></span></p>
<p>Defining the above interval as <span class="math inline">\(\text{CI}^{\text{sim}}(\boldsymbol{x_0}^T \boldsymbol{\beta})\)</span> gives us the simultaneous coverage property (<a href="#eq-simultaneous-contrasts"><span>2.16</span></a>). Comparing to equation (<a href="#eq-pointwise-contrast-interval"><span>2.12</span></a>), we see that the simultaneous interval is the pointwise interval expanded by a factor of <span class="math inline">\(\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)\)</span>. Specializing to the case <span class="math inline">\(\boldsymbol{x_0} \equiv \boldsymbol{e_j}\)</span>, we get an expression for the simultaneous intervals for each coordinate:</p>
<p><span id="eq-simultaneous-coordinatewise-se"><span class="math display">\[
\text{CI}^{\text{sim}}(\beta_j) \equiv \widehat{\beta_j} \pm \widehat{\sigma} \sqrt{(\boldsymbol{X}^T \boldsymbol{X})^{-1}_{jj}} \sqrt{pF_{p, n-p}(1-\alpha)} \equiv \text{SE}(\widehat{\beta_j})\sqrt{pF_{p, n-p}(1-\alpha)},
\tag{2.18}\]</span></span></p>
<p>which again is the pointwise interval (<a href="#eq-pointwise-interval-beta"><span>2.11</span></a>) expanded by a factor of <span class="math inline">\(\sqrt{pF_{p, n-p}(1-\alpha)}/t_{n-p}(1-\alpha/2)\)</span>. These simultaneous intervals are called <em>Working-Hotelling intervals</em>.</p>
</section>
</section>
<section id="practical-considerations" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="practical-considerations"><span class="header-section-number">2.5</span> Practical considerations</h2>
<section id="practical-versus-statistical-significance" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="practical-versus-statistical-significance"><span class="header-section-number">2.5.1</span> Practical versus statistical significance</h3>
<p>You can have a statistically significant effect that is not practically significant. The hypothesis testing framework is most useful in the case when the signal-to-noise ratio is relatively small. Otherwise, constructing a confidence interval for the effect size is a more meaningful approach.</p>
</section>
<section id="correlation-versus-causation-and-simpsons-paradox" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="correlation-versus-causation-and-simpsons-paradox"><span class="header-section-number">2.5.2</span> Correlation versus causation, and Simpson’s paradox</h3>
<p>Causation can be elusive for several reasons. One is reverse causation, where it is not clear whether <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> or <span class="math inline">\(Y\)</span> causes <span class="math inline">\(X\)</span>. Another is confounding, where there is a third variable <span class="math inline">\(Z\)</span> that causes both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. For the latter reason, linear regression coefficients can be sensitive to the choice of other predictors to include and can be misleading if you omit important variables from the regression. A special and sometimes overlooked case of this is <em>Simpson’s paradox</em>, where an important discrete variable is omitted. Consider the example in Figure <a href="#fig-simpson-paradox"><span>2.5</span></a>. Sometimes this discrete variable may seem benign, such as the year in which the data was collected. Such variables might or might not be measured.</p>
<div id="fig-simpson-paradox" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/kidney-stones.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2.5: An example of Simpson’s paradox (source: Wikipedia).</figcaption>
</figure>
</div>
</section>
<section id="dealing-with-correlated-predictors" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="dealing-with-correlated-predictors"><span class="header-section-number">2.5.3</span> Dealing with correlated predictors</h3>
<p>It depends on the goal. If we’re trying to tease apart effects of correlated predictors, then we have no choice but to proceed as usual despite lower power. Otherwise, we can test predictors in groups via the <span class="math inline">\(F\)</span>-test to get higher power at the cost of lower “resolution.” Sometimes, it is recommended to simply remove predictors that are correlated with other predictors. This practice, however, is somewhat arbitrary and not recommended.</p>
</section>
<section id="model-selection" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">2.5.4</span> Model selection</h3>
<p>We need to ask ourselves: Why do we want to do model selection? It can either be for prediction purposes or for inferential purposes. If it is for prediction purposes, then we can apply cross-validation to select a model and we don’t need to think very hard about statistical significance. If it is for inference, then we need to be more careful. There are various classical model selection criteria (e.g., AIC, BIC), but it is not entirely clear what statistical guarantee we are getting for the resulting models. A simpler approach is to apply a <span class="math inline">\(t\)</span>-test for each variable in the model, apply a multiple testing correction to the resulting <span class="math inline">\(p\)</span>-values, and report the set of significant variables and the associated guarantee. Re-fitting the linear regression after model selection leads us into some dicey inferential territory due to selection bias. This is the subject of ongoing research, and the jury is still out on the best way of doing this.</p>
</section>
</section>
<section id="r-demo" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="r-demo"><span class="header-section-number">2.6</span> R demo</h2>
<p><em>See also Agresti 3.4.1, 3.4.3, Dunn and Smyth 2.6, 2.14</em></p>
<p>Let’s put into practice what we’ve learned in this chapter by analyzing data about house prices.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>houses_data <span class="ot">&lt;-</span> <span class="fu">read_tsv</span>(<span class="st">"data/Houses.dat"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>houses_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 100 × 7
    case taxes  beds baths   new price  size
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1     1  3104     4     2     0  280.  2048
 2     2  1173     2     1     0  146.   912
 3     3  3076     4     2     0  238.  1654
 4     4  1608     3     2     0  200   2068
 5     5  1454     3     3     0  160.  1477
 6     6  2997     3     2     1  500.  3153
 7     7  4054     3     2     0  266.  1355
 8     8  3002     3     2     1  290.  2075
 9     9  6627     5     4     0  587   3990
10    10   320     3     2     0   70   1160
# ℹ 90 more rows</code></pre>
</div>
</div>
<section id="exploration" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="exploration"><span class="header-section-number">2.6.1</span> Exploration</h3>
<p>Let’s first do a bit of exploration:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize distribution of housing prices, superimposing the mean</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> price)) <span class="sc">+</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">bins =</span> <span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(price)),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="st">"red"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">"dashed"</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="384"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare median and mean price</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean_price =</span> <span class="fu">mean</span>(price),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">median_price =</span> <span class="fu">median</span>(price)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
  mean_price median_price
       &lt;dbl&gt;        &lt;dbl&gt;
1       155.         133.</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a pairs plot of continuous variables</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(price, size, taxes) <span class="sc">|&gt;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggpairs</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="432"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see how price relates to beds</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(beds), <span class="at">y =</span> price)) <span class="sc">+</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">"dodgerblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="288"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see how price relates to baths</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(baths), <span class="at">y =</span> price)) <span class="sc">+</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">"dodgerblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="288"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see how price relates to new</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(new), <span class="at">y =</span> price)) <span class="sc">+</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">"dodgerblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="192"></p>
</div>
</div>
</section>
<section id="hypothesis-testing-1" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="hypothesis-testing-1"><span class="header-section-number">2.6.2</span> Hypothesis testing</h3>
<p>Let’s run a linear regression and interpret the summary. But first, we must decide whether to model beds/baths as categorical or continuous? We should probably model these as categorical, given the potentially nonlinear trend observed in the box plots.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(beds) <span class="sc">+</span> <span class="fu">factor</span>(baths) <span class="sc">+</span> new <span class="sc">+</span> size,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> houses_data</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = price ~ factor(beds) + factor(baths) + new + size, 
    data = houses_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-179.306  -32.037   -2.899   19.115  152.718 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -19.26307   18.01344  -1.069 0.287730    
factor(beds)3   -16.46430   15.04669  -1.094 0.276749    
factor(beds)4   -12.48561   21.12357  -0.591 0.555936    
factor(beds)5  -101.14581   55.83607  -1.811 0.073366 .  
factor(baths)2    2.39872   15.44014   0.155 0.876885    
factor(baths)3   -0.70410   26.45512  -0.027 0.978825    
factor(baths)4  273.20079   83.65764   3.266 0.001540 ** 
new              66.94940   18.50445   3.618 0.000487 ***
size              0.10882    0.01234   8.822 7.46e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 51.17 on 91 degrees of freedom
Multiple R-squared:  0.7653,    Adjusted R-squared:  0.7446 
F-statistic: 37.08 on 8 and 91 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can read off the test statistics and <span class="math inline">\(p\)</span>-values for each variable from the regression summary, as well as for the <span class="math inline">\(F\)</span>-test against the constant model from the bottom of the summary.</p>
<p>Let’s use an <span class="math inline">\(F\)</span>-test to assess whether the categorical <code>baths</code> variable is important.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>lm_fit_partial <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(beds) <span class="sc">+</span> new <span class="sc">+</span> size,</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> houses_data</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_fit_partial, lm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: price ~ factor(beds) + new + size
Model 2: price ~ factor(beds) + factor(baths) + new + size
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1     94 273722                                
2     91 238289  3     35433 4.5104 0.005374 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>What if we had not coded <code>baths</code> as a factor?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>lm_fit_not_factor <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(beds) <span class="sc">+</span> baths <span class="sc">+</span> new <span class="sc">+</span> size,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> houses_data</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_fit_partial, lm_fit_not_factor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: price ~ factor(beds) + new + size
Model 2: price ~ factor(beds) + baths + new + size
  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
1     94 273722                           
2     93 273628  1     94.33 0.0321 0.8583</code></pre>
</div>
</div>
<p>If we want to test for the equality of means across groups of a categorical predictor, without adjusting for other variables, we can use the ANOVA <span class="math inline">\(F\)</span>-test. There are several equivalent ways of doing so:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># just use the summary function</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>lm_fit_baths <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(baths), <span class="at">data =</span> houses_data)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_fit_baths)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = price ~ factor(baths), data = houses_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-146.44  -45.88   -7.89   22.22  352.01 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       90.21      19.51   4.624 1.17e-05 ***
factor(baths)2    57.68      21.72   2.656  0.00927 ** 
factor(baths)3   174.52      31.13   5.607 1.97e-07 ***
factor(baths)4   496.79      82.77   6.002 3.45e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 80.44 on 96 degrees of freedom
Multiple R-squared:  0.3881,    Adjusted R-squared:  0.369 
F-statistic:  20.3 on 3 and 96 DF,  p-value: 2.865e-10</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use the anova function as before</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>lm_fit_const <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> houses_data)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_fit_const, lm_fit_baths)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: price ~ 1
Model 2: price ~ factor(baths)
  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
1     99 1015150                                  
2     96  621130  3    394020 20.299 2.865e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use the aov function</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>aov_fit <span class="ot">&lt;-</span> <span class="fu">aov</span>(price <span class="sc">~</span> <span class="fu">factor</span>(baths), <span class="at">data =</span> houses_data)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(aov_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
factor(baths)  3 394020  131340    20.3 2.86e-10 ***
Residuals     96 621130    6470                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>We can also use an <span class="math inline">\(F\)</span>-test to test for the presence of an interaction with a multi-class categorical predictor.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>lm_fit_interaction <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> size <span class="sc">*</span> <span class="fu">factor</span>(beds), <span class="at">data =</span> houses_data)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_fit_interaction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = price ~ size * factor(beds), data = houses_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-232.643  -25.938   -0.942   19.172  155.517 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          50.12619   48.22282   1.039 0.301310    
size                  0.05037    0.04210   1.197 0.234565    
factor(beds)3      -103.85734   52.20373  -1.989 0.049620 *  
factor(beds)4      -143.90213   67.31359  -2.138 0.035185 *  
factor(beds)5      -507.88205  144.10191  -3.524 0.000663 ***
size:factor(beds)3    0.07589    0.04368   1.738 0.085633 .  
size:factor(beds)4    0.09234    0.04704   1.963 0.052638 .  
size:factor(beds)5    0.21147    0.05957   3.550 0.000609 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 53.35 on 92 degrees of freedom
Multiple R-squared:  0.7421,    Adjusted R-squared:  0.7225 
F-statistic: 37.81 on 7 and 92 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>lm_fit_size <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> size <span class="sc">+</span> <span class="fu">factor</span>(beds), <span class="at">data =</span> houses_data)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_fit_size, lm_fit_interaction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: price ~ size + factor(beds)
Model 2: price ~ size * factor(beds)
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1     95 300953                                
2     92 261832  3     39121 4.5819 0.004905 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Contrasts of regression coefficients can be tested using the <code>glht()</code> function from the <code>multcomp</code> package.</p>
</section>
<section id="confidence-intervals" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">2.6.3</span> Confidence intervals</h3>
<p>We can construct pointwise confidence intervals for each coefficient using <code>confint()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                       2.5 %      97.5 %
(Intercept)     -55.04455734  16.5184161
factor(beds)3   -46.35270691  13.4241025
factor(beds)4   -54.44498235  29.4737689
factor(beds)5  -212.05730801   9.7656895
factor(baths)2  -28.27123130  33.0686620
factor(baths)3  -53.25394742  51.8457394
factor(baths)4  107.02516067 439.3764122
new              30.19258305 103.7062177
size              0.08431972   0.1333284</code></pre>
</div>
</div>
<p>To create simultaneous confidence intervals, we need a somewhat more manual approach. We start with the coefficients and standard errors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(lm_fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   Estimate  Std. Error     t value     Pr(&gt;|t|)
(Intercept)     -19.2630706 18.01344052 -1.06937209 2.877304e-01
factor(beds)3   -16.4643022 15.04669172 -1.09421410 2.767490e-01
factor(beds)4   -12.4856067 21.12356937 -0.59107467 5.559357e-01
factor(beds)5  -101.1458092 55.83607248 -1.81147786 7.336590e-02
factor(baths)2    2.3987153 15.44014266  0.15535578 8.768849e-01
factor(baths)3   -0.7041040 26.45511871 -0.02661504 9.788251e-01
factor(baths)4  273.2007864 83.65764044  3.26570036 1.540093e-03
new              66.9494004 18.50445029  3.61801617 4.872475e-04
size              0.1088241  0.01233621  8.82151661 7.460814e-14</code></pre>
</div>
</div>
<p>Then we add lower and upper confidence interval endpoints based on the formula (<a href="#eq-simultaneous-coordinatewise-se"><span>2.18</span></a>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(houses_data)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">coef</span>(lm_fit))</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>f_quantile <span class="ot">&lt;-</span> <span class="fu">qf</span>(<span class="dv">1</span> <span class="sc">-</span> alpha, <span class="at">df1 =</span> p, <span class="at">df2 =</span> n <span class="sc">-</span> p)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(lm_fit)) <span class="sc">|&gt;</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">|&gt;</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>(<span class="at">var =</span> <span class="st">"Variable"</span>) <span class="sc">|&gt;</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Variable, Estimate, <span class="st">`</span><span class="at">Std. Error</span><span class="st">`</span>) <span class="sc">|&gt;</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">CI_lower =</span> Estimate <span class="sc">-</span> <span class="st">`</span><span class="at">Std. Error</span><span class="st">`</span> <span class="sc">*</span> <span class="fu">sqrt</span>(p <span class="sc">*</span> f_quantile),</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">CI_upper =</span> Estimate <span class="sc">+</span> <span class="st">`</span><span class="at">Std. Error</span><span class="st">`</span> <span class="sc">*</span> <span class="fu">sqrt</span>(p <span class="sc">*</span> f_quantile)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Variable     Estimate  Std. Error      CI_lower    CI_upper
1    (Intercept)  -19.2630706 18.01344052  -95.38917389  56.8630327
2  factor(beds)3  -16.4643022 15.04669172  -80.05271036  47.1241059
3  factor(beds)4  -12.4856067 21.12356937 -101.75533960  76.7841262
4  factor(beds)5 -101.1458092 55.83607248 -337.11309238 134.8214739
5 factor(baths)2    2.3987153 15.44014266  -62.85244495  67.6498756
6 factor(baths)3   -0.7041040 26.45511871 -112.50535022 111.0971422
7 factor(baths)4  273.2007864 83.65764044  -80.34245635 626.7440292
8            new   66.9494004 18.50445029  -11.25174573 145.1505465
9           size    0.1088241  0.01233621    0.05669037   0.1609578</code></pre>
</div>
</div>
<p>Note that the simultaneous intervals are substantially larger.</p>
<p>To construct pointwise confidence intervals for the fit, we can use the <code>predict()</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm_fit, <span class="at">newdata =</span> houses_data, <span class="at">interval =</span> <span class="st">"confidence"</span>) <span class="sc">|&gt;</span> <span class="fu">head</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        fit       lwr      upr
1 193.52176 165.22213 221.8214
2  79.98449  51.91430 108.0547
3 150.64507 122.28397 179.0062
4 191.71955 172.27396 211.1651
5 124.30169  81.34488 167.2585
6 376.74308 333.44559 420.0406</code></pre>
</div>
</div>
<p>To get pointwise prediction intervals, we switch <code>"confidence"</code> to <code>"prediction"</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm_fit, <span class="at">newdata =</span> houses_data, <span class="at">interval =</span> <span class="st">"prediction"</span>) <span class="sc">|&gt;</span> <span class="fu">head</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        fit       lwr      upr
1 193.52176  88.00908 299.0344
2  79.98449 -25.46688 185.4359
3 150.64507  45.11589 256.1743
4 191.71955  88.22951 295.2096
5 124.30169  13.95069 234.6527
6 376.74308 266.25901 487.2271</code></pre>
</div>
</div>
<p>To construct simultaneous confidence intervals for the fit or predictions, we again need a slightly more manual approach. We call <code>predict()</code> again, but this time asking it for the standard errors rather than the confidence intervals:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm_fit, <span class="at">newdata =</span> houses_data, <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predictions<span class="sc">$</span>fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2         3         4         5         6 
193.52176  79.98449 150.64507 191.71955 124.30169 376.74308 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predictions<span class="sc">$</span>se.fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2         3         4         5         6 
14.246855 14.131352 14.277804  9.789472 21.625709 21.797212 </code></pre>
</div>
</div>
<p>Now we can construct the simultaneous confidence intervals via the formula (<a href="#eq-simultaneous-fit-se"><span>2.17</span></a>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>f_quantile <span class="ot">&lt;-</span> <span class="fu">qf</span>(<span class="dv">1</span> <span class="sc">-</span> alpha, <span class="at">df1 =</span> p, <span class="at">df2 =</span> n <span class="sc">-</span> p)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> predictions<span class="sc">$</span>fit <span class="sc">-</span> predictions<span class="sc">$</span>se.fit <span class="sc">*</span> <span class="fu">sqrt</span>(p <span class="sc">*</span> f_quantile),</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">upper =</span> predictions<span class="sc">$</span>fit <span class="sc">+</span> predictions<span class="sc">$</span>se.fit <span class="sc">*</span> <span class="fu">sqrt</span>(p <span class="sc">*</span> f_quantile)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 100 × 2
   lower upper
   &lt;dbl&gt; &lt;dbl&gt;
 1 133.   254.
 2  20.3  140.
 3  90.3  211.
 4 150.   233.
 5  32.9  216.
 6 285.   469.
 7  82.8  145.
 8 188.   331.
 9 371.   803.
10  57.3  128.
# ℹ 90 more rows</code></pre>
</div>
</div>
<p>In the case of simple linear regression, we can plot these pointwise and simultaneous confidence intervals as bands:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to produce confidence intervals for fits in general, use the predict() function</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(houses_data)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> size, <span class="at">data =</span> houses_data)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm_fit, <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>t_quantile <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="dv">1</span> <span class="sc">-</span> alpha <span class="sc">/</span> <span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span> p)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>f_quantile <span class="ot">&lt;-</span> <span class="fu">qf</span>(<span class="dv">1</span> <span class="sc">-</span> alpha, <span class="at">df1 =</span> p, <span class="at">df2 =</span> n <span class="sc">-</span> p)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">fit =</span> predictions<span class="sc">$</span>fit,</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">se =</span> predictions<span class="sc">$</span>se.fit,</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">ptwise_width =</span> t_quantile <span class="sc">*</span> se,</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">simultaneous_width =</span> <span class="fu">sqrt</span>(p <span class="sc">*</span> f_quantile) <span class="sc">*</span> se</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> size)) <span class="sc">+</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> price)) <span class="sc">+</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> fit), <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> fit <span class="sc">+</span> ptwise_width, <span class="at">color =</span> <span class="st">"Pointwise"</span>)) <span class="sc">+</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> fit <span class="sc">-</span> ptwise_width, <span class="at">color =</span> <span class="st">"Pointwise"</span>)) <span class="sc">+</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> fit <span class="sc">+</span> simultaneous_width, <span class="at">color =</span> <span class="st">"Simultaneous"</span>)) <span class="sc">+</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> fit <span class="sc">-</span> simultaneous_width, <span class="at">color =</span> <span class="st">"Simultaneous"</span>)) <span class="sc">+</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="288"></p>
</div>
</div>
</section>
<section id="predictor-competition-and-collaboration" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="predictor-competition-and-collaboration"><span class="header-section-number">2.6.4</span> Predictor competition and collaboration</h3>
<p>Let’s look at the power of detecting the association between <code>price</code> and <code>beds</code>. We can imagine that <code>beds</code> and <code>baths</code> are correlated:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>houses_data <span class="sc">|&gt;</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> beds, <span class="at">y =</span> baths)) <span class="sc">+</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_count</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="linear-models-inference_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="336"></p>
</div>
</div>
<p>So let’s see how significant <code>beds</code> is, with and without <code>baths</code> in the model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>lm_fit_only_beds <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(beds), <span class="at">data =</span> houses_data)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_fit_only_beds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = price ~ factor(beds), data = houses_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-234.35  -50.63  -15.69   24.56  365.86 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     105.94      21.48   4.931 3.43e-06 ***
factor(beds)3    44.69      24.47   1.827 0.070849 .  
factor(beds)4   105.70      32.35   3.268 0.001504 ** 
factor(beds)5   246.71      69.62   3.544 0.000611 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 93.65 on 96 degrees of freedom
Multiple R-squared:  0.1706,    Adjusted R-squared:  0.1447 
F-statistic: 6.583 on 3 and 96 DF,  p-value: 0.0004294</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>lm_fit_only_baths <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(baths), <span class="at">data =</span> houses_data)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>lm_fit_beds_baths <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> <span class="fu">factor</span>(beds) <span class="sc">+</span> <span class="fu">factor</span>(baths), <span class="at">data =</span> houses_data)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_fit_only_baths, lm_fit_beds_baths)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: price ~ factor(baths)
Model 2: price ~ factor(beds) + factor(baths)
  Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  
1     96 621130                             
2     93 572436  3     48693 2.637 0.05424 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>We see that the significance of <code>beds</code> dropped by two orders of magnitude. This is an example of predictor competition.</p>
<p>On the other hand, note that the variable <code>new</code> is not very correlated with <code>beds</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(new <span class="sc">~</span> beds, <span class="at">data =</span> houses_data)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = new ~ beds, data = houses_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.15762 -0.11000 -0.11000 -0.08619  0.91381 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.03857    0.14950   0.258    0.797
beds         0.02381    0.04871   0.489    0.626

Residual standard error: 0.3157 on 98 degrees of freedom
Multiple R-squared:  0.002432,  Adjusted R-squared:  -0.007747 
F-statistic: 0.2389 on 1 and 98 DF,  p-value: 0.6261</code></pre>
</div>
</div>
<p>but we know it has a substantial impact on <code>price</code>. Let’s look at the significance of the test that <code>beds</code> is not important when we add <code>new</code> to the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>lm_fit_only_new <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> new, <span class="at">data =</span> houses_data)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>lm_fit_beds_new <span class="ot">&lt;-</span> <span class="fu">lm</span>(price <span class="sc">~</span> new <span class="sc">+</span> <span class="fu">factor</span>(beds), <span class="at">data =</span> houses_data)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm_fit_only_new, lm_fit_beds_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: price ~ new
Model 2: price ~ new + factor(beds)
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 787781                                  
2     95 619845  3    167936 8.5795 4.251e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Adding <code>new</code> to the model made the <span class="math inline">\(p\)</span>-value more significant by a factor of 10. This is an example of predictor collaboration.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./linear-models-estimation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear models: Estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./linear-models-misspecification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear models: Misspecification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>